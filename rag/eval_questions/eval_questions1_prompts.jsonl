{"id": "q001", "question": "strict exogeneityとは？", "topic": "econometrics", "gold_chunk_ids": ["Wooldridge - Cross-section and Panel Data::p293::c0", "Wooldridge - Cross-section and Panel Data::p298::c0"], "prompt": "あなたは厳密な研究アシスタントです。以下の CONTEXT（抜粋）のみを根拠に、日本語で回答してください。\n\n【ルール（引用強制モード）】\n- CONTEXT以外の知識は一切使わない（推測・一般常識の補足も禁止）。\n- 重要な主張は「1文=1主張」を意識し、各文の末尾に必ず引用を付ける： (chunk_id, p#)\n- 1つの文で複数の主張をしない（分割する）。\n- CONTEXTから直接言えない場合は、必ず「不足」と明示し、何が不足かを書き、追加で必要な検索クエリ案を3つ提示する。\n\n【出力フォーマット】\nA) 結論（各文に引用）\nB) 根拠（箇条書き：各行=1主張 + 引用）\nC) 不足している情報（あれば）\nD) 追加retrievalクエリ案（3つ）\n\n質問:\nstrict exogeneityとは？\n\nCONTEXT:\n## Evidence (high priority)\n\n---\n\n[#1 | w=1.00 | score=22.3883 | Wooldridge - Cross-section and Panel Data::p298::c0 | p298 | Econometric Analysis of Cross Section and Panel Data | topic=['econometrics']]\nChapter 11.) In addition, correlation between uit and xis for s 0 t causes FD and FE to be inconsistent. When lagged xit is correlated with uit, we can solve lack of strict exogeneity by including lags and interpreting the equation as a distributed lag model. More problematical is when uit is correlated with future xit: only rarely does putting future values of explanatory variables in an equation lead to an interesting economic model. In Chapter 11 we show how to estimate the parameters consistently when there is feedback from uit to xis, s > t. We can formally test the assumptions underlying the consistency of the FE and FD estimators by using a Hausman test. It might be important to use a robust form of the Hausman test that maintains neither Assumption FE.3 nor Assumption FD.3 under the null hypothesis. This approach is not di‰cult—see Problem 10.6—but we focus here on regression-based tests, which are easier to compute. If T ¼ 2, it is easy to test for strict exogeneity. In the equation Dyi ¼ Dxib þ Dui, neither xi1 nor xi2 should be signiﬁcant as additional explanatory variables in the ﬁrst-di¤erenced equation. We simply add, say, xi2 to the FD equation and carry out an F test for signiﬁcance of xi2. With more than two time periods, a test of strict exogeneity is a test of H0: g ¼ 0 in the expanded equation Dyt ¼ Dxtb þ wtg þ Dut; t ¼ 2; . . . ; T where wt is a subset of \n\n---\n\n[#2 | w=0.91 | score=20.2807 | Wooldridge - Cross-section and Panel Data::p297::c0 | p297 | Econometric Analysis of Cross Section and Panel Data | topic=['econometrics']]\nDyi2 ¼ y2 þ Dzi2g þ d1progi2 þ Dui2 ð10:72Þ The e¤ect of the policy can be obtained by regressing the change in y on the change in z and the policy indicator. When Dzi2 is omitted, the estimate of d1 from equation (10.72) is the di¤erence-in-di¤erences (DID) estimator (see Problem 10.4): ^d1 ¼ Dytreat \u0003 Dycontrol. This is similar to the DID estimator from Section 6.3—see equation (6.32)—but there is an important di¤erence: with panel data, the di¤erences over time are for the same cross section units. If some people participated in the program in the ﬁrst time period, or if more than two periods are involved, equation (10.72) can give misleading answers. In general, the equation that should be estimated is Dyit ¼ xt þ Dzitg þ d1Dprogit þ Duit ð10:73Þ where the program participation indicator is di¤erenced along with everything else, and the xt are new period intercepts. Example 10.6 is one such case. Extensions of the model, where progit appears in other forms, are discussed in Chapter 11. 10.7 Comparison of Estimators 10.7.1 Fixed E¤ects versus First Di¤erencing When we have only two time periods, ﬁxed e¤ects estimation and ﬁrst di¤erencing produce identical estimates and inference, as you are asked to show in Problem 10.3. First di¤erencing is easier to implement, and all procedures that can be applied to a single cross section—such as heteroskedasticity-robust inference—can \n\n---\n\n[#3 | w=0.82 | score=18.3439 | Wooldridge - Cross-section and Panel Data::p305::c0 | p305 | Econometric Analysis of Cross Section and Panel Data | topic=['econometrics']]\ne. Discuss whether strict exogeneity is reasonable for the two variables taxit and disasterit; assume that neither of these variables has a lagged e¤ect on capital investment. 10.2. Suppose you have T ¼ 2 years of data on the same group of N working indi- viduals. Consider the following model of wage determination: logðwageitÞ ¼ y1 þ y2d2t þ zitg þ d1 femalei þ d2d2t \u0002 femalei þ ci þ uit The unobserved e¤ect ci is allowed to be correlated with zit and femalei. The variable d2t is a time period indicator, where d2t ¼ 1 if t ¼ 2 and d2t ¼ 0 if t ¼ 1. In what follows, assume that Eðuit j femalei; zi1; zi2; ciÞ ¼ 0; t ¼ 1; 2 a. Without further assumptions, what parameters in the log wage equation can be consistently estimated? b. Interpret the coe‰cients y2 and d2. c. Write the log wage equation explicitly for the two time periods. Show that the di¤erenced equation can be written as DlogðwageiÞ ¼ y2 þ Dzig þ d2 femalei þ Dui where DlogðwageiÞ ¼ logðwagei2Þ \u0003 logðwagei1Þ, and so on. 10.3. For T ¼ 2 consider the standard unoberved e¤ects model yit ¼ xitb þ ci þ uit; t ¼ 1; 2 Let ^bFE and ^bFD denote the ﬁxed e¤ects and ﬁrst di¤erence estimators, respectively. a. Show that the FE and FD estimates are numerically identical. b. Show that the error variance estimates from the FE and FD methods are numer- ically identical. 10.4. A common setup for program evaluation with two periods of pa\n\n---\n\n## Support (lower priority)\n\n---\n\n[#4 | w=0.75 | score=16.7011 | Wooldridge - Cross-section and Panel Data::p325::c0 | p325 | Econometric Analysis of Cross Section and Panel Data | topic=['econometrics']]\nwhere w\u0004 it is measured with error. Write rit ¼ wit \u0001 w\u0004 it, and assume strict exogeneity along with redundancy of wit: Eðuit j zi; w\u0004 i ; wi; ciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð11:34Þ Replacing w\u0004 it with wit and ﬁrst di¤erencing gives Dyit ¼ Dzitg þ dDwit þ Duit \u0001 dDrit ð11:35Þ The standard CEV assumption in the current context can be stated as Eðrit j zi; w\u0004 i ; ciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð11:36Þ which implies that rit is uncorrelated with zis, w\u0004 is for all t and s. (As always in the\n\n---\n\n[#5 | w=0.74 | score=16.6310 | Wooldridge - Cross-section and Panel Data::p328::c0 | p328 | Econometric Analysis of Cross Section and Panel Data | topic=['econometrics']]\nOne approach to estimating b is to di¤erence away ci: Dyit ¼ gi þ Dxitb þ Duit; t ¼ 2; 3; . . . ; T ð11:41Þ where we have used the fact that git \u0001 giðt \u0001 1Þ ¼ gi. Now equation (11.41) is just the standard unobserved e¤ects model we studied in Chapter 10. The key strict exo- geneity assumption, EðDuit j gi; Dxi2; . . . ; DxiTÞ ¼ 0, t ¼ 2; 3; . . . ; T, holds under as- sumption (11.39). Therefore, we can apply ﬁxed e¤ects or ﬁrst-di¤erencing methods to equation (11.41) in order to estim\n\n---\n\n[#6 | w=0.73 | score=16.2490 | Wooldridge - Cross-section and Panel Data::p294::c0 | p294 | Econometric Analysis of Cross Section and Panel Data | topic=['econometrics']]\ntotically e‰cient in the class of estimators using the strict exogeneity assumption FE.1. Therefore, the ﬁrst di¤erence estimator is less e‰cient than ﬁxed e¤ects under Assumptions FE.1–FE.3. Assumption FE.3 is key to the e‰ciency of FE. It assumes homoskedasticity and no serial correlation in uit. Assuming that the fuit: t ¼ 1; 2; . . . Tg are serially uncorrelated may be too strong. An alternative assumption is that the ﬁrst di¤erence of the idiosyncratic errors, feit 1 Duit; t ¼ 2;\n\n---\n\n[#7 | w=0.72 | score=16.1131 | Wooldridge - Cross-section and Panel Data::p320::c0 | p320 | Econometric Analysis of Cross Section and Panel Data | topic=['econometrics']]\nbut where we allow wit to be contemporaneously correlated with uit. This correlation can be due to any of the three problems that we studied earlier: omission of an im- portant time-varying explanatory variable, measurement error in some elements of wit, or simultaneity between yit and one or more elements of wit. We assume that equation (11.18) is the equation of interest. In a simultaneous equations model with panel data, equation (11.18) represents a single equation. A system appro\n\n---\n\n[#8 | w=0.70 | score=15.5998 | Wooldridge - Cross-section and Panel Data::p311::c0 | p311 | Econometric Analysis of Cross Section and Panel Data | topic=['econometrics']]\n11 More Topics in Linear Unobserved E¤ects Models This chapter continues our treatment of linear, unobserved e¤ects panel data models. We ﬁrst cover estimation of models where the strict exogeneity Assumption FE.1 fails but sequential moment conditions hold. A simple approach to consistent esti- mation involves di¤erencing combined with instrumental variables methods. We also cover models with individual slopes, where unobservables can interact with explana- tory variables, and models\n", "prompt_meta": {"retriever": "bm25", "rerank": "none", "top_k": 8, "pool_k": 30, "max_chars": 1400, "mode": "cite_strict"}}
{"id": "q002", "question": "PPOのclip objectiveを説明して", "topic": "reinforcement_learning", "gold_chunk_ids": ["PolicyOptimizationTDlearninginContinuoustime::p1::c0", "PolicyOptimizationTDlearninginContinuoustime::p3::c0"], "prompt": "あなたは厳密な研究アシスタントです。以下の CONTEXT（抜粋）のみを根拠に、日本語で回答してください。\n\n【ルール（引用強制モード）】\n- CONTEXT以外の知識は一切使わない（推測・一般常識の補足も禁止）。\n- 重要な主張は「1文=1主張」を意識し、各文の末尾に必ず引用を付ける： (chunk_id, p#)\n- 1つの文で複数の主張をしない（分割する）。\n- CONTEXTから直接言えない場合は、必ず「不足」と明示し、何が不足かを書き、追加で必要な検索クエリ案を3つ提示する。\n\n【出力フォーマット】\nA) 結論（各文に引用）\nB) 根拠（箇条書き：各行=1主張 + 引用）\nC) 不足している情報（あれば）\nD) 追加retrievalクエリ案（3つ）\n\n質問:\nPPOのclip objectiveを説明して\n\nCONTEXT:\n## Evidence (high priority)\n\n---\n\n[#1 | w=1.00 | score=8.9111 | PolicyOptimizationTDlearninginContinuoustime::p1::c0 | p1 | PolicyOptimizationTDlearninginContinuoustime | topic=['reinforcement_learning']]\nJournal of Machine Learning Research 23 (2022) 1-55 Submitted 8/21; Revised 1/22; Published 4/22 Policy Evaluation and Temporal–Diﬀerence Learning in Continuous Time and Space: A Martingale Approach Yanwei Jia yj2650@columbia.edu Department of Industrial Engineering and Operations Research Columbia University New York, NY 10027, USA Xun Yu Zhou xz2574@columbia.edu Department of Industrial Engineering and Operations Research & The Data Science Institute Columbia University New York, NY 10027, USA Editor: Csaba Szepesvari Abstract We propose a uniﬁed framework to study policy evaluation (PE) and the associated tempo- ral diﬀerence (TD) methods for reinforcement learning in continuous time and space. We show that PE is equivalent to maintaining the martingale condition of a process. From this perspective, we ﬁnd that the mean–square TD error approximates the quadratic variation of the martingale and thus is not a suitable objective for PE. We present two methods to use the martingale characterization for designing PE algorithms. The ﬁrst one minimizes a “martingale loss function”, whose solution is proved to be the best approximation of the true value function in the mean–square sense. This method interprets the classical gradi- ent Monte-Carlo algorithm. The second method is based on a system of equations called the “martingale orthogonality conditions” with test functions. Solvi\n\n---\n\n[#2 | w=0.85 | score=7.5892 | PolicyOptimizationTDlearninginContinuoustime::p3::c0 | p3 | PolicyOptimizationTDlearninginContinuoustime | topic=['reinforcement_learning']]\nPolicy Evaluation and TD Learning in Continuous Time formulation for trading oﬀexploration and exploitation in continuous time and space, and derive the continuous version of the Boltzmann distribution (Gibbs measure) as the optimal exploratory policy. When the problem is linear–quadratic (LQ), namely the dynamic is linear and the payoﬀis quadratic in state and action, the optimal strategy specializes to Gaussian exploration. Wang and Zhou (2020) apply this general theory to a mean–variance ﬁnancial portfolio selection problem, which is inherently of an LQ structure, and design an algorithm for extensive simulation and empirical experiments. Dai et al. (2020) further consider the equilibrium mean–variance strategies addressing the time-inconsistent issue of the problem. Guo et al. (2022) extend the formulation and results of Wang et al. (2020) to mean-ﬁeld games. Gao et al. (2020) use the idea of Wang et al. (2020) to a non-learning problem – simulated annealing for nonconvex optimization formulated as controlling the temperature of a Langevin diﬀusion. For PE, there are generally two aspects one should address. First and more funda- mentally, one speciﬁes a mathematical objective against which a learning task is evaluated. Usually, such an objective is described by either an optimization problem (to minimize a loss/error function) or a system of equations. Second and on the im\n\n---\n\n[#3 | w=0.74 | score=6.6140 | PolicyOptimizationTDlearninginContinuoustime::p41::c0 | p41 | PolicyOptimizationTDlearninginContinuoustime | topic=['reinforcement_learning']]\nPolicy Evaluation and TD Learning in Continuous Time Appendix A: A Summary of Popular PE Methods The following Table 1 summarizes popular PE methods and algorithms, and the interpre- tations we have discovered in this paper in terms of objectives (loss/error functions to be minimized or equations to be solved) and limiting points of convergent algorithms. Method Representative algorithms Online Objective Converging point Monte Carloa gradient Monte Carlo No minimize martingale loss function minimizers of mean-square value function error Residual gradientb na¨ıve residual gradient Yes minimize mean-square TD error minimizers of quadratic variation Semi-gradient TD learningc TD(λ) LSTD(λ) Yes solve moment conditions zeros to moment conditions Gradient TD learningd GTD(0) GTD2 TDC Yes minimize quadratic form of moment conditions minimizers of mean-square projected Bellman error a. Sutton and Barto (2018). b. Baird (1995). c. Sutton (1988); Bradtke and Barto (1996). This terminology is taken from Sutton and Barto (2018, Chapter 9). d. Sutton et al. (2008, 2009). Table 1: Summary of popular PE methods in RL literature. The table summarizes diﬀerent PE methods. The ﬁrst three columns indicate the names of the methods, those of the representative algorithms, and whether applicable online and/or oﬄine. The last two columns reveal the objectives and the converging points of the correspo\n\n---\n\n## Support (lower priority)\n\n---\n\n[#4 | w=0.64 | score=5.6997 | PolicyOptimizationTDlearninginContinuoustime::p6::c0 | p6 | PolicyOptimizationTDlearninginContinuoustime | topic=['reinforcement_learning']]\nJia and Zhou Unlike most RL problems that are formulated in an inﬁnite planning horizon (known as continuing tasks), the current paper mainly focuses on a ﬁnite horizon setting (known as episodic tasks).5 Finite horizons reﬂect limited lifespans of real-life tasks, e.g., a trader sells a ﬁnancial contract with a maturity date, a robot ﬁnishes a task before a deadline, and a video gamer strives to pass a checkpoint given a time limit. The PE task is, for a ﬁxed given policy (which is s\n\n---\n\n[#5 | w=0.63 | score=5.6382 | PolicyOptimizationTDlearninginContinuoustime::p21::c0 | p21 | PolicyOptimizationTDlearninginContinuoustime | topic=['reinforcement_learning']]\nPolicy Evaluation and TD Learning in Continuous Time Figure 5: ML and CTD(λ) methods converge to diﬀerent points for Example 3. Applying ML algorithm leads to θ∗ ML = 4 15, which is the minimizer of MSVE. CTD methods converge to θ∗ moment = 0, which is the solution to the moment condition. In this case, the moment conditions associated with CTD(0) and CTD(1) have the same solution so the two algorithms converge to the same point. We repeat the experiment for 100 times to calculate the\n\n---\n\n[#6 | w=0.63 | score=5.5833 | Reinforcement learning and stochastic optimisation::p9::c1 | p9 | Reinforcement learning and stochastic optimisation | topic=['reinforcement_learning']]\nthe action space is itself continuous. To circumvent this is- sue, policy gradient (PG) methods were developed in Silver et al. [104], and extended to deterministic policy gradient (DPG) methods in Lillicrap et al. [84]. DPG is based on the following set of observations. First, with F(x′) := P[x1 ≤x′], deﬁne the (un- normalised) discounted state distribution from following the policy π as ρπ(x) = t=1 γ t−1P[xt = x|x1 = x′,π]dF(x′). Then the value function V (π) associated with a polic\n\n---\n\n[#7 | w=0.60 | score=5.3173 | PolicyOptimizationTDlearninginContinuoustime::p26::c0 | p26 | PolicyOptimizationTDlearninginContinuoustime | topic=['reinforcement_learning']]\nJia and Zhou Henceforth we impose the following assumption on the test functions used for moment conditions. Assumption 5 A test function ξ = {ξt, 0 ≤t ≤T} is an RL′-valued adapted process satisfying |ξ| ∈L2 F([0, T]; Mθ) and E[|ξt′ −ξt|2] ≤C(θ)|t′ −t|α for any t, t′ ∈[0, T], where C(θ) is a continuous function of θ and 0 < α ≤2 is a given constant. The following is about the convergence of the TD type algorithms when ∆t →0. Theorem 5 Denote by θ∗ moment(∆t) the solution to the discre\n\n---\n\n[#8 | w=0.59 | score=5.2183 | Reinforcement learning and stochastic optimisation::p3::c0 | p3 | Reinforcement learning and stochastic optimisation | topic=['reinforcement_learning']]\nReinforcement learning and stochastic optimisation 105 Fig. 1 Directed graph representation of the state–action–reward evolution 2 Deep reinforcement learning This section provides an overview of deep RL approaches that I believe we as ﬁnan- cial mathematicians should be aware of, and hopefully improve upon by blending these ideas with our traditional approaches. In the simplest RL setting, the evolution of the environment, action and reward may be viewed as in the directed graph show\n", "prompt_meta": {"retriever": "bm25", "rerank": "none", "top_k": 8, "pool_k": 30, "max_chars": 1400, "mode": "cite_strict"}}
