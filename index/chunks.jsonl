{"chunk_id": "Wooldridge - Cross-section and Panel Data::p2::c0", "text": "Econometric Analysis of Cross Section and Panel Data Je¤rey M. Wooldridge The MIT Press Cambridge, Massachusetts London, England", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 2, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p3::c0", "text": "Contents Preface xvii Acknowledgments xxiii I INTRODUCTION AND BACKGROUND 1 1 Introduction 3 1.1 Causal Relationships and Ceteris Paribus Analysis 3 1.2 The Stochastic Setting and Asymptotic Analysis 4 1.2.1 Data Structures 4 1.2.2 Asymptotic Analysis 7 1.3 Some Examples 7 1.4 Why Not Fixed Explanatory Variables? 9 2 Conditional Expectations and Related Concepts in Econometrics 13 2.1 The Role of Conditional Expectations in Econometrics 13 2.2 Features of Conditional Expectations 14 2.2.1 Deﬁnition and Examples 14 2.2.2 Partial E¤ects, Elasticities, and Semielasticities 15 2.2.3 The Error Form of Models of Conditional Expectations 18 2.2.4 Some Properties of Conditional Expectations 19 2.2.5 Average Partial E¤ects 22 2.3 Linear Projections 24 Problems 27 Appendix 2A 29 2.A.1 Properties of Conditional Expectations 29 2.A.2 Properties of Conditional Variances 31 2.A.3 Properties of Linear Projections 32 3 Basic Asymptotic Theory 35 3.1 Convergence of Deterministic Sequences 35 3.2 Convergence in Probability and Bounded in Probability 36 3.3 Convergence in Distribution 38 3.4 Limit Theorems for Random Samples 39 3.5 Limiting Behavior of Estimators and Test Statistics 40 3.5.1 Asymptotic Properties of Estimators 40 3.5.2 Asymptotic Properties of Test Statistics 43 Problems 45", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 3, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p4::c0", "text": "II LINEAR MODELS 47 4 The Single-Equation Linear Model and OLS Estimation 49 4.1 Overview of the Single-Equation Linear Model 49 4.2 Asymptotic Properties of OLS 51 4.2.1 Consistency 52 4.2.2 Asymptotic Inference Using OLS 54 4.2.3 Heteroskedasticity-Robust Inference 55 4.2.4 Lagrange Multiplier (Score) Tests 58 4.3 OLS Solutions to the Omitted Variables Problem 61 4.3.1 OLS Ignoring the Omitted Variables 61 4.3.2 The Proxy Variable–OLS Solution 63 4.3.3 Models with Interactions in Unobservables 67 4.4 Properties of OLS under Measurement Error 70 4.4.1 Measurement Error in the Dependent Variable 71 4.4.2 Measurement Error in an Explanatory Variable 73 Problems 76 5 Instrumental Variables Estimation of Single-Equation Linear Models 83 5.1 Instrumental Variables and Two-Stage Least Squares 83 5.1.1 Motivation for Instrumental Variables Estimation 83 5.1.2 Multiple Instruments: Two-Stage Least Squares 90 5.2 General Treatment of 2SLS 92 5.2.1 Consistency 92 5.2.2 Asymptotic Normality of 2SLS 94 5.2.3 Asymptotic E‰ciency of 2SLS 96 5.2.4 Hypothesis Testing with 2SLS 97 5.2.5 Heteroskedasticity-Robust Inference for 2SLS 100 5.2.6 Potential Pitfalls with 2SLS 101 5.3 IV Solutions to the Omitted Variables and Measurement Error Problems 105 5.3.1 Leaving the Omitted Factors in the Error Term 105 5.3.2 Solutions Using Indicators of the Unobservables 105 Problems 107 6 Additional Single-Equation Topics 115 6.1 Estimation with Generated Regressors and Instruments 115 Contents vi", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 4, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p5::c0", "text": "6.1.1 OLS with Generated Regressors 115 6.1.2 2SLS with Generated Instruments 116 6.1.3 Generated Instruments and Regressors 117 6.2 Some Speciﬁcation Tests 118 6.2.1 Testing for Endogeneity 118 6.2.2 Testing Overidentifying Restrictions 122 6.2.3 Testing Functional Form 124 6.2.4 Testing for Heteroskedasticity 125 6.3 Single-Equation Methods under Other Sampling Schemes 128 6.3.1 Pooled Cross Sections over Time 128 6.3.2 Geographically Stratiﬁed Samples 132 6.3.3 Spatial Dependence 134 6.3.4 Cluster Samples 134 Problems 135 Appendix 6A 139 7 Estimating Systems of Equations by OLS and GLS 143 7.1 Introduction 143 7.2 Some Examples 143 7.3 System OLS Estimation of a Multivariate Linear System 147 7.3.1 Preliminaries 147 7.3.2 Asymptotic Properties of System OLS 148 7.3.3 Testing Multiple Hypotheses 153 7.4 Consistency and Asymptotic Normality of Generalized Least Squares 153 7.4.1 Consistency 153 7.4.2 Asymptotic Normality 156 7.5 Feasible GLS 157 7.5.1 Asymptotic Properties 157 7.5.2 Asymptotic Variance of FGLS under a Standard Assumption 160 7.6 Testing Using FGLS 162 7.7 Seemingly Unrelated Regressions, Revisited 163 7.7.1 Comparison between OLS and FGLS for SUR Systems 164 7.7.2 Systems with Cross Equation Restrictions 167 7.7.3 Singular Variance Matrices in SUR Systems 167 Contents vii", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 5, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p6::c0", "text": "7.8 The Linear Panel Data Model, Revisited 169 7.8.1 Assumptions for Pooled OLS 170 7.8.2 Dynamic Completeness 173 7.8.3 A Note on Time Series Persistence 175 7.8.4 Robust Asymptotic Variance Matrix 175 7.8.5 Testing for Serial Correlation and Heteroskedasticity after Pooled OLS 176 7.8.6 Feasible GLS Estimation under Strict Exogeneity 178 Problems 179 8 System Estimation by Instrumental Variables 183 8.1 Introduction and Examples 183 8.2 A General Linear System of Equations 186 8.3 Generalized Method of Moments Estimation 188 8.3.1 A General Weighting Matrix 188 8.3.2 The System 2SLS Estimator 191 8.3.3 The Optimal Weighting Matrix 192 8.3.4 The Three-Stage Least Squares Estimator 194 8.3.5 Comparison between GMM 3SLS and Traditional 3SLS 196 8.4 Some Considerations When Choosing an Estimator 198 8.5 Testing Using GMM 199 8.5.1 Testing Classical Hypotheses 199 8.5.2 Testing Overidentiﬁcation Restrictions 201 8.6 More E‰cient Estimation and Optimal Instruments 202 Problems 205 9 Simultaneous Equations Models 209 9.1 The Scope of Simultaneous Equations Models 209 9.2 Identiﬁcation in a Linear System 211 9.2.1 Exclusion Restrictions and Reduced Forms 211 9.2.2 General Linear Restrictions and Structural Equations 215 9.2.3 Unidentiﬁed, Just Identiﬁed, and Overidentiﬁed Equations 220 9.3 Estimation after Identiﬁcation 221 9.3.1 The Robustness-E‰ciency Trade-o¤ 221 9.3.2 When Are 2SLS and 3SLS Equivalent? 224 9.3.3 Estimating the Reduced Form Parameters 224 9.4 Additional Topics in Linear SEMs 225 Contents viii", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 6, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p7::c0", "text": "9.4.1 Using Cross Equation Restrictions to Achieve Identiﬁcation 225 9.4.2 Using Covariance Restrictions to Achieve Identiﬁcation 227 9.4.3 Subtleties Concerning Identiﬁcation and E‰ciency in Linear Systems 229 9.5 SEMs Nonlinear in Endogenous Variables 230 9.5.1 Identiﬁcation 230 9.5.2 Estimation 235 9.6 Di¤erent Instruments for Di¤erent Equations 237 Problems 239 10 Basic Linear Unobserved E¤ects Panel Data Models 247 10.1 Motivation: The Omitted Variables Problem 247 10.2 Assumptions about the Unobserved E¤ects and Explanatory Variables 251 10.2.1 Random or Fixed E¤ects? 251 10.2.2 Strict Exogeneity Assumptions on the Explanatory Variables 252 10.2.3 Some Examples of Unobserved E¤ects Panel Data Models 254 10.3 Estimating Unobserved E¤ects Models by Pooled OLS 256 10.4 Random E¤ects Methods 257 10.4.1 Estimation and Inference under the Basic Random E¤ects Assumptions 257 10.4.2 Robust Variance Matrix Estimator 262 10.4.3 A General FGLS Analysis 263 10.4.4 Testing for the Presence of an Unobserved E¤ect 264 10.5 Fixed E¤ects Methods 265 10.5.1 Consistency of the Fixed E¤ects Estimator 265 10.5.2 Asymptotic Inference with Fixed E¤ects 269 10.5.3 The Dummy Variable Regression 272 10.5.4 Serial Correlation and the Robust Variance Matrix Estimator 274 10.5.5 Fixed E¤ects GLS 276 10.5.6 Using Fixed E¤ects Estimation for Policy Analysis 278 10.6 First Di¤erencing Methods 279 10.6.1 Inference 279 10.6.2 Robust Variance Matrix 282 Contents ix", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 7, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p8::c0", "text": "10.6.3 Testing for Serial Correlation 282 10.6.4 Policy Analysis Using First Di¤erencing 283 10.7 Comparison of Estimators 284 10.7.1 Fixed E¤ects versus First Di¤erencing 284 10.7.2 The Relationship between the Random E¤ects and Fixed E¤ects Estimators 286 10.7.3 The Hausman Test Comparing the RE and FE Estimators 288 Problems 291 11 More Topics in Linear Unobserved E¤ects Models 299 11.1 Unobserved E¤ects Models without the Strict Exogeneity Assumption 299 11.1.1 Models under Sequential Moment Restrictions 299 11.1.2 Models with Strictly and Sequentially Exogenous Explanatory Variables 305 11.1.3 Models with Contemporaneous Correlation between Some Explanatory Variables and the Idiosyncratic Error 307 11.1.4 Summary of Models without Strictly Exogenous Explanatory Variables 314 11.2 Models with Individual-Speciﬁc Slopes 315 11.2.1 A Random Trend Model 315 11.2.2 General Models with Individual-Speciﬁc Slopes 317 11.3 GMM Approaches to Linear Unobserved E¤ects Models 322 11.3.1 Equivalence between 3SLS and Standard Panel Data Estimators 322 11.3.2 Chamberlain’s Approach to Unobserved E¤ects Models 323 11.4 Hausman and Taylor-Type Models 325 11.5 Applying Panel Data Methods to Matched Pairs and Cluster Samples 328 Problems 332 III GENERAL APPROACHES TO NONLINEAR ESTIMATION 339 12 M-Estimation 341 12.1 Introduction 341 12.2 Identiﬁcation, Uniform Convergence, and Consistency 345 12.3 Asymptotic Normality 349 Contents x", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 8, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p9::c0", "text": "12.4 Two-Step M-Estimators 353 12.4.1 Consistency 353 12.4.2 Asymptotic Normality 354 12.5 Estimating the Asymptotic Variance 356 12.5.1 Estimation without Nuisance Parameters 356 12.5.2 Adjustments for Two-Step Estimation 361 12.6 Hypothesis Testing 362 12.6.1 Wald Tests 362 12.6.2 Score (or Lagrange Multiplier) Tests 363 12.6.3 Tests Based on the Change in the Objective Function 369 12.6.4 Behavior of the Statistics under Alternatives 371 12.7 Optimization Methods 372 12.7.1 The Newton-Raphson Method 372 12.7.2 The Berndt, Hall, Hall, and Hausman Algorithm 374 12.7.3 The Generalized Gauss-Newton Method 375 12.7.4 Concentrating Parameters out of the Objective Function 376 12.8 Simulation and Resampling Methods 377 12.8.1 Monte Carlo Simulation 377 12.8.2 Bootstrapping 378 Problems 380 13 Maximum Likelihood Methods 385 13.1 Introduction 385 13.2 Preliminaries and Examples 386 13.3 General Framework for Conditional MLE 389 13.4 Consistency of Conditional MLE 391 13.5 Asymptotic Normality and Asymptotic Variance Estimation 392 13.5.1 Asymptotic Normality 392 13.5.2 Estimating the Asymptotic Variance 395 13.6 Hypothesis Testing 397 13.7 Speciﬁcation Testing 398 13.8 Partial Likelihood Methods for Panel Data and Cluster Samples 401 13.8.1 Setup for Panel Data 401 13.8.2 Asymptotic Inference 405 13.8.3 Inference with Dynamically Complete Models 408 13.8.4 Inference under Cluster Sampling 409 Contents xi", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 9, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p10::c0", "text": "13.9 Panel Data Models with Unobserved E¤ects 410 13.9.1 Models with Strictly Exogenous Explanatory Variables 410 13.9.2 Models with Lagged Dependent Variables 412 13.10 Two-Step MLE 413 Problems 414 Appendix 13A 418 14 Generalized Method of Moments and Minimum Distance Estimation 421 14.1 Asymptotic Properties of GMM 421 14.2 Estimation under Orthogonality Conditions 426 14.3 Systems of Nonlinear Equations 428 14.4 Panel Data Applications 434 14.5 E‰cient Estimation 436 14.5.1 A General E‰ciency Framework 436 14.5.2 E‰ciency of MLE 438 14.5.3 E‰cient Choice of Instruments under Conditional Moment Restrictions 439 14.6 Classical Minimum Distance Estimation 442 Problems 446 Appendix 14A 448 IV NONLINEAR MODELS AND RELATED TOPICS 451 15 Discrete Response Models 453 15.1 Introduction 453 15.2 The Linear Probability Model for Binary Response 454 15.3 Index Models for Binary Response: Probit and Logit 457 15.4 Maximum Likelihood Estimation of Binary Response Index Models 460 15.5 Testing in Binary Response Index Models 461 15.5.1 Testing Multiple Exclusion Restrictions 461 15.5.2 Testing Nonlinear Hypotheses about b 463 15.5.3 Tests against More General Alternatives 463 15.6 Reporting the Results for Probit and Logit 465 15.7 Speciﬁcation Issues in Binary Response Models 470 15.7.1 Neglected Heterogeneity 470 15.7.2 Continuous Endogenous Explanatory Variables 472 Contents xii", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 10, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p11::c0", "text": "15.7.3 A Binary Endogenous Explanatory Variable 477 15.7.4 Heteroskedasticity and Nonnormality in the Latent Variable Model 479 15.7.5 Estimation under Weaker Assumptions 480 15.8 Binary Response Models for Panel Data and Cluster Samples 482 15.8.1 Pooled Probit and Logit 482 15.8.2 Unobserved E¤ects Probit Models under Strict Exogeneity 483 15.8.3 Unobserved E¤ects Logit Models under Strict Exogeneity 490 15.8.4 Dynamic Unobserved E¤ects Models 493 15.8.5 Semiparametric Approaches 495 15.8.6 Cluster Samples 496 15.9 Multinomial Response Models 497 15.9.1 Multinomial Logit 497 15.9.2 Probabilistic Choice Models 500 15.10 Ordered Response Models 504 15.10.1 Ordered Logit and Ordered Probit 504 15.10.2 Applying Ordered Probit to Interval-Coded Data 508 Problems 509 16 Corner Solution Outcomes and Censored Regression Models 517 16.1 Introduction and Motivation 517 16.2 Derivations of Expected Values 521 16.3 Inconsistency of OLS 524 16.4 Estimation and Inference with Censored Tobit 525 16.5 Reporting the Results 527 16.6 Speciﬁcation Issues in Tobit Models 529 16.6.1 Neglected Heterogeneity 529 16.6.2 Endogenous Explanatory Variables 530 16.6.3 Heteroskedasticity and Nonnormality in the Latent Variable Model 533 16.6.4 Estimation under Conditional Median Restrictions 535 16.7 Some Alternatives to Censored Tobit for Corner Solution Outcomes 536 16.8 Applying Censored Regression to Panel Data and Cluster Samples 538 16.8.1 Pooled Tobit 538 16.8.2 Unobserved E¤ects Tobit Models under Strict Exogeneity 540 Contents xiii", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 11, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p12::c0", "text": "16.8.3 Dynamic Unobserved E¤ects Tobit Models 542 Problems 544 17 Sample Selection, Attrition, and Stratiﬁed Sampling 551 17.1 Introduction 551 17.2 When Can Sample Selection Be Ignored? 552 17.2.1 Linear Models: OLS and 2SLS 552 17.2.2 Nonlinear Models 556 17.3 Selection on the Basis of the Response Variable: Truncated Regression 558 17.4 A Probit Selection Equation 560 17.4.1 Exogenous Explanatory Variables 560 17.4.2 Endogenous Explanatory Variables 567 17.4.3 Binary Response Model with Sample Selection 570 17.5 A Tobit Selection Equation 571 17.5.1 Exogenous Explanatory Variables 571 17.5.2 Endogenous Explanatory Variables 573 17.6 Estimating Structural Tobit Equations with Sample Selection 575 17.7 Sample Selection and Attrition in Linear Panel Data Models 577 17.7.1 Fixed E¤ects Estimation with Unbalanced Panels 578 17.7.2 Testing and Correcting for Sample Selection Bias 581 17.7.3 Attrition 585 17.8 Stratiﬁed Sampling 590 17.8.1 Standard Stratiﬁed Sampling and Variable Probability Sampling 590 17.8.2 Weighted Estimators to Account for Stratiﬁcation 592 17.8.3 Stratiﬁcation Based on Exogenous Variables 596 Problems 598 18 Estimating Average Treatment E¤ects 603 18.1 Introduction 603 18.2 A Counterfactual Setting and the Self-Selection Problem 603 18.3 Methods Assuming Ignorability of Treatment 607 18.3.1 Regression Methods 608 18.3.2 Methods Based on the Propensity Score 614 18.4 Instrumental Variables Methods 621 18.4.1 Estimating the ATE Using IV 621 Contents xiv", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 12, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p13::c0", "text": "18.4.2 Estimating the Local Average Treatment E¤ect by IV 633 18.5 Further Issues 636 18.5.1 Special Considerations for Binary and Corner Solution Responses 636 18.5.2 Panel Data 637 18.5.3 Nonbinary Treatments 638 18.5.4 Multiple Treatments 642 Problems 642 19 Count Data and Related Models 645 19.1 Why Count Data Models? 645 19.2 Poisson Regression Models with Cross Section Data 646 19.2.1 Assumptions Used for Poisson Regression 646 19.2.2 Consistency of the Poisson QMLE 648 19.2.3 Asymptotic Normality of the Poisson QMLE 649 19.2.4 Hypothesis Testing 653 19.2.5 Speciﬁcation Testing 654 19.3 Other Count Data Regression Models 657 19.3.1 Negative Binomial Regression Models 657 19.3.2 Binomial Regression Models 659 19.4 Other QMLEs in the Linear Exponential Family 660 19.4.1 Exponential Regression Models 661 19.4.2 Fractional Logit Regression 661 19.5 Endogeneity and Sample Selection with an Exponential Regression Function 663 19.5.1 Endogeneity 663 19.5.2 Sample Selection 666 19.6 Panel Data Methods 668 19.6.1 Pooled QMLE 668 19.6.2 Specifying Models of Conditional Expectations with Unobserved E¤ects 670 19.6.3 Random E¤ects Methods 671 19.6.4 Fixed E¤ects Poisson Estimation 674 19.6.5 Relaxing the Strict Exogeneity Assumption 676 Problems 678 Contents xv", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 13, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p14::c0", "text": "20 Duration Analysis 685 20.1 Introduction 685 20.2 Hazard Functions 686 20.2.1 Hazard Functions without Covariates 686 20.2.2 Hazard Functions Conditional on Time-Invariant Covariates 690 20.2.3 Hazard Functions Conditional on Time-Varying Covariates 691 20.3 Analysis of Single-Spell Data with Time-Invariant Covariates 693 20.3.1 Flow Sampling 694 20.3.2 Maximum Likelihood Estimation with Censored Flow Data 695 20.3.3 Stock Sampling 700 20.3.4 Unobserved Heterogeneity 703 20.4 Analysis of Grouped Duration Data 706 20.4.1 Time-Invariant Covariates 707 20.4.2 Time-Varying Covariates 711 20.4.3 Unobserved Heterogeneity 713 20.5 Further Issues 714 20.5.1 Cox’s Partial Likelihood Method for the Proportional Hazard Model 714 20.5.2 Multiple-Spell Data 714 20.5.3 Competing Risks Models 715 Problems 715 References 721 Index 737 Contents xvi", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 14, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p15::c0", "text": "Acknowledgments My interest in panel data econometrics began in earnest when I was an assistant professor at MIT, after I attended a seminar by a graduate student, Leslie Papke, who would later become my wife. Her empirical research using nonlinear panel data methods piqued my interest and eventually led to my research on estimating non- linear panel data models without distributional assumptions. I dedicate this text to Leslie. My former colleagues at MIT, particularly Jerry Hausman, Daniel McFadden, Whitney Newey, Danny Quah, and Thomas Stoker, played signiﬁcant roles in en- couraging my interest in cross section and panel data econometrics. I also have learned much about the modern approach to panel data econometrics from Gary Chamberlain of Harvard University. I cannot discount the excellent training I received from Robert Engle, Clive Granger, and especially Halbert White at the University of California at San Diego. I hope they are not too disappointed that this book excludes time series econometrics. I did not teach a course in cross section and panel data methods until I started teaching at Michigan State. Fortunately, my colleague Peter Schmidt encouraged me to teach the course at which this book is aimed. Peter also suggested that a text on panel data methods that uses ‘‘vertical bars’’ would be a worthwhile contribution. Several classes of students at Michigan State were subjected to this book in manu- script form at various stages of development. I would like to thank these students for their perseverance, helpful comments, and numerous corrections. I want to speciﬁcally mention Scott Baier, Linda Bailey, Ali Berker, Yi-Yi Chen, William Horrace, Robin Poston, Kyosti Pietola, Hailong Qian, Wendy Stock, and Andrew Toole. Naturally, they are not responsible for any remaining errors. I was fortunate to have several capable, conscientious reviewers for the manuscript. Jason Abrevaya (University of Chicago), Joshua Angrist (MIT), David Drukker (Stata Corporation), Brian McCall (University of Minnesota), James Ziliak (Uni- versity of Oregon), and three anonymous reviewers provided excellent suggestions, many of which improved the book’s organization and coverage. The people at MIT Press have been remarkably patient, and I have very much enjoyed working with them. I owe a special debt to Terry Vaughn (now at Princeton University Press) for initiating this project and then giving me the time to produce a manuscript with which I felt comfortable. I am grateful to Jane McDonald and Elizabeth Murry for reenergizing the project and for allowing me signiﬁcant leeway in crafting the ﬁnal manuscript. Finally, Peggy Gordon and her crew at P. M. Gordon Associates, Inc., did an expert job in editing the manuscript and in producing the ﬁnal text.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 15, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p16::c0", "text": "Preface This book is intended primarily for use in a second-semester course in graduate econometrics, after a ﬁrst course at the level of Goldberger (1991) or Greene (1997). Parts of the book can be used for special-topics courses, and it should serve as a general reference. My focus on cross section and panel data methods—in particular, what is often dubbed microeconometrics—is novel, and it recognizes that, after coverage of the basic linear model in a ﬁrst-semester course, an increasingly popular approach is to treat advanced cross section and panel data methods in one semester and time series methods in a separate semester. This division reﬂects the current state of econometric practice. Modern empirical research that can be ﬁtted into the classical linear model para- digm is becoming increasingly rare. For instance, it is now widely recognized that a student doing research in applied time series analysis cannot get very far by ignoring recent advances in estimation and testing in models with trending and strongly de- pendent processes. This theory takes a very di¤erent direction from the classical lin- ear model than does cross section or panel data analysis. Hamilton’s (1994) time series text demonstrates this di¤erence unequivocally. Books intended to cover an econometric sequence of a year or more, beginning with the classical linear model, tend to treat advanced topics in cross section and panel data analysis as direct applications or minor extensions of the classical linear model (if they are treated at all). Such treatment needlessly limits the scope of appli- cations and can result in poor econometric practice. The focus in such books on the algebra and geometry of econometrics is appropriate for a ﬁrst-semester course, but it results in oversimpliﬁcation or sloppiness in stating assumptions. Approaches to estimation that are acceptable under the ﬁxed regressor paradigm so prominent in the classical linear model can lead one badly astray under practically important depar- tures from the ﬁxed regressor assumption. Books on ‘‘advanced’’ econometrics tend to be high-level treatments that focus on general approaches to estimation, thereby attempting to cover all data conﬁgurations— including cross section, panel data, and time series—in one framework, without giving special attention to any. A hallmark of such books is that detailed regularity con- ditions are treated on par with the practically more important assumptions that have economic content. This is a burden for students learning about cross section and panel data methods, especially those who are empirically oriented: deﬁnitions and limit theorems about dependent processes need to be included among the regularity conditions in order to cover time series applications. In this book I have attempted to ﬁnd a middle ground between more traditional approaches and the more recent, very uniﬁed approaches. I present each model and", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 16, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p17::c0", "text": "method with a careful discussion of assumptions of the underlying population model. These assumptions, couched in terms of correlations, conditional expectations, con- ditional variances and covariances, or conditional distributions, usually can be given behavioral content. Except for the three more technical chapters in Part III, regularity conditions—for example, the existence of moments needed to ensure that the central limit theorem holds—are not discussed explicitly, as these have little bearing on ap- plied work. This approach makes the assumptions relatively easy to understand, while at the same time emphasizing that assumptions concerning the underlying population and the method of sampling need to be carefully considered in applying any econo- metric method. A unifying theme in this book is the analogy approach to estimation, as exposited by Goldberger (1991) and Manski (1988). [For nonlinear estimation methods with cross section data, Manski (1988) covers several of the topics included here in a more compact format.] Loosely, the analogy principle states that an estimator is chosen to solve the sample counterpart of a problem solved by the population parameter. The analogy approach is complemented nicely by asymptotic analysis, and that is the focus here. By focusing on asymptotic properties I do not mean to imply that small-sample properties of estimators and test statistics are unimportant. However, one typically ﬁrst applies the analogy principle to devise a sensible estimator and then derives its asymptotic properties. This approach serves as a relatively simple guide to doing inference, and it works well in large samples (and often in samples that are not so large). Small-sample adjustments may improve performance, but such considerations almost always come after a large-sample analysis and are often done on a case-by- case basis. The book contains proofs or outlines the proofs of many assertions, focusing on the role played by the assumptions with economic content while downplaying or ignoring regularity conditions. The book is primarily written to give applied researchers a very ﬁrm understanding of why certain methods work and to give students the background for developing new methods. But many of the arguments used throughout the book are representative of those made in modern econometric research (sometimes without the technical details). Students interested in doing research in cross section or panel data methodology will ﬁnd much here that is not available in other graduate texts. I have also included several empirical examples with included data sets. Most of the data sets come from published work or are intended to mimic data sets used in modern empirical analysis. To save space I illustrate only the most commonly used methods on the most common data structures. Not surprisingly, these overlap con- Preface xviii", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 17, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p18::c0", "text": "siderably with methods that are packaged in econometric software programs. Other examples are of models where, given access to the appropriate data set, one could undertake an empirical analysis. The numerous end-of-chapter problems are an important component of the book. Some problems contain important points that are not fully described in the text; others cover new ideas that can be analyzed using the tools presented in the current and previous chapters. Several of the problems require using the data sets that are included with the book. As with any book, the topics here are selective and reﬂect what I believe to be the methods needed most often by applied researchers. I also give coverage to topics that have recently become important but are not adequately treated in other texts. Part I of the book reviews some tools that are elusive in mainstream econometrics books— in particular, the notion of conditional expectations, linear projections, and various convergence results. Part II begins by applying these tools to the analysis of single- equation linear models using cross section data. In principle, much of this material should be review for students having taken a ﬁrst-semester course. But starting with single-equation linear models provides a bridge from the classical analysis of linear models to a more modern treatment, and it is the simplest vehicle to illustrate the application of the tools in Part I. In addition, several methods that are used often in applications—but rarely covered adequately in texts—can be covered in a single framework. I approach estimation of linear systems of equations with endogenous variables from a di¤erent perspective than traditional treatments. Rather than begin with simul- taneous equations models, we study estimation of a general linear system by instru- mental variables. This approach allows us to later apply these results to models with the same statistical structure as simultaneous equations models, including panel data models. Importantly, we can study the generalized method of moments estimator from the beginning and easily relate it to the more traditional three-stage least squares estimator. The analysis of general estimation methods for nonlinear models in Part III begins with a general treatment of asymptotic theory of estimators obtained from non- linear optimization problems. Maximum likelihood, partial maximum likelihood, and generalized method of moments estimation are shown to be generally applicable estimation approaches. The method of nonlinear least squares is also covered as a method for estimating models of conditional means. Part IV covers several nonlinear models used by modern applied researchers. Chapters 15 and 16 treat limited dependent variable models, with attention given to Preface xix", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 18, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p19::c0", "text": "handling certain endogeneity problems in such models. Panel data methods for binary response and censored variables, including some new estimation approaches, are also covered in these chapters. Chapter 17 contains a treatment of sample selection problems for both cross sec- tion and panel data, including some recent advances. The focus is on the case where the population model is linear, but some results are given for nonlinear models as well. Attrition in panel data models is also covered, as are methods for dealing with stratiﬁed samples. Recent approaches to estimating average treatment e¤ects are treated in Chapter 18. Poisson and related regression models, both for cross section and panel data, are treated in Chapter 19. These rely heavily on the method of quasi-maximum likeli- hood estimation. A brief but modern treatment of duration models is provided in Chapter 20. I have given short shrift to some important, albeit more advanced, topics. The setting here is, at least in modern parlance, essentially parametric. I have not included detailed treatment of recent advances in semiparametric or nonparametric analysis. In many cases these topics are not conceptually di‰cult. In fact, many semiparametric methods focus primarily on estimating a ﬁnite dimensional parameter in the presence of an inﬁnite dimensional nuisance parameter—a feature shared by traditional par- ametric methods, such as nonlinear least squares and partial maximum likelihood. It is estimating inﬁnite dimensional parameters that is conceptually and technically challenging. At the appropriate point, in lieu of treating semiparametric and nonparametric methods, I mention when such extensions are possible, and I provide references. A beneﬁt of a modern approach to parametric models is that it provides a seamless transition to semiparametric and nonparametric methods. General surveys of semi- parametric and nonparametric methods are available in Volume 4 of the Handbook of Econometrics—see Powell (1994) and Ha¨rdle and Linton (1994)—as well as in Volume 11 of the Handbook of Statistics—see Horowitz (1993) and Ullah and Vinod (1993). I only brieﬂy treat simulation-based methods of estimation and inference. Com- puter simulations can be used to estimate complicated nonlinear models when tradi- tional optimization methods are ine¤ective. The bootstrap method of inference and conﬁdence interval construction can improve on asymptotic analysis. Volume 4 of the Handbook of Econometrics and Volume 11 of the Handbook of Statistics contain nice surveys of these topics (Hajivassilou and Ruud, 1994; Hall, 1994; Hajivassilou, 1993; and Keane, 1993). Preface xx", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 19, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p20::c0", "text": "On an organizational note, I refer to sections throughout the book ﬁrst by chapter number followed by section number and, sometimes, subsection number. Therefore, Section 6.3 refers to Section 3 in Chapter 6, and Section 13.8.3 refers to Subsection 3 of Section 8 in Chapter 13. By always including the chapter number, I hope to minimize confusion. Possible Course Outlines If all chapters in the book are covered in detail, there is enough material for two semesters. For a one-semester course, I use a lecture or two to review the most im- portant concepts in Chapters 2 and 3, focusing on conditional expectations and basic limit theory. Much of the material in Part I can be referred to at the appropriate time. Then I cover the basics of ordinary least squares and two-stage least squares in Chapters 4, 5, and 6. Chapter 7 begins the topics that most students who have taken one semester of econometrics have not previously seen. I spend a fair amount of time on Chapters 10 and 11, which cover linear unobserved e¤ects panel data models. Part III is technically more di‰cult than the rest of the book. Nevertheless, it is fairly easy to provide an overview of the analogy approach to nonlinear estimation, along with computing asymptotic variances and test statistics, especially for maxi- mum likelihood and partial maximum likelihood methods. In Part IV, I focus on binary response and censored regression models. If time permits, I cover the rudiments of quasi-maximum likelihood in Chapter 19, especially for count data, and give an overview of some important issues in modern duration analysis (Chapter 20). For topics courses that focus entirely on nonlinear econometric methods for cross section and panel data, Part III is a natural starting point. A full-semester course would carefully cover the material in Parts III and IV, probably supplementing the parametric approach used here with popular semiparametric methods, some of which are referred to in Part IV. Parts III and IV can also be used for a half-semester course on nonlinear econometrics, where Part III is not covered in detail if the course has an applied orientation. A course in applied econometrics can select topics from all parts of the book, emphasizing assumptions but downplaying derivations. The several empirical exam- ples and data sets can be used to teach students how to use advanced econometric methods. The data sets can be accessed by visiting the website for the book at MIT Press: http://mitpress.mit.edu/Wooldridge-EconAnalysis. Preface xxi", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 20, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p21::c0", "text": "I INTRODUCTION AND BACKGROUND In this part we introduce the basic approach to econometrics taken throughout the book and cover some background material that is important to master before reading the remainder of the text. Students who have a solid understanding of the algebra of conditional expectations, conditional variances, and linear projections could skip Chapter 2, referring to it only as needed. Chapter 3 contains a summary of the asymptotic analysis needed to read Part II and beyond. In Part III we introduce ad- ditional asymptotic tools that are needed to study nonlinear estimation.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 21, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p22::c0", "text": "1Introduction 1.1 Causal Relationships and Ceteris Paribus Analysis The goal of most empirical studies in economics and other social sciences is to de- termine whether a change in one variable, say w, causes a change in another variable, say y. For example, does having another year of education cause an increase in monthly salary? Does reducing class size cause an improvement in student per- formance? Does lowering the business property tax rate cause an increase in city economic activity? Because economic variables are properly interpreted as random variables, we should use ideas from probability to formalize the sense in which a change in w causes a change in y. The notion of ceteris paribus—that is, holding all other (relevant) factors ﬁxed—is at the crux of establishing a causal relationship. Simply ﬁnding that two variables are correlated is rarely enough to conclude that a change in one variable causes a change in another. This result is due to the nature of economic data: rarely can we run a controlled experiment that allows a simple correlation analysis to uncover causality. Instead, we can use econometric methods to e¤ectively hold other factors ﬁxed. If we focus on the average, or expected, response, a ceteris paribus analysis entails estimating Eðy j w; cÞ, the expected value of y conditional on w and c. The vector c— whose dimension is not important for this discussion—denotes a set of control vari- ables that we would like to explicitly hold ﬁxed when studying the e¤ect of w on the expected value of y. The reason we control for these variables is that we think w is correlated with other factors that also inﬂuence y. If w is continuous, interest centers on qEðy j w; cÞ=qw, which is usually called the partial e¤ect of w on Eðy j w; cÞ. If w is discrete, we are interested in Eðy j w; cÞ evaluated at di¤erent values of w, with the elements of c ﬁxed at the same speciﬁed values. Deciding on the list of proper controls is not always straightforward, and using di¤erent controls can lead to di¤erent conclusions about a causal relationship be- tween y and w. This is where establishing causality gets tricky: it is up to us to decide which factors need to be held ﬁxed. If we settle on a list of controls, and if all ele- ments of c can be observed, then estimating the partial e¤ect of w on Eðy j w; cÞ is relatively straightforward. Unfortunately, in economics and other social sciences, many elements of c are not observed. For example, in estimating the causal e¤ect of education on wage, we might focus on Eðwage j educ; exper; abilÞ where educ is years of schooling, exper is years of workforce experience, and abil is innate ability. In this case, c ¼ ðexper; abil Þ, where exper is observed but abil is not. (It is widely agreed among labor economists that experience and ability are two factors we should hold ﬁxed to obtain the causal e¤ect of education on wages. Other factors, such as years", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 22, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p23::c0", "text": "with the current employer, might belong as well. We can all agree that something such as the last digit of one’s social security number need not be included as a con- trol, as it has nothing to do with wage or education.) As a second example, consider establishing a causal relationship between student attendance and performance on a ﬁnal exam in a principles of economics class. We might be interested in Eðscore j attend; SAT; priGPAÞ, where score is the ﬁnal exam score, attend is the attendance rate, SAT is score on the scholastic aptitude test, and priGPA is grade point average at the beginning of the term. We can reasonably col- lect data on all of these variables for a large group of students. Is this setup enough to decide whether attendance has a causal e¤ect on performance? Maybe not. While SAT and priGPA are general measures reﬂecting student ability and study habits, they do not necessarily measure one’s interest in or aptitude for econonomics. Such attributes, which are di‰cult to quantify, may nevertheless belong in the list of con- trols if we are going to be able to infer that attendance rate has a causal e¤ect on performance. In addition to not being able to obtain data on all desired controls, other problems can interfere with estimating causal relationships. For example, even if we have good measures of the elements of c, we might not have very good measures of y or w. A more subtle problem—which we study in detail in Chapter 9—is that we may only observe equilibrium values of y and w when these variables are simultaneously de- termined. An example is determining the causal e¤ect of conviction rates ðwÞ on city crime rates ðyÞ. A ﬁrst course in econometrics teaches students how to apply multiple regression analysis to estimate ceteris paribus e¤ects of explanatory variables on a response variable. In the rest of this book, we will study how to estimate such e¤ects in a variety of situations. Unlike most introductory treatments, we rely heavily on con- ditional expectations. In Chapter 2 we provide a detailed summary of properties of conditional expectations. 1.2 The Stochastic Setting and Asymptotic Analysis 1.2.1 Data Structures In order to give proper treatment to modern cross section and panel data methods, we must choose a stochastic setting that is appropriate for the kinds of cross section and panel data sets collected for most econometric applications. Naturally, all else equal, it is best if the setting is as simple as possible. It should allow us to focus on Chapter 1 4", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 23, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p24::c0", "text": "interpreting assumptions with economic content while not having to worry too much about technical regularity conditions. (Regularity conditions are assumptions in- volving things such as the number of absolute moments of a random variable that must be ﬁnite.) For much of this book we adopt a random sampling assumption. More precisely, we assume that (1) a population model has been speciﬁed and (2) an independent, identically distributed (i.i.d.) sample can be drawn from the population. Specifying a population model—which may be a model of Eðy j w; cÞ, as in Section 1.1—requires us ﬁrst to clearly deﬁne the population of interest. Deﬁning the relevant population may seem to be an obvious requirement. Nevertheless, as we will see in later chapters, it can be subtle in some cases. An important virtue of the random sampling assumption is that it allows us to separate the sampling assumption from the assumptions made on the population model. In addition to putting the proper emphasis on assumptions that impinge on economic behavior, stating all assumptions in terms of the population is actually much easier than the traditional approach of stating assumptions in terms of full data matrices. Because we will rely heavily on random sampling, it is important to know what it allows and what it rules out. Random sampling is often reasonable for cross section data, where, at a given point in time, units are selected at random from the popula- tion. In this setup, any explanatory variables are treated as random outcomes along with data on response variables. Fixed regressors cannot be identically distributed across observations, and so the random sampling assumption technically excludes the classical linear model. This result is actually desirable for our purposes. In Section 1.4 we provide a brief discussion of why it is important to treat explanatory variables as random for modern econometric analysis. We should not confuse the random sampling assumption with so-called experi- mental data. Experimental data fall under the ﬁxed explanatory variables paradigm. With experimental data, researchers set values of the explanatory variables and then observe values of the response variable. Unfortunately, true experiments are quite rare in economics, and in any case nothing practically important is lost by treating explanatory variables that are set ahead of time as being random. It is safe to say that no one ever went astray by assuming random sampling in place of independent sampling with ﬁxed explanatory variables. Random sampling does exclude cases of some interest for cross section analysis. For example, the identical distribution assumption is unlikely to hold for a pooled cross section, where random samples are obtained from the population at di¤erent Introduction 5", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 24, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p25::c0", "text": "points in time. This case is covered by independent, not identically distributed (i.n.i.d.) observations. Allowing for non-identically distributed observations under indepen- dent sampling is not di‰cult, and its practical e¤ects are easy to deal with. We will mention this case at several points in the book after the analyis is done under random sampling. We do not cover the i.n.i.d. case explicitly in derivations because little is to be gained from the additional complication. A situation that does require special consideration occurs when cross section ob- servations are not independent of one another. An example is spatial correlation models. This situation arises when dealing with large geographical units that cannot be assumed to be independent draws from a large population, such as the 50 states in the United States. It is reasonable to expect that the unemployment rate in one state is correlated with the unemployment rate in neighboring states. While standard esti- mation methods—such as ordinary least squares and two-stage least squares—can usually be applied in these cases, the asymptotic theory needs to be altered. Key sta- tistics often (although not always) need to be modiﬁed. We will brieﬂy discuss some of the issues that arise in this case for single-equation linear models, but otherwise this subject is beyond the scope of this book. For better or worse, spatial correlation is often ignored in applied work because correcting the problem can be di‰cult. Cluster sampling also induces correlation in a cross section data set, but in most cases it is relatively easy to deal with econometrically. For example, retirement saving of employees within a ﬁrm may be correlated because of common (often unobserved) characteristics of workers within a ﬁrm or because of features of the ﬁrm itself (such as type of retirement plan). Each ﬁrm represents a group or cluster, and we may sample several workers from a large number of ﬁrms. As we will see later, provided the number of clusters is large relative to the cluster sizes, standard methods can correct for the presence of within-cluster correlation. Another important issue is that cross section samples often are, either intentionally or unintentionally, chosen so that they are not random samples from the population of interest. In Chapter 17 we discuss such problems at length, including sample selection and stratiﬁed sampling. As we will see, even in cases of nonrandom samples, the assumptions on the population model play a central role. For panel data (or longitudinal data), which consist of repeated observations on the same cross section of, say, individuals, households, ﬁrms, or cities, over time, the random sampling assumption initially appears much too restrictive. After all, any reasonable stochastic setting should allow for correlation in individual or ﬁrm be- havior over time. But the random sampling assumption, properly stated, does allow for temporal correlation. What we will do is assume random sampling in the cross Chapter 1 6", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 25, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p26::c0", "text": "section dimension. The dependence in the time series dimension can be entirely un- restricted. As we will see, this approach is justiﬁed in panel data applications with many cross section observations spanning a relatively short time period. We will also be able to cover panel data sample selection and stratiﬁcation issues within this paradigm. A panel data setup that we will not adequately cover—although the estimation methods we cover can be usually used—is seen when the cross section dimension and time series dimensions are roughly of the same magnitude, such as when the sample consists of countries over the post–World War II period. In this case it makes little sense to ﬁx the time series dimension and let the cross section dimension grow. The research on asymptotic analysis with these kinds of panel data sets is still in its early stages, and it requires special limit theory. See, for example, Quah (1994), Pesaran and Smith (1995), Kao (1999), and Phillips and Moon (1999). 1.2.2 Asymptotic Analysis Throughout this book we focus on asymptotic properties, as opposed to ﬁnite sample properties, of estimators. The primary reason for this emphasis is that ﬁnite sample properties are intractable for most of the estimators we study in this book. In fact, most of the estimators we cover will not have desirable ﬁnite sample properties such as unbiasedness. Asymptotic analysis allows for a uniﬁed treatment of estimation procedures, and it (along with the random sampling assumption) allows us to state all assumptions in terms of the underlying population. Naturally, asymptotic analysis is not without its drawbacks. Occasionally, we will mention when asymptotics can lead one astray. In those cases where ﬁnite sample properties can be derived, you are sometimes asked to derive such properties in the problems. In cross section analysis the asymptotics is as the number of observations, denoted N throughout this book, tends to inﬁnity. Usually what is meant by this statement is obvious. For panel data analysis, the asymptotics is as the cross section dimension gets large while the time series dimension is ﬁxed. 1.3 Some Examples In this section we provide two examples to emphasize some of the concepts from the previous sections. We begin with a standard example from labor economics. Example 1.1 (Wage O¤er Function): Suppose that the natural log of the wage o¤er, wageo, is determined as Introduction 7", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 26, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p27::c0", "text": "logðwageoÞ ¼ b0 þ b1educ þ b2exper þ b3married þ u ð1:1Þ where educ is years of schooling, exper is years of labor market experience, and married is a binary variable indicating marital status. The variable u, called the error term or disturbance, contains unobserved factors that a¤ect the wage o¤er. Interest lies in the unknown parameters, the bj. We should have a concrete population in mind when specifying equation (1.1). For example, equation (1.1) could be for the population of all working women. In this case, it will not be di‰cult to obtain a random sample from the population. All assumptions can be stated in terms of the population model. The crucial assumptions involve the relationship between u and the observable explanatory vari- ables, educ, exper, and married. For example, is the expected value of u given the explanatory variables educ, exper, and married equal to zero? Is the variance of u conditional on the explanatory variables constant? There are reasons to think the answer to both of these questions is no, something we discuss at some length in Chapters 4 and 5. The point of raising them here is to emphasize that all such ques- tions are most easily couched in terms of the population model. What happens if the relevant population is all women over age 18? A problem arises because a random sample from this population will include women for whom the wage o¤er cannot be observed because they are not working. Nevertheless, we can think of a random sample being obtained, but then wageo is unobserved for women not working. For deriving the properties of estimators, it is often useful to write the population model for a generic draw from the population. Equation (1.1) becomes logðwageo i Þ ¼ b0 þ b1educi þ b2experi þ b3marriedi þ ui; ð1:2Þ where i indexes person. Stating assumptions in terms of ui and xi 1 ðeduci; experi; marriediÞ is the same as stating assumptions in terms of u and x. Throughout this book, the i subscript is reserved for indexing cross section units, such as individual, ﬁrm, city, and so on. Letters such as j, g, and h will be used to index variables, parameters, and equations. Before ending this example, we note that using matrix notation to write equation (1.2) for all N observations adds nothing to our understanding of the model or sam- pling scheme; in fact, it just gets in the way because it gives the mistaken impression that the matrices tell us something about the assumptions in the underlying popula- tion. It is much better to focus on the population model (1.1). The next example is illustrative of panel data applications. Chapter 1 8", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 27, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p28::c0", "text": "Example 1.2 (E¤ect of Spillovers on Firm Output): Suppose that the population is all manufacturing ﬁrms in a country operating during a given three-year period. A production function describing output in the population of ﬁrms is logðoutputtÞ ¼ dt þ b1 logðlabortÞ þ b2 logðcapitaltÞ þ b3spillovert þ quality þ ut; t ¼ 1; 2; 3 ð1:3Þ Here, spillovert is a measure of foreign ﬁrm concentration in the region containing the ﬁrm. The term quality contains unobserved factors—such as unobserved managerial or worker quality—which a¤ect productivity and are constant over time. The error ut represents unobserved shocks in each time period. The presence of the parameters dt, which represent di¤erent intercepts in each year, allows for aggregate productivity to change over time. The coe‰cients on labort, capitalt, and spillovert are assumed constant across years. As we will see when we study panel data methods, there are several issues in deciding how best to estimate the bj. An important one is whether the unobserved productivity factors (quality) are correlated with the observable inputs. Also, can we assume that spillovert at, say, t ¼ 3 is uncorrelated with the error terms in all time periods? For panel data it is especially useful to add an i subscript indicating a generic cross section observation—in this case, a randomly sampled ﬁrm: logðoutputitÞ ¼ dt þ b1 logðlaboritÞ þ b2 logðcapitalitÞ þ b3spilloverit þ qualityi þ uit; t ¼ 1; 2; 3 ð1:4Þ Equation (1.4) makes it clear that qualityi is a ﬁrm-speciﬁc term that is constant over time and also has the same e¤ect in each time period, while uit changes across time and ﬁrm. Nevertheless, the key issues that we must address for estimation can be discussed for a generic i, since the draws are assumed to be randomly made from the population of all manufacturing ﬁrms. Equation (1.4) is an example of another convention we use throughout the book: the subscript t is reserved to index time, just as i is reserved for indexing the cross section. 1.4 Why Not Fixed Explanatory Variables? We have seen two examples where, generally speaking, the error in an equation can be correlated with one or more of the explanatory variables. This possibility is Introduction 9", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 28, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p29::c0", "text": "so prevalent in social science applications that it makes little sense to adopt an assumption—namely, the assumption of ﬁxed explanatory variables—that rules out such correlation a priori. In a ﬁrst course in econometrics, the method of ordinary least squares (OLS) and its extensions are usually learned under the ﬁxed regressor assumption. This is ap- propriate for understanding the mechanics of least squares and for gaining experience with statistical derivations. Unfortunately, reliance on ﬁxed regressors or, more gen- erally, ﬁxed ‘‘exogenous’’ variables, can have unintended consequences, especially in more advanced settings. For example, in Chapters 7, 10, and 11 we will see that as- suming ﬁxed regressors or ﬁxed instrumental variables in panel data models imposes often unrealistic restrictions on dynamic economic behavior. This is not just a tech- nical point: estimation methods that are consistent under the ﬁxed regressor as- sumption, such as generalized least squares, are no longer consistent when the ﬁxed regressor assumption is relaxed in interesting ways. To illustrate the shortcomings of the ﬁxed regressor assumption in a familiar con- text, consider a linear model for cross section data, written for each observation i as yi ¼ b0 þ xib þ ui; i ¼ 1; 2; . . . ; N where xi is a 1 \u0001 K vector and b is a K \u0001 1 vector. It is common to see the ‘‘ideal’’ assumptions for this model stated as ‘‘The errors fui: i ¼ 1; 2; . . . ; Ng are i.i.d. with EðuiÞ ¼ 0 and VarðuiÞ ¼ s2.’’ (Sometimes the ui are also assumed to be normally distributed.) The problem with this statement is that it omits the most important consideration: What is assumed about the relationship between ui and xi? If the xi are taken as nonrandom—which, evidently, is very often the implicit assumption—then ui and xi are independent of one another. In nonexperimental environments this as- sumption rules out too many situations of interest. Some important questions, such as e‰ciency comparisons across models with di¤erent explanatory variables, cannot even be asked in the context of ﬁxed regressors. (See Problems 4.5 and 4.15 of Chapter 4 for speciﬁc examples.) In a random sampling context, the ui are always independent and identically dis- tributed, regardless of how they are related to the xi. Assuming that the population mean of the error is zero is without loss of generality when an intercept is included in the model. Thus, the statement ‘‘The errors fui: i ¼ 1; 2; . . . ; Ng are i.i.d. with EðuiÞ ¼ 0 and VarðuiÞ ¼ s2’’ is vacuous in a random sampling context. Viewing the xi as random draws along with yi forces us to think about the relationship between the error and the explanatory variables in the population. For example, in the popu- lation model y ¼ b0 þ xb þ u, is the expected value of u given x equal to zero? Is u correlated with one or more elements of x? Is the variance of u given x constant, or Chapter 1 10", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 29, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p30::c0", "text": "does it depend on x? These are the assumptions that are relevant for estimating b and for determining how to perform statistical inference. Because our focus is on asymptotic analysis, we have the luxury of allowing for random explanatory variables throughout the book, whether the setting is linear models, nonlinear models, single-equation analysis, or system analysis. An incidental but nontrivial beneﬁt is that, compared with frameworks that assume ﬁxed explan- atory variables, the unifying theme of random sampling actually simpliﬁes the asymptotic analysis. We will never state assumptions in terms of full data matrices, because such assumptions can be imprecise and can impose unintended restrictions on the population model. Introduction 11", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 30, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p31::c0", "text": "2 Conditional Expectations and Related Concepts in Econometrics 2.1 The Role of Conditional Expectations in Econometrics As we suggested in Section 1.1, the conditional expectation plays a crucial role in modern econometric analysis. Although it is not always explicitly stated, the goal of most applied econometric studies is to estimate or test hypotheses about the ex- pectation of one variable—called the explained variable, the dependent variable, the regressand, or the response variable, and usually denoted y—conditional on a set of explanatory variables, independent variables, regressors, control variables, or covari- ates, usually denoted x ¼ ðx1; x2; . . . ; xKÞ. A substantial portion of research in econometric methodology can be interpreted as ﬁnding ways to estimate conditional expectations in the numerous settings that arise in economic applications. As we brieﬂy discussed in Section 1.1, most of the time we are interested in conditional expectations that allow us to infer causality from one or more explanatory variables to the response variable. In the setup from Section 1.1, we are interested in the e¤ect of a variable w on the expected value of y, holding ﬁxed a vector of controls, c. The conditional expectation of interest is Eðy j w; cÞ, which we will call a structural conditional expectation. If we can collect data on y, w, and c in a random sample from the underlying population of interest, then it is fairly straightforward to estimate Eðy j w; cÞ—especially if we are willing to make an assumption about its functional form—in which case the e¤ect of w on Eðy j w; cÞ, holding c ﬁxed, is easily estimated. Unfortunately, complications often arise in the collection and analysis of economic data because of the nonexperimental nature of economics. Observations on economic variables can contain measurement error, or they are sometimes properly viewed as the outcome of a simultaneous process. Sometimes we cannot obtain a random sample from the population, which may not allow us to estimate Eðy j w; cÞ. Perhaps the most prevalent problem is that some variables we would like to control for (ele- ments of c) cannot be observed. In each of these cases there is a conditional expec- tation (CE) of interest, but it generally involves variables for which the econometrician cannot collect data or requires an experiment that cannot be carried out. Under additional assumptions—generally called identiﬁcation assumptions—we can sometimes recover the structural conditional expectation originally of interest, even if we cannot observe all of the desired controls, or if we only observe equilib- rium outcomes of variables. As we will see throughout this text, the details di¤er depending on the context, but the notion of conditional expectation is fundamental. In addition to providing a uniﬁed setting for interpreting economic models, the CE operator is useful as a tool for manipulating structural equations into estimable equations. In the next section we give an overview of the important features of the", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 31, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p32::c0", "text": "conditional expectations operator. The appendix to this chapter contains a more ex- tensive list of properties. 2.2 Features of Conditional Expectations 2.2.1 Deﬁnition and Examples Let y be a random variable, which we refer to in this section as the explained variable, and let x 1 ðx1; x2; . . . ; xKÞ be a 1 \u0001 K random vector of explanatory variables. If EðjyjÞ < y, then there is a function, say m: RK ! R, such that Eðy j x1; x2; . . . ; xKÞ ¼ mðx1; x2; . . . ; xKÞ ð2:1Þ or Eðy j xÞ ¼ mðxÞ. The function mðxÞ determines how the average value of y changes as elements of x change. For example, if y is wage and x contains various individual characteristics, such as education, experience, and IQ, then Eðwage j educ; exper; IQÞ is the average value of wage for the given values of educ, exper, and IQ. Technically, we should distinguish Eðy j xÞ—which is a random variable because x is a random vector deﬁned in the population—from the conditional expectation when x takes on a particular value, such as x0: Eðy j x ¼ x0Þ. Making this distinction soon becomes cumbersome and, in most cases, is not overly important; for the most part we avoid it. When discussing probabilistic features of Eðy j xÞ, x is necessarily viewed as a random variable. Because Eðy j xÞ is an expectation, it can be obtained from the conditional density of y given x by integration, summation, or a combination of the two (depending on the nature of y). It follows that the conditional expectation operator has the same linearity properties as the unconditional expectation operator, and several additional properties that are consequences of the randomness of mðxÞ. Some of the statements we make are proven in the appendix, but general proofs of other assertions require measure-theoretic probabability. You are referred to Billingsley (1979) for a detailed treatment. Most often in econometrics a model for a conditional expectation is speciﬁed to depend on a ﬁnite set of parameters, which gives a parametric model of Eðy j xÞ. This considerably narrows the list of possible candidates for mðxÞ. Example 2.1: For K ¼ 2 explanatory variables, consider the following examples of conditional expectations: Eðy j x1; x2Þ ¼ b0 þ b1x1 þ b2x2 ð2:2Þ Chapter 2 14", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 32, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p33::c0", "text": "Eðy j x1; x2Þ ¼ b0 þ b1x1 þ b2x2 þ b3x2 2 ð2:3Þ Eðy j x1; x2Þ ¼ b0 þ b1x1 þ b2x2 þ b3x1x2 ð2:4Þ Eðy j x1; x2Þ ¼ exp½b0 þ b1 logðx1Þ þ b2x2\u0002; y b 0; x1 > 0 ð2:5Þ The model in equation (2.2) is linear in the explanatory variables x1 and x2. Equation (2.3) is an example of a conditional expectation nonlinear in x2, although it is linear in x1. As we will review shortly, from a statistical perspective, equations (2.2) and (2.3) can be treated in the same framework because they are linear in the parameters bj. The fact that equation (2.3) is nonlinear in x has important implications for interpreting the bj, but not for estimating them. Equation (2.4) falls into this same class: it is nonlinear in x ¼ ðx1; x2Þ but linear in the bj. Equation (2.5) di¤ers fundamentally from the ﬁrst three examples in that it is a nonlinear function of the parameters bj, as well as of the xj. Nonlinearity in the parameters has implications for estimating the bj; we will see how to estimate such models when we cover nonlinear methods in Part III. For now, you should note that equation (2.5) is reasonable only if y b 0. 2.2.2 Partial E¤ects, Elasticities, and Semielasticities If y and x are related in a deterministic fashion, say y ¼ f ðxÞ, then we are often interested in how y changes when elements of x change. In a stochastic setting we cannot assume that y ¼ f ðxÞ for some known function and observable vector x be- cause there are always unobserved factors a¤ecting y. Nevertheless, we can deﬁne the partial e¤ects of the xj on the conditional expectation Eðy j xÞ. Assuming that mð\u0003Þ is appropriately di¤erentiable and xj is a continuous variable, the partial derivative qmðxÞ=qxj allows us to approximate the marginal change in Eðy j xÞ when xj is increased by a small amount, holding x1; . . . ; xj\u00041; xjþ1; . . . xK constant: DEðy j xÞA qmðxÞ qxj \u0003 Dxj; holding x1; . . . ; xj\u00041; xjþ1; . . . xK ﬁxed ð2:6Þ The partial derivative of Eðy j xÞ with respect to xj is usually called the partial e¤ect of xj on Eðy j xÞ (or, to be somewhat imprecise, the partial e¤ect of xj on y). Inter- preting the magnitudes of coe‰cients in parametric models usually comes from the approximation in equation (2.6). If xj is a discrete variable (such as a binary variable), partial e¤ects are computed by comparing Eðy j xÞ at di¤erent settings of xj (for example, zero and one when xj is binary), holding other variables ﬁxed. Conditional Expectations and Related Concepts in Econometrics 15", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 33, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p34::c0", "text": "Example 2.1 (continued): In equation (2.2) we have qEðy j xÞ qx1 ¼ b1; qEðy j xÞ qx2 ¼ b2 As expected, the partial e¤ects in this model are constant. In equation (2.3), qEðy j xÞ qx1 ¼ b1; qEðy j xÞ qx2 ¼ b2 þ 2b3x2 so that the partial e¤ect of x1 is constant but the partial e¤ect of x2 depends on the level of x2. In equation (2.4), qEðy j xÞ qx1 ¼ b1 þ b3x2; qEðy j xÞ qx2 ¼ b2 þ b3x1 so that the partial e¤ect of x1 depends on x2, and vice versa. In equation (2.5), qEðy j xÞ qx1 ¼ expð\u0003Þðb1=x1Þ; qEðy j xÞ qx2 ¼ expð\u0003Þb2 ð2:7Þ where expð\u0003Þ denotes the function Eðy j xÞ in equation (2.5). In this case, the partial e¤ects of x1 and x2 both depend on x ¼ ðx1; x2Þ. Sometimes we are interested in a particular function of a partial e¤ect, such as an elasticity. In the determinstic case y ¼ f ðxÞ, we deﬁne the elasticity of y with respect to xj as qy qxj \u0003 xj y ¼ qf ðxÞ qxj \u0003 xj f ðxÞ ð2:8Þ again assuming that xj is continuous. The right-hand side of equation (2.8) shows that the elasticity is a function of x. When y and x are random, it makes sense to use the right-hand side of equation (2.8), but where f ðxÞ is the conditional mean, mðxÞ. Therefore, the (partial) elasticity of Eðy j xÞ with respect to xj, holding x1; . . . ; xj\u00041; xjþ1; . . . ; xK constant, is qEðy j xÞ qxj \u0003 xj Eðy j xÞ ¼ qmðxÞ qxj \u0003 xj mðxÞ : ð2:9Þ If Eðy j xÞ > 0 and xj > 0 (as is often the case), equation (2.9) is the same as q log½Eðy j xÞ\u0002 q logðxjÞ ð2:10Þ Chapter 2 16", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 34, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p35::c0", "text": "This latter expression gives the elasticity its interpretation as the approximate per- centage change in Eðy j xÞ when xj increases by 1 percent. Example 2.1 (continued): In equations (2.2) to (2.5), most elasticities are not con- stant. For example, in equation (2.2), the elasticity of Eðy j xÞ with respect to x1 is ðb1x1Þ=ðb0 þ b1x1 þ b2x2Þ, which clearly depends on x1 and x2. However, in equa- tion (2.5) the elasticity with respect to x1 is constant and equal to b1. How does equation (2.10) compare with the deﬁnition of elasticity from a model linear in the natural logarithms? If y > 0 and xj > 0, we could deﬁne the elasticity as qE½logðyÞ j x\u0002 q logðxjÞ ð2:11Þ This is the natural deﬁnition in a model such as logðyÞ ¼ gðxÞ þ u, where gðxÞ is some function of x and u is an unobserved disturbance with zero mean conditional on x. How do equations (2.10) and (2.11) compare? Generally, they are di¤erent (since the expected value of the log and the log of the expected value can be very di¤erent). If u is independent of x, then equations (2.10) and (2.11) are the same, because then Eðy j xÞ ¼ d \u0003 exp½gðxÞ\u0002 where d 1 E½expðuÞ\u0002. (If u and x are independent, so are expðuÞ and exp½gðxÞ\u0002.) As a speciﬁc example, if logðyÞ ¼ b0 þ b1 logðx1Þ þ b2x2 þ u ð2:12Þ where u has zero mean and is independent of ðx1; x2Þ, then the elasticity of y with respect to x1 is b1 using either deﬁnition of elasticity. If Eðu j xÞ ¼ 0 but u and x are not independent, the deﬁnitions are generally di¤erent. For the most part, little is lost by treating equations (2.10) and (2.11) as the same when y > 0. We will view models such as equation (2.12) as constant elasticity models of y with respect to x1 whenever logðyÞ and logðxjÞ are well deﬁned. Deﬁni- tion (2.10) is more general because sometimes it applies even when logðyÞ is not deﬁned. (We will need the general deﬁnition of an elasticity in Chapters 16 and 19.) The percentage change in Eðy j xÞ when xj is increased by one unit is approximated as 100 \u0003 qEðy j xÞ qxj \u0003 1 Eðy j xÞ ð2:13Þ which equals Conditional Expectations and Related Concepts in Econometrics 17", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 35, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p36::c0", "text": "100 \u0003 q log½Eðy j xÞ\u0002 qxj ð2:14Þ if Eðy j xÞ > 0. This is sometimes called the semielasticity of Eðy j xÞ with respect to xj. Example 2.1 (continued): In equation (2.5) the semielasticity with respect to x2 is constant and equal to 100 \u0003 b2. No other semielasticities are constant in these equations. 2.2.3 The Error Form of Models of Conditional Expectations When y is a random variable we would like to explain in terms of observable vari- ables x, it is useful to decompose y as y ¼ Eðy j xÞ þ u ð2:15Þ Eðu j xÞ ¼ 0 ð2:16Þ In other words, equations (2.15) and (2.16) are deﬁnitional: we can always write y as its conditional expectation, Eðy j xÞ, plus an error term or disturbance term that has conditional mean zero. The fact that Eðu j xÞ ¼ 0 has the following important implications: (1) EðuÞ ¼ 0; (2) u is uncorrelated with any function of x1; x2; . . . ; xK, and, in particular, u is uncorrelated with each of x1; x2; . . . ; xK. That u has zero unconditional expectation follows as a special case of the law of iterated expectations (LIE ), which we cover more generally in the next subsection. Intuitively, it is quite reasonable that Eðu j xÞ ¼ 0 implies EðuÞ ¼ 0. The second implication is less obvious but very important. The fact that u is uncorrelated with any function of x is much stronger than merely saying that u is uncorrelated with x1; . . . ; xK. As an example, if equation (2.2) holds, then we can write y ¼ b0 þ b1x1 þ b2x2 þ u; Eðu j x1; x2Þ ¼ 0 ð2:17Þ and so EðuÞ ¼ 0; Covðx1; uÞ ¼ 0; Covðx2; uÞ ¼ 0 ð2:18Þ But we can say much more: under equation (2.17), u is also uncorrelated with any other function we might think of, such as x2 1; x2 2; x1x2; expðx1Þ, and logðx2 2 þ 1Þ. This fact ensures that we have fully accounted for the e¤ects of x1 and x2 on the expected value of y; another way of stating this point is that we have the functional form of Eðy j xÞ properly speciﬁed. Chapter 2 18", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 36, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p37::c0", "text": "If we only assume equation (2.18), then u can be correlated with nonlinear func- tions of x1 and x2, such as quadratics, interactions, and so on. If we hope to estimate the partial e¤ect of each xj on Eðy j xÞ over a broad range of values for x, we want Eðu j xÞ ¼ 0. [In Section 2.3 we discuss the weaker assumption (2.18) and its uses.] Example 2.2: Suppose that housing prices are determined by the simple model hprice ¼ b0 þ b1sqrft þ b2distance þ u; where sqrft is the square footage of the house and distance is distance of the house from a city incinerator. For b2 to represent qEðhprice j sqrft; distanceÞ=q distance, we must assume that Eðu j sqrft; distanceÞ ¼ 0. 2.2.4 Some Properties of Conditional Expectations One of the most useful tools for manipulating conditional expectations is the law of iterated expectations, which we mentioned previously. Here we cover the most gen- eral statement needed in this book. Suppose that w is a random vector and y is a random variable. Let x be a random vector that is some function of w, say x ¼ fðwÞ. (The vector x could simply be a subset of w.) This statement implies that if we know the outcome of w, then we know the outcome of x. The most general statement of the LIE that we will need is Eðy j xÞ ¼ E½Eðy j wÞ j x\u0002 ð2:19Þ In other words, if we write m1ðwÞ 1 Eðy j wÞ and m2ðxÞ 1 Eðy j xÞ, we can obtain m2ðxÞ by computing the expected value of m2ðwÞ given x: m1ðxÞ ¼ E½m1ðwÞ j x\u0002. There is another result that looks similar to equation (2.19) but is much simpler to verify. Namely, Eðy j xÞ ¼ E½Eðy j xÞ j w\u0002 ð2:20Þ Note how the positions of x and w have been switched on the right-hand side of equation (2.20) compared with equation (2.19). The result in equation (2.20) follows easily from the conditional aspect of the expection: since x is a function of w, know- ing w implies knowing x; given that m2ðxÞ ¼ Eðy j xÞ is a function of x, the expected value of m2ðxÞ given w is just m2ðxÞ. Some ﬁnd a phrase useful for remembering both equations (2.19) and (2.20): ‘‘The smaller information set always dominates.’’ Here, x represents less information than w, since knowing w implies knowing x, but not vice versa. We will use equations (2.19) and (2.20) almost routinely throughout the book. Conditional Expectations and Related Concepts in Econometrics 19", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 37, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p38::c0", "text": "For many purposes we need the following special case of the general LIE (2.19). If x and z are any random vectors, then Eðy j xÞ ¼ E½Eðy j x; zÞ j x\u0002 ð2:21Þ or, deﬁning m1ðx; zÞ 1 Eðy j x; zÞ and m2ðxÞ 1 Eðy j xÞ, m2ðxÞ ¼ E½m1ðx; zÞ j x\u0002 ð2:22Þ For many econometric applications, it is useful to think of m1ðx; zÞ ¼ Eðy j x; zÞ as a structural conditional expectation, but where z is unobserved. If interest lies in Eðy j x; zÞ, then we want the e¤ects of the xj holding the other elements of x and z ﬁxed. If z is not observed, we cannot estimate Eðy j x; zÞ directly. Nevertheless, since y and x are observed, we can generally estimate Eðy j xÞ. The question, then, is whether we can relate Eðy j xÞ to the original expectation of interest. (This is a ver- sion of the identiﬁcation problem in econometrics.) The LIE provides a convenient way for relating the two expectations. Obtaining E½m1ðx; zÞ j x\u0002 generally requires integrating (or summing) m1ðx; zÞ against the conditional density of z given x, but in many cases the form of Eðy j x; zÞ is simple enough not to require explicit integration. For example, suppose we begin with the model Eðy j x1; x2; zÞ ¼ b0 þ b1x1 þ b2x2 þ b3z ð2:23Þ but where z is unobserved. By the LIE, and the linearity of the CE operator, Eðy j x1; x2Þ ¼ Eðb0 þ b1x1 þ b2x2 þ b3z j x1; x2Þ ¼ b0 þ b1x1 þ b2x2 þ b3Eðz j x1; x2Þ ð2:24Þ Now, if we make an assumption about Eðz j x1; x2Þ, for example, that it is linear in x1 and x2, Eðz j x1; x2Þ ¼ d0 þ d1x1 þ d2x2 ð2:25Þ then we can plug this into equation (2.24) and rearrange: ¼ b0 þ b1x1 þ b2x2 þ b3ðd0 þ d1x1 þ d2x2Þ ¼ ðb0 þ b3d0Þ þ ðb1 þ b3d1Þx1 þ ðb2 þ b3d2Þx2 This last expression is Eðy j x1; x2Þ; given our assumptions it is necessarily linear in ðx1; x2Þ. Chapter 2 20", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 38, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p39::c0", "text": "Now suppose equation (2.23) contains an interaction in x1 and z: Eðy j x1; x2; zÞ ¼ b0 þ b1x1 þ b2x2 þ b3z þ b4x1z ð2:26Þ Then, again by the LIE, Eðy j x1; x2Þ ¼ b0 þ b1x1 þ b2x2 þ b3Eðz j x1; x2Þ þ b4x1Eðz j x1; x2Þ If Eðz j x1; x2Þ is again given in equation (2.25), you can show that Eðy j x1; x2Þ has terms linear in x1 and x2 and, in addition, contains x2 1 and x1x2. The usefulness of such derivations will become apparent in later chapters. The general form of the LIE has other useful implications. Suppose that for some (vector) function fðxÞ and a real-valued function gð\u0003Þ, Eðy j xÞ ¼ g½fðxÞ\u0002. Then E½y j fðxÞ\u0002 ¼ Eðy j xÞ ¼ g½fðxÞ\u0002 ð2:27Þ There is another way to state this relationship: If we deﬁne z 1 fðxÞ, then Eðy j zÞ ¼ gðzÞ. The vector z can have smaller or greater dimension than x. This fact is illus- trated with the following example. Example 2.3: If a wage equation is Eðwage j educ; experÞ ¼ b0 þ b1educ þ b2exper þ b3exper2 þ b4educ\u0003exper then Eðwage j educ; exper; exper2; educ\u0003experÞ ¼ b0 þ b1educ þ b2exper þ b3exper2 þ b4educ\u0003exper: In other words, once educ and exper have been conditioned on, it is redundant to condition on exper2 and educ\u0003exper. The conclusion in this example is much more general, and it is helpful for analyz- ing models of conditional expectations that are linear in parameters. Assume that, for some functions g1ðxÞ; g2ðxÞ; . . . ; gMðxÞ, Eðy j xÞ ¼ b0 þ b1g1ðxÞ þ b2g2ðxÞ þ \u0003 \u0003 \u0003 þ bMgMðxÞ ð2:28Þ This model allows substantial ﬂexibility, as the explanatory variables can appear in all kinds of nonlinear ways; the key restriction is that the model is linear in the bj. If we deﬁne z1 1 g1ðxÞ; . . . ; zM 1 gMðxÞ, then equation (2.27) implies that Eðy j z1; z2; . . . ; zMÞ ¼ b0 þ b1z1 þ b2z2 þ \u0003 \u0003 \u0003 þ bMzM ð2:29Þ Conditional Expectations and Related Concepts in Econometrics 21", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 39, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p40::c0", "text": "This equation shows that any conditional expectation linear in parameters can be written as a conditional expectation linear in parameters and linear in some conditioning variables. If we write equation (2.29) in error form as y ¼ b0 þ b1z1 þ b2z2 þ \u0003 \u0003 \u0003 þ bMzM þ u, then, because Eðu j xÞ ¼ 0 and the zj are functions of x, it follows that u is uncorrelated with z1; . . . ; zM (and any functions of them). As we will see in Chapter 4, this result allows us to cover models of the form (2.28) in the same framework as models linear in the original explanatory variables. We also need to know how the notion of statistical independence relates to condi- tional expectations. If u is a random variable independent of the random vector x, then Eðu j xÞ ¼ EðuÞ, so that if EðuÞ ¼ 0 and u and x are independent, then Eðu j xÞ ¼ 0. The converse of this is not true: Eðu j xÞ ¼ EðuÞ does not imply statistical inde- pendence between u and x ( just as zero correlation between u and x does not imply independence). 2.2.5 Average Partial E¤ects When we explicitly allow the expectation of the response variable, y, to depend on unobservables—usually called unobserved heterogeneity—we must be careful in specifying the partial e¤ects of interest. Suppose that we have in mind the (structural) conditional mean Eðy j x; qÞ ¼ m1ðx; qÞ, where x is a vector of observable explanatory variables and q is an unobserved random variable—the unobserved heterogeneity. (We take q to be a scalar for simplicity; the discussion for a vector is essentially the same.) For continuous xj, the partial e¤ect of immediate interest is yjðx; qÞ 1 qEðy j x; qÞ=qxj ¼ qm1ðx; qÞ=qxj ð2:30Þ (For discrete xj, we would simply look at di¤erences in the regression function for xj at two di¤erent values, when the other elements of x and q are held ﬁxed.) Because yjðx; qÞ generally depends on q, we cannot hope to estimate the partial e¤ects across many di¤erent values of q. In fact, even if we could estimate yjðx; qÞ for all x and q, we would generally have little guidance about inserting values of q into the mean function. In many cases we can make a normalization such as EðqÞ ¼ 0, and estimate yjðx; 0Þ, but q ¼ 0 typically corresponds to a very small segment of the population. (Technically, q ¼ 0 corresponds to no one in the population when q is continuously distributed.) Usually of more interest is the partial e¤ect averaged across the popu- lation distribution of q; this is called the average partial e¤ect (APE ). For emphasis, let xo denote a ﬁxed value of the covariates. The average partial e¤ect evaluated at xo is djðxoÞ 1 Eq½yjðxo; qÞ\u0002 ð2:31Þ Chapter 2 22", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 40, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p41::c0", "text": "where Eq½ \u0003 \u0002 denotes the expectation with respect to q. In other words, we simply average the partial e¤ect yjðxo; qÞ across the population distribution of q. Deﬁnition (2.31) holds for any population relationship between q and x; in particular, they need not be inde- pendent. But remember, in deﬁnition (2.31), xo is a nonrandom vector of numbers. For concreteness, assume that q has a continuous distribution with density func- tion gð\u0003Þ, so that djðxoÞ ¼ ð R yjðxo; qÞgðqÞ dq ð2:32Þ where q is simply the dummy argument in the integration. The question we answer here is, Is it possible to estimate djðxoÞ from conditional expectations that depend only on observable conditioning variables? Generally, the answer must be no, as q and x can be arbitrarily related. Nevertheless, if we appropriately restrict the rela- tionship between q and x, we can obtain a very useful equivalance. One common assumption in nonlinear models with unobserved heterogeneity is that q and x are independent. We will make the weaker assumption that q and x are independent conditional on a vector of observables, w: Dðq j x; wÞ ¼ Dðq j wÞ ð2:33Þ where Dð\u0003 j \u0003Þ denotes conditional distribution. (If we take w to be empty, we get the special case of independence between q and x.) In many cases, we can interpret equation (2.33) as implying that w is a vector of good proxy variables for q, but equation (2.33) turns out to be fairly widely applicable. We also assume that w is redundant or ignorable in the structural expectation Eðy j x; q; wÞ ¼ Eðy j x; qÞ ð2:34Þ As we will see in subsequent chapters, many econometric methods hinge on being able to exclude certain variables from the equation of interest, and equation (2.34) makes this assumption precise. Of course, if w is empty, then equation (2.34) is trivi- ally true. Under equations (2.33) and (2.34), we can show the following important result, provided that we can interchange a certain integral and partial derivative: djðxoÞ ¼ Ew½qEðy j xo; wÞ=qxj\u0002 ð2:35Þ where Ew½ \u0003 \u0002 denotes the expectation with respect to the distribution of w. Before we verify equation (2.35) for the special case of continuous, scalar q, we must understand its usefulness. The point is that the unobserved heterogeneity, q, has disappeared en- tirely, and the conditional expectation Eðy j x; wÞ can be estimated quite generally Conditional Expectations and Related Concepts in Econometrics 23", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 41, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p42::c0", "text": "because we assume that a random sample can be obtained on ðy; x; wÞ. [Alternatively, when we write down parametric econometric models, we will be able to derive Eðy j x; wÞ.] Then, estimating the average partial e¤ect at any chosen xo amounts to averaging q^m2ðxo; wiÞ=qxj across the random sample, where m2ðx; wÞ 1 Eðy j x; wÞ. Proving equation (2.35) is fairly simple. First, we have m2ðx; wÞ ¼ E½Eðy j x; q; wÞ j x; w\u0002 ¼ E½m1ðx; qÞ j x; w\u0002 ¼ ð R m1ðx; qÞgðq j wÞ dq where the ﬁrst equality follows from the law of iterated expectations, the second equality follows from equation (2.34), and the third equality follows from equation (2.33). If we now take the partial derivative with respect to xj of the equality m2ðx; wÞ ¼ ð R m1ðx; qÞgðq j wÞ dq ð2:36Þ and interchange the partial derivative and the integral, we have, for any ðx; wÞ, qm2ðx; wÞ=qxj ¼ ð R yjðx; qÞgðq j wÞ dq ð2:37Þ For ﬁxed xo, the right-hand side of equation (2.37) is simply E½yjðxo; qÞ j w\u0002, and so another application of iterated expectations gives, for any xo, Ew½qm2ðxo; wÞ=qxj\u0002 ¼ EfE½yjðxo; qÞ j w\u0002g ¼ djðxoÞ which is what we wanted to show. As mentioned previously, equation (2.35) has many applications in models where unobserved heterogeneity enters a conditional mean function in a nonadditive fash- ion. We will use this result (in simpliﬁed form) in Chapter 4, and also extensively in Part III. The special case where q is independent of x—and so we do not need the proxy variables w—is very simple: the APE of xj on Eðy j x; qÞ is simply the partial e¤ect of xj on m2ðxÞ ¼ Eðy j xÞ. In other words, if we focus on average partial e¤ects, there is no need to introduce heterogeneity. If we do specify a model with heteroge- neity independent of x, then we simply ﬁnd Eðy j xÞ by integrating Eðy j x; qÞ over the distribution of q. 2.3 Linear Projections In the previous section we saw some examples of how to manipulate conditional expectations. While structural equations are usually stated in terms of CEs, making Chapter 2 24", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 42, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p43::c0", "text": "linearity assumptions about CEs involving unobservables or auxiliary variables is undesirable, especially if such assumptions can be easily relaxed. By using the notion of a linear projection we can often relax linearity assumptions in auxiliary conditional expectations. Typically this is done by ﬁrst writing down a structural model in terms of a CE and then using the linear projection to obtain an estimable equation. As we will see in Chapters 4 and 5, this approach has many applications. Generally, let y; x1; . . . ; xK be random variables representing some population such that Eðy2Þ < y, Eðx2 j Þ < y, j ¼ 1; 2; . . . ; K. These assumptions place no practical restrictions on the joint distribution of ðy; x1; x2; . . . ; xKÞ: the vector can contain dis- crete and continuous variables, as well as variables that have both characteristics. In many cases y and the xj are nonlinear functions of some underlying variables that are initially of interest. Deﬁne x 1 ðx1; . . . ; xKÞ as a 1 \u0001 K vector, and make the assumption that the K \u0001 K variance matrix of x is nonsingular (positive deﬁnite). Then the linear projec- tion of y on 1; x1; x2; . . . ; xK always exists and is unique: Lðy j 1; x1; . . . xKÞ ¼ Lðy j 1; xÞ ¼ b0 þ b1x1 þ \u0003 \u0003 \u0003 þ bKxK ¼ b0 þ xb ð2:38Þ where, by deﬁnition, b 1 ½VarðxÞ\u0002\u00041 Covðx; yÞ ð2:39Þ b0 1 EðyÞ \u0004 EðxÞb ¼ EðyÞ \u0004 b1Eðx1Þ \u0004 \u0003 \u0003 \u0003 \u0004 bKEðxKÞ ð2:40Þ The matrix VarðxÞ is the K \u0001 K symmetric matrix with ð j; kÞth element given by Covðxj; xkÞ, while Covðx; yÞ is the K \u0001 1 vector with jth element Covðxj; yÞ. When K ¼ 1 we have the familiar results b1 1 Covðx1; yÞ=Varðx1Þ and b0 1 EðyÞ \u0004 b1Eðx1Þ. As its name suggests, Lðy j 1; x1; x2; . . . ; xKÞ is always a linear function of the xj. Other authors use a di¤erent notation for linear projections, the most common being E\u0005ð\u0003 j \u0003Þ and Pð\u0003 j \u0003Þ. [For example, Chamberlain (1984) and Goldberger (1991) use E\u0005ð\u0003 j \u0003Þ.] Some authors omit the 1 in the deﬁnition of a linear projection because it is assumed that an intercept is always included. Although this is usually the case, we put unity in explicitly to distinguish equation (2.38) from the case that a zero in- tercept is intended. The linear projection of y on x1; x2; . . . ; xK is deﬁned as Lðy j xÞ ¼ Lðy j x1; x2; . . . ; xKÞ ¼ g1x1 þ g2x2 þ \u0003 \u0003 \u0003 þ gKxK ¼ xg where g 1 ðEðx0xÞÞ\u00041Eðx0yÞ. Note that g 0 b unless EðxÞ ¼ 0. Later, we will include unity as an element of x, in which case the linear projection including an intercept can be written as Lðy j xÞ. Conditional Expectations and Related Concepts in Econometrics 25", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 43, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p44::c0", "text": "The linear projection is just another way of writing down a population linear model where the disturbance has certain properties. Given the linear projection in equation (2.38) we can always write y ¼ b0 þ b1x1 þ \u0003 \u0003 \u0003 þ bKxK þ u ð2:41Þ where the error term u has the following properties (by deﬁnition of a linear projec- tion): Eðu2Þ < y and EðuÞ ¼ 0; Covðxj; uÞ ¼ 0; j ¼ 1; 2; . . . ; K ð2:42Þ In other words, u has zero mean and is uncorrelated with every xj. Conversely, given equations (2.41) and (2.42), the parameters bj in equation (2.41) must be the param- eters in the linear projection of y on 1; x1; . . . ; xK given by deﬁnitions (2.39) and (2.40). Sometimes we will write a linear projection in error form, as in equations (2.41) and (2.42), but other times the notation (2.38) is more convenient. It is important to emphasize that when equation (2.41) represents the linear pro- jection, all we can say about u is contained in equation (2.42). In particular, it is not generally true that u is independent of x or that Eðu j xÞ ¼ 0. Here is another way of saying the same thing: equations (2.41) and (2.42) are deﬁnitional. Equation (2.41) under Eðu j xÞ ¼ 0 is an assumption that the conditional expectation is linear. The linear projection is sometimes called the minimum mean square linear predictor or the least squares linear predictor because b0 and b can be shown to solve the fol- lowing problem: min b0;b A RK E½ðy \u0004 b0 \u0004 xbÞ2\u0002 ð2:43Þ (see Property LP.6 in the appendix). Because the CE is the minimum mean square predictor—that is, it gives the smallest mean square error out of all (allowable) functions (see Property CE.8)—it follows immediately that if Eðy j xÞ is linear in x then the linear projection coincides with the conditional expectation. As with the conditional expectation operator, the linear projection operator sat- isﬁes some important iteration properties. For vectors x and z, Lðy j 1; xÞ ¼ L½Lðy j 1; x; zÞ j 1; x\u0002 ð2:44Þ This simple fact can be used to derive omitted variables bias in a general setting as well as proving properties of estimation methods such as two-stage least squares and certain panel data methods. Another iteration property that is useful involves taking the linear projection of a conditional expectation: Chapter 2 26", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 44, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p45::c0", "text": "Lðy j 1; xÞ ¼ L½Eðy j x; zÞ j 1; x\u0002 ð2:45Þ Often we specify a structural model in terms of a conditional expectation Eðy j x; zÞ (which is frequently linear), but, for a variety of reasons, the estimating equations are based on the linear projection Lðy j 1; xÞ. If Eðy j x; zÞ is linear in x and z, then equations (2.45) and (2.44) say the same thing. For example, assume that Eðy j x1; x2Þ ¼ b0 þ b1x1 þ b2x2 þ b3x1x2 and deﬁne z1 1 x1x2. Then, from Property CE.3, Eðy j x1; x2; z1Þ ¼ b0 þ b1x1 þ b2x2 þ b3z1 ð2:46Þ The right-hand side of equation (2.46) is also the linear projection of y on 1; x1; x2, and z1; it is not generally the linear projection of y on 1; x1; x2. Our primary use of linear projections will be to obtain estimable equations involving the parameters of an underlying conditional expectation of interest. Prob- lems 2.2 and 2.3 show how the linear projection can have an interesting interpreta- tion in terms of the structural parameters. Problems 2.1. Given random variables y, x1, and x2, consider the model Eðy j x1; x2Þ ¼ b0 þ b1x1 þ b2x2 þ b3x2 2 þ b4x1x2 a. Find the partial e¤ects of x1 and x2 on Eðy j x1; x2Þ. b. Writing the equation as y ¼ b0 þ b1x1 þ b2x2 þ b3x2 2 þ b4x1x2 þ u what can be said about Eðu j x1; x2Þ? What about Eðu j x1; x2; x2 2; x1x2Þ? c. In the equation of part b, what can be said about Varðu j x1; x2Þ? 2.2. Let y and x be scalars such that Eðy j xÞ ¼ d0 þ d1ðx \u0004 mÞ þ d2ðx \u0004 mÞ2 where m ¼ EðxÞ. a. Find qEðy j xÞ=qx, and comment on how it depends on x. b. Show that d1 is equal to qEðy j xÞ=qx averaged across the distribution of x. Conditional Expectations and Related Concepts in Econometrics 27", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 45, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p46::c0", "text": "c. Suppose that x has a symmetric distribution, so that E½ðx \u0004 mÞ3\u0002 ¼ 0. Show that Lðy j 1; xÞ ¼ a0 þ d1x for some a0. Therefore, the coe‰cient on x in the linear pro- jection of y on ð1; xÞ measures something useful in the nonlinear model for Eðy j xÞ: it is the partial e¤ect qEðy j xÞ=qx averaged across the distribution of x. 2.3. Suppose that Eðy j x1; x2Þ ¼ b0 þ b1x1 þ b2x2 þ b3x1x2 ð2:47Þ a. Write this expectation in error form (call the error u), and describe the properties of u. b. Suppose that x1 and x2 have zero means. Show that b1 is the expected value of qEðy j x1; x2Þ=qx1 (where the expectation is across the population distribution of x2). Provide a similar interpretation for b2. c. Now add the assumption that x1 and x2 are independent of one another. Show that the linear projection of y on ð1; x1; x2Þ is Lðy j 1; x1; x2Þ ¼ b0 þ b1x1 þ b2x2 ð2:48Þ (Hint: Show that, under the assumptions on x1 and x2, x1x2 has zero mean and is uncorrelated with x1 and x2.) d. Why is equation (2.47) generally more useful than equation (2.48)? 2.4. For random scalars u and v and a random vector x, suppose that Eðu j x; vÞ is a linear function of ðx; vÞ and that u and v each have zero mean and are uncorrelated with the elements of x. Show that Eðu j x; vÞ ¼ Eðu j vÞ ¼ r1v for some r1. 2.5. Consider the two representations y ¼ m1ðx; zÞ þ u1; Eðu1 j x; zÞ ¼ 0 y ¼ m2ðxÞ þ u2; Eðu2 j xÞ ¼ 0 Assuming that Varðy j x; zÞ and Varðy j xÞ are both constant, what can you say about the relationship between Varðu1Þ and Varðu2Þ? (Hint: Use Property CV.4 in the appendix.) 2.6. Let x be a 1 \u0001 K random vector, and let q be a random scalar. Suppose that q can be expressed as q ¼ q\u0005 þ e, where EðeÞ ¼ 0 and Eðx0eÞ ¼ 0. Write the linear projection of q\u0005 onto ð1; xÞ as q\u0005 ¼ d0 þ d1x1 þ \u0003 \u0003 \u0003 þ dKxK þ r\u0005, where Eðr\u0005Þ ¼ 0 and Eðx0r\u0005Þ ¼ 0. Chapter 2 28", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 46, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p47::c0", "text": "a. Show that Lðq j 1; xÞ ¼ d0 þ d1x1 þ \u0003 \u0003 \u0003 þ dKxK b. Find the projection error r 1 q \u0004 Lðq j 1; xÞ in terms of r\u0005 and e. 2.7. Consider the conditional expectation Eðy j x; zÞ ¼ gðxÞ þ zb where gð\u0003Þ is a general function of x and b is a 1 \u0001 M vector. Show that Eð~y j~zÞ ¼ ~zb where ~y 1 y \u0004 Eðy j xÞ and ~z 1 z \u0004 Eðz j xÞ. Appendix 2A 2.A.1 Properties of Conditional Expectations property CE.1: Let a1ðxÞ; . . . ; aGðxÞ and bðxÞ be scalar functions of x, and let y1; . . . ; yG be random scalars. Then E X G j¼1 ajðxÞyj þ bðxÞ j x ! ¼ X G j¼1 ajðxÞEðyj j xÞ þ bðxÞ provided that EðjyjjÞ < y, E½jajðxÞyjj\u0002 < y, and E½jbðxÞj\u0002 < y. This is the sense in which the conditional expectation is a linear operator. property CE.2: EðyÞ ¼ E½Eðy j xÞ\u0002 1 E½mðxÞ\u0002. Property CE.2 is the simplest version of the law of iterated expectations. As an illustration, suppose that x is a discrete random vector taking on values c1; c2; . . . ; cM with probabilities p1; p2; . . . ; pM. Then the LIE says EðyÞ ¼ p1Eðy j x ¼ c1Þ þ p2Eðy j x ¼ c2Þ þ \u0003 \u0003 \u0003 þ pMEðy j x ¼ cMÞ ð2:49Þ In other words, EðyÞ is simply a weighted average of the Eðy j x ¼ cjÞ, where the weight pj is the probability that x takes on the value cj. property CE.3: (1) Eðy j xÞ ¼ E½Eðy j wÞ j x\u0002, where x and w are vectors with x ¼ fðwÞ for some nonstochastic function fð\u0003Þ. (This is the general version of the law of iterated expectations.) (2) As a special case of part 1, Eðy j xÞ ¼ E½Eðy j x; zÞ j x\u0002 for vectors x and z. Conditional Expectations and Related Concepts in Econometrics 29", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 47, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p48::c0", "text": "property CE.4: If fðxÞ A RJ is a function of x such that Eðy j xÞ ¼ g½fðxÞ\u0002 for some scalar function gð\u0003Þ, then E½y j fðxÞ\u0002 ¼ Eðy j xÞ. property CE.5: If the vector ðu; vÞ is independent of the vector x, then Eðu j x; vÞ ¼ Eðu j vÞ. property CE.6: If u 1 y \u0004 Eðy j xÞ, then E½gðxÞu\u0002 ¼ 0 for any function gðxÞ, pro- vided that E½jgjðxÞuj\u0002 < y, j ¼ 1; . . . ; J, and EðjujÞ < y. In particular, EðuÞ ¼ 0 and Covðxj; uÞ ¼ 0, j ¼ 1; . . . ; K. Proof: First, note that Eðu j xÞ ¼ E½ðy \u0004 Eðy j xÞÞ j x\u0002 ¼ E½ðy \u0004 mðxÞÞ j x\u0002 ¼ Eðy j xÞ \u0004 mðxÞ ¼ 0 Next, by property CE.2, E½gðxÞu\u0002 ¼ EðE½gðxÞu j x\u0002Þ ¼ E½gðxÞEðu j xÞ\u0002 (by property CE.1) ¼ 0 because Eðu j xÞ ¼ 0. property CE.7 (Conditional Jensen’s Inequality): If c: R ! R is a convex function deﬁned on R and E½jyj\u0002 < y, then c½Eðy j xÞ\u0002 a E½cðyÞ j x\u0002 Technically, we should add the statement ‘‘almost surely-Px,’’ which means that the inequality holds for all x in a set that has probability equal to one. As a special case, ½EðyÞ\u00022 a Eðy2Þ. Also, if y > 0, then \u0004log½EðyÞ\u0002 a E½\u0004logðyÞ\u0002, or E½logðyÞ\u0002 a log½EðyÞ\u0002. property CE.8: If Eðy2Þ < y and mðxÞ 1 Eðy j xÞ, then m is a solution to min m A M E½ðy \u0004 mðxÞÞ2\u0002 where M is the set of functions m: RK ! R such that E½mðxÞ2\u0002 < y. In other words, mðxÞ is the best mean square predictor of y based on information contained in x. Proof: By the conditional Jensen’s inequality, if follows that Eðy2Þ < y implies E½mðxÞ2\u0002 < y, so that m A M. Next, for any m A M, write E½ðy \u0004 mðxÞÞ2\u0002 ¼ E½fðy \u0004 mðxÞÞ þ ðmðxÞ \u0004 mðxÞÞg2\u0002 ¼ E½ðy \u0004 mðxÞÞ2\u0002 þ E½ðmðxÞ \u0004 mðxÞÞ2\u0002 þ 2E½ðmðxÞ \u0004 mðxÞÞu\u0002 where u 1 y \u0004 mðxÞ. Thus, by CE.6, E½ðy \u0004 mðxÞÞ2\u0002 ¼ Eðu2Þ þ E½ðmðxÞ \u0004 mðxÞÞ2\u0002: The right-hand side is clearly minimized at m 1 m. Chapter 2 30", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 48, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p49::c0", "text": "2.A.2 Properties of Conditional Variances The conditional variance of y given x is deﬁned as Varðy j xÞ 1 s2ðxÞ 1 E½fy \u0004 Eðy j xÞg2 j x\u0002 ¼ Eðy2 j xÞ \u0004 ½Eðy j xÞ\u00022 The last representation is often useful for computing Varðy j xÞ. As with the con- ditional expectation, s2ðxÞ is a random variable when x is viewed as a random vector. property CV.1: Var½aðxÞy þ bðxÞ j x\u0002 ¼ ½aðxÞ\u00022 Varðy j xÞ. property CV.2: VarðyÞ ¼ E½Varðy j xÞ\u0002 þ Var½Eðy j xÞ\u0002 ¼ E½s2ðxÞ\u0002 þ Var½mðxÞ\u0002. Proof: VarðyÞ 1 E½ðy \u0004 EðyÞÞ2\u0002 ¼ E½ðy \u0004 Eðy j xÞ þ Eðy j xÞ þ EðyÞÞ2\u0002 ¼ E½ðy \u0004 Eðy j xÞÞ2\u0002 þ E½ðEðy j xÞ \u0004 EðyÞÞ2\u0002 þ 2E½ðy \u0004 Eðy j xÞÞðEðy j xÞ \u0004 EðyÞÞ\u0002 By CE.6, E½ðy \u0004 Eðy j xÞÞðEðy j xÞ \u0004 EðyÞÞ\u0002 ¼ 0; so VarðyÞ ¼ E½ðy \u0004 Eðy j xÞÞ2\u0002 þ E½ðEðy j xÞ \u0004 EðyÞÞ2\u0002 ¼ EfE½ðy \u0004 Eðy j xÞÞ2 j x\u0002g þ E½ðEðy j xÞ \u0004 E½Eðy j xÞ\u0002Þ2 by the law of iterated expectations 1 E½Varðy j xÞ\u0002 þ Var½Eðy j xÞ\u0002 An extension of Property CV.2 is often useful, and its proof is similar: property CV.3: Varðy j xÞ ¼ E½Varðy j x; zÞ j x\u0002 þ Var½Eðy j x; zÞ j x\u0002. Consequently, by the law of iterated expectations CE.2, property CV.4: E½Varðy j xÞ\u0002 b E½Varðy j x; zÞ\u0002. For any function mð\u0003Þ deﬁne the mean squared error as MSEðy; mÞ 1 E½ðy \u0004 mðxÞÞ2\u0002. Then CV.4 can be loosely stated as MSE½y; Eðy j xÞ\u0002 b MSE½y; Eðy j x; zÞ\u0002. In other words, in the population one never does worse for predicting y when additional vari- ables are conditioned on. In particular, if Varðy j xÞ and Varðy j x; zÞ are both con- stant, then Varðy j xÞ b Varðy j x; zÞ. Conditional Expectations and Related Concepts in Econometrics 31", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 49, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p50::c0", "text": "2.A.3 Properties of Linear Projections In what follows, y is a scalar, x is a 1 \u0001 K vector, and z is a 1 \u0001 J vector. We allow the ﬁrst element of x to be unity, although the following properties hold in either case. All of the variables are assumed to have ﬁnite second moments, and the ap- propriate variance matrices are assumed to be nonsingular. property LP.1: If Eðy j xÞ ¼ xb, then Lðy j xÞ ¼ xb. More generally, if Eðy j xÞ ¼ b1g1ðxÞ þ b2g2ðxÞ þ \u0003 \u0003 \u0003 þ bMgMðxÞ then Lðy j w1; . . . ; wMÞ ¼ b1w1 þ b2w2 þ \u0003 \u0003 \u0003 þ bMwM where wj 1 gjðxÞ, j ¼ 1; 2; . . . ; M. This property tells us that, if Eðy j xÞ is known to be linear in some functions gjðxÞ, then this linear function also represents a linear projection. property LP.2: Deﬁne u 1 y \u0004 Lðy j xÞ ¼ y \u0004 xb. Then Eðx0uÞ ¼ 0. property LP.3: Suppose yj, j ¼ 1; 2; . . . ; G are each random scalars, and a1; . . . ; aG are constants. Then L X G j¼1 ajyj j x ! ¼ X G j¼1 ajLðyj j xÞ Thus, the linear projection is a linear operator. property LP.4 (Law of Iterated Projections): Lðy j xÞ ¼ L½Lðy j x; zÞ j x\u0002. More precisely, let Lðy j x; zÞ 1 xb þ zg and Lðy j xÞ ¼ xd For each element of z, write Lðzj j xÞ ¼ xpj, j ¼ 1; . . . ; J, where pj is K \u0001 1. Then Lðz j xÞ ¼ xP where P is the K \u0001 J matrix P 1 ðp1; p2; . . . ; pJÞ. Property LP.4 implies that Lðy j xÞ ¼ Lðxb þ zg j xÞ ¼ Lðx j xÞb þ Lðz j xÞg ðby LP:3Þ ¼ xb þ ðxPÞg ¼ xðb þ PgÞ ð2:50Þ Thus, we have shown that d ¼ b þ Pg. This is, in fact, the population analogue of the omitted variables bias formula from standard regression theory, something we will use in Chapter 4. Chapter 2 32", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 50, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p51::c0", "text": "Another iteration property involves the linear projection and the conditional expectation: property LP.5: Lðy j xÞ ¼ L½Eðy j x; zÞ j x\u0002. Proof: Write y ¼ mðx; zÞ þ u, where mðx; zÞ ¼ Eðy j x; zÞ. But Eðu j x; zÞ ¼ 0; so Eðx0uÞ ¼ 0, which implies by LP.3 that Lðy j xÞ ¼ L½mðx; zÞ j x\u0002 þ Lðu j xÞ ¼ L½mðx; zÞ j x\u0002 ¼ L½Eðy j x; zÞ j x\u0002. A useful special case of Property LP.5 occurs when z is empty. Then Lðy j xÞ ¼ L½Eðy j xÞ j x\u0002. property LP.6: b is a solution to min b A RK E½ðy \u0004 xbÞ2\u0002 ð2:51Þ If Eðx0xÞ is positive deﬁnite, then b is the unique solution to this problem. Proof: For any b, write y \u0004 xb ¼ ðy \u0004 xbÞ þ ðxb \u0004 xbÞ. Then ðy \u0004 xbÞ2 ¼ ðy \u0004 xbÞ2 þ ðxb \u0004 xbÞ2 þ 2ðxb \u0004 xbÞðy \u0004 xbÞ ¼ ðy \u0004 xbÞ2 þ ðb \u0004 bÞ0x0xðb \u0004 bÞ þ 2ðb \u0004 bÞ0x0ðy \u0004 xbÞ Therefore, E½ðy \u0004 xbÞ2\u0002 ¼ E½ðy \u0004 xbÞ2\u0002 þ ðb \u0004 bÞ0Eðx0xÞðb \u0004 bÞ þ 2ðb \u0004 bÞ0E½x0ðy \u0004 xbÞ\u0002 ¼ E½ðy \u0004 xbÞ2\u0002 þ ðb \u0004 bÞ0Eðx0xÞðb \u0004 bÞ ð2:52Þ because E½x0ðy \u0004 xbÞ\u0002 ¼ 0 by LP.2. When b ¼ b, the right-hand side of equation (2.52) is minimized. Further, if Eðx0xÞ is positive deﬁnite, ðb \u0004 bÞ0Eðx0xÞðb \u0004 bÞ > 0 if b 0 b; so in this case b is the unique minimizer. Property LP.6 states that the linear projection is the minimum mean square linear predictor. It is not necessarily the minimum mean square predictor: if Eðy j xÞ ¼ mðxÞ is not linear in x, then E½ðy \u0004 mðxÞÞ2\u0002 < E½ðy \u0004 xbÞ2\u0002 ð2:53Þ property LP.7: This is a partitioned projection formula, which is useful in a variety of circumstances. Write Lðy j x; zÞ ¼ xb þ zg ð2:54Þ Conditional Expectations and Related Concepts in Econometrics 33", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 51, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p52::c0", "text": "Deﬁne the 1 \u0001 K vector of population residuals from the projection of x on z as r 1 x \u0004 Lðx j zÞ. Further, deﬁne the population residual from the projection of y on z as v 1 y \u0004 Lðy j zÞ. Then the following are true: Lðv j rÞ ¼ rb ð2:55Þ and Lðy j rÞ ¼ rb ð2:56Þ The point is that the b in equations (2.55) and (2.56) is the same as that appearing in equation (2.54). Another way of stating this result is b ¼ ½Eðr0rÞ\u0002\u00041Eðr0vÞ ¼ ½Eðr0rÞ\u0002\u00041Eðr0yÞ: ð2:57Þ Proof: From equation (2.54) write y ¼ xb þ zg þ u; Eðx0uÞ ¼ 0; Eðz0uÞ ¼ 0 ð2:58Þ Taking the linear projection gives Lðy j zÞ ¼ Lðx j zÞb þ zg ð2:59Þ Subtracting equation (2.59) from (2.58) gives y \u0004 Lðy j zÞ ¼ ½x \u0004 Lðx j zÞ\u0002b þ u, or v ¼ rb þ u ð2:60Þ Since r is a linear combination of ðx; zÞ, Eðr0uÞ ¼ 0. Multiplying equation (2.60) through by r0 and taking expectations, it follows that b ¼ ½Eðr0rÞ\u0002\u00041Eðr0vÞ [We assume that Eðr0rÞ is nonsingular.] Finally, Eðr0vÞ ¼ E½r0ðy \u0004 Lðy j zÞÞ\u0002 ¼ Eðr0yÞ, since Lðy j zÞ is linear in z and r is orthogonal to any linear function of z. Chapter 2 34", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 52, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p53::c0", "text": "3 Basic Asymptotic Theory This chapter summarizes some deﬁnitions and limit theorems that are important for studying large-sample theory. Most claims are stated without proof, as several re- quire tedious epsilon-delta arguments. We do prove some results that build on fun- damental deﬁnitions and theorems. A good, general reference for background in asymptotic analysis is White (1984). In Chapter 12 we introduce further asymptotic methods that are required for studying nonlinear models. 3.1 Convergence of Deterministic Sequences Asymptotic analysis is concerned with the various kinds of convergence of sequences of estimators as the sample size grows. We begin with some deﬁnitions regarding nonstochastic sequences of numbers. When we apply these results in econometrics, N is the sample size, and it runs through all positive integers. You are assumed to have some familiarity with the notion of a limit of a sequence. definition 3.1: (1) A sequence of nonrandom numbers faN: N ¼ 1; 2; . . .g con- verges to a (has limit a) if for all e > 0, there exists Ne such that if N > Ne then jaN \u0001 aj < e. We write aN ! a as N ! y. (2) A sequence faN: N ¼ 1; 2; . . .g is bounded if and only if there is some b < y such that jaNj a b for all N ¼ 1; 2; . . . : Otherwise, we say that faNg is unbounded. These deﬁnitions apply to vectors and matrices element by element. Example 3.1: (1) If aN ¼ 2 þ 1=N, then aN ! 2. (2) If aN ¼ ð\u00011ÞN, then aN does not have a limit, but it is bounded. (3) If aN ¼ N 1=4, aN is not bounded. Because aN increases without bound, we write aN ! y. definition 3.2: (1) A sequence faNg is OðN lÞ (at most of order N l) if N\u0001laN is bounded. When l ¼ 0, faNg is bounded, and we also write aN ¼ Oð1Þ (big oh one). (2) faNg is oðN lÞ if N\u0001laN ! 0. When l ¼ 0, aN converges to zero, and we also write aN ¼ oð1Þ (little oh one). From the deﬁnitions, it is clear that if aN ¼ oðN lÞ, then aN ¼ OðN lÞ; in particular, if aN ¼ oð1Þ, then aN ¼ Oð1Þ. If each element of a sequence of vectors or matrices is OðN lÞ, we say the sequence of vectors or matrices is OðN lÞ, and similarly for oðN lÞ. Example 3.2: (1) If aN ¼ logðNÞ, then aN ¼ oðN lÞ for any l > 0. (2) If aN ¼ 10 þ ﬃﬃﬃﬃ N p , then aN ¼ OðN 1=2Þ and aN ¼ oðNð1=2þgÞÞ for any g > 0.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 53, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p54::c0", "text": "3.2 Convergence in Probability and Bounded in Probability definition 3.3: (1) A sequence of random variables fxN: N ¼ 1; 2; . . .g converges in probability to the constant a if for all e > 0, P½jxN \u0001 aj > e\u0002 ! 0 as N ! y We write xN ! p a and say that a is the probability limit (plim) of xN: plim xN ¼ a. (2) In the special case where a ¼ 0, we also say that fxNg is opð1Þ (little oh p one). We also write xN ¼ opð1Þ or xN ! p 0. (3) A sequence of random variables fxNg is bounded in probability if and only if for every e > 0, there exists a be < y and an integer Ne such that P½jxNj b be\u0002 < e for all N b Ne We write xN ¼ Opð1Þ (fxNg is big oh p one). If cN is a nonrandom sequence, then cN ¼ Opð1Þ if and only if cN ¼ Oð1Þ; cN ¼ opð1Þ if and only if cN ¼ oð1Þ. A simple, and very useful, fact is that if a sequence converges in probability to any real number, then it is bounded in probability. lemma 3.1: If xN ! p a, then xN ¼ Opð1Þ. This lemma also holds for vectors and matrices. The proof of Lemma 3.1 is not di‰cult; see Problem 3.1. definition 3.4: (1) A random sequence fxN: N ¼ 1; 2; . . .g is opðaNÞ, where faNg is a nonrandom, positive sequence, if xN=aN ¼ opð1Þ. We write xN ¼ opðaNÞ. (2) A random sequence fxN: N ¼ 1; 2; . . .g is OpðaNÞ, where faNg is a non- random, positive sequence, if xN=aN ¼ Opð1Þ. We write xN ¼ OpðaNÞ. We could have started by deﬁning a sequence fxNg to be opðN dÞ for d A R if N\u0001dxN ! p 0, in which case we obtain the deﬁnition of opð1Þ when d ¼ 0. This is where the one in opð1Þ comes from. A similar remark holds for Opð1Þ. Example 3.3: If z is a random variable, then xN 1 ﬃﬃﬃﬃ N p z is OpðN 1=2Þ and xN ¼ opðN dÞ for any d > 1 2. lemma 3.2: If wN ¼ opð1Þ, xN ¼ opð1Þ, yN ¼ Opð1Þ, and zN ¼ Opð1Þ, then (1) wN þ xN ¼ opð1Þ; (2) yN þ zN ¼ Opð1Þ; (3) yNzN ¼ Opð1Þ; and (4) xNzN ¼ opð1Þ. In derivations, we will write relationships 1 to 4 as opð1Þ þ opð1Þ ¼ opð1Þ, Opð1Þ þ Opð1Þ ¼ Opð1Þ, Opð1Þ \u0003 Opð1Þ ¼ Opð1Þ, and opð1Þ \u0003 Opð1Þ ¼ opð1Þ, respectively. Be- Chapter 3 36", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 54, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p55::c0", "text": "cause a opð1Þ sequence is Opð1Þ, Lemma 3.2 also implies that opð1Þ þ Opð1Þ ¼ Opð1Þ and opð1Þ \u0003 opð1Þ ¼ opð1Þ. All of the previous deﬁnitions apply element by element to sequences of random vectors or matrices. For example, if fxNg is a sequence of random K \u0004 1 random vectors, xN ! p a, where a is a K \u0004 1 nonrandom vector, if and only if xNj ! p aj, j ¼ 1; . . . ; K. This is equivalent to kxN \u0001 ak ! p 0, where kbk 1 ðb0bÞ1=2 denotes the Euclidean length of the K \u0004 1 vector b. Also, ZN ! p B, where ZN and B are M \u0004 K, is equivalent to kZN \u0001 Bk ! p 0, where kAk 1 ½trðA0AÞ\u00021=2 and trðCÞ denotes the trace of the square matrix C. A result that we often use for studying the large-sample properties of estimators for linear models is the following. It is easily proven by repeated application of Lemma 3.2 (see Problem 3.2). lemma 3.3: Let fZN: N ¼ 1; 2; . . .g be a sequence of J \u0004 K matrices such that ZN ¼ opð1Þ, and let fxNg be a sequence of J \u0004 1 random vectors such that xN ¼ Opð1Þ. Then Z0 NxN ¼ opð1Þ. The next lemma is known as Slutsky’s theorem. lemma 3.4: Let g: RK ! RJ be a function continuous at some point c A RK. Let fxN: N ¼ 1; 2; . . .g be sequence of K \u0004 1 random vectors such that xN ! p c. Then gðxNÞ ! p gðcÞ as N ! y. In other words, plim gðxNÞ ¼ gðplim xNÞ ð3:1Þ if gð\u0003Þ is continuous at plim xN. Slutsky’s theorem is perhaps the most useful feature of the plim operator: it shows that the plim passes through nonlinear functions, provided they are continuous. The expectations operator does not have this feature, and this lack makes ﬁnite sample analysis di‰cult for many estimators. Lemma 3.4 shows that plims behave just like regular limits when applying a continuous function to the sequence. definition 3.5: Let ðW; F; PÞ be a probability space. A sequence of events fWN: N ¼ 1; 2; . . .g H F is said to occur with probability approaching one (w.p.a.1) if and only if PðWNÞ ! 1 as N ! y. Deﬁnition 3.5 allows that Wc N, the complement of WN, can occur for each N, but its chance of occuring goes to zero as N ! y. corollary 3.1: Let fZN: N ¼ 1; 2; . . .g be a sequence of random K \u0004 K matrices, and let A be a nonrandom, invertible K \u0004 K matrix. If ZN ! p A then Basic Asymptotic Theory 37", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 55, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p56::c0", "text": "(1) Z\u00011 N exists w.p.a.1; (2) Z\u00011 N ! p A\u00011 or plim Z\u00011 N ¼ A\u00011 (in an appropriate sense). Proof: Because the determinant is a continuous function on the space of all square matrices, detðZNÞ ! p detðAÞ. Because A is nonsingular, detðAÞ 0 0. Therefore, it follows that P½detðZNÞ 0 0\u0002 ! 1 as N ! y. This completes the proof of part 1. Part 2 requires a convention about how to deﬁne Z\u00011 N when ZN is nonsingular. Let WN be the set of o (outcomes) such that ZNðoÞ is nonsingular for o A WN; we just showed that PðWNÞ ! 1 as N ! y. Deﬁne a new sequence of matrices by ~ZNðoÞ 1 ZNðoÞ when o A WN; ~ZNðoÞ 1 IK when o B WN Then Pð ~ZN ¼ ZNÞ ¼ PðWNÞ ! 1 as N ! y. Then, because ZN ! p A, ~ZN ! p A. The inverse operator is continuous on the space of invertible matrices, so ~Z\u00011 N ! p A\u00011. This is what we mean by Z\u00011 N ! p A\u00011; the fact that ZN can be singular with vanishing probability does not a¤ect asymptotic analysis. 3.3 Convergence in Distribution definition 3.6: A sequence of random variables fxN: N ¼ 1; 2; . . .g converges in distribution to the continuous random variable x if and only if FNðxÞ ! FðxÞ as N ! y for all x A R where FN is the cumulative distribution function (c.d.f.) of xN and F is the (continu- ous) c.d.f. of x. We write xN ! d x. When x @ Normalðm; s2Þ we write xN ! d Normalðm; s2Þ or xN @ a Normalðm; s2Þ (xN is asymptotically normal). In Deﬁnition 3.6, xN is not required to be continuous for any N. A good example of where xN is discrete for all N but has an asymptotically normal distribution is the Demoivre-Laplace theorem (a special case of the central limit theorem given in Section 3.4), which says that xN 1 ðsN \u0001 NpÞ=½Npð1 \u0001 pÞ\u00021=2 has a limiting standard normal distribution, where sN has the binomial ðN; pÞ distribution. definition 3.7: A sequence of K \u0004 1 random vectors fxN: N ¼ 1; 2; . . .g converges in distribution to the continuous random vector x if and only if for any K \u0004 1 non- random vector c such that c0c ¼ 1, c0xN ! d c0x, and we write xN ! d x. When x @ Normalðm; VÞ the requirement in Deﬁnition 3.7 is that c0xN ! d Normalðc0m; c0VcÞ for every c A RK such that c0c ¼ 1; in this case we write xN ! d Normalðm; VÞ or xN @ a Normalðm; VÞ. For the derivations in this book, m ¼ 0. Chapter 3 38", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 56, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p57::c0", "text": "lemma 3.5: If xN ! d x, where x is any K \u0004 1 random vector, then xN ¼ Opð1Þ. As we will see throughout this book, Lemma 3.5 turns out to be very useful for establishing that a sequence is bounded in probability. Often it is easiest to ﬁrst verify that a sequence converges in distribution. lemma 3.6: Let fxNg be a sequence of K \u0004 1 random vectors such that xN ! d x. If g: RK ! RJ is a continuous function, then gðxNÞ ! d gðxÞ. The usefulness of Lemma 3.6, which is called the continuous mapping theorem, cannot be overstated. It tells us that once we know the limiting distribution of xN, we can ﬁnd the limiting distribution of many interesting functions of xN. This is espe- cially useful for determining the asymptotic distribution of test statistics once the limiting distribution of an estimator is known; see Section 3.5. The continuity of g is not necessary in Lemma 3.6, but some restrictions are needed. We will only need the form stated in Lemma 3.6. corollary 3.2: If fzNg is a sequence of K \u0004 1 random vectors such that zN ! d Normalð0; VÞ then (1) For any K \u0004 M nonrandom matrix A, A0zN ! d Normalð0; A0VAÞ. (2) z0 NV\u00011zN ! d w2 K (or z0 NV\u00011zN @ a w2 K). lemma 3.7: Let fxNg and fzNg be sequences of K \u0004 1 random vectors. If zN ! d z and xN \u0001 zN ! p 0, then xN ! d z. Lemma 3.7 is called the asymptotic equivalence lemma. In Section 3.5.1 we discuss generally how Lemma 3.7 is used in econometrics. We use the asymptotic equiva- lence lemma so frequently in asymptotic analysis that after a while we will not even mention that we are using it. 3.4 Limit Theorems for Random Samples In this section we state two classic limit theorems for independent, identically dis- tributed (i.i.d.) sequences of random vectors. These apply when sampling is done randomly from a population. theorem 3.1: Let fwi: i ¼ 1; 2; . . .g be a sequence of independent, identically dis- tributed G \u0004 1 random vectors such that EðjwigjÞ < y, g ¼ 1; . . . ; G. Then the sequence satisﬁes the weak law of large numbers (WLLN): N\u00011 PN i¼1 wi ! p mw, where mw 1 EðwiÞ. Basic Asymptotic Theory 39", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 57, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p58::c0", "text": "theorem 3.2 (Lindeberg-Levy): Let fwi: i ¼ 1; 2; . . .g be a sequence of independent, identically distributed G \u0004 1 random vectors such that Eðw2 igÞ < y, g ¼ 1; . . . ; G, and EðwiÞ ¼ 0. Then fwi: i ¼ 1; 2; . . .g satisﬁes the central limit theorem (CLT); that is, N\u00011=2 X N i¼1 wi ! d Normalð0; BÞ where B ¼ VarðwiÞ ¼ Eðwiw0 iÞ is necessarily positive semideﬁnite. For our purposes, B is almost always positive deﬁnite. 3.5 Limiting Behavior of Estimators and Test Statistics In this section, we apply the previous concepts to sequences of estimators. Because estimators depend on the random outcomes of data, they are properly viewed as random vectors. 3.5.1 Asymptotic Properties of Estimators definition 3.8: Let f^yN: N ¼ 1; 2; . . .g be a sequence of estimators of the P \u0004 1 vector y A Y, where N indexes the sample size. If ^yN ! p y ð3:2Þ for any value of y, then we say ^yN is a consistent estimator of y. Because there are other notions of convergence, in the theoretical literature condi- tion (3.2) is often referred to as weak consistency. This is the only kind of consistency we will be concerned with, so we simply call condition (3.2) consistency. (See White, 1984, Chapter 2, for other kinds of convergence.) Since we do not know y, the con- sistency deﬁnition requires condition (3.2) for any possible value of y. definition 3.9: Let f^yN: N ¼ 1; 2; . . .g be a sequence of estimators of the P \u0004 1 vector y A Y. Suppose that ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ ! d Normalð0; VÞ ð3:3Þ where V is a P \u0004 P positive semideﬁnite matrix. Then we say that ^yN is ﬃﬃﬃﬃ N p - asymptotically normally distributed and V is the asymptotic variance of ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ, denoted Avar ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ ¼ V. Even though V=N ¼ Varð^yNÞ holds only in special cases, and ^yN rarely has an exact normal distribution, we treat ^yN as if Chapter 3 40", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 58, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p59::c0", "text": "^yN @ Normalðy; V=NÞ ð3:4Þ whenever statement (3.3) holds. For this reason, V=N is called the asymptotic vari- ance of ^yN, and we write Avarð^yNÞ ¼ V=N ð3:5Þ However, the only sense in which ^yN is approximately normally distributed with mean y and variance V=N is contained in statement (3.3), and this is what is needed to perform inference about y. Statement (3.4) is a heuristic statement that leads to the appropriate inference. When we discuss consistent estimation of asymptotic variances—a topic that will arise often—we should technically focus on estimation of V 1 Avar ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ. In most cases, we will be able to ﬁnd at least one, and usually more than one, consistent estimator ^VN of V. Then the corresponding estimator of Avarð^yNÞ is ^VN=N, and we write Av^arð^yNÞ ¼ ^VN=N ð3:6Þ The division by N in equation (3.6) is practically very important. What we call the asymptotic variance of ^yN is estimated as in equation (3.6). Unfortunately, there has not been a consistent usage of the term ‘‘asymptotic variance’’ in econometrics. Taken literally, a statement such as ‘‘^VN=N is consistent for Avarð^yNÞ’’ is not very meaningful because V=N converges to 0 as N ! y; typically, ^VN=N ! p 0 whether or not ^VN is not consistent for V. Nevertheless, it is useful to have an admittedly imprecise shorthand. In what follows, if we say that ‘‘^VN=N consistently estimates Avarð^yNÞ,’’ we mean that ^VN consistently estimates Avar ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ. definition 3.10: If ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ @ a Normalð0; VÞ where V is positive deﬁnite with jth diagonal vjj, and ^VN ! p V, then the asymptotic standard error of ^yNj, denoted seð^yNjÞ, is ð^vNjj=NÞ1=2. In other words, the asymptotic standard error of an estimator, which is almost always reported in applied work, is the square root of the appropriate diagonal ele- ment of ^VN=N. The asymptotic standard errors can be loosely thought of as estimating the standard deviations of the elements of ^yN, and they are the appropriate quantities to use when forming (asymptotic) t statistics and conﬁdence intervals. Obtaining valid asymptotic standard errors (after verifying that the estimator is asymptotically normally distributed) is often the biggest challenge when using a new estimator. If statement (3.3) holds, it follows by Lemma 3.5 that ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ ¼ Opð1Þ, or ^yN \u0001 y ¼ OpðN\u00011=2Þ, and we say that ^yN is a ﬃﬃﬃﬃ N p -consistent estimator of y. ﬃﬃﬃﬃ N p - Basic Asymptotic Theory 41", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 59, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p60::c0", "text": "consistency certainly implies that plim ^yN ¼ y, but it is much stronger because it tells us that the rate of convergence is almost the square root of the sample size N: ^yN \u0001 y ¼ opðN\u0001cÞ for any 0 a c < 1 2. In this book, almost every consistent estimator we will study—and every one we consider in any detail—is ﬃﬃﬃﬃ N p -asymptotically nor- mal, and therefore ﬃﬃﬃﬃ N p -consistent, under reasonable assumptions. If one ﬃﬃﬃﬃ N p -asymptotically normal estimator has an asymptotic variance that is smaller than another’s asymptotic variance (in the matrix sense), it makes it easy to choose between the estimators based on asymptotic considerations. definition 3.11: Let ^yN and ~yN be estimators of y each satisfying statement (3.3), with asymptotic variances V ¼ Avar ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ and D ¼ Avar ﬃﬃﬃﬃ N p ð~yN \u0001 yÞ (these generally depend on the value of y, but we suppress that consideration here). (1) ^yN is asymptotically e‰cient relative to ~yN if D \u0001 V is positive semideﬁnite for all y; (2) ^yN and ~yN are ﬃﬃﬃﬃ N p -equivalent if ﬃﬃﬃﬃ N p ð^yN \u0001 ~yNÞ ¼ opð1Þ. When two estimators are ﬃﬃﬃﬃ N p -equivalent, they have the same limiting distribution (multivariate normal in this case, with the same asymptotic variance). This conclu- sion follows immediately from the asymptotic equivalence lemma (Lemma 3.7). Sometimes, to ﬁnd the limiting distribution of, say, ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ, it is easiest to ﬁrst ﬁnd the limiting distribution of ﬃﬃﬃﬃ N p ð~yN \u0001 yÞ, and then to show that ^yN and ~yN are ﬃﬃﬃﬃ N p -equivalent. A good example of this approach is in Chapter 7, where we ﬁnd the limiting distribution of the feasible generalized least squares estimator, after we have found the limiting distribution of the GLS estimator. definition 3.12: Partition ^yN satisfying statement (3.3) into vectors ^yN1 and ^yN2. Then ^yN1 and ^yN2 are asymptotically independent if V ¼ V1 0 0 V2 \u0002 \u0003 where V1 is the asymptotic variance of ﬃﬃﬃﬃ N p ð^yN1 \u0001 y1Þ and similarly for V2. In other words, the asymptotic variance of ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ is block diagonal. Throughout this section we have been careful to index estimators by the sample size, N. This is useful to ﬁx ideas on the nature of asymptotic analysis, but it is cum- bersome when applying asymptotics to particular estimation methods. After this chapter, an estimator of y will be denoted ^y, which is understood to depend on the sample size N. When we write, for example, ^y ! p y, we mean convergence in proba- bility as the sample size N goes to inﬁnity. Chapter 3 42", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 60, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p61::c0", "text": "3.5.2 Asymptotic Properties of Test Statistics We begin with some important deﬁnitions in the large-sample analysis of test statistics. definition 3.13: (1) The asymptotic size of a testing procedure is deﬁned as the limiting probability of rejecting H0 when it is true. Mathematically, we can write this as limN!y PN(reject H0 j H0), where the N subscript indexes the sample size. (2) A test is said to be consistent against the alternative H1 if the null hypothesis is rejected with probability approaching one when H1 is true: limN!y PN(reject H0 j H1Þ ¼ 1. In practice, the asymptotic size of a test is obtained by ﬁnding the limiting distribu- tion of a test statistic—in our case, normal or chi-square, or simple modiﬁcations of these that can be used as t distributed or F distributed—and then choosing a critical value based on this distribution. Thus, testing using asymptotic methods is practically the same as testing using the classical linear model. A test is consistent against alternative H1 if the probability of rejecting H1 tends to unity as the sample size grows without bound. Just as consistency of an estimator is a minimal requirement, so is consistency of a test statistic. Consistency rarely allows us to choose among tests: most tests are consistent against alternatives that they are supposed to have power against. For consistent tests with the same asymptotic size, we can use the notion of local power analysis to choose among tests. We will cover this brieﬂy in Chapter 12 on nonlinear estimation, where we introduce the notion of local alternatives—that is, alternatives to H0 that converge to H0 at rate 1= ﬃﬃﬃﬃ N p . Generally, test statistics will have desirable asymptotic properties when they are based on estimators with good asymptotic properties (such as e‰ciency). We now derive the limiting distribution of a test statistic that is used very often in econometrics. lemma 3.8: Suppose that statement (3.3) holds, where V is positive deﬁnite. Then for any nonstochastic matrix Q \u0004 P matrix R, Q a P, with rankðRÞ ¼ Q, ﬃﬃﬃﬃ N p Rð^yN \u0001 yÞ @ a Normalð0; RVR0Þ and ½ ﬃﬃﬃﬃ N p Rð^yN \u0001 yÞ\u00020½RVR0\u0002\u00011½ ﬃﬃﬃﬃ N p Rð^yN \u0001 yÞ\u0002 @ a w2 Q In addition, if plim ^VN ¼ V then ½ ﬃﬃﬃﬃ N p Rð^yN \u0001 yÞ\u00020½R^VNR0\u0002\u00011½ ﬃﬃﬃﬃ N p Rð^yN \u0001 yÞ\u0002 ¼ ð^yN \u0001 yÞ0R0½Rð^VN=NÞR0\u0002\u00011Rð^yN \u0001 yÞ @ a w2 Q Basic Asymptotic Theory 43", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 61, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p62::c0", "text": "For testing the null hypothesis H0: Ry ¼ r, where r is a Q \u0004 1 nonrandom vector, deﬁne the Wald statistic for testing H0 against H1: Ry 0 r as WN 1 ðR^yN \u0001 rÞ0½Rð^VN=NÞR0\u0002\u00011ðR^yN \u0001 rÞ ð3:7Þ Under H0, WN @ a w2 Q. If we abuse the asymptotics and treat ^yN as being distributed as Normalðy; ^VN=NÞ, we get equation (3.7) exactly. lemma 3.9: Suppose that statement (3.3) holds, where V is positive deﬁnite. Let c: Y ! RQ be a continuously di¤erentiable function on the parameter space Y H RP, where Q a P, and assume that y is in the interior of the parameter space. Deﬁne CðyÞ 1 ‘ycðyÞ as the Q \u0004 P Jacobian of c. Then ﬃﬃﬃﬃ N p ½cð^yNÞ \u0001 cðyÞ\u0002 @ a Normal½0; CðyÞVCðyÞ0\u0002 ð3:8Þ and f ﬃﬃﬃﬃ N p ½cð^yNÞ \u0001 cðyÞ\u0002g0½CðyÞVCðyÞ0\u0002\u00011f ﬃﬃﬃﬃ N p ½cð^yNÞ \u0001 cðyÞ\u0002g @ a w2 Q Deﬁne ^CN 1 Cð^yNÞ. Then plim ^CN ¼ CðyÞ. If plim ^VN ¼ V, then f ﬃﬃﬃﬃ N p ½cð^yNÞ \u0001 cðyÞ\u0002g0½ ^CN ^VN ^C0 N\u0002\u00011f ﬃﬃﬃﬃ N p ½cð^yNÞ \u0001 cðyÞ\u0002g @ a w2 Q ð3:9Þ Equation (3.8) is very useful for obtaining asymptotic standard errors for nonlin- ear functions of ^yN. The appropriate estimator of Avar½cð^yNÞ\u0002 is ^CNð^VN=NÞ ^C0 N ¼ ^CN½Avarð^yNÞ\u0002 ^C0 N. Thus, once Avarð^yNÞ and the estimated Jacobian of c are ob- tained, we can easily obtain Avar½cð^yNÞ\u0002 ¼ ^CN½Avarð^yNÞ\u0002 ^C0 N ð3:10Þ The asymptotic standard errors are obtained as the square roots of the diagonal elements of equation (3.10). In the scalar case ^gN ¼ cð^yNÞ, the asymptotic standard error of ^gN is ½‘ycð^yNÞ½Avarð^yNÞ\u0002‘ycð^yNÞ0\u00021=2. Equation (3.9) is useful for testing nonlinear hypotheses of the form H0: cðyÞ ¼ 0 against H1: cðyÞ 0 0. The Wald statistic is WN ¼ ﬃﬃﬃﬃ N p cð^yNÞ0½ ^CN ^VN ^C0 N\u0002\u00011 ﬃﬃﬃﬃ N p cð^yNÞ ¼ cð^yNÞ0½ ^CNð^VN=NÞ ^C0 N\u0002\u00011cð^yNÞ ð3:11Þ Under H0, WN @ a w2 Q. The method of establishing equation (3.8), given that statement (3.3) holds, is often called the delta method, and it is used very often in econometrics. It gets its name from its use of calculus. The argument is as follows. Because y is in the interior of Y, and because plim ^yN ¼ y, ^yN is in an open, convex subset of Y containing y with Chapter 3 44", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 62, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p63::c0", "text": "probability approaching one, therefore w.p.a.1 we can use a mean value expansion cð^yNÞ ¼ cðyÞ þ €CN \u0003 ð^yN \u0001 yÞ, where €CN denotes the matrix CðyÞ with rows eval- uated at mean values between ^yN and y. Because these mean values are trapped be- tween ^yN and y, they converge in probability to y. Therefore, by Slutsky’s theorem, €CN ! p CðyÞ, and we can write ﬃﬃﬃﬃ N p ½cð^yNÞ \u0001 cðyÞ\u0002 ¼ €CN \u0003 ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ ¼ CðyÞ ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ þ ½€CN \u0001 CðyÞ\u0002 ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ ¼ CðyÞ ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ þ opð1Þ \u0003 Opð1Þ ¼ CðyÞ ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ þ opð1Þ We can now apply the asymptotic equivalence lemma and Lemma 3.8 [with R 1 CðyÞ\u0002 to get equation (3.8). Problems 3.1. Prove Lemma 3.1. 3.2. Using Lemma 3.2, prove Lemma 3.3. 3.3. Explain why, under the assumptions of Lemma 3.4, gðxNÞ ¼ Opð1Þ. 3.4. Prove Corollary 3.2. 3.5. Let fyi: i ¼ 1; 2; . . .g be an independent, identically distributed sequence with Eðy2 i Þ < y. Let m ¼ EðyiÞ and s2 ¼ VarðyiÞ. a. Let yN denote the sample average based on a sample size of N. Find Var½ ﬃﬃﬃﬃ N p ðyN \u0001 mÞ\u0002. b. What is the asymptotic variance of ﬃﬃﬃﬃ N p ðyN \u0001 mÞ? c. What is the asymptotic variance of yN? Compare this with VarðyNÞ. d. What is the asymptotic standard deviation of yN? e. How would you obtain the asymptotic standard error of yN? 3.6. Give a careful (albeit short) proof of the following statement: If ﬃﬃﬃﬃ N p ð^yN \u0001 yÞ ¼ Opð1Þ, then ^yN \u0001 y ¼ opðN\u0001cÞ for any 0 a c < 1 2. 3.7. Let ^y be a ﬃﬃﬃﬃ N p -asymptotically normal estimator for the scalar y > 0. Let ^g ¼ logð^yÞ be an estimator of g ¼ logðyÞ. a. Why is ^g a consistent estimator of g? Basic Asymptotic Theory 45", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 63, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p64::c0", "text": "b. Find the asymptotic variance of ﬃﬃﬃﬃ N p ð^g \u0001 gÞ in terms of the asymptotic variance of ﬃﬃﬃﬃ N p ð^y \u0001 yÞ. c. Suppose that, for a sample of data, ^y ¼ 4 and seð^yÞ ¼ 2. What is ^g and its (asymptotic) standard error? d. Consider the null hypothesis H0: y ¼ 1. What is the asymptotic t statistic for testing H0, given the numbers from part c? e. Now state H0 from part d equivalently in terms of g, and use ^g and seð^gÞ to test H0. What do you conclude? 3.8. Let ^y ¼ ð^y1; ^y2Þ0 be a ﬃﬃﬃﬃ N p -asymptotically normal estimator for y ¼ ðy1; y2Þ0, with y2 0 0. Let ^g ¼ ^y1=^y2 be an estimator of g ¼ y1=y2. a. Show that plim ^g ¼ g. b. Find Avarð^gÞ in terms of y and Avarð^yÞ using the delta method. c. If, for a sample of data, ^y ¼ ð\u00011:5; :5Þ0 and Avarð^yÞ is estimated as 1 \u0001:4 \u0001:4 2 \u0002 \u0003 , ﬁnd the asymptotic standard error of ^g. 3.9. Let ^y and ~y be two consistent, ﬃﬃﬃﬃ N p -asymptotically normal estimators of the P \u0004 1 parameter vector y, with Avar ﬃﬃﬃﬃ N p ð^y \u0001 yÞ ¼ V1 and Avar ﬃﬃﬃﬃ N p ð~y \u0001 yÞ ¼ V2. Deﬁne a Q \u0004 1 parameter vector by g ¼ gðyÞ, where gð\u0003Þ is a continuously di¤er- entiable function. Show that, if ^y is asymptotically more e‰cient than ~y, then ^g 1 gð^yÞ is asymptotically e‰cient relative to ~g 1 gð~yÞ. Chapter 3 46", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 64, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p65::c0", "text": "II LINEAR MODELS In this part we begin our econometric analysis of linear models for cross section and panel data. In Chapter 4 we review the single-equation linear model and discuss ordinary least squares estimation. Although this material is, in principle, review, the approach is likely to be di¤erent from an introductory linear models course. In ad- dition, we cover several topics that are not traditionally covered in texts but that have proven useful in empirical work. Chapter 5 discusses instrumental variables estima- tion of the linear model, and Chapter 6 covers some remaining topics to round out our treatment of the single-equation model. Chapter 7 begins our analysis of systems of equations. The general setup is that the number of population equations is small relative to the (cross section) sample size. This allows us to cover seemingly unrelated regression models for cross section data as well as begin our analysis of panel data. Chapter 8 builds on the framework from Chapter 7 but considers the case where some explanatory variables may be uncorre- lated with the error terms. Generalized method of moments estimation is the unifying theme. Chapter 9 applies the methods of Chapter 8 to the estimation of simultaneous equations models, with an emphasis on the conceptual issues that arise in applying such models. Chapter 10 explicitly introduces unobserved-e¤ects linear panel data models. Under the assumption that the explanatory variables are strictly exogenous conditional on the unobserved e¤ect, we study several estimation methods, including ﬁxed e¤ects, ﬁrst di¤erencing, and random e¤ects. The last method assumes, at a minimum, that the unobserved e¤ect is uncorrelated with the explanatory variables in all time periods. Chapter 11 considers extensions of the basic panel data model, including failure of the strict exogeneity assumption.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 65, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p66::c0", "text": "4 The Single-Equation Linear Model and OLS Estimation 4.1 Overview of the Single-Equation Linear Model This and the next couple of chapters cover what is still the workhorse in empirical economics: the single-equation linear model. Though you are assumed to be com- fortable with ordinary least squares (OLS) estimation, we begin with OLS for a couple of reasons. First, it provides a bridge between more traditional approaches to econometrics—which treats explanatory variables as ﬁxed—and the current ap- proach, which is based on random sampling with stochastic explanatory variables. Second, we cover some topics that receive at best cursory treatment in ﬁrst-semester texts. These topics, such as proxy variable solutions to the omitted variable problem, arise often in applied work. The population model we study is linear in its parameters, y ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKxK þ u ð4:1Þ where y; x1; x2; x3; . . . ; xK are observable random scalars (that is, we can observe them in a random sample of the population), u is the unobservable random distur- bance or error, and b0; b1; b2; . . . ; bK are the parameters (constants) we would like to estimate. The error form of the model in equation (4.1) is useful for presenting a uniﬁed treatment of the statistical properties of various econometric procedures. Neverthe- less, the steps one uses for getting to equation (4.1) are just as important. Goldberger (1972) deﬁnes a structural model as one representing a causal relationship, as opposed to a relationship that simply captures statistical associations. A structural equation can be obtained from an economic model, or it can be obtained through informal reasoning. Sometimes the structural model is directly estimable. Other times we must combine auxiliary assumptions about other variables with algebraic manipulations to arrive at an estimable model. In addition, we will often have reasons to estimate nonstructural equations, sometimes as a precursor to estimating a structural equation. The error term u can consist of a variety of things, including omitted variables and measurement error (we will see some examples shortly). The parameters bj hopefully correspond to the parameters of interest, that is, the parameters in an un- derlying structural model. Whether this is the case depends on the application and the assumptions made. As we will see in Section 4.2, the key condition needed for OLS to consistently estimate the bj (assuming we have available a random sample from the population) is that the error (in the population) has mean zero and is uncorrelated with each of the regressors: EðuÞ ¼ 0; Covðxj; uÞ ¼ 0; j ¼ 1; 2; . . . ; K ð4:2Þ", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 66, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p67::c0", "text": "The zero-mean assumption is for free when an intercept is included, and we will restrict attention to that case in what follows. It is the zero covariance of u with each xj that is important. From Chapter 2 we know that equation (4.1) and assumption (4.2) are equivalent to deﬁning the linear projection of y onto ð1; x1; x2; . . . ; xKÞ as b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKxK. Su‰cient for assumption (4.2) is the zero conditional mean assumption Eðu j x1; x2; . . . ; xKÞ ¼ Eðu j xÞ ¼ 0 ð4:3Þ Under equation (4.1) and assumption (4.3) we have the population regression function Eðy j x1; x2; . . . ; xKÞ ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKxK ð4:4Þ As we saw in Chapter 2, equation (4.4) includes the case where the xj are nonlinear functions of underlying explanatory variables, such as Eðsavings j income; size; age; collegeÞ ¼ b0 þ b1 logðincomeÞ þ b2size þ b3age þ b4college þ b5college\u0001age We will study the asymptotic properties of OLS primarily under assumption (4.2), since it is weaker than assumption (4.3). As we discussed in Chapter 2, assumption (4.3) is natural when a structural model is directly estimable because it ensures that no additional functions of the explanatory variables help to explain y. An explanatory variable xj is said to be endogenous in equation (4.1) if it is corre- lated with u. You should not rely too much on the meaning of ‘‘endogenous’’ from other branches of economics. In traditional usage, a variable is endogenous if it is determined within the context of a model. The usage in econometrics, while related to traditional deﬁnitions, is used broadly to describe any situation where an explanatory variable is correlated with the disturbance. If xj is uncorrelated with u, then xj is said to be exogenous in equation (4.1). If assumption (4.3) holds, then each explanatory variable is necessarily exogenous. In applied econometrics, endogeneity usually arises in one of three ways: Omitted Variables Omitted variables appear when we would like to control for one or more additional variables but, usually because of data unavailability, we cannot include them in a regression model. Speciﬁcally, suppose that Eðy j x; qÞ is the con- ditional expectation of interest, which can be written as a function linear in parame- ters and additive in q. If q is unobserved, we can always estimate Eðy j xÞ, but this need have no particular relationship to Eðy j x; qÞ when q and x are allowed to be correlated. One way to represent this situation is to write equation (4.1) where q is part of the error term u. If q and xj are correlated, then xj is endogenous. The cor- Chapter 4 50", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 67, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p68::c0", "text": "relation of explanatory variables with unobservables is often due to self-selection: if agents choose the value of xj, this might depend on factors ðqÞ that are unobservable to the analyst. A good example is omitted ability in a wage equation, where an indi- vidual’s years of schooling are likely to be correlated with unobserved ability. We discuss the omitted variables problem in detail in Section 4.3. Measurement Error In this case we would like to measure the (partial) e¤ect of a variable, say x\u0002 K, but we can observe only an imperfect measure of it, say xK. When we plug xK in for x\u0002 K —thereby arriving at the estimable equation (4.1)—we neces- sarily put a measurement error into u. Depending on assumptions about how x\u0002 K and xK are related, u and xK may or may not be correlated. For example, x\u0002 K might denote a marginal tax rate, but we can only obtain data on the average tax rate. We will study the measurement error problem in Section 4.4. Simultaneity Simultaneity arises when at least one of the explanatory variables is determined simultaneously along with y. If, say, xK is determined partly as a function of y, then xK and u are generally correlated. For example, if y is city murder rate and xK is size of the police force, size of the police force is partly determined by the murder rate. Conceptually, this is a more di‰cult situation to analyze, because we must be able to think of a situation where we could vary xK exogenously, even though in the data that we collect y and xK are generated simultaneously. Chapter 9 treats simultaneous equations models in detail. The distinctions among the three possible forms of endogeneity are not always sharp. In fact, an equation can have more than one source of endogeneity. For ex- ample, in looking at the e¤ect of alcohol consumption on worker productivity (as typically measured by wages), we would worry that alcohol usage is correlated with unobserved factors, possibly related to family background, that also a¤ect wage; this is an omitted variables problem. In addition, alcohol demand would generally de- pend on income, which is largely determined by wage; this is a simultaneity problem. And measurement error in alcohol usage is always a possibility. For an illuminating discussion of the three kinds of endogeneity as they arise in a particular ﬁeld, see Deaton’s (1995) survey chapter on econometric issues in development economics. 4.2 Asymptotic Properties of OLS We now brieﬂy review the asymptotic properties of OLS for random samples from a population, focusing on inference. It is convenient to write the population equation of interest in vector form as The Single-Equation Linear Model and OLS Estimation 51", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 68, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p69::c0", "text": "y ¼ xb þ u ð4:5Þ where x is a 1 \u0003 K vector of regressors and b 1 ðb1; b2; . . . ; bKÞ0 is a K \u0003 1 vector. Since most equations contain an intercept, we will just assume that x1 1 1, as this assumption makes interpreting the conditions easier. We assume that we can obtain a random sample of size N from the population in order to estimate b; thus, fðxi; yiÞ: i ¼ 1; 2; . . . ; Ng are treated as independent, iden- tically distributed random variables, where xi is 1 \u0003 K and yi is a scalar. For each observation i we have yi ¼ xib þ ui ð4:6Þ which is convenient for deriving statistical properties of estimators. As for stating and interpreting assumptions, it is easiest to focus on the population model (4.5). 4.2.1 Consistency As discussed in Section 4.1, the key assumption for OLS to consistently estimate b is the population orthogonality condition: assumption OLS.1: Eðx0uÞ ¼ 0. Because x contains a constant, Assumption OLS.1 is equivalent to saying that u has mean zero and is uncorrelated with each regressor, which is how we will refer to Assumption OLS.1. Su‰cient for Assumption OLS.1 is the zero conditional mean assumption (4.3). The other assumption needed for consistency of OLS is that the expected outer product matrix of x has full rank, so that there are no exact linear relationships among the regressors in the population. This is stated succinctly as follows: assumption OLS.2: rank Eðx0xÞ ¼ K. As with Assumption OLS.1, Assumption OLS.2 is an assumption about the popu- lation. Since Eðx0xÞ is a symmetric K \u0003 K matrix, Assumption OLS.2 is equivalent to assuming that Eðx0xÞ is positive deﬁnite. Since x1 ¼ 1, Assumption OLS.2 is also equivalent to saying that the (population) variance matrix of the K \u0004 1 nonconstant elements in x is nonsingular. This is a standard assumption, which fails if and only if at least one of the regressors can be written as a linear function of the other regressors (in the population). Usually Assumption OLS.2 holds, but it can fail if the population model is improperly speciﬁed [for example, if we include too many dummy variables in x or mistakenly use something like logðageÞ and logðage2Þ in the same equation]. Under Assumptions OLS.1 and OLS.2, the parameter vector b is identiﬁed. In the context of models that are linear in the parameters under random sampling, identi- Chapter 4 52", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 69, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p70::c0", "text": "ﬁcation of b simply means that b can be written in terms of population moments in observable variables. (Later, when we consider nonlinear models, the notion of identiﬁcation will have to be more general. Also, special issues arise if we cannot obtain a random sample from the population, something we treat in Chapter 17.) To see that b is identiﬁed under Assumptions OLS.1 and OLS.2, premultiply equation (4.5) by x0, take expectations, and solve to get b ¼ ½Eðx0xÞ\u0005\u00041Eðx0 yÞ Because ðx; yÞ is observed, b is identiﬁed. The analogy principle for choosing an esti- mator says to turn the population problem into its sample counterpart (see Gold- berger, 1968; Manski, 1988). In the current application this step leads to the method of moments: replace the population moments Eðx0xÞ and Eðx0 yÞ with the corre- sponding sample averages. Doing so leads to the OLS estimator: ^b ¼ N\u00041 X N i¼1 x0 ixi !\u00041 N\u00041 X N i¼1 x0 i yi ! ¼ b þ N\u00041 X N i¼1 x0 ixi !\u00041 N\u00041 X N i¼1 x0 iui ! which can be written in full matrix form as ðX0XÞ\u00041X0Y, where X is the N \u0003 K data matrix of regressors with ith row xi and Y is the N \u0003 1 data vector with ith element yi. Under Assumption OLS.2, X0X is nonsingular with probability approaching one and plim½ðN\u00041 PN i¼1 x0 ixiÞ\u00041\u0005 ¼ A\u00041, where A 1 Eðx0xÞ (see Corollary 3.1). Further, under Assumption OLS.1, plimðN\u00041 PN i¼1 x0 iuiÞ ¼ Eðx0uÞ ¼ 0. Therefore, by Slutsky’s theorem (Lemma 3.4), plim ^b ¼ b þ A\u00041 \u0001 0 ¼ b. We summarize with a theorem: theorem 4.1 (Consistency of OLS): Under Assumptions OLS.1 and OLS.2, the OLS estimator ^b obtained from a random sample following the population model (4.5) is consistent for b. The simplicity of the proof of Theorem 4.1 should not undermine its usefulness. Whenever an equation can be put into the form (4.5) and Assumptions OLS.1 and OLS.2 hold, OLS using a random sample consistently estimates b. It does not matter where this equation comes from, or what the bj actually represent. As we will see in Sections 4.3 and 4.4, often an estimable equation is obtained only after manipulating an underlying structural equation. An important point to remember is that, once the linear (in parameters) equation has been speciﬁed with an additive error and Assumptions OLS.1 and OLS.2 are veriﬁed, there is no need to reprove Theorem 4.1. Under the assumptions of Theorem 4.1, xb is the linear projection of y on x. Thus, Theorem 4.1 shows that OLS consistently estimates the parameters in a linear pro- jection, subject to the rank condition in Assumption OLS.2. This is very general, as it places no restrictions on the nature of y—for example, y could be a binary variable The Single-Equation Linear Model and OLS Estimation 53", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 70, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p71::c0", "text": "or some other variable with discrete characteristics. Since a conditional expectation that is linear in parameters is also the linear projection, Theorem 4.1 also shows that OLS consistently estimates conditional expectations that are linear in parameters. We will use this fact often in later sections. There are a few ﬁnal points worth emphasizing. First, if either Assumption OLS.1 or OLS.2 fails, then b is not identiﬁed (unless we make other assumptions, as in Chapter 5). Usually it is correlation between u and one or more elements of x that causes lack of identiﬁcation. Second, the OLS estimator is not necessarily unbiased even under Assumptions OLS.1 and OLS.2. However, if we impose the zero condi- tional mean assumption (4.3), then it can be shown that Eð ^b j XÞ ¼ b if X0X is non- singular; see Problem 4.2. By iterated expectations, ^b is then also unconditionally unbiased, provided the expected value Eð ^bÞ exists. Finally, we have not made the much more restrictive assumption that u and x are independent. If EðuÞ ¼ 0 and u is independent of x, then assumption (4.3) holds, but not vice versa. For example, Varðu j xÞ is entirely unrestricted under assumption (4.3), but Varðu j xÞ is necessarily constant if u and x are independent. 4.2.2 Asymptotic Inference Using OLS The asymptotic distribution of the OLS estimator is derived by writing ﬃﬃﬃﬃ N p ð ^b \u0004 bÞ ¼ N\u00041 X N i¼1 x0 ixi !\u00041 N\u00041=2 X N i¼1 x0 iui ! As we saw in Theorem 4.1, ðN\u00041 PN i¼1 x0 ixiÞ\u00041 \u0004 A\u00041 ¼ opð1Þ. Also, fðx0 iuiÞ:i ¼ 1; 2; . . .g is an i.i.d. sequence with zero mean, and we assume that each element has ﬁnite variance. Then the central limit theorem (Theorem 3.2) implies that N\u00041=2 PN i¼1 x0 iui ! d Normalð0; BÞ, where B is the K \u0003 K matrix B 1 Eðu2x0xÞ ð4:7Þ This implies N\u00041=2 PN i¼1 x0 iui ¼ Opð1Þ, and so we can write ﬃﬃﬃﬃ N p ð ^b \u0004 bÞ ¼ A\u00041 N\u00041=2 X N i¼1 x0 iui ! þ opð1Þ ð4:8Þ since opð1Þ \u0001 Opð1Þ ¼ opð1Þ. We can use equation (4.8) to immediately obtain the asymptotic distribution of ﬃﬃﬃﬃ N p ð ^b \u0004 bÞ. A homoskedasticity assumption simpliﬁes the form of OLS asymptotic variance: assumption OLS.3: Eðu2x0xÞ ¼ s2Eðx0xÞ, where s2 1 Eðu2Þ. Chapter 4 54", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 71, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p72::c0", "text": "Because EðuÞ ¼ 0, s2 is also equal to VarðuÞ. Assumption OLS.3 is the weakest form of the homoskedasticity assumption. If we write out the K \u0003 K matrices in Assump- tion OLS.3 element by element, we see that Assumption OLS.3 is equivalent to assuming that the squared error, u2, is uncorrelated with each xj, x2 j , and all cross products of the form xjxk. By the law of iterated expectations, su‰cient for As- sumption OLS.3 is Eðu2 j xÞ ¼ s2, which is the same as Varðu j xÞ ¼ s2 when Eðu j xÞ ¼ 0. The constant conditional variance assumption for u given x is the easiest to interpret, but it is stronger than needed. theorem 4.2 (Asymptotic Normality of OLS): Under Assumptions OLS.1–OLS.3, ﬃﬃﬃﬃ N p ð ^b \u0004 bÞ @ a Normalð0; s2A\u00041Þ ð4:9Þ Proof: From equation (4.8) and deﬁnition of B, it follows from Lemma 3.7 and Corollary 3.2 that ﬃﬃﬃﬃ N p ð ^b \u0004 bÞ @ a Normalð0; A\u00041BA\u00041Þ Under Assumption OLS.3, B ¼ s2A, which proves the result. Practically speaking, equation (4.9) allows us to treat ^b as approximately normal with mean b and variance s2½Eðx0xÞ\u0005\u00041=N. The usual estimator of s2, ^s2 1 SSR= ðN \u0004 KÞ, where SSR ¼ PN i¼1 ^u2 i is the OLS sum of squared residuals, is easily shown to be consistent. (Using N or N \u0004 K in the denominator does not a¤ect consistency.) When we also replace Eðx0xÞ with the sample average N\u00041 PN i¼1 x0 ixi ¼ ðX0X=NÞ, we get Av^arð ^bÞ ¼ ^s2ðX0XÞ\u00041 ð4:10Þ The right-hand side of equation (4.10) should be familiar: it is the usual OLS variance matrix estimator under the classical linear model assumptions. The bottom line of Theorem 4.2 is that, under Assumptions OLS.1–OLS.3, the usual OLS standard errors, t statistics, and F statistics are asymptotically valid. Showing that the F sta- tistic is approximately valid is done by deriving the Wald test for linear restrictions of the form Rb ¼ r (see Chapter 3). Then the F statistic is simply a degrees-of-freedom- adjusted Wald statistic, which is where the F distribution (as opposed to the chi- square distribution) arises. 4.2.3 Heteroskedasticity-Robust Inference If Assumption OLS.1 fails, we are in potentially serious trouble, as OLS is not even consistent. In the next chapter we discuss the important method of instrumental variables that can be used to obtain consistent estimators of b when Assumption The Single-Equation Linear Model and OLS Estimation 55", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 72, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p73::c0", "text": "OLS.1 fails. Assumption OLS.2 is also needed for consistency, but there is rarely any reason to examine its failure. Failure of Assumption OLS.3 has less serious consequences than failure of As- sumption OLS.1. As we have already seen, Assumption OLS.3 has nothing to do with consistency of ^b. Further, the proof of asymptotic normality based on equation (4.8) is still valid without Assumption OLS.3, but the ﬁnal asymptotic variance is di¤erent. We have assumed OLS.3 for deriving the limiting distribution because it implies the asymptotic validity of the usual OLS standard errors and test statistics. All regression packages assume OLS.3 as the default in reporting statistics. Often there are reasons to believe that Assumption OLS.3 might fail, in which case equation (4.10) is no longer a valid estimate of even the asymptotic variance matrix. If we make the zero conditional mean assumption (4.3), one solution to violation of Assumption OLS.3 is to specify a model for Varðy j xÞ, estimate this model, and apply weighted least squares (WLS): for observation i, yi and every element of xi (including unity) are divided by an estimate of the conditional standard deviation ½Varðyi j xiÞ\u00051=2, and OLS is applied to the weighted data (see Wooldridge, 2000a, Chapter 8, for details). This procedure leads to a di¤erent estimator of b. We discuss WLS in the more general context of nonlinear regression in Chapter 12. Lately, it has become more popular to estimate b by OLS even when heteroskedasticity is sus- pected but to adjust the standard errors and test statistics so that they are valid in the presence of arbitrary heteroskedasticity. Since these standard errors are valid whether or not Assumption OLS.3 holds, this method is much easier than a weighted least squares procedure. What we sacriﬁce is potential e‰ciency gains from weighted least squares (WLS) (see Chapter 14). But, e‰ciency gains from WLS are guaranteed only if the model for Varðy j xÞ is correct. Further, WLS is generally inconsistent if Eðu j xÞ 0 0 but Assumption OLS.1 holds, so WLS is inappropriate for estimating linear projections. Especially with large sample sizes, the presence of heteroskeda- sticity need not a¤ect one’s ability to perform accurate inference using OLS. But we need to compute standard errors and test statistics appropriately. The adjustment needed to the asymptotic variance follows from the proof of The- orem 4.2: without OLS.3, the asymptotic variance of ^b is Avarð ^bÞ ¼ A\u00041BA\u00041=N, where the K \u0003 K matrices A and B were deﬁned earlier. We already know how to consistently estimate A. Estimation of B is also straightforward. First, by the law of large numbers, N\u00041 PN i¼1 u2 i x0 ixi ! p Eðu2x0xÞ ¼ B. Now, since the ui are not observed, we replace ui with the OLS residual ^ui ¼ yi \u0004 xi ^b. This leads to the con- sistent estimator ^B 1 N\u00041 PN i¼1 ^u2 i x0 ixi. See White (1984) and Problem 4.5. The heteroskedasticity-robust variance matrix estimator of ^b is ^A\u00041^B^A\u00041=N or, after cancellations, Chapter 4 56", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 73, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p74::c0", "text": "Av^arð ^bÞ ¼ ðX0XÞ\u00041 X N i¼1 ^u2 i x0 ixi ! ðX0XÞ\u00041 ð4:11Þ This matrix was introduced in econometrics by White (1980b), although some attri- bute it to either Eicker (1967) or Huber (1967), statisticians who discovered robust variance matrices. The square roots of the diagonal elements of equation (4.11) are often called the White standard errors or Huber standard errors, or some hyphenated combination of the names Eicker, Huber, and White. It is probably best to just call them heteroskedasticity-robust standard errors, since this term describes their purpose. Remember, these standard errors are asymptotically valid in the presence of any kind of heteroskedasticity, including homoskedasticity. Robust standard errors are often reported in applied cross-sectional work, espe- cially when the sample size is large. Sometimes they are reported along with the usual OLS standard errors; sometimes they are presented in place of them. Several regres- sion packages now report these standard errors as an option, so it is easy to obtain heteroskedasticity-robust standard errors. Sometimes, as a degrees-of-freedom correction, the matrix in equation (4.11) is multiplied by N=ðN \u0004 KÞ. This procedure guarantees that, if the ^u2 i were constant across i (an unlikely event in practice, but the strongest evidence of homoskedasticity possible), then the usual OLS standard errors would be obtained. There is some evi- dence that the degrees-of-freedom adjustment improves ﬁnite sample performance. There are other ways to adjust equation (4.11) to improve its small-sample properties— see, for example, MacKinnon and White (1985)—but if N is large relative to K, these adjustments typically make little di¤erence. Once standard errors are obtained, t statistics are computed in the usual way. These are robust to heteroskedasticity of unknown form, and can be used to test single restrictions. The t statistics computed from heteroskedasticity robust standard errors are heteroskedasticity-robust t statistics. Conﬁdence intervals are also obtained in the usual way. When Assumption OLS.3 fails, the usual F statistic is not valid for testing multiple linear restrictions, even asymptotically. Some packages allow robust testing with a simple command, while others do not. If the hypotheses are written as H0: Rb ¼ r ð4:12Þ where R is Q \u0003 K and has rank Q a K, and r is Q \u0003 1, then the heteroskedasticity- robust Wald statistic for testing equation (4.12) is W ¼ ðR^b \u0004 rÞ0ðR^VR0Þ\u00041ðR^b \u0004 rÞ ð4:13Þ The Single-Equation Linear Model and OLS Estimation 57", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 74, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p75::c0", "text": "where ^V is given in equation (4.11). Under H0, W @ a w2 Q. The Wald statistic can be turned into an approximate FQ;N\u0004K random variable by dividing it by Q (and usu- ally making the degrees-of-freedom adjustment to ^V). But there is nothing wrong with using equation (4.13) directly. 4.2.4 Lagrange Multiplier (Score) Tests In the partitioned model y ¼ x1b1 þ x2b2 þ u ð4:14Þ under Assumptions OLS.1–OLS.3, where x1 is 1 \u0003 K1 and x2 is 1 \u0003 K2, we know that the hypothesis H0: b2 ¼ 0 is easily tested (asymptotically) using a standard F test. There is another approach to testing such hypotheses that is sometimes useful, espe- cially for computing heteroskedasticity-robust tests and for nonlinear models. Let ~b1 be the estimator of b1 under the null hypothesis H0: b2 ¼ 0; this is called the estimator from the restricted model. Deﬁne the restricted OLS residuals as ~ui ¼ yi \u0004 xi1 ~b1, i ¼ 1; 2; . . . ; N. Under H0, xi2 should be, up to sample variation, uncor- related with ~ui in the sample. The Lagrange multiplier or score principle is based on this observation. It turns out that a valid test statistic is obtained as follows: Run the OLS regression ~u on x1; x2 ð4:15Þ (where the observation index i has been suppressed). Assuming that x1 contains a constant (that is, the null model contains a constant), let R2 u denote the usual R- squared from the regression (4.15). Then the Lagrange multiplier (LM) or score sta- tistic is LM 1 NR2 u. These names come from di¤erent features of the constrained optimization problem; see Rao (1948), Aitchison and Silvey (1958), and Chapter 12. Because of its form, LM is also referred to as an N-R-squared test. Under H0, LM @ a w2 K2, where K2 is the number of restrictions being tested. If NR2 u is su‰- ciently large, then ~u is signiﬁcantly correlated with x2, and the null hypothesis will be rejected. It is important to include x1 along with x2 in regression (4.15). In other words, the OLS residuals from the null model should be regressed on all explanatory variables, even though ~u is orthogonal to x1 in the sample. If x1 is excluded, then the resulting statistic generally does not have a chi-square distribution when x2 and x1 are corre- lated. If Eðx0 1x2Þ ¼ 0, then we can exclude x1 from regression (4.15), but this ortho- gonality rarely holds in applications. If x1 does not include a constant, R2 u should be the uncentered R-squared: the total sum of squares in the denominator is obtained Chapter 4 58", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 75, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p76::c0", "text": "without demeaning the dependent variable, ~u. When x1 includes a constant, the usual centered R-squared and uncentered R-squared are identical because PN i¼1 ~ui ¼ 0. Example 4.1 (Wage Equation for Married, Working Women): Consider a wage equation for married, working women: logðwageÞ ¼ b0 þ b1exper þ b2exper2 þ b3educ þ b4age þ b5kidslt6 þ b6kidsge6 þ u ð4:16Þ where the last three variables are the woman’s age, number of children less than six, and number of children at least six years of age, respectively. We can test whether, after the productivity variables experience and education are controlled for, women are paid di¤erently depending on their age and number of children. The F statistic for the hypothesis H0: b4 ¼ 0; b5 ¼ 0; b6 ¼ 0 is F ¼ ½ðR2 ur \u0004 R2 r Þ=ð1 \u0004 R2 urÞ\u0005 \u0001 ½ðN \u0004 7Þ=3\u0005, where R2 ur and R2 r are the unrestricted and restricted R-squareds; under H0 (and homoskedasticity), F @ F3;N\u00047. To obtain the LM statistic, we estimate the equation without age, kidslt6, and kidsge6; let ~u denote the OLS residuals. Then, the LM sta- tistic is NR2 u from the regression ~u on 1, exper, exper2, educ, age, kidslt6, and kidsge6, where the 1 denotes that we include an intercept. Under H0 and homoskedasticity, NR2 u @ a w2 3. Using the data on the 428 working, married women in MROZ.RAW (from Mroz, 1987), we obtain the following estimated equation: logð^wageÞ ¼ \u0004:421 ð:317Þ ½:316\u0005 þ :040 ð:013Þ ½:015\u0005 exper \u0004 :00078 ð:00040Þ ½:00041\u0005 exper2 þ :108 ð:014Þ ½:014\u0005 educ \u0004 :0015 ð:0053Þ ½:0059\u0005 age \u0004 :061 ð:089Þ ½:105\u0005 kidslt6 \u0004 :015 ð:028Þ ½:029\u0005 kidsge6; R2 ¼ :158 where the quantities in brackets are the heteroskedasticity-robust standard errors. The F statistic for joint signiﬁcance of age, kidslt6, and kidsge6 turns out to be about .24, which gives p-valueA:87. Regressing the residuals ~u from the restricted model on all exogenous variables gives an R-squared of .0017, so LM ¼ 428ð:0017Þ ¼ :728, and p-valueA:87. Thus, the F and LM tests give virtually identical results. The test from regression (4.15) maintains Assumption OLS.3 under H0, just like the usual F test. It turns out to be easy to obtain a heteroskedasticity-robust LM The Single-Equation Linear Model and OLS Estimation 59", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 76, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p77::c0", "text": "statistic. To see how to do so, let us look at the formula for the LM statistic from regression (4.15) in more detail. After some algebra we can write LM ¼ N\u00041=2 X N i¼1 ^r0 i ~ui !0 ~s2N\u00041 X N i¼1 ^r0 i^ri !\u00041 N\u00041=2 X N i¼1 ^r0 i ~ui ! where ~s2 1 N\u00041 PN i¼1 ~u2 i and each ^ri is a 1 \u0003 K2 vector of OLS residuals from the (multivariate) regression of xi2 on xi1, i ¼ 1; 2; . . . ; N. This statistic is not robust to heteroskedasticity because the matrix in the middle is not a consistent estimator of the asymptotic variance of ðN\u00041=2 PN i¼1 ^r0 i ~uiÞ under heteroskedasticity. Following the reasoning in Section 4.2.3, a heteroskedasticity-robust statistic is LM ¼ N\u00041=2 X N i¼1 ^r0 i ~ui !0 N\u00041 X N i¼1 ~u2 i ^r0 i^ri !\u00041 N\u00041=2 X N i¼1 ^r0 i ~ui ! ¼ X N i¼1 ^r0 i ~ui !0 X N i¼1 ~u2 i ^r0 i^ri !\u00041 X N i¼1 ^r0 i ~ui ! Dropping the i subscript, this is easily obtained, as N \u0004 SSR0 from the OLS regres- sion (without an intercept) 1 on ~u \u0001 ^r ð4:17Þ where ~u \u0001 ^r ¼ ð~u \u0001 ^r1; ~u \u0001 ^r2; . . . ; ~u \u0001 ^rK2Þ is the 1 \u0003 K2 vector obtained by multiplying ~u by each element of ^r and SSR0 is just the usual sum of squared residuals from re- gression (4.17). Thus, we ﬁrst regress each element of x2 onto all of x1 and collect the residuals in ^r. Then we form ~u \u0001 ^r (observation by observation) and run the regression in (4.17); N \u0004 SSR0 from this regression is distributed asymptotically as w2 K2. (Do not be thrown o¤ by the fact that the dependent variable in regression (4.17) is unity for each observation; a nonzero sum of squared residuals is reported when you run OLS without an intercept.) For more details, see Davidson and MacKinnon (1985, 1993) or Wooldridge (1991a, 1995b). Example 4.1 (continued): To obtain the heteroskedasticity-robust LM statistic for H0: b4 ¼ 0; b5 ¼ 0; b6 ¼ 0 in equation (4.16), we estimate the restricted model as before and obtain ~u. Then, we run the regressions (1) age on 1, exper, exper2, educ; (2) kidslt6 on 1, exper, exper2, educ; (3) kidsge6 on 1, exper, exper2, educ; and obtain the residuals ^r1, ^r2, and ^r3, respectively. The LM statistic is N \u0004 SSR0 from the re- gression 1 on ~u \u0001 ^r1, ~u \u0001 ^r2, ~u \u0001 ^r3, and N \u0004 SSR0 @ a w2 3. Chapter 4 60", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 77, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p78::c0", "text": "When we apply this result to the data in MROZ.RAW we get LM ¼ :51, which is very small for a w2 3 random variable: p-valueA:92. For comparison, the hetero- skedasticity-robust Wald statistic (scaled by Stata9 to have an approximate F distri- bution) also yields p-valueA:92. 4.3 OLS Solutions to the Omitted Variables Problem 4.3.1 OLS Ignoring the Omitted Variables Because it is so prevalent in applied work, we now consider the omitted variables problem in more detail. A model that assumes an additive e¤ect of the omitted vari- able is Eðy j x1; x2; . . . ; xK; qÞ ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKxK þ gq ð4:18Þ where q is the omitted factor. In particular, we are interested in the bj, which are the partial e¤ects of the observed explanatory variables holding the other explanatory variables constant, including the unobservable q. In the context of this additive model, there is no point in allowing for more than one unobservable; any omitted factors are lumped into q. Henceforth we simply refer to q as the omitted variable. A good example of equation (4.18) is seen when y is logðwageÞ and q includes ability. If xK denotes a measure of education, bK in equation (4.18) measures the partial e¤ect of education on wages controlling for—or holding ﬁxed—the level of ability (as well as other observed characteristics). This e¤ect is most interesting from a policy perspective because it provides a causal interpretation of the return to edu- cation: bK is the expected proportionate increase in wage if someone from the work- ing population is exogenously given another year of education. Viewing equation (4.18) as a structural model, we can always write it in error form as y ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKxK þ gq þ v ð4:19Þ Eðv j x1; x2; . . . ; xK; qÞ ¼ 0 ð4:20Þ where v is the structural error. One way to handle the nonobservability of q is to put it into the error term. In doing so, nothing is lost by assuming EðqÞ ¼ 0 because an intercept is included in equation (4.19). Putting q into the error term means we re- write equation (4.19) as y ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKxK þ u ð4:21Þ The Single-Equation Linear Model and OLS Estimation 61", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 78, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p79::c0", "text": "u 1 gq þ v ð4:22Þ The error u in equation (4.21) consists of two parts. Under equation (4.20), v has zero mean and is uncorrelated with x1; x2; . . . ; xK (and q). By normalization, q also has zero mean. Thus, EðuÞ ¼ 0. However, u is uncorrelated with x1; x2; . . . ; xK if and only if q is uncorrelated with each of the observable regressors. If q is correlated with any of the regressors, then so is u, and we have an endogeneity problem. We cannot ex- pect OLS to consistently estimate any bj. Although Eðu j xÞ 0 EðuÞ in equation (4.21), the bj do have a structural interpretation because they appear in equation (4.19). It is easy to characterize the plims of the OLS estimators when the omitted variable is ignored; we will call this the OLS omitted variables inconsistency or OLS omitted variables bias (even though the latter term is not always precise). Write the linear projection of q onto the observable explanatory variables as q ¼ d0 þ d1x1 þ \u0001 \u0001 \u0001 þ dKxK þ r ð4:23Þ where, by deﬁnition of a linear projection, EðrÞ ¼ 0, Covðxj; rÞ ¼ 0, j ¼ 1; 2; . . . ; K. Then we can easily infer the plim of the OLS estimators from regressing y onto 1; x1; . . . ; xK by ﬁnding an equation that does satisfy Assumptions OLS.1 and OLS.2. Plugging equation (4.23) into equation (4.19) and doing simple algrebra gives y ¼ ðb0 þ gd0Þ þ ðb1 þ gd1Þx1 þ ðb2 þ gd2Þx2 þ \u0001 \u0001 \u0001 þ ðbK þ gdKÞxK þ v þ gr Now, the error v þ gr has zero mean and is uncorrelated with each regressor. It fol- lows that we can just read o¤ the plim of the OLS estimators from the regression of y on 1; x1; . . . ; xK: plim ^bj ¼ bj þ gdj. Sometimes it is assumed that most of the dj are zero. When the correlation between q and a particular variable, say xK, is the focus, a common (usually implicit) assumption is that all dj in equation (4.23) except the intercept and coe‰cient on xK are zero. Then plim ^bj ¼ bj, j ¼ 1; . . . ; K \u0004 1, and plim ^bK ¼ bK þ g½CovðxK; qÞ=VarðxKÞ\u0005 ð4:24Þ [since dK ¼ CovðxK; qÞ=VarðxKÞ in this case]. This formula gives us a simple way to determine the sign, and perhaps the magnitude, of the inconsistency in ^bK. If g > 0 and xK and q are positively correlated, the asymptotic bias is positive. The other combinations are easily worked out. If xK has substantial variation in the population relative to the covariance between xK and q, then the bias can be small. In the general case of equation (4.23), it is di‰cult to sign dK because it measures a partial correla- tion. It is for this reason that dj ¼ 0, j ¼ 1; . . . ; K \u0004 1 is often maintained for deter- mining the likely asymptotic bias in ^bK when only xK is endogenous. Chapter 4 62", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 79, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p80::c0", "text": "Example 4.2 (Wage Equation with Unobserved Ability): Write a structural wage equation explicitly as logðwageÞ ¼ b0 þ b1exper þ b2exper2 þ b3educ þ g abil þ v where v has the structural error property Eðv j exper; educ; abilÞ ¼ 0. If abil is uncor- related with exper and exper2 once educ has been partialed out—that is, abil ¼ d0 þ d3educ þ r with r uncorrelated with exper and exper2—then plim ^b3 ¼ b3 þ gd3. Un- der these assumptions the coe‰cients on exper and exper2 are consistently estimated by the OLS regression that omits ability. If d3 > 0 then plim ^b3 > b3 (because g > 0 by deﬁnition), and the return to education is likely to be overestimated in large samples. 4.3.2 The Proxy Variable–OLS Solution Omitted variables bias can be eliminated, or at least mitigated, if a proxy variable is available for the unobserved variable q. There are two formal requirements for a proxy variable for q. The ﬁrst is that the proxy variable should be redundant (some- times called ignorable) in the structural equation. If z is a proxy variable for q, then the most natural statement of redundancy of z in equation (4.18) is Eðy j x; q; zÞ ¼ Eðy j x; qÞ ð4:25Þ Condition (4.25) is easy to interpret: z is irrelevant for explaining y, in a conditional mean sense, once x and q have been controlled for. This assumption on a proxy variable is virtually always made (sometimes only implicitly), and it is rarely contro- versial: the only reason we bother with z in the ﬁrst place is that we cannot get data on q. Anyway, we cannot get very far without condition (4.25). In the wage-education example, let q be ability and z be IQ score. By deﬁnition it is ability that a¤ects wage: IQ would not matter if true ability were known. Condition (4.25) is somewhat stronger than needed when unobservables appear additively as in equation (4.18); it su‰ces to assume that v in equation (4.19) is simply uncorrelated with z. But we will focus on condition (4.25) because it is natu- ral, and because we need it to cover models where q interacts with some observed covariates. The second requirement of a good proxy variable is more complicated. We require that the correlation between the omitted variable q and each xj be zero once we par- tial out z. This is easily stated in terms of a linear projection: Lðq j 1; x1; . . . ; xK; zÞ ¼ Lðq j 1; zÞ ð4:26Þ It is also helpful to see this relationship in terms of an equation with an unobserved error. Write q as a linear function of z and an error term as The Single-Equation Linear Model and OLS Estimation 63", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 80, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p81::c0", "text": "q ¼ y0 þ y1z þ r ð4:27Þ where, by deﬁnition, EðrÞ ¼ 0 and Covðz; rÞ ¼ 0 because y0 þ y1z is the linear pro- jection of q on 1, z. If z is a reasonable proxy for q, y1 0 0 (and we usually think in terms of y1 > 0). But condition (4.26) assumes much more: it is equivalent to Covðxj; rÞ ¼ 0; j ¼ 1; 2; . . . ; K This condition requires z to be closely enough related to q so that once it is included in equation (4.27), the xj are not partially correlated with q. Before showing why these two proxy variable requirements do the trick, we should head o¤ some possible confusion. The deﬁnition of proxy variable here is not uni- versal. While a proxy variable is always assumed to satisfy the redundancy condition (4.25), it is not always assumed to have the second property. In Chapter 5 we will use the notion of an indicator of q, which satisﬁes condition (4.25) but not the second proxy variable assumption. To obtain an estimable equation, replace q in equation (4.19) with equation (4.27) to get y ¼ ðb0 þ gy0Þ þ b1x1 þ \u0001 \u0001 \u0001 þ bKxK þ gy1z þ ðgr þ vÞ ð4:28Þ Under the assumptions made, the composite error term u 1 gr þ v is uncorrelated with xj for all j; redundancy of z in equation (4.18) means that z is uncorrelated with v and, by deﬁnition, z is uncorrelated with r. It follows immediately from Theorem 4.1 that the OLS regression y on 1; x1; x2; . . . ; xK, z produces consistent estimators of ðb0 þ gy0Þ; b1; b2; . . . ; bK, and gy1. Thus, we can estimate the partial e¤ect of each of the xj in equation (4.18) under the proxy variable assumptions. When z is an imperfect proxy, then r in equation (4.27) is correlated with one or more of the xj. Generally, when we do not impose condition (4.26) and write the linear projection as q ¼ y0 þ r1x1 þ \u0001 \u0001 \u0001 þ rKxK þ y1z þ r the proxy variable regression gives plim ^bj ¼ bj þ grj. Thus, OLS with an imperfect proxy is inconsistent. The hope is that the rj are smaller in magnitude than if z were omitted from the linear projection, and this can usually be argued if z is a reasonable proxy for q. If including z induces substantial collinearity, it might be better to use OLS with- out the proxy variable. However, in making these decisions we must recognize that including z reduces the error variance if y1 0 0: Varðgr þ vÞ < Varðgq þ vÞ because VarðrÞ < VarðqÞ, and v is uncorrelated with both r and q. Including a proxy variable can actually reduce asymptotic variances as well as mitigate bias. Chapter 4 64", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 81, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p82::c0", "text": "Example 4.3 (Using IQ as a Proxy for Ability): We apply the proxy variable method to the data on working men in NLS80.RAW, which was used by Blackburn and Neumark (1992), to estimate the structural model logðwageÞ ¼ b0 þ b1exper þ b2tenure þ b3married þ b4south þ b5urban þ b6black þ b7educ þ g abil þ v ð4:29Þ where exper is labor market experience, married is a dummy variable equal to unity if married, south is a dummy variable for the southern region, urban is a dummy vari- able for living in an SMSA, black is a race indicator, and educ is years of schooling. We assume that IQ satisﬁes the proxy variable assumptions: in the linear projection abil ¼ y0 þ y1IQ þ r, where r has zero mean and is uncorrelated with IQ, we also assume that r is uncorrelated with experience, tenure, education, and other factors appearing in equation (4.29). The estimated equations without and with IQ are logð^wageÞ ¼ 5:40 ð0:11Þ þ :014 ð:003Þ exper þ :012 ð:002Þ tenure þ :199 ð:039Þ married \u0004 :091 ð:026Þ south þ :184 ð:027Þ urban \u0004 :188 ð:038Þ black þ :065 ð:006Þ educ N ¼ 935; R2 ¼ :253 logð^wageÞ ¼ 5:18 ð0:13Þ þ :014 ð:003Þ exper þ :011 ð:002Þ tenure þ :200 ð:039Þ married \u0004 :080 ð:026Þ south þ :182 ð:027Þ urban \u0004 :143 ð:039Þ black þ :054 ð:007Þ educ þ :0036 ð:0010Þ IQ N ¼ 935; R2 ¼ :263 Notice how the return to schooling has fallen from about 6.5 percent to about 5.4 percent when IQ is added to the regression. This is what we expect to happen if ability and schooling are (partially) positively correlated. Of course, these are just the ﬁndings from one sample. Adding IQ explains only one percentage point more of the variation in logðwageÞ, and the equation predicts that 15 more IQ points (one standard deviation) increases wage by about 5.4 percent. The standard error on the return to education has increased, but the 95 percent conﬁdence interval is still fairly tight. The Single-Equation Linear Model and OLS Estimation 65", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 82, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p83::c0", "text": "Often the outcome of the dependent variable from an earlier time period can be a useful proxy variable. Example 4.4 (E¤ects of Job Training Grants on Worker Productivity): The data in JTRAIN1.RAW are for 157 Michigan manufacturing ﬁrms for the years 1987, 1988, and 1989. These data are from Holzer, Block, Cheatham, and Knott (1993). The goal is to determine the e¤ectiveness of job training grants on ﬁrm productivity. For this exercise, we use only the 54 ﬁrms in 1988 which reported nonmissing values of the scrap rate (number of items out of 100 that must be scrapped). No ﬁrms were awarded grants in 1987; in 1988, 19 of the 54 ﬁrms were awarded grants. If the training grant has the intended e¤ect, the average scrap rate should be lower among ﬁrms receiving a grant. The problem is that the grants were not randomly assigned: whether or not a ﬁrm received a grant could be related to other factors unobservable to the econometrician that a¤ect productivity. In the simplest case, we can write (for the 1988 cross section) logðscrapÞ ¼ b0 þ b1grant þ gq þ v where v is orthogonal to grant but q contains unobserved productivity factors that might be correlated with grant, a binary variable equal to unity if the ﬁrm received a job training grant. Since we have the scrap rate in the previous year, we can use logðscrap\u00041Þ as a proxy variable for q: q ¼ y0 þ y1 logðscrap\u00041Þ þ r where r has zero mean and, by deﬁnition, is uncorrelated with logðscrap\u00041Þ. We hope that r has no or little correlation with grant. Plugging in for q gives the estimable model logðscrapÞ ¼ d0 þ b1grant þ gy1 logðscrap\u00041Þ þ r þ v From this equation, we see that b1 measures the proportionate di¤erence in scrap rates for two ﬁrms having the same scrap rates in the previous year, but where one ﬁrm received a grant and the other did not. This is intuitively appealing. The esti- mated equations are logðs^crapÞ ¼ :409 ð:240Þ þ :057 ð:406Þ grant N ¼ 54; R2 ¼ :0004 logðs^crapÞ ¼ :021 ð:089Þ \u0004 :254 ð:147Þ grant þ :831 ð:044Þ logðscrap\u00041Þ N ¼ 54; R2 ¼ :873 Chapter 4 66", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 83, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p84::c0", "text": "Without the lagged scrap rate, we see that the grant appears, if anything, to reduce productivity (by increasing the scrap rate), although the coe‰cient is statistically in- signiﬁcant. When the lagged dependent variable is included, the coe‰cient on grant changes signs, becomes economically large—ﬁrms awarded grants have scrap rates about 25.4 percent less than those not given grants—and the e¤ect is signiﬁcant at the 5 percent level against a one-sided alternative. [The more accurate estimate of the percentage e¤ect is 100 \u0001 ½expð\u0004:254Þ \u0004 1\u0005 ¼ \u000422:4%; see Problem 4.1(a).] We can always use more than one proxy for xK. For example, it might be that Eðq j x; z1; z2Þ ¼ Eðq j z1; z2Þ ¼ y0 þ y1z1 þ y2z2, in which case including both z1 and z2 as regressors along with x1; . . . ; xK solves the omitted variable problem. The weaker condition that the error r in the equation q ¼ y0 þ y1z1 þ y2z2 þ r is uncor- related with x1; . . . ; xK also su‰ces. The data set NLS80.RAW also contains each man’s score on the knowledge of the world of work (KWW ) test. Problem 4.11 asks you to reestimate equation (4.29) when KWW and IQ are both used as proxies for ability. 4.3.3 Models with Interactions in Unobservables In some cases we might be concerned about interactions between unobservables and observable explanatory variables. Obtaining consistent estimators is more di‰cult in this case, but a good proxy variable can again solve the problem. Write the structural model with unobservable q as y ¼ b0 þ b1x1 þ \u0001 \u0001 \u0001 þ bKxK þ g1q þ g2xKq þ v ð4:30Þ where we make a zero conditional mean assumption on the structural error v: Eðv j x; qÞ ¼ 0 ð4:31Þ For simplicity we have interacted q with only one explanatory variable, xK. Before discussing estimation of equation (4.30), we should have an interpretation for the parameters in this equation, as the interaction xKq is unobservable. (We dis- cussed this topic more generally in Section 2.2.5.) If xK is an essentially continuous variable, the partial e¤ect of xK on Eðy j x; qÞ is qEðy j x; qÞ qxK ¼ bK þ g2q ð4:32Þ Thus, the partial e¤ect of xK actually depends on the level of q. Because q is not observed for anyone in the population, equation (4.32) can never be estimated, even if we could estimate g2 (which we cannot, in general). But we can average equation The Single-Equation Linear Model and OLS Estimation 67", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 84, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p85::c0", "text": "(4.32) across the population distribution of q. Assuming EðqÞ ¼ 0, the average partial e¤ect (APE ) of xK is EðbK þ g2qÞ ¼ bK ð4:33Þ A similar interpretation holds for discrete xK. For example, if xK is binary, then Eðy j x1; . . . ; xK\u00041; 1; qÞ \u0004 Eðy j x1; . . . ; xK\u00041; 0; qÞ ¼ bK þ g2q, and bK is the average of this di¤erence over the distribution of q. In this case, bK is called the average treatment e¤ect (ATE). This name derives from the case where xK represents receiv- ing some ‘‘treatment,’’ such as participation in a job training program or partici- pation in an income maintenence program. We will consider the binary treatment case further in Chapter 18, where we introduce a counterfactual framework for esti- mating average treatment e¤ects. It turns out that the assumption EðqÞ ¼ 0 is without loss of generality. Using sim- ple algebra we can show that, if mq 1 EðqÞ 0 0, then we can consistently estimate bK þ g2mq, which is the average partial e¤ect. If the elements of x are exogenous in the sense that Eðq j xÞ ¼ 0, then we can con- sistently estimate each of the bj by an OLS regression, where q and xKq are just part of the error term. This result follows from iterated expectations applied to equation (4.30), which shows that Eðy j xÞ ¼ b0 þ b1x1 þ \u0001 \u0001 \u0001 þ bKxK if Eðq j xÞ ¼ 0. The resulting equation probably has heteroskedasticity, but this is easily dealt with. Inci- dentally, this is a case where only assuming that q and x are uncorrelated would not be enough to ensure consistency of OLS: xKq and x can be correlated even if q and x are uncorrelated. If q and x are correlated, we can consistently estimate the bj by OLS if we have a suitable proxy variable for q. We still assume that the proxy variable, z, satisﬁes the redundancy condition (4.25). In the current model we must make a stronger proxy variable assumption than we did in Section 4.3.2: Eðq j x; zÞ ¼ Eðq j zÞ ¼ y1z ð4:34Þ where now we assume z has a zero mean in the population. Under these two proxy variable assumptions, iterated expectations gives Eðy j x; zÞ ¼ b0 þ b1x1 þ \u0001 \u0001 \u0001 þ bKxK þ g1y1z þ g2y1xKz ð4:35Þ and the parameters are consistently estimated by OLS. If we do not deﬁne our proxy to have zero mean in the population, then estimating equation (4.35) by OLS does not consistently estimate bK. If EðzÞ 0 0, then we would have to write Eðq j zÞ ¼ y0 þ y1z, in which case the coe‰cient on xK in equation (4.35) would be bK þ y0g2. In practice, we may not know the population mean of the Chapter 4 68", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 85, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p86::c0", "text": "proxy variable, in which case the proxy variable should be demeaned in the sample before interacting it with xK. If we maintain homoskedasticity in the structural model—that is, Varðy j x; q; zÞ ¼ Varðy j x; qÞ ¼ s2—then there must be heteroskedasticity in Varðy j x; zÞ. Using Property CV.3 in Appendix 2A, it can be shown that Varðy j x; zÞ ¼ s2 þ ðg1 þ g2xKÞ2 Varðq j x; zÞ Even if Varðq j x; zÞ is constant, Varðy j x; zÞ depends on xK. This situation is most easily dealt with by computing heteroskedasticity-robust statistics, which allows for heteroskedasticity of arbitrary form. Example 4.5 (Return to Education Depends on Ability): Consider an extension of the wage equation (4.29): logðwageÞ ¼ b0 þ b1exper þ b2tenure þ b3married þ b4south þ b5urban þ b6black þ b7educ þ g1abil þ g2educ\u0001abil þ v ð4:36Þ so that educ and abil have separate e¤ects but also have an interactive e¤ect. In this model the return to a year of schooling depends on abil: b7 þ g2abil. Normalizing abil to have zero population mean, we see that the average of the return to education is simply b7. We estimate this equation under the assumption that IQ is redundant in equation (4.36) and Eðabil j x; IQÞ ¼ Eðabil j IQÞ ¼ y1ðIQ \u0004 100Þ 1 y1IQ0, where IQ0 is the population-demeaned IQ (IQ is constructed to have mean 100 in the pop- ulation). We can estimate the bj in equation (4.36) by replacing abil with IQ0 and educ\u0001abil with educ\u0001IQ0 and doing OLS. Using the sample of men in NLS80.RAW gives the following: logð^wageÞ ¼ \u0001 \u0001 \u0001 þ :052 ð:007Þ educ \u0004 :00094 ð:00516Þ IQ0 þ :00034 ð:00038Þ educ \u0001 IQ0 N ¼ 935; R2 ¼ :263 where the usual OLS standard errors are reported (if g2 ¼ 0, homoskedasticity may be reasonable). The interaction term educ\u0001IQ0 is not statistically signiﬁcant, and the return to education at the average IQ, 5.2 percent, is similar to the estimate when the return to education is assumed to be constant. Thus there is little evidence for an in- teraction between education and ability. Incidentally, the F test for joint signiﬁcance of IQ0 and educ\u0001IQ0 yields a p-value of about .0011, but the interaction term is not needed. The Single-Equation Linear Model and OLS Estimation 69", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 86, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p87::c0", "text": "In this case, we happen to know the population mean of IQ, but in most cases we will not know the population mean of a proxy variable. Then, we should use the sample average to demean the proxy before interacting it with xK; see Problem 4.8. Technically, using the sample average to estimate the population average should be reﬂected in the OLS standard errors. But, as you are asked to show in Problem 6.10 in Chapter 6, the adjustments generally have very small impacts on the standard errors and can safely be ignored. In his study on the e¤ects of computer usage on the wage structure in the United States, Krueger (1993) uses computer usage at home as a proxy for unobservables that might be correlated with computer usage at work; he also includes an interaction between the two computer usage dummies. Krueger does not demean the ‘‘uses computer at home’’ dummy before constructing the interaction, so his estimate on ‘‘uses a computer at work’’ does not have an average treatment e¤ect interpreta- tion. However, just as in Example 4.5, Krueger found that the interaction term is insigniﬁcant. 4.4 Properties of OLS under Measurement Error As we saw in Section 4.1, another way that endogenous explanatory variables can arise in economic applications occurs when one or more of the variables in our model contains measurement error. In this section, we derive the consequences of measure- ment error for ordinary least squares estimation. The measurement error problem has a statistical structure similar to the omitted variable–proxy variable problem discussed in the previous section. However, they are conceptually very di¤erent. In the proxy variable case, we are looking for a variable that is somehow associated with the unobserved variable. In the measurement error case, the variable that we do not observe has a well-deﬁned, quantitative meaning (such as a marginal tax rate or annual income), but our measures of it may contain error. For example, reported annual income is a measure of actual annual income, whereas IQ score is a proxy for ability. Another important di¤erence between the proxy variable and measurement error problems is that, in the latter case, often the mismeasured explanatory variable is the one whose e¤ect is of primary interest. In the proxy variable case, we cannot estimate the e¤ect of the omitted variable. Before we turn to the analysis, it is important to remember that measurement error is an issue only when the variables on which we can collect data di¤er from the vari- ables that inﬂuence decisions by individuals, families, ﬁrms, and so on. For example, Chapter 4 70", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 87, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p88::c0", "text": "suppose we are estimating the e¤ect of peer group behavior on teenage drug usage, where the behavior of one’s peer group is self-reported. Self-reporting may be a mis- measure of actual peer group behavior, but so what? We are probably more inter- ested in the e¤ects of how a teenager perceives his or her peer group. 4.4.1 Measurement Error in the Dependent Variable We begin with the case where the dependent variable is the only variable measured with error. Let y\u0002 denote the variable (in the population, as always) that we would like to explain. For example, y\u0002 could be annual family saving. The regression model has the usual linear form y\u0002 ¼ b0 þ b1x1 þ \u0001 \u0001 \u0001 þ bKxK þ v ð4:37Þ and we assume that it satisﬁes at least Assumptions OLS.1 and OLS.2. Typically, we are interested in Eðy\u0002 j x1; . . . ; xKÞ. We let y represent the observable measure of y\u0002 where y 0 y\u0002. The population measurement error is deﬁned as the di¤erence between the ob- served value and the actual value: e0 ¼ y \u0004 y\u0002 ð4:38Þ For a random draw i from the population, we can write ei0 ¼ yi \u0004 y\u0002 i , but what is important is how the measurement error in the population is related to other factors. To obtain an estimable model, we write y\u0002 ¼ y \u0004 e0, plug this into equation (4.37), and rearrange: y ¼ b0 þ b1x1 þ \u0001 \u0001 \u0001 þ bKxK þ v þ e0 ð4:39Þ Since y; x1; x2; . . . ; xK are observed, we can estimate this model by OLS. In e¤ect, we just ignore the fact that y is an imperfect measure of y\u0002 and proceed as usual. When does OLS with y in place of y\u0002 produce consistent estimators of the bj? Since the original model (4.37) satisﬁes Assumption OLS.1, v has zero mean and is uncorrelated with each xj. It is only natural to assume that the measurement error has zero mean; if it does not, this fact only a¤ects estimation of the intercept, b0. Much more important is what we assume about the relationship between the mea- surement error e0 and the explanatory variables xj. The usual assumption is that the measurement error in y is statistically independent of each explanatory variable, which implies that e0 is uncorrelated with x. Then, the OLS estimators from equation (4.39) are consistent (and possibly unbiased as well). Further, the usual OLS infer- ence procedures (t statistics, F statistics, LM statistics) are asymptotically valid under appropriate homoskedasticity assumptions. The Single-Equation Linear Model and OLS Estimation 71", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 88, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p89::c0", "text": "If e0 and v are uncorrelated, as is usually assumed, then Varðv þ e0Þ ¼ s2 v þ s2 0 > s2 v. Therefore, measurement error in the dependent variable results in a larger error variance than when the dependent variable is not measured with error. This result is hardly surprising and translates into larger asymptotic variances for the OLS esti- mators than if we could observe y\u0002. But the larger error variance violates none of the assumptions needed for OLS estimation to have its desirable large-sample properties. Example 4.6 (Saving Function with Measurement Error): Consider a saving function Eðsav\u0002 j inc; size; educ; ageÞ ¼ b0 þ b1inc þ b2size þ b3educ þ b4age but where actual saving ðsav\u0002Þ may deviate from reported saving (sav). The question is whether the size of the measurement error in sav is systematically related to the other variables. It may be reasonable to assume that the measurement error is not correlated with inc, size, educ, and age, but we might expect that families with higher incomes, or more education, report their saving more accurately. Unfortunately, without more information, we cannot know whether the measurement error is cor- related with inc or educ. When the dependent variable is in logarithmic form, so that logðy\u0002Þ is the depen- dent variable, a natural measurement error equation is logðyÞ ¼ logðy\u0002Þ þ e0 ð4:40Þ This follows from a multiplicative measurement error for y: y ¼ y\u0002a0 where a0 > 0 and e0 ¼ logða0Þ. Example 4.7 (Measurement Error in Firm Scrap Rates): In Example 4.4, we might think that the ﬁrm scrap rate is mismeasured, leading us to postulate the model logðscrap\u0002Þ ¼ b0 þ b1grant þ v, where scrap\u0002 is the true scrap rate. The measurement error equation is logðscrapÞ ¼ logðscrap\u0002Þ þ e0. Is the measurement error e0 inde- pendent of whether the ﬁrm receives a grant? Not if a ﬁrm receiving a grant is more likely to underreport its scrap rate in order to make it look as if the grant had the intended e¤ect. If underreporting occurs, then, in the estimable equation logðscrapÞ ¼ b0 þ b1grant þ v þ e0, the error u ¼ v þ e0 is negatively correlated with grant. This result would produce a downward bias in b1, tending to make the training program look more e¤ective than it actually was. These examples show that measurement error in the dependent variable can cause biases in OLS if the measurement error is systematically related to one or more of the explanatory variables. If the measurement error is uncorrelated with the explanatory variables, OLS is perfectly appropriate. Chapter 4 72", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 89, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p90::c0", "text": "4.4.2 Measurement Error in an Explanatory Variable Traditionally, measurement error in an explanatory variable has been considered a much more important problem than measurement error in the response variable. This point was suggested by Example 4.2, and in this subsection we develop the general case. We consider the model with a single explanatory measured with error: y ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKx\u0002 K þ v ð4:41Þ where y; x1; . . . ; xK\u00041 are observable but x\u0002 K is not. We assume at a minimum that v has zero mean and is uncorrelated with x1; x2; . . . ; xK\u00041, x\u0002 K; in fact, we usually have in mind the structural model Eðy j x1; . . . ; xK\u00041; x\u0002 KÞ ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKx\u0002 K. If x\u0002 K were observed, OLS estimation would produce consistent estimators. Instead, we have a measure of x\u0002 K; call it xK. A maintained assumption is that v is also uncorrelated with xK. This follows under the redundancy assumption Eðy j x1; . . . ; xK\u00041; x\u0002 K; xKÞ ¼ Eðy j x1; . . . ; xK\u00041; x\u0002 KÞ, an assumption we used in the proxy variable solution to the omitted variable problem. This means that xK has no e¤ect on y once the other explanatory variables, including x\u0002 K, have been con- trolled for. Since x\u0002 K is assumed to be the variable that a¤ects y, this assumption is uncontroversial. The measurement error in the population is simply eK ¼ xK \u0004 x\u0002 K ð4:42Þ and this can be positive, negative, or zero. We assume that the average measurement error in the population is zero: EðeKÞ ¼ 0, which has no practical consequences be- cause we include an intercept in equation (4.41). Since v is assumed to be uncorre- lated with x\u0002 K and xK, v is also uncorrelated with eK. We want to know the properties of OLS if we simply replace x\u0002 K with xK and run the regression of y on 1; x1; x2; . . . ; xK. These depend crucially on the assumptions we make about the measurement error. An assumption that is almost always maintained is that eK is uncorrelated with the explanatory variables not measured with error: EðxjeKÞ ¼ 0, j ¼ 1; . . . ; K \u0004 1. The key assumptions involve the relationship between the measurement error and x\u0002 K and xK. Two assumptions have been the focus in the econometrics literature, and these represent polar extremes. The ﬁrst assumption is that eK is uncorrelated with the observed measure, xK: CovðxK; eKÞ ¼ 0 ð4:43Þ The Single-Equation Linear Model and OLS Estimation 73", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 90, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p91::c0", "text": "From equation (4.42), if assumption (4.43) is true, then eK must be correlated with the unobserved variable x\u0002 K. To determine the properties of OLS in this case, we write x\u0002 K ¼ xK \u0004 eK and plug this into equation (4.41): y ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKxK þ ðv \u0004 bKeKÞ ð4:44Þ Now, we have assumed that v and eK both have zero mean and are uncorrelated with each xj, including xK; therefore, v \u0004 bKeK has zero mean and is uncorrelated with the xj. It follows that OLS estimation with xK in place of x\u0002 K produces consistent esti- mators of all of the bj (assuming the standard rank condition Assumption OLS.2). Since v is uncorrelated with eK, the variance of the error in equation (4.44) is Varðv \u0004 bKeKÞ ¼ s2 v þ b2 Ks2 eK. Therefore, except when bK ¼ 0, measurement error increases the error variance, which is not a surprising ﬁnding and violates none of the OLS assumptions. The assumption that eK is uncorrelated with xK is analogous to the proxy variable assumption we made in the Section 4.3.2. Since this assumption implies that OLS has all its nice properties, this is not usually what econometricians have in mind when referring to measurement error in an explanatory variable. The classical errors-in- variables (CEV ) assumption replaces assumption (4.43) with the assumption that the measurement error is uncorrelated with the unobserved explanatory variable: Covðx\u0002 K; eKÞ ¼ 0 ð4:45Þ This assumption comes from writing the observed measure as the sum of the true explanatory variable and the measurement error, xK ¼ x\u0002 K þ eK, and then assuming the two components of xK are uncorrelated. (This has nothing to do with assump- tions about v; we are always maintaining that v is uncorrelated with x\u0002 K and xK, and therefore with eK.) If assumption (4.45) holds, then xK and eK must be correlated: CovðxK; eKÞ ¼ EðxKeKÞ ¼ Eðx\u0002 KeKÞ þ Eðe2 KÞ ¼ s2 eK ð4:46Þ Thus, under the CEV assumption, the covariance between xK and eK is equal to the variance of the measurement error. Looking at equation (4.44), we see that correlation between xK and eK causes problems for OLS. Because v and xK are uncorrelated, the covariance between xK and the composite error v \u0004 bKeK is CovðxK; v \u0004 bKeKÞ ¼ \u0004bK CovðxK; eKÞ ¼ \u0004bKs2 eK. It follows that, in the CEV case, the OLS regression of y on x1; x2; . . . ; xK generally gives inconsistent estimators of all of the bj. Chapter 4 74", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 91, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p92::c0", "text": "The plims of the ^bj for j 0 K are di‰cult to characterize except under special assumptions. If x\u0002 K is uncorrelated with xj, all j 0 K, then so is xK, and it follows that plim ^bj ¼ bj, all j 0 K. The plim of ^bK can be characterized in any case. Problem 4.10 asks you to show that plimð ^bKÞ ¼ bK s2 r\u0002 K s2 r \u0002 K þ s2 eK ! ð4:47Þ where r\u0002 K is the linear projection error in x\u0002 K ¼ d0 þ d1x1 þ d2x2 þ \u0001 \u0001 \u0001 þ dK\u00041xK\u00041 þ r\u0002 K An important implication of equation (4.47) is that, because the term multiplying bK is always between zero and one, jplimð ^bKÞj < jbKj. This is called the attenuation bias in OLS due to classical errors-in-variables: on average (or in large samples), the esti- mated OLS e¤ect will be attenuated as a result of the presence of classical errors-in- variables. If bK is positive, ^bK will tend to underestimate bK; if bK is negative, ^bK will tend to overestimate bK. In the case of a single explanatory variable (K ¼ 1) measured with error, equation (4.47) becomes plim ^b1 ¼ b1 s2 x \u0002 1 s2 x \u0002 1 þ s2 e1 ! ð4:48Þ The term multiplying b1 in equation (4.48) is Varðx\u0002 1Þ=Varðx1Þ, which is always less than unity under the CEV assumption (4.45). As Varðe1Þ shrinks relative to Varðx\u0002 1Þ, the attentuation bias disappears. In the case with multiple explanatory variables, equation (4.47) shows that it is not s2 x\u0002 K that a¤ects plimð ^bKÞ but the variance in x\u0002 K after netting out the other explana- tory variables. Thus, the more collinear x\u0002 K is with the other explanatory variables, the worse is the attenuation bias. Example 4.8 (Measurement Error in Family Income): Consider the problem of estimating the causal e¤ect of family income on college grade point average, after controlling for high school grade point average and SAT score: colGPA ¼ b0 þ b1 faminc\u0002 þ b2hsGPA þ b3SAT þ v where faminc\u0002 is actual annual family income. Precise data on colGPA, hsGPA, and SAT are relatively easy to obtain from school records. But family income, especially The Single-Equation Linear Model and OLS Estimation 75", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 92, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p93::c0", "text": "as reported by students, could be mismeasured. If faminc ¼ faminc\u0002 þ e1, and the CEV assumptions hold, then using reported family income in place of actual family income will bias the OLS estimator of b1 toward zero. One consequence is that a hypothesis test of H0: b1 ¼ 0 will have a higher probability of Type II error. If measurement error is present in more than one explanatory variable, deriving the inconsistency in the OLS estimators under extensions of the CEV assumptions is complicated and does not lead to very usable results. In some cases it is clear that the CEV assumption (4.45) cannot be true. For ex- ample, suppose that frequency of marijuana usage is to be used as an explanatory variable in a wage equation. Let smoked \u0002 be the number of days, out of the last 30, that a worker has smoked marijuana. The variable smoked is the self-reported num- ber of days. Suppose we postulate the standard measurement error model, smoked ¼ smoked \u0002 þ e1, and let us even assume that people try to report the truth. It seems very likely that people who do not smoke marijuana at all—so that smoked \u0002 ¼ 0— will also report smoked ¼ 0. In other words, the measurement error is zero for people who never smoke marijuana. When smoked \u0002 > 0 it is more likely that someone mis- counts how many days he or she smoked marijuana. Such miscounting almost cer- tainly means that e1 and smoked \u0002 are correlated, a ﬁnding which violates the CEV assumption (4.45). A general situation where assumption (4.45) is necessarily false occurs when the observed variable xK has a smaller population variance than the unobserved variable x\u0002 K. Of course, we can rarely know with certainty whether this is the case, but we can sometimes use introspection. For example, consider actual amount of schooling versus reported schooling. In many cases, reported schooling will be a rounded-o¤ version of actual schooling; therefore, reported schooling is less variable than actual schooling. Problems 4.1. Consider a standard logðwageÞ equation for men under the assumption that all explanatory variables are exogenous: logðwageÞ ¼ b0 þ b1married þ b2educ þ zg þ u ð4:49Þ Eðu j married; educ; zÞ ¼ 0 where z contains factors other than marital status and education that can a¤ect wage. When b1 is small, 100 \u0001 b1 is approximately the ceteris paribus percentage di¤erence Chapter 4 76", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 93, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p94::c0", "text": "in wages between married and unmarried men. When b1 is large, it is preferable to use the exact percentage di¤erence in Eðwage j married; educ; zÞ. Call this y1. a. Show that, if u is independent of all explanatory variables in equation (4.49), then y1 ¼ 100 \u0001 ½expðb1Þ \u0004 1\u0005. [Hint: Find Eðwage j married; educ; zÞ for married ¼ 1 and married ¼ 0, and ﬁnd the percentage di¤erence.] A natural, consistent, estimator of y1 is ^y1 ¼ 100 \u0001 ½expð ^b1Þ \u0004 1\u0005, where ^b1 is the OLS estimator from equation (4.49). b. Use the delta method (see Section 3.5.2) to show that asymptotic standard error of ^y1 is ½100 \u0001 expð ^b1Þ\u0005 \u0001 seð ^b1Þ. c. Repeat parts a and b by ﬁnding the exact percentage change in Eðwage j married; educ; zÞ for any given change in educ, Deduc. Call this y2. Explain how to estimate y2 and obtain its asymptotic standard error. d. Use the data in NLS80.RAW to estimate equation (4.49), where z contains the remaining variables in equation (4.29) (except ability, of course). Find ^y1 and its standard error; ﬁnd ^y2 and its standard error when Deduc ¼ 4. 4.2. a. Show that, under random sampling and the zero conditional mean as- sumption Eðu j xÞ ¼ 0, Eð ^b j XÞ ¼ b if X0X is nonsingular. (Hint: Use Property CE.5 in the appendix to Chapter 2.) b. In addition to the assumptions from part a, assume that Varðu j xÞ ¼ s2. Show that Varð ^b j XÞ ¼ s2ðX0XÞ\u00041. 4.3. Suppose that in the linear model (4.5), Eðx0uÞ ¼ 0 (where x contains unity), Varðu j xÞ ¼ s2, but Eðu j xÞ 0 EðuÞ. a. Is it true that Eðu2 j xÞ ¼ s2? b. What relevance does part a have for OLS estimation? 4.4. Show that the estimator ^B 1 N\u00041 PN i¼1 ^u2 i x0 ixi is consistent for B ¼ Eðu2x0xÞ by showing that N\u00041 PN i¼1 ^u2 i x0 ixi ¼ N\u00041 PN i¼1 u2 i x0 ixi þ opð1Þ. [Hint: Write ^u2 i ¼ u2 i \u0004 2xiuið ^b \u0004 bÞ þ ½xið ^b \u0004 b\u00052, and use the facts that sample averages are Opð1Þ when expectations exist and that ^b \u0004 b ¼ opð1Þ. Assume that all necessary expectations exist and are ﬁnite.] 4.5. Let y and z be random scalars, and let x be a 1 \u0003 K random vector, where one element of x can be unity to allow for a nonzero intercept. Consider the population model Eðy j x; zÞ ¼ xb þ gz ð4:50Þ Varðy j x; zÞ ¼ s2 ð4:51Þ The Single-Equation Linear Model and OLS Estimation 77", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 94, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p95::c0", "text": "where interest lies in the K \u0003 1 vector b. To rule out trivialities, assume that g 0 0. In addition, assume that x and z are orthogonal in the population: Eðx0zÞ ¼ 0. Consider two estimators of b based on N independent and identically distributed observations: (1) ^b (obtained along with ^g) is from the regression of y on x and z; (2) ~b is from the regression of y on x. Both estimators are consistent for b under equa- tion (4.50) and Eðx0zÞ ¼ 0 (along with the standard rank conditions). a. Show that, without any additional assumptions (except those needed to apply the law of large numbers and central limit theorem), Avar ﬃﬃﬃﬃ N p ð ~b \u0004 bÞ \u0004 Avar ﬃﬃﬃﬃ N p ð ^b \u0004 bÞ is always positive semideﬁnite (and usually positive deﬁnite). Therefore—from the standpoint of asymptotic analysis—it is always better under equations (4.50) and (4.51) to include variables in a regression model that are uncorrelated with the variables of interest. b. Consider the special case where z ¼ ðxK \u0004 mKÞ2, where mK 1 EðxKÞ, and xK is symetrically distributed: E½ðxK \u0004 mKÞ3\u0005 ¼ 0. Then bK is the partial e¤ect of xK on Eðy j xÞ evaluated at xK ¼ mK. Is it better to estimate the average partial e¤ect with or without ðxK \u0004 mKÞ2 included as a regressor? c. Under the setup in Problem 2.3, with Varðy j xÞ ¼ s2, is it better to estimate b1 and b2 with or without x1x2 in the regression? 4.6. Let the variable nonwhite be a binary variable indicating race: nonwhite ¼ 1 if the person is a race other than white. Given that race is determined at birth and is beyond an individual’s control, explain how nonwhite can be an endogenous explan- atory variable in a regression model. In particular, consider the three kinds of endo- geneity discussed in Section 4.1. 4.7. Consider estimating the e¤ect of personal computer ownership, as represented by a binary variable, PC, on college GPA, colGPA. With data on SAT scores and high school GPA you postulate the model colGPA ¼ b0 þ b1hsGPA þ b2SAT þ b3PC þ u a. Why might u and PC be positively correlated? b. If the given equation is estimated by OLS using a random sample of college students, is ^b3 likely to have an upward or downward asymptotic bias? c. What are some variables that might be good proxies for the unobservables in u that are correlated with PC? 4.8. Consider a population regression with two explanatory variables, but where they have an interactive e¤ect and x2 appears as a quadratic: Chapter 4 78", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 95, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p96::c0", "text": "Eðy j x1; x2Þ ¼ b0 þ b1x1 þ b2x2 þ b3x1x2 þ b4x2 2 Let m1 1 Eðx1Þ and m2 1 Eðx2Þ be the population means of the explanatory variables. a. Let a1 denote the average partial e¤ect (across the distribution of the explanatory variables) of x1 on Eðy j x1; x2Þ, and let a2 be the same for x2. Find a1 and a2 in terms of the bj and mj. b. Rewrite the regression function so that a1 and a2 appear directly. (Note that m1 and m2 will also appear.) c. Given a random sample, what regression would you run to estimate a1 and a2 directly? What if you do not know m1 and m2? d. Apply part c to the data in NLS80.RAW, where y ¼ logðwageÞ, x1 ¼ educ, and x2 ¼ exper. (You will have to plug in the sample averages of educ and exper.) Com- pare coe‰cients and standard errors when the interaction term is educ\u0001exper instead, and discuss. 4.9. Consider a linear model where the dependent variable is in logarithmic form, and the lag of logðyÞ is also an explanatory variable: logðyÞ ¼ b0 þ xb þ a1 logðy\u00041Þ þ u; Eðu j x; y\u00041Þ ¼ 0 where the inclusion of logðy\u00041Þ might be to control for correlation between policy variables in x and a previous value of y; see Example 4.4. a. For estimating b, why do we obtain the same estimator if the growth in y, logðyÞ \u0004 logðy\u00041Þ, is used instead as the dependent variable? b. Suppose that there are no covariates x in the equation. Show that, if the dis- tributions of y and y\u00041 are identical, then ja1j < 1. This is the regression-to-the-mean phenomenon in a dynamic setting. {Hint: Show that a1 ¼ Corr½logðyÞ; logðy\u00041Þ\u0005.} 4.10. Use Property LP.7 from Chapter 2 [particularly equation (2.56)] and Problem 2.6 to derive equation (4.47). (Hint: First use Problem 2.6 to show that the popula- tion residual rK, in the linear projection of xK on 1; x1; . . . ; xK\u00041, is r\u0002 K þ eK. Then ﬁnd the projection of y on rK and use Property LP.7.) 4.11. a. In Example 4.3, use KWW and IQ simultaneously as proxies for ability in equation (4.29). Compare the estimated return to education without a proxy for ability and with IQ as the only proxy for ability. b. Test KWW and IQ for joint signiﬁcance in the estimated equation from part a. c. When KWW and IQ are used as proxies for abil, does the wage di¤erential be- tween nonblacks and blacks disappear? What is the estimated di¤erential? The Single-Equation Linear Model and OLS Estimation 79", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 96, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p97::c0", "text": "d. Add the interactions educðIQ \u0004 100Þ and educðKWW \u0004 KWWÞ to the regression from part a, where KWW is the average score in the sample. Are these terms jointly signiﬁcant using a standard F test? Does adding them a¤ect any important con- clusions? 4.12. Redo Example 4.4, adding the variable union—a dummy variable indicat- ing whether the workers at the plant are unionized—as an additional explanatory variable. 4.13. Use the data in CORNWELL.RAW (from Cornwell and Trumball, 1994) to estimate a model of county level crime rates, using the year 1987 only. a. Using logarithms of all variables, estimate a model relating the crime rate to the deterrent variables prbarr, prbconv, prbpris, and avgsen. b. Add logðcrmrteÞ for 1986 as an additional explanatory variable, and comment on how the estimated elasticities di¤er from part a. c. Compute the F statistic for joint signiﬁcance of all of the wage variables (again in logs), using the restricted model from part b. d. Redo part c but make the test robust to heteroskedasticity of unknown form. 4.14. Use the data in ATTEND.RAW to answer this question. a. To determine the e¤ects of attending lecture on ﬁnal exam performance, estimate a model relating stndfnl (the standardized ﬁnal exam score) to atndrte (the percent of lectures attended). Include the binary variables frosh and soph as explanatory vari- ables. Interpret the coe‰cient on atndrte, and discuss its signiﬁcance. b. How conﬁdent are you that the OLS estimates from part a are estimating the causal e¤ect of attendence? Explain. c. As proxy variables for student ability, add to the regression priGPA (prior cumu- lative GPA) and ACT (achievement test score). Now what is the e¤ect of atndrte? Discuss how the e¤ect di¤ers from that in part a. d. What happens to the signiﬁcance of the dummy variables in part c as compared with part a? Explain. e. Add the squares of priGPA and ACT to the equation. What happens to the co- e‰cient on atndrte? Are the quadratics jointly signiﬁcant? f. To test for a nonlinear e¤ect of atndrte, add its square to the equation from part e. What do you conclude? 4.15. Assume that y and each xj have ﬁnite second moments, and write the linear projection of y on ð1; x1; . . . ; xKÞ as Chapter 4 80", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 97, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p98::c0", "text": "y ¼ b0 þ b1x1 þ \u0001 \u0001 \u0001 þ bKxK þ u ¼ b0 þ xb þ u EðuÞ ¼ 0; EðxjuÞ ¼ 0; j ¼ 1; 2; . . . ; K a. Show that s2 y ¼ VarðxbÞ þ s2 u. b. For a random draw i from the population, write yi ¼ b0 þ xib þ ui. Evaluate the following assumption, which has been known to appear in econometrics textbooks: ‘‘VarðuiÞ ¼ s2 ¼ VarðyiÞ for all i.’’ c. Deﬁne the population R-squared by r2 1 1 \u0004 s2 u=s2 y ¼ VarðxbÞ=s2 y. Show that the R-squared, R2 ¼ 1 \u0004 SSR=SST, is a consistent estimator of r2, where SSR is the OLS sum of squared residuals and SST ¼ PN i¼1ðyi \u0004 yÞ2 is the total sum of squares. d. Evaluate the following statement: ‘‘In the presence of heteroskedasticity, the R- squared from an OLS regression is meaningless.’’ (This kind of statement also tends to appear in econometrics texts.) The Single-Equation Linear Model and OLS Estimation 81", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 98, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p99::c0", "text": "5 Instrumental Variables Estimation of Single-Equation Linear Models In this chapter we treat instrumental variables estimation, which is probably second only to ordinary least squares in terms of methods used in empirical economic re- search. The underlying population model is the same as in Chapter 4, but we explic- itly allow the unobservable error to be correlated with the explanatory variables. 5.1 Instrumental Variables and Two-Stage Least Squares 5.1.1 Motivation for Instrumental Variables Estimation To motivate the need for the method of instrumental variables, consider a linear population model y ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKxK þ u ð5:1Þ EðuÞ ¼ 0; Covðxj; uÞ ¼ 0; j ¼ 1; 2; . . . ; K \u0002 1 ð5:2Þ but where xK might be correlated with u. In other words, the explanatory variables x1, x2; . . . ; xK\u00021 are exogenous, but xK is potentially endogenous in equation (5.1). The endogeneity can come from any of the sources we discussed in Chapter 4. To ﬁx ideas it might help to think of u as containing an omitted variable that is uncorrelated with all explanatory variables except xK. So, we may be interested in a conditional expectation as in equation (4.18), but we do not observe q, and q is correlated with xK. As we saw in Chapter 4, OLS estimation of equation (5.1) generally results in in- consistent estimators of all the bj if CovðxK; uÞ 0 0. Further, without more informa- tion, we cannot consistently estimate any of the parameters in equation (5.1). The method of instrumental variables (IV) provides a general solution to the problem of an endogenous explanatory variable. To use the IV approach with xK endogenous, we need an observable variable, z1, not in equation (5.1) that satisﬁes two conditions. First, z1 must be uncorrelated with u: Covðz1; uÞ ¼ 0 ð5:3Þ In other words, like x1; . . . ; xK\u00021, z1 is exogenous in equation (5.1). The second requirement involves the relationship between z1 and the endogenous variable, xK. A precise statement requires the linear projection of xK onto all the exogenous variables: xK ¼ d0 þ d1x1 þ d2x2 þ \u0001 \u0001 \u0001 þ dK\u00021xK\u00021 þ y1z1 þ rK ð5:4Þ where, by deﬁnition of a linear projection error, EðrKÞ ¼ 0 and rK is uncorrelated with x1, x2; . . . ; xK\u00021, and z1. The key assumption on this linear projection is that the", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 99, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p100::c0", "text": "coe‰cient on z1 is nonzero: y1 0 0 ð5:5Þ This condition is often loosely described as ‘‘z1 is correlated with xK,’’ but that statement is not quite correct. The condition y1 0 0 means that z1 is partially corre- lated with xK once the other exogenous variables x1; . . . ; xK\u00021 have been netted out. If xK is the only explanatory variable in equation (5.1), then the linear projection is xK ¼ d0 þ y1z1 þ rK, where y1 ¼ Covðz1; xKÞ=Varðz1Þ, and so condition (5.5) and Covðz1; xKÞ 0 0 are the same. At this point we should mention that we have put no restrictions on the distribu- tion of xK or z1. In many cases xK and z1 will be both essentially continuous, but sometimes xK, z1, or both are discrete. In fact, one or both of xK and z1 can be binary variables, or have continuous and discrete characteristics at the same time. Equation (5.4) is simply a linear projection, and this is always deﬁned when second moments of all variables are ﬁnite. When z1 satisﬁes conditions (5.3) and (5.5), then it is said to be an instrumental variable (IV) candidate for xK. (Sometimes z1 is simply called an instrument for xK.) Because x1; . . . ; xK\u00021 are already uncorrelated with u, they serve as their own instru- mental variables in equation (5.1). In other words, the full list of instrumental vari- ables is the same as the list of exogenous variables, but we often just refer to the instrument for the endogenous explanatory variable. The linear projection in equation (5.4) is called a reduced form equation for the endogenous explanatory variable xK. In the context of single-equation linear models, a reduced form always involves writing an endogenous variable as a linear projection onto all exogenous variables. The ‘‘reduced form’’ terminology comes from simulta- neous equations analysis, and it makes more sense in that context. We use it in all IV contexts because it is a concise way of stating that an endogenous variable has been linearly projected onto the exogenous variables. The terminology also conveys that there is nothing necessarily structural about equation (5.4). From the structural equation (5.1) and the reduced form for xK, we obtain a reduced form for y by plugging equation (5.4) into equation (5.1) and rearranging: y ¼ a0 þ a1x1 þ \u0001 \u0001 \u0001 þ aK\u00021xK\u00021 þ l1z1 þ v ð5:6Þ where v ¼ u þ bKrK is the reduced form error, aj ¼ bj þ bK dj, and l1 ¼ bKy1. By our assumptions, v is uncorrelated with all explanatory variables in equation (5.6), and so OLS consistently estimates the reduced form parameters, the aj and l1. Estimates of the reduced form parameters are sometimes of interest in their own right, but estimating the structural parameters is generally more useful. For example, at the ﬁrm level, suppose that xK is job training hours per worker and y is a measure Chapter 5 84", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 100, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p101::c0", "text": "of average worker productivity. Suppose that job training grants were randomly assigned to ﬁrms. Then it is natural to use for z1 either a binary variable indicating whether a ﬁrm received a job training grant or the actual amount of the grant per worker (if the amount varies by ﬁrm). The parameter bK in equation (5.1) is the e¤ect of job training on worker productivity. If z1 is a binary variable for receiving a job training grant, then l1 is the e¤ect of receiving this particular job training grant on worker productivity, which is of some interest. But estimating the e¤ect of an hour of general job training is more valuable. We can now show that the assumptions we have made on the IV z1 solve the identiﬁcation problem for the bj in equation (5.1). By identiﬁcation we mean that we can write the bj in terms of population moments in observable variables. To see how, write equation (5.1) as y ¼ xb þ u ð5:7Þ where the constant is absorbed into x so that x ¼ ð1; x2; . . . ; xKÞ. Write the 1 \u0003 K vector of all exogenous variables as z 1 ð1; x2; . . . ; xK\u00021; z1Þ Assumptions (5.2) and (5.3) imply the K population orthogonality conditions Eðz0uÞ ¼ 0 ð5:8Þ Multiplying equation (5.7) through by z0, taking expectations, and using equation (5.8) gives ½Eðz0xÞ\u0004b ¼ Eðz0yÞ ð5:9Þ where Eðz0xÞ is K \u0003 K and Eðz0yÞ is K \u0003 1. Equation (5.9) represents a system of K linear equations in the K unknowns b1, b2; . . . ; bK. This system has a unique solution if and only if the K \u0003 K matrix Eðz0xÞ has full rank; that is, rank Eðz0xÞ ¼ K ð5:10Þ in which case the solution is b ¼ ½Eðz0xÞ\u0004\u00021Eðz0yÞ ð5:11Þ The expectations Eðz0xÞ and Eðz0yÞ can be consistently estimated using a random sample on ðx; y; z1Þ, and so equation (5.11) identiﬁes the vector b. It is clear that condition (5.3) was used to obtain equation (5.11). But where have we used condition (5.5)? Let us maintain that there are no linear dependencies among the exogenous variables, so that Eðz0zÞ has full rank K; this simply rules out perfect Instrumental Variables Estimation of Single-Equation Linear Models 85", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 101, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p102::c0", "text": "collinearity in z in the population. Then, it can be shown that equation (5.10) holds if and only if y1 0 0. (A more general case, which we cover in Section 5.1.2, is covered in Problem 5.12.) Therefore, along with the exogeneity condition (5.3), assumption (5.5) is the key identiﬁcation condition. Assumption (5.10) is the rank condition for identiﬁcation, and we return to it more generally in Section 5.2.1. Given a random sample fðxi; yi; zi1Þ: i ¼ 1; 2; . . . ; Ng from the population, the in- strumental variables estimator of b is ^b ¼ N\u00021 X N i¼1 z0 ixi !\u00021 N\u00021 X N i¼1 z0 i yi ! ¼ ðZ0 XÞ\u00021Z0Y where Z and X are N \u0003 K data matrices and Y is the N \u0003 1 data vector on the yi. The consistency of this estimator is immediate from equation (5.11) and the law of large numbers. We consider a more general case in Section 5.2.1. When searching for instruments for an endogenous explanatory variable, con- ditions (5.3) and (5.5) are equally important in identifying b. There is, however, one practically important di¤erence between them: condition (5.5) can be tested, whereas condition (5.3) must be maintained. The reason for this disparity is simple: the covariance in condition (5.3) involves the unobservable u, and therefore we cannot test anything about Covðz1; uÞ. Testing condition (5.5) in the reduced form (5.4) is a simple matter of computing a t test after OLS estimation. Nothing guarantees that rK satisﬁes the requisite homo- skedasticity assumption (Assumption OLS.3), so a heteroskedasticity-robust t statis- tic for ^y1 is often warranted. This statement is especially true if xK is a binary variable or some other variable with discrete characteristics. A word of caution is in order here. Econometricians have been known to say that ‘‘it is not possible to test for identiﬁcation.’’ In the model with one endogenous vari- able and one instrument, we have just seen the sense in which this statement is true: assumption (5.3) cannot be tested. Nevertheless, the fact remains that condition (5.5) can and should be tested. In fact, recent work has shown that the strength of the re- jection in condition (5.5) (in a p-value sense) is important for determining the ﬁnite sample properties, particularly the bias, of the IV estimator. We return to this issue in Section 5.2.6. In the context of omitted variables, an instrumental variable, like a proxy variable, must be redundant in the structural model [that is, the model that explicitly contains the unobservables; see condition (4.25)]. However, unlike a proxy variable, an IV for xK should be uncorrelated with the omitted variable. Remember, we want a proxy variable to be highly correlated with the omitted variable. Chapter 5 86", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 102, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p103::c0", "text": "Example 5.1 (Instrumental Variables for Education in a Wage Equation): Consider a wage equation for the U.S. working population logðwageÞ ¼ b0 þ b1exper þ b2exper2 þ b3educ þ u ð5:12Þ where u is thought to be correlated with educ because of omitted ability, as well as other factors, such as quality of education and family background. Suppose that we can collect data on mother’s education, motheduc. For this to be a valid instrument for educ we must assume that motheduc is uncorrelated with u and that y1 0 0 in the reduced form equation educ ¼ d0 þ d1exper þ d2exper2 þ y1motheduc þ r There is little doubt that educ and motheduc are partially correlated, and this corre- lation is easily tested given a random sample from the population. The potential problem with motheduc as an instrument for educ is that motheduc might be corre- lated with the omitted factors in u: mother’s education is likely to be correlated with child’s ability and other family background characteristics that might be in u. A variable such as the last digit of one’s social security number makes a poor IV candidate for the opposite reason. Because the last digit is randomly determined, it is independent of other factors that a¤ect earnings. But it is also independent of edu- cation. Therefore, while condition (5.3) holds, condition (5.5) does not. By being clever it is often possible to come up with more convincing instruments. Angrist and Krueger (1991) propose using quarter of birth as an IV for education. In the simplest case, let frstqrt be a dummy variable equal to unity for people born in the ﬁrst quarter of the year and zero otherwise. Quarter of birth is arguably independent of unobserved factors such as ability that a¤ect wage (although there is disagreement on this point; see Bound, Jaeger, and Baker, 1995). In addition, we must have y1 0 0 in the reduced form educ ¼ d0 þ d1exper þ d2exper2 þ y1 frstqrt þ r How can quarter of birth be (partially) correlated with educational attainment? Angrist and Krueger (1991) argue that compulsory school attendence laws induce a relationship between educ and frstqrt: at least some people are forced, by law, to at- tend school longer than they otherwise would, and this fact is correlated with quarter of birth. We can determine the strength of this association in a particular sample by estimating the reduced form and obtaining the t statistic for H0: y1 ¼ 0. This example illustrates that it can be very di‰cult to ﬁnd a good instrumental variable for an endogenous explanatory variable because the variable must satisfy Instrumental Variables Estimation of Single-Equation Linear Models 87", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 103, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p104::c0", "text": "two di¤erent, often conﬂicting, criteria. For motheduc, the issue in doubt is whether condition (5.3) holds. For frstqrt, the initial concern is with condition (5.5). Since condition (5.5) can be tested, frstqrt has more appeal as an instrument. However, the partial correlation between educ and frstqrt is small, and this can lead to ﬁnite sample problems (see Section 5.2.6). A more subtle issue concerns the sense in which we are estimating the return to education for the entire population of working people. As we will see in Chapter 18, if the return to education is not constant across people, the IV estimator that uses frstqrt as an IV estimates the return to education only for those people induced to obtain more schooling because they were born in the ﬁrst quarter of the year. These make up a relatively small fraction of the population. Convincing instruments sometimes arise in the context of program evaluation, where individuals are randomly selected to be eligible for the program. Examples include job training programs and school voucher programs. Actual participation is almost always voluntary, and it may be endogenous because it can depend on unob- served factors that a¤ect the response. However, it is often reasonable to assume that eligibility is exogenous. Because participation and eligibility are correlated, the latter can be used as an IV for the former. A valid instrumental variable can also come from what is called a natural experi- ment. A natural experiment occurs when some (often unintended) feature of the setup we are studying produces exogenous variation in an otherwise endogenous explana- tory variable. The Angrist and Krueger (1991) example seems, at least initially, to be a good natural experiment. Another example is given by Angrist (1990), who studies the e¤ect of serving in the Vietnam war on the earnings of men. Participation in the military is not necessarily exogenous to unobserved factors that a¤ect earnings, even after controlling for education, nonmilitary experience, and so on. Angrist used the following observation to obtain an instrumental variable for the binary Vietnam war participation indicator: men with a lower draft lottery number were more likely to serve in the war. Angrist veriﬁes that the probability of serving in Vietnam is indeed related to draft lottery number. Because the lottery number is randomly determined, it seems like an ideal IV for serving in Vietnam. There are, however, some potential problems. It might be that men who were assigned a low lottery number chose to obtain more education as a way of increasing the chance of obtaining a draft defer- ment. If we do not control for education in the earnings equation, lottery number could be endogenous. Further, employers may have been willing to invest in job training for men who are unlikely to be drafted. Again, unless we can include mea- sures of job training in the earnings equation, condition (5.3) may be violated. (This reasoning assumes that we are interested in estimating the pure e¤ect of serving in Vietnam, as opposed to including indirect e¤ects such as reduced job training.) Chapter 5 88", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 104, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p105::c0", "text": "Hoxby (1994) uses topographical features, in particular the natural boundaries created by rivers, as IVs for the concentration of public schools within a school dis- trict. She uses these IVs to estimate the e¤ects of competition among public schools on student performance. Cutler and Glaeser (1997) use the Hoxby instruments, as well as others, to estimate the e¤ects of segregation on schooling and employment outcomes for blacks. Levitt (1997) provides another example of obtaining instrumen- tal variables from a natural experiment. He uses the timing of mayoral and guber- natorial elections as instruments for size of the police force in estimating the e¤ects of police on city crime rates. (Levitt actually uses panel data, something we will discuss in Chapter 11.) Sensible IVs need not come from natural experiments. For example, Evans and Schwab (1995) study the e¤ect of attending a Catholic high school on various out- comes. They use a binary variable for whether a student is Catholic as an IV for attending a Catholic high school, and they spend much e¤ort arguing that religion is exogenous in their versions of equation (5.7). [In this application, condition (5.5) is easy to verify.] Economists often use regional variation in prices or taxes as instru- ments for endogenous explanatory variables appearing in individual-level equations. For example, in estimating the e¤ects of alcohol consumption on performance in college, the local price of alcohol can be used as an IV for alcohol consumption, provided other regional factors that a¤ect college performance have been appropri- ately controlled for. The idea is that the price of alcohol, including any taxes, can be assumed to be exogenous to each individual. Example 5.2 (College Proximity as an IV for Education): Using wage data for 1976, Card (1995) uses a dummy variable that indicates whether a man grew up in the vicinity of a four-year college as an instrumental variable for years of schooling. He also includes several other controls. In the equation with experience and its square, a black indicator, southern and urban indicators, and regional and urban indicators for 1966, the instrumental variables estimate of the return to schooling is .132, or 13.2 percent, while the OLS estimate is 7.5 percent. Thus, for this sample of data, the IV estimate is almost twice as large as the OLS estimate. This result would be counterintuitive if we thought that an OLS analysis su¤ered from an upward omitted variable bias. One interpretation is that the OLS estimators su¤er from the attenuation bias as a result of measurement error, as we discussed in Section 4.4.2. But the classical errors-in-variables assumption for education is questionable. Another interpretation is that the instrumental variable is not exogenous in the wage equation: location is not entirely exogenous. The full set of estimates, including standard errors and t statistics, can be found in Card (1995). Or, you can replicate Card’s results in Problem 5.4. Instrumental Variables Estimation of Single-Equation Linear Models 89", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 105, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p106::c0", "text": "5.1.2 Multiple Instruments: Two-Stage Least Squares Consider again the model (5.1) and (5.2), where xK can be correlated with u. Now, however, assume that we have more than one instrumental variable for xK. Let z1, z2; . . . ; zM be variables such that Covðzh; uÞ ¼ 0; h ¼ 1; 2; . . . ; M ð5:13Þ so that each zh is exogenous in equation (5.1). If each of these has some partial cor- relation with xK, we could have M di¤erent IV estimators. Actually, there are many more than this—more than we can count—since any linear combination of x1, x2; . . . ; xK\u00021, z1, z2; . . . ; zM is uncorrelated with u. So which IV estimator should we use? In Section 5.2.3 we show that, under certain assumptions, the two-stage least squares (2SLS) estimator is the most e‰cient IV estimator. For now, we rely on intuition. To illustrate the method of 2SLS, deﬁne the vector of exogenous variables again by z 1 ð1; x1; x2; . . . ; xK\u00021; z1; . . . ; zMÞ, a 1 \u0003 L vector ðL ¼ K þ MÞ. Out of all possible linear combinations of z that can be used as an instrument for xK, the method of 2SLS chooses that which is most highly correlated with xK. If xK were exogenous, then this choice would imply that the best instrument for xK is simply itself. Ruling this case out, the linear combination of z most highly correlated with xK is given by the linear projection of xK on z. Write the reduced form for xK as xK ¼ d0 þ d1x1 þ \u0001 \u0001 \u0001 þ dK\u00021xK\u00021 þ y1z1 þ \u0001 \u0001 \u0001 þ yMzM þ rK ð5:14Þ where, by deﬁnition, rK has zero mean and is uncorrelated with each right-hand-side variable. As any linear combination of z is uncorrelated with u, x\u0005 K 1 d0 þ d1x1 þ \u0001 \u0001 \u0001 þ dK\u00021xK\u00021 þ y1z1 þ \u0001 \u0001 \u0001 þ yMzM ð5:15Þ is uncorrelated with u. In fact, x\u0005 K is often interpreted as the part of xK that is uncorrelated with u. If xK is endogenous, it is because rK is correlated with u. If we could observe x\u0005 K, we would use it as an instrument for xK in equation (5.1) and use the IV estimator from the previous subsection. Since the dj and yj are pop- ulation parameters, x\u0005 K is not a usable instrument. However, as long as we make the standard assumption that there are no exact linear dependencies among the exoge- nous variables, we can consistently estimate the parameters in equation (5.14) by OLS. The sample analogues of the x\u0005 iK for each observation i are simply the OLS ﬁtted values: ^xiK ¼ ^d0 þ ^d1xi1 þ \u0001 \u0001 \u0001 þ ^dK\u00021xi;K\u00021 þ ^y1zi1 þ \u0001 \u0001 \u0001 þ ^yMziM ð5:16Þ Chapter 5 90", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 106, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p107::c0", "text": "Now, for each observation i, deﬁne the vector ^xi 1 ð1; xi1; . . . ; xi;K\u00021; ^xiKÞ, i ¼ 1; 2; . . . ; N. Using ^xi as the instruments for xi gives the IV estimator ^b ¼ X N i¼1 ^x0 ixi !\u00021 X N i¼1 ^x0 iyi ! ¼ ð^X0 XÞ\u00021 ^X0 Y ð5:17Þ where unity is also the ﬁrst element of xi. The IV estimator in equation (5.17) turns out to be an OLS estimator. To see this fact, note that the N \u0003 ðK þ 1Þ matrix ^X can be expressed as ^X ¼ ZðZ0ZÞ\u00021Z0X ¼ PZX, where the projection matrix PZ ¼ ZðZ0ZÞ\u00021Z0 is idempotent and symmetric. Therefore, ^X0 X ¼ X0PZX ¼ ðPZXÞ0PZX ¼ ^X0 ^X. Plugging this expression into equa- tion (5.17) shows that the IV estimator that uses instruments ^xi can be written as ^b ¼ ð^X0 ^XÞ\u00021 ^X0 Y. The name ‘‘two-stage least squares’’ comes from this procedure. To summarize, ^b can be obtained from the following steps: 1. Obtain the ﬁtted values ^xK from the regression xK on 1; x1; . . . ; xK\u00021; z1; . . . ; zM ð5:18Þ where the i subscript is omitted for simplicity. This is called the ﬁrst-stage regression. 2. Run the OLS regression y on 1; x1; . . . ; xK\u00021; ^xK ð5:19Þ This is called the second-stage regression, and it produces the ^bj. In practice, it is best to use a software package with a 2SLS command rather than explicitly carry out the two-step procedure. Carrying out the two-step procedure explicitly makes one susceptible to harmful mistakes. For example, the following, seemingly sensible, two-step procedure is generally inconsistent: (1) regress xK on 1; z1; . . . ; zM and obtain the ﬁtted values, say ~xK; (2) run the regression in (5.19) with ~xK in place of ^xK. Problem 5.11 asks you to show that omitting x1; . . . ; xK\u00021 in the ﬁrst-stage regression and then explicitly doing the second-stage regression produces inconsistent estimators of the bj. Another reason to avoid the two-step procedure is that the OLS standard errors reported with regression (5.19) will be incorrect, something that will become clear later. Sometimes for hypothesis testing we need to carry out the second-stage regres- sion explicitly—see Section 5.2.4. The 2SLS estimator and the IV estimator from Section 5.1.1 are identical when there is only one instrument for xK. Unless stated otherwise, we mean 2SLS whenever we talk about IV estimation of a single equation. Instrumental Variables Estimation of Single-Equation Linear Models 91", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 107, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p108::c0", "text": "What is the analogue of the condition (5.5) when more than one instrument is available with one endogenous explanatory variable? Problem 5.12 asks you to show that Eðz0xÞ has full column rank if and only if at least one of the yj in equation (5.14) is nonzero. The intuition behind this requirement is pretty clear: we need at least one exogenous variable that does not appear in equation (5.1) to induce variation in xK that cannot be explained by x1; . . . ; xK\u00021. Identiﬁcation of b does not depend on the values of the dh in equation (5.14). Testing the rank condition with a single endogenous explanatory variable and multiple instruments is straightforward. In equation (5.14) we simply test the null hypothesis H0: y1 ¼ 0; y2 ¼ 0; . . . ; yM ¼ 0 ð5:20Þ against the alternative that at least one of the yj is di¤erent from zero. This test gives a compelling reason for explicitly running the ﬁrst-stage regression. If rK in equation (5.14) satisﬁes the OLS homoskedasticity assumption OLS.3, a standard F statistic or Lagrange multiplier statistic can be used to test hypothesis (5.20). Often a hetero- skedasticity-robust statistic is more appropriate, especially if xK has discrete charac- teristics. If we cannot reject hypothesis (5.20) against the alternative that at least one yh is di¤erent from zero, at a reasonably small signiﬁcance level, then we should have serious reservations about the proposed 2SLS procedure: the instruments do not pass a minimal requirement. The model with a single endogenous variable is said to be overidentiﬁed when M > 1 and there are M \u0002 1 overidentifying restrictions. This terminology comes from the fact that, if each zh has some partial correlation with xK, then we have M \u0002 1 more exogenous variables than needed to identify the parameters in equation (5.1). For example, if M ¼ 2, we could discard one of the instruments and still achieve identi- ﬁcation. In Chapter 6 we will show how to test the validity of any overidentifying restrictions. 5.2 General Treatment of 2SLS 5.2.1 Consistency We now summarize asymptotic results for 2SLS in a single-equation model with perhaps several endogenous variables among the explanatory variables. Write the population model as in equation (5.7), where x is 1 \u0003 K and generally includes unity. Several elements of x may be correlated with u. As usual, we assume that a random sample is available from the population. Chapter 5 92", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 108, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p109::c0", "text": "assumption 2SLS.1: For some 1 \u0003 L vector z, Eðz0uÞ ¼ 0. Here we do not specify where the elements of z come from, but any exogenous ele- ments of x, including a constant, are included in z. Unless every element of x is ex- ogenous, z will have to contain variables obtained from outside the model. The zero conditional mean assumption, Eðu j zÞ ¼ 0, implies Assumption 2SLS.1. The next assumption contains the general rank condition for single-equation analysis. assumption 2SLS.2: (a) rank Eðz0zÞ ¼ L; (b) rank Eðz0xÞ ¼ K. Technically, part a of this assumption is needed, but it is not especially important, since the exogenous variables, unless chosen unwisely, will be linearly independent in the population (as well as in a typical sample). Part b is the crucial rank condition for identiﬁcation. In a precise sense it means that z is su‰ciently linearly related to x so that rank Eðz0xÞ has full column rank. We discussed this concept in Section 5.1 for the situation in which x contains a single endogenous variable. When x is exogenous, so that z ¼ x, Assumption 2SLS.1 reduces to Assumption OLS.1 and Assumption 2SLS.2 reduces to Assumption OLS.2. Necessary for the rank condition is the order condition, L b K. In other words, we must have at least as many instruments as we have explanatory variables. If we do not have as many instruments as right-hand-side variables, then b is not identiﬁed. However, L b K is no guarantee that 2SLS.2b holds: the elements of z might not be appropriately correlated with the elements of x. We already know how to test Assumption 2SLS.2b with a single endogenous ex- planatory variable. In the general case, it is possible to test Assumption 2SLS.2b, given a random sample on ðx; zÞ, essentially by performing tests on the sample ana- logue of Eðz0xÞ, Z0 X=N. The tests are somewhat complicated; see, for example Cragg and Donald (1996). Often we estimate the reduced form for each endogenous ex- planatory variable to make sure that at least one element of z not in x is signiﬁcant. This is not su‰cient for the rank condition in general, but it can help us determine if the rank condition fails. Using linear projections, there is a simple way to see how Assumptions 2SLS.1 and 2SLS.2 identify b. First, assuming that Eðz0zÞ is nonsingular, we can always write the linear projection of x onto z as x\u0005 ¼ zP, where P is the L \u0003 K matrix P ¼ ½Eðz0zÞ\u0004\u00021Eðz0xÞ. Since each column of P can be consistently estimated by regressing the appropriate element of x onto z, for the purposes of identiﬁcation of b, we can treat P as known. Write x ¼ x\u0005 þ r, where Eðz0rÞ ¼ 0 and so Eðx\u00050rÞ ¼ 0. Now, the 2SLS estimator is e¤ectively the IV estimator using instruments x\u0005. Multiplying Instrumental Variables Estimation of Single-Equation Linear Models 93", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 109, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p110::c0", "text": "equation (5.7) by x\u00050, taking expectations, and rearranging gives Eðx\u00050xÞb ¼ Eðx\u00050yÞ ð5:21Þ since Eðx\u00050uÞ ¼ 0. Thus, b is identiﬁed by b ¼ ½Eðx\u00050xÞ\u0004\u00021Eðx\u00050yÞ provided Eðx\u00050xÞ is nonsingular. But Eðx\u00050xÞ ¼ P0Eðz0xÞ ¼ Eðx0zÞ½Eðz0zÞ\u0004\u00021Eðz0xÞ and this matrix is nonsingular if and only if Eðz0xÞ has rank K; that is, if and only if Assumption 2SLS.2b holds. If 2SLS.2b fails, then Eðx\u00050xÞ is singular and b is not identiﬁed. [Note that, because x ¼ x\u0005 þ r with Eðx\u00050rÞ ¼ 0, Eðx\u00050xÞ ¼ Eðx\u00050x\u0005Þ. So b is identiﬁed if and only if rank Eðx\u00050x\u0005Þ ¼ K.] The 2SLS estimator can be written as in equation (5.17) or as ^b ¼ X N i¼1 x0 izi ! X N i¼1 z0 izi !\u00021 X N i¼1 z0 ixi ! 2 4 3 5 \u00021 X N i¼1 x0 izi ! X N i¼1 z0 izi !\u00021 X N i¼1 z0 iyi ! ð5:22Þ We have the following consistency result. theorem 5.1 (Consistency of 2SLS): Under Assumptions 2SLS.1 and 2SLS.2, the 2SLS estimator obtained from a random sample is consistent for b. Proof: Write ^b ¼ b þ N\u00021 X N i¼1 x0 izi ! N\u00021 X N i¼1 z0 izi !\u00021 N\u00021 X N i¼1 z0 ixi ! 2 4 3 5 \u00021 \u0001 N\u00021 X N i¼1 x0 izi ! N\u00021 X N i¼1 z0 izi !\u00021 N\u00021 X N i¼1 z0 iui ! and, using Assumptions 2SLS.1 and 2SLS.2, apply the law of large numbers to each term along with Slutsky’s theorem. 5.2.2 Asymptotic Normality of 2SLS The asymptotic normality of ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ follows from the asymptotic normality of N\u00021=2 PN i¼1 z0 iui, which follows from the central limit theorem under Assumption 2SLS.1 and mild ﬁnite second-moment assumptions. The asymptotic variance is simplest under a homoskedasticity assumption: Chapter 5 94", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 110, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p111::c0", "text": "assumption 2SLS.3: Eðu2z0zÞ ¼ s2Eðz0zÞ, where s2 ¼ Eðu2Þ. This assumption is the same as Assumption OLS.3 except that the vector of instru- ments appears in place of x. By the usual LIE argument, su‰cient for Assumption 2SLS.3 is the assumption Eðu2 j zÞ ¼ s2 ð5:23Þ which is the same as Varðu j zÞ ¼ s2 if Eðu j zÞ ¼ 0. [When x contains endogenous elements, it makes no sense to make assumptions about Varðu j xÞ.] theorem 5.2 (Asymptotic Normality of 2SLS): Under Assumptions 2SLS.1–2SLS.3, ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ is asymptotically normally distributed with mean zero and variance matrix s2fEðx0zÞ½Eðz0zÞ\u0004\u00021Eðz0xÞg\u00021 ð5:24Þ The proof of Theorem 5.2 is similar to Theorem 4.2 for OLS and is therefore omitted. The matrix in expression (5.24) is easily estimated using sample averages. To esti- mate s2 we will need appropriate estimates of the ui. Deﬁne the 2SLS residuals as ^ui ¼ yi \u0002 xi ^b; i ¼ 1; 2; . . . ; N ð5:25Þ Note carefully that these residuals are not the residuals from the second-stage OLS regression that can be used to obtain the 2SLS estimates. The residuals from the second-stage regression are yi \u0002 ^xi ^b. Any 2SLS software routine will compute equa- tion (5.25) as the 2SLS residuals, and these are what we need to estimate s2. Given the 2SLS residuals, a consistent (though not unbiased) estimator of s2 under Assumptions 2SLS.1–2SLS.3 is ^s2 1 ðN \u0002 KÞ\u00021 X N i¼1 ^u2 i ð5:26Þ Many regression packages use the degrees of freedom adjustment N \u0002 K in place of N, but this usage does not a¤ect the consistency of the estimator. The K \u0003 K matrix ^s2 X N i¼1 ^x0 i^xi !\u00021 ¼ ^s2ð^X0 ^XÞ\u00021 ð5:27Þ is a valid estimator of the asymptotic variance of ^b under Assumptions 2SLS.1– 2SLS.3. The (asymptotic) standard error of ^bj is just the square root of the jth diag- onal element of matrix (5.27). Asymptotic conﬁdence intervals and t statistics are obtained in the usual fashion. Instrumental Variables Estimation of Single-Equation Linear Models 95", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 111, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p112::c0", "text": "Example 5.3 (Parents’ and Husband’s Education as IVs): We use the data on the 428 working, married women in MROZ.RAW to estimate the wage equation (5.12). We assume that experience is exogenous, but we allow educ to be correlated with u. The instruments we use for educ are motheduc, fatheduc, and huseduc. The reduced form for educ is educ ¼ d0 þ d1exper þ d2exper2 þ y1motheduc þ y2 fatheduc þ y3huseduc þ r Assuming that motheduc, fatheduc, and huseduc are exogenous in the logðwageÞ equation (a tenuous assumption), equation (5.12) is identiﬁed if at least one of y1, y2, and y3 is nonzero. We can test this assumption using an F test (under homoskedas- ticity). The F statistic (with 3 and 422 degrees of freedom) turns out to be 104.29, which implies a p-value of zero to four decimal places. Thus, as expected, educ is fairly strongly related to motheduc, fatheduc, and huseduc. (Each of the three t sta- tistics is also very signiﬁcant.) When equation (5.12) is estimated by 2SLS, we get the following: logð^wageÞ ¼ \u0002:187 ð:285Þ þ :043 ð:013Þ exper \u0002 :00086 ð:00040Þ exper2 þ :080 ð:022Þ educ where standard errors are in parentheses. The 2SLS estimate of the return to educa- tion is about 8 percent, and it is statistically signiﬁcant. For comparison, when equation (5.12) is estimated by OLS, the estimated coe‰cient on educ is about .107 with a standard error of about .014. Thus, the 2SLS estimate is notably below the OLS estimate and has a larger standard error. 5.2.3 Asymptotic E‰ciency of 2SLS The appeal of 2SLS comes from its e‰ciency in a class of IV estimators: theorem 5.3 (Relative E‰ciency of 2SLS): Under Assumptions 2SLS.1–2SLS.3, the 2SLS estimator is e‰cient in the class of all instrumental variables estimators using instruments linear in z. Proof: Let ^b be the 2SLS estimator, and let ~b be any other IV estimator using instruments linear in z. Let the instruments for ~b be ~x 1 zG, where G is an L \u0003 K nonstochastic matrix. (Note that z is the 1 \u0003 L random vector in the population.) We assume that the rank condition holds for ~x. For 2SLS, the choice of IVs is e¤ectively x\u0005 ¼ zP, where P ¼ ½Eðz0zÞ\u0004\u00021Eðz0xÞ 1 D\u00021C. (In both cases, we can re- place G and P with ﬃﬃﬃﬃ N p -consistent estimators without changing the asymptotic vari- ances.) Now, under Assumptions 2SLS.1–2SLS.3, we know the asymptotic variance Chapter 5 96", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 112, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p113::c0", "text": "of ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ is s2½Eðx\u00050x\u0005Þ\u0004\u00021, where x\u0005 ¼ zP. It is straightforward to show that Avar½ ﬃﬃﬃﬃ N p ð ~b \u0002 bÞ\u0004 ¼ s2½Eð~x0xÞ\u0004\u00021½Eð~x0~xÞ\u0004½Eðx0~xÞ\u0004\u00021. To show that Avar½ ﬃﬃﬃﬃ N p ð ~b \u0002 bÞ\u0004 \u0002 Avar½ ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ\u0004 is positive semideﬁnite (p.s.d.), it su‰ces to show that Eðx\u00050x\u0005Þ \u0002 Eðx0~xÞ½Eð~x0~xÞ\u0004\u00021Eð~x0xÞ is p.s.d. But x ¼ x\u0005 þ r, where Eðz0rÞ ¼ 0, and so Eð~x0rÞ ¼ 0. It follows that Eð~x0xÞ ¼ Eð~x0x\u0005Þ, and so Eðx\u00050x\u0005Þ \u0002 Eðx0~xÞ½Eð~x0~xÞ\u0004\u00021Eð~x0xÞ ¼ Eðx\u00050x\u0005Þ \u0002 Eðx\u00050~xÞ½Eð~x0~xÞ\u0004\u00021Eð~x0x\u0005Þ ¼ Eðs\u00050s\u0005Þ where s\u0005 ¼ x\u0005 \u0002 Lðx\u0005 j ~xÞ is the population residual from the linear projection of x\u0005 on ~x. Because Eðs\u00050s\u0005Þ is p.s.d, the proof is complete. Theorem 5.3 is vacuous when L ¼ K because any (nonsingular) choice of G leads to the same estimator: the IV estimator derived in Section 5.1.1. When x is exogenous, Theorem 5.3 implies that, under Assumptions 2SLS.1– 2SLS.3, the OLS estimator is e‰cient in the class of all estimators using instruments linear in exogenous variables z. This statement is true because x is a subset of z and so Lðx j zÞ ¼ x. Another important implication of Theorem 5.3 is that, asymptotically, we always do better by using as many instruments as are available, at least under homo- skedasticity. This conclusion follows because using a subset of z as instruments cor- responds to using a particular linear combination of z. For certain subsets we might achieve the same e‰ciency as 2SLS using all of z, but we can do no better. This ob- servation makes it tempting to add many instruments so that L is much larger than K. Unfortunately, 2SLS estimators based on many overidentifying restrictions can cause ﬁnite sample problems; see Section 5.2.6. Since Assumption 2SLS.3 is assumed for Theorem 5.3, it is not surprising that more e‰cient estimators are available if Assumption 2SLS.3 fails. If L > K, a more e‰cient estimator than 2SLS exists, as shown by Hansen (1982) and White (1982b, 1984). In fact, even if x is exogenous and Assumption OLS.3 holds, OLS is not gen- erally asymptotically e‰cient if, for x H z, Assumptions 2SLS.1 and 2SLS.2 hold but Assumption 2SLS.3 does not. Obtaining the e‰cient estimator falls under the rubric of generalized method of moments estimation, something we cover in Chapter 8. 5.2.4 Hypothesis Testing with 2SLS We have already seen that testing hypotheses about a single bj is straightforward us- ing an asymptotic t statistic, which has an asymptotic normal distribution under the null; some prefer to use the t distribution when N is small. Generally, one should be Instrumental Variables Estimation of Single-Equation Linear Models 97", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 113, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p114::c0", "text": "aware that the normal and t approximations can be poor if N is small. Hypotheses about single linear combinations involving the bj are also easily carried out using a t statistic. The easiest procedure is to deﬁne the linear combination of interest, say y 1 a1b1 þ a2b2 þ \u0001 \u0001 \u0001 þ aKbK, and then to write one of the bj in terms of y and the other elements of b. Then, substitute into the equation of interest so that y appears directly, and estimate the resulting equation by 2SLS to get the standard error of ^y. See Problem 5.9 for an example. To test multiple linear restrictions of the form H0: Rb ¼ r, the Wald statistic is just as in equation (4.13), but with ^V given by equation (5.27). The Wald statistic, as usual, is a limiting null w2 Q distribution. Some econometrics packages, such as Stata=, compute the Wald statistic (actually, its F statistic counterpart, obtained by dividing the Wald statistic by Q) after 2SLS estimation using a simple test command. A valid test of multiple restrictions can be computed using a residual-based method, analogous to the usual F statistic from OLS analysis. Any kind of linear re- striction can be recast as exclusion restrictions, and so we explicitly cover exclusion restrictions. Write the model as y ¼ x1b1 þ x2b2 þ u ð5:28Þ where x1 is 1 \u0003 K1 and x2 is 1 \u0003 K2, and interest lies in testing the K2 restrictions H0: b2 ¼ 0 against H1: b2 0 0 ð5:29Þ Both x1 and x2 can contain endogenous and exogenous variables. Let z denote the L b K1 þ K2 vector of instruments, and we assume that the rank condition for identiﬁcation holds. Justiﬁcation for the following statistic can be found in Wooldridge (1995b). Let ^ui be the 2SLS residuals from estimating the unrestricted model using zi as instruments. Using these residuals, deﬁne the 2SLS unrestricted sum of squared residuals by SSRur 1 X N i¼1 ^u2 i ð5:30Þ In order to deﬁne the F statistic for 2SLS, we need the sum of squared residuals from the second-stage regressions. Thus, let ^xi1 be the 1 \u0003 K1 ﬁtted values from the ﬁrst- stage regression xi1 on zi. Similarly, ^xi2 are the ﬁtted values from the ﬁrst-stage re- gression xi2 on zi. Deﬁne S^SRur as the usual sum of squared residuals from the unrestricted second-stage regression y on ^x1, ^x2. Similarly, S^SRr is the sum of squared residuals from the restricted second-stage regression, y on ^x1. It can be shown that, Chapter 5 98", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 114, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p115::c0", "text": "under H0: b2 ¼ 0 (and Assumptions 2SLS.1–2SLS.3), N \u0001 ðS^SRr \u0002 S^SRurÞ=SSRur @ a w2 K2. It is just as legitimate to use an F-type statistic: F 1 ðS^SRr \u0002 S^SRurÞ SSRur \u0001 ðN \u0002 KÞ K2 ð5:31Þ is distributed approximately as FK2;N\u0002K. Note carefully that S^SRr and S^SRur appear in the numerator of (5.31). These quantities typically need to be computed directly from the second-stage regression. In the denominator of F is SSRur, which is the 2SLS sum of squared residuals. This is what is reported by the 2SLS commands available in popular regression packages. For 2SLS it is important not to use a form of the statistic that would work for OLS, namely, ðSSRr \u0002 SSRurÞ SSRur \u0001 ðN \u0002 KÞ K2 ð5:32Þ where SSRr is the 2SLS restricted sum of squared residuals. Not only does expression (5.32) not have a known limiting distribution, but it can also be negative with positive probability even as the sample size tends to inﬁnity; clearly such a statistic cannot have an approximate F distribution, or any other distribution typically associated with multiple hypothesis testing. Example 5.4 (Parents’ and Husband’s Education as IVs, continued): We add the number of young children (kidslt6) and older children (kidsge6) to equation (5.12) and test for their joint signiﬁcance using the Mroz (1987) data. The statistic in equa- tion (5.31) is F ¼ :31; with two and 422 degrees of freedom, the asymptotic p-value is about .737. There is no evidence that number of children a¤ects the wage for working women. Rather than equation (5.31), we can compute an LM-type statistic for testing hy- pothesis (5.29). Let ~ui be the 2SLS residuals from the restricted model. That is, obtain ~b1 from the model y ¼ x1b1 þ u using instruments z, and let ~ui 1 yi \u0002 xi1 ~b1. Letting ^xi1 and ^xi2 be deﬁned as before, the LM statistic is obtained as NR2 u from the regression ~ui on ^xi1; ^xi2; i ¼ 1; 2; . . . ; N ð5:33Þ where R2 u is generally the uncentered R-squared. (That is, the total sum of squares in the denominator of R-squared is not demeaned.) When f~uig has a zero sample aver- age, the uncentered R-squared and the usual R-squared are the same. This is the case when the null explanatory variables x1 and the instruments z both contain unity, the Instrumental Variables Estimation of Single-Equation Linear Models 99", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 115, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p116::c0", "text": "typical case. Under H0 and Assumptions 2SLS.1–2SLS.3, LM @ a w2 K2. Whether one uses this statistic or the F statistic in equation (5.31) is primarily a matter of taste; asymptotically, there is nothing that distinguishes the two. 5.2.5 Heteroskedasticity-Robust Inference for 2SLS Assumption 2SLS.3 can be restrictive, so we should have a variance matrix estimator that is robust in the presence of heteroskedasticity of unknown form. As usual, we need to estimate B along with A. Under Assumptions 2SLS.1 and 2SLS.2 only, Avarð ^bÞ can be estimated as ð^X0 ^XÞ\u00021 X N i¼1 ^u2 i ^x0 i^xi ! ð^X0 ^XÞ\u00021 ð5:34Þ Sometimes this matrix is multiplied by N=ðN \u0002 KÞ as a degrees-of-freedom adjust- ment. This heteroskedasticity-robust estimator can be used anywhere the estimator ^s2ð^X0 ^XÞ\u00021 is. In particular, the square roots of the diagonal elements of the matrix (5.34) are the heteroskedasticity-robust standard errors for 2SLS. These can be used to construct (asymptotic) t statistics in the usual way. Some packages compute these standard errors using a simple command. For example, using Stata=, rounded to three decimal places the heteroskedasticity-robust standard error for educ in Example 5.3 is .022, which is the same as the usual standard error rounded to three decimal places. The robust standard error for exper is .015, somewhat higher than the non- robust one (.013). Sometimes it is useful to compute a robust standard error that can be computed with any regression package. Wooldridge (1995b) shows how this procedure can be carried out using an auxiliary linear regression for each parameter. Consider com- puting the robust standard error for ^bj. Let ‘‘seð ^bjÞ’’ denote the standard error com- puted using the usual variance matrix (5.27); we put this in quotes because it is no longer appropriate if Assumption 2SLS.3 fails. The ^s is obtained from equation (5.26), and ^ui are the 2SLS residuals from equation (5.25). Let ^rij be the residuals from the regression ^xij on ^xi1; ^xi2; . . . ; ^xi; j\u00021; ^xi; jþ1; . . . ; ^xiK; i ¼ 1; 2; . . . ; N and deﬁne ^mj 1 PN i¼1 ^rij^ui. Then, a heteroskedasticity-robust standard error of ^bj can be tabulated as seð ^bjÞ ¼ ½N=ðN \u0002 KÞ\u00041=2½‘‘seð ^bjÞ’’=^s\u00042=ð ^mjÞ1=2 ð5:35Þ Many econometrics packages compute equation (5.35) for you, but it is also easy to compute directly. Chapter 5 100", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 116, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p117::c0", "text": "To test multiple linear restrictions using the Wald approach, we can use the usual statistic but with the matrix (5.34) as the estimated variance. For example, the heteroskedasticity-robust version of the test in Example 5.4 gives F ¼ :25; asymp- totically, F can be treated as an F2;422 variate. The asymptotic p-value is .781. The Lagrange multiplier test for omitted variables is easily made heteroskedasticity- robust. Again, consider the model (5.28) with the null (5.29), but this time with- out the homoskedasticity assumptions. Using the notation from before, let ^ri 1 ð^ri1; ^ri2; . . . ; ^riK2Þ be the 1 \u0003 K2 vectors of residuals from the multivariate regression ^xi2 on ^xi1, i ¼ 1; 2; . . . ; N. (Again, this procedure can be carried out by regressing each element of ^xi2 on all of ^xi1.) Then, for each observation, form the 1 \u0003 K2 vector ~ui \u0001 ^ri 1 ð~ui \u0001 ^ri1; . . . ; ~ui \u0001 ^riK2Þ. Then, the robust LM test is N \u0002 SSR0 from the regres- sion 1 on ~ui \u0001 ^ri1; . . . ; ~ui \u0001 ^riK2, i ¼ 1; 2; . . . ; N. Under H0; N \u0002 SSR0 @ a w2 K2. This pro- cedure can be justiﬁed in a manner similar to the tests in the context of OLS. You are referred to Wooldridge (1995b) for details. 5.2.6 Potential Pitfalls with 2SLS When properly applied, the method of instrumental variables can be a powerful tool for estimating structural equations using nonexperimental data. Nevertheless, there are some problems that one can encounter when applying IV in practice. One thing to remember is that, unlike OLS under a zero conditional mean as- sumption, IV methods are never unbiased when at least one explanatory variable is endogenous in the model. In fact, under standard distributional assumptions, the expected value of the 2SLS estimator does not even exist. As shown by Kinal (1980), in the case when all endogenous variables have homoskedastic normal distributions with expectations linear in the exogenous variables, the number of moments of the 2SLS estimator that exist is one less than the number of overidentifying restrictions. This ﬁnding implies that when the number of instruments equals the number of ex- planatory variables, the IV estimator does not have an expected value. This is one reason we rely on large-sample analysis to justify 2SLS. Even in large samples IV methods can be ill-behaved if the instruments are weak. Consider the simple model y ¼ b0 þ b1x1 þ u, where we use z1 as an instrument for x1. Assuming that Covðz1; x1Þ 0 0, the plim of the IV estimator is easily shown to be plim ^b1 ¼ b1 þ Covðz1; uÞ=Covðz1; x1Þ ð5:36Þ When Covðz1; uÞ ¼ 0 we obtain the consistency result from earlier. However, if z1 has some correlation with u, the IV estimator is, not surprisingly, inconsistent. Rewrite equation (5.36) as plim ^b1 ¼ b1 þ ðsu=sx1Þ½Corrðz1; uÞ=Corrðz1; x1Þ\u0004 ð5:37Þ Instrumental Variables Estimation of Single-Equation Linear Models 101", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 117, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p118::c0", "text": "where Corrð\u0001 ; \u0001Þ denotes correlation. From this equation we see that if z1 and u are correlated, the inconsistency in the IV estimator gets arbitrarily large as Corrðz1; x1Þ gets close to zero. Thus seemingly small correlations between z1 and u can cause severe inconsistency—and therefore severe ﬁnite sample bias—if z1 is only weakly correlated with x1. In such cases it may be better to just use OLS, even if we only focus on the inconsistency in the estimators: the plim of the OLS estimator is gen- erally b1 þ ðsu=sx1Þ Corrðx1; uÞ. Unfortunately, since we cannot observe u, we can never know the size of the inconsistencies in IV and OLS. But we should be con- cerned if the correlation between z1 and x1 is weak. Similar considerations arise with multiple explanatory variables and instruments. Another potential problem with applying 2SLS and other IV procedures is that the 2SLS standard errors have a tendency to be ‘‘large.’’ What is typically meant by this statement is either that 2SLS coe‰cients are statistically insigniﬁcant or that the 2SLS standard errors are much larger than the OLS standard errors. Not suprisingly, the magnitudes of the 2SLS standard errors depend, among other things, on the quality of the instrument(s) used in estimation. For the following discussion we maintain the standard 2SLS Assumptions 2SLS.1– 2SLS.3 in the model y ¼ b0 þ b1x1 þ b2x2 þ \u0001 \u0001 \u0001 þ bKxK þ u ð5:38Þ Let ^b be the vector of 2SLS estimators using instruments z. For concreteness, we focus on the asymptotic variance of ^bK. Technically, we should study Avar ﬃﬃﬃﬃ N p ð ^bK \u0002 bKÞ, but it is easier to work with an expression that contains the same information. In particular, we use the fact that Avarð ^bKÞA s2 S^SRK ð5:39Þ where S^SRK is the sum of squared residuals from the regression ^xK on 1; ^x1; . . . ; ^xK\u00021 ð5:40Þ (Remember, if xj is exogenous for any j, then ^xj ¼ xj.) If we replace s2 in regression (5.39) with ^s2, then expression (5.39) is the usual 2SLS variance estimator. For the current discussion we are interested in the behavior of S^SRK. From the deﬁnition of an R-squared, we can write S^SRK ¼ S^STKð1 \u0002 ^R2 KÞ ð5:41Þ where S^STK is the total sum of squares of ^xK in the sample, S^STK ¼ PN i¼1ð^xiK \u0002 ^xKÞ, and ^R2 K is the R-squared from regression (5.40). In the context of OLS, the term Chapter 5 102", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 118, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p119::c0", "text": "ð1 \u0002 ^R2 KÞ in equation (5.41) is viewed as a measure of multicollinearity, whereas S^STK measures the total variation in ^xK. We see that, in addition to traditional multicol- linearity, 2SLS can have an additional source of large variance: the total variation in ^xK can be small. When is S^STK small? Remember, ^xK denotes the ﬁtted values from the regression xK on z ð5:42Þ Therefore, S^STK is the same as the explained sum of squares from the regression (5.42). If xK is only weakly related to the IVs, then the explained sum of squares from regression (5.42) can be quite small, causing a large asymptotic variance for ^bK. If xK is highly correlated with z, then S^STK can be almost as large as the total sum of squares of xK and SSTK, and this fact reduces the 2SLS variance estimate. When xK is exogenous—whether or not the other elements of x are—S^STK ¼ SSTK. While this total variation can be small, it is determined only by the sample variation in fxiK: i ¼ 1; 2; . . . ; Ng. Therefore, for exogenous elements appearing among x, the quality of instruments has no bearing on the size of the total sum of squares term in equation (5.41). This fact helps explain why the 2SLS estimates on exogenous explanatory variables are often much more precise than the coe‰- cients on endogenous explanatory variables. In addition to making the term S^STK small, poor quality of instruments can lead to ^R2 K close to one. As an illustration, consider a model in which xK is the only endog- enous variable and there is one instrument z1 in addition to the exogenous variables ð1; x1; . . . ; xK\u00021Þ. Therefore, z 1 ð1; x1; . . . ; xK\u00021; z1Þ. (The same argument works for multiple instruments.) The ﬁtted values ^xK come from the regression xK on 1; x1; . . . ; xK\u00021; z1 ð5:43Þ Because all other regressors are exogenous (that is, they are included in z), ^R2 K comes from the regression ^xK on 1; x1; . . . ; xK\u00021 ð5:44Þ Now, from basic least squares mechanics, if the coe‰cient on z1 in regression (5.43) is exactly zero, then the R-squared from regression (5.44) is exactly unity, in which case the 2SLS estimator does not even exist. This outcome virtually never happens, but z1 could have little explanatory value for xK once x1; . . . ; xK\u00021 have been controlled for, in which case ^R2 K can be close to one. Identiﬁcation, which only has to do with whether we can consistently estimate b, requires only that z1 appear with nonzero coe‰cient in the population analogue of regression (5.43). But if the explanatory power of z1 is weak, the asymptotic variance of the 2SLS estimator can be quite Instrumental Variables Estimation of Single-Equation Linear Models 103", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 119, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p120::c0", "text": "large. This is another way to illustrate why nonzero correlation between xK and z1 is not enough for 2SLS to be e¤ective: the partial correlation is what matters for the asymptotic variance. As always, we must keep in mind that there are no absolute standards for deter- mining when the denominator of equation (5.39) is ‘‘large enough.’’ For example, it is quite possible that, say, xK and z are only weakly linearly related but the sample size is su‰ciently large so that the term S^STK is large enough to produce a small enough standard error (in the sense that conﬁdence intervals are tight enough to re- ject interesting hypotheses). Provided there is some linear relationship between xK and z in the population, S^STK ! p y as N ! y. Further, in the preceding example, if the coe‰cent y1 on z1 in the population regression (5.4) is di¤erent from zero, then ^R2 K converges in probability to a number less than one; asymptotically, multicol- linearity is not a problem. We are in a di‰cult situation when the 2SLS standard errors are so large that nothing is signiﬁcant. Often we must choose between a possibly inconsistent estima- tor that has relatively small standard errors (OLS) and a consistent estimator that is so imprecise that nothing interesting can be concluded (2SLS). One approach is to use OLS unless we can reject exogeneity of the explanatory variables. We show how to test for endogeneity of one or more explanatory variables in Section 6.2.1. There has been some important recent work on the ﬁnite sample properties of 2SLS that emphasizes the potentially large biases of 2SLS, even when sample sizes seem to be quite large. Remember that the 2SLS estimator is never unbiased (pro- vided one has at least one truly endogenous variable in x). But we hope that, with a very large sample size, we need only weak instruments to get an estimator with small bias. Unfortunately, this hope is not fulﬁlled. For example, Bound, Jaeger, and Baker (1995) show that in the setting of Angrist and Krueger (1991) the 2SLS estimator can be expected to behave quite poorly, an alarming ﬁnding because Angrist and Krueger use 300,000 to 500,000 observations! The problem is that the instruments— representing quarters of birth and various interactions of these with year of birth and state of birth—are very weak, and they are too numerous relative to their contribu- tion in explaining years of education. One lesson is that, even with a very large sample size and zero correlation between the instruments and error, we should not use too many overidentifying restrictions. Staiger and Stock (1997) provide a theoretical analysis of the 2SLS estimator with weak instruments and conclude that, even with large sample sizes, instruments that have small partial correlation with an endogenous explanatory variable can lead to substantial biases in 2SLS. One lesson that comes out of the Staiger-Stock work is Chapter 5 104", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 120, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p121::c0", "text": "that we should always compute the F statistic from the ﬁrst-stage regression (or the t statistic with a single instrumental variable). Staiger and Stock (1997) provide some guidelines about how large this F statistic should be (equivalently, how small the p- value should be) for 2SLS to have acceptable properties. 5.3 IV Solutions to the Omitted Variables and Measurement Error Problems In this section, we brieﬂy survey the di¤erent approaches that have been suggested for using IV methods to solve the omitted variables problem. Section 5.3.2 covers an approach that applies to measurement error as well. 5.3.1 Leaving the Omitted Factors in the Error Term Consider again the omitted variable model y ¼ b0 þ b1x1 þ \u0001 \u0001 \u0001 þ bKxK þ gq þ v ð5:45Þ where q represents the omitted variable and Eðv j x; qÞ ¼ 0. The solution that would follow from Section 5.1.1 is to put q in the error term, and then to ﬁnd instruments for any element of x that is correlated with q. It is useful to think of the instruments satisfying the following requirements: (1) they are redundant in the structural model Eðy j x; qÞ; (2) they are uncorrelated with the omitted variable, q; and (3) they are su‰ciently correlated with the endogenous elements of x (that is, those elements that are correlated with q). Then 2SLS applied to equation (5.45) with u 1 gq þ v pro- duces consistent and asymptotically normal estimators. 5.3.2 Solutions Using Indicators of the Unobservables An alternative solution to the omitted variable problem is similar to the OLS proxy variable solution but requires IV rather than OLS estimation. In the OLS proxy variable solution we assume that we have z1 such that q ¼ y0 þ y1z1 þ r1 where r1 is uncorrelated with z1 (by deﬁnition) and is uncorrelated with x1; . . . ; xK (the key proxy variable assumption). Suppose instead that we have two indicators of q. Like a proxy variable, an indicator of q must be redundant in equation (5.45). The key di¤erence is that an indicator can be written as q1 ¼ d0 þ d1q þ a1 ð5:46Þ where Covðq; a1Þ ¼ 0; Covðx; a1Þ ¼ 0 ð5:47Þ Instrumental Variables Estimation of Single-Equation Linear Models 105", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 121, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p122::c0", "text": "This assumption contains the classical errors-in-variables model as a special case, where q is the unobservable, q1 is the observed measurement, d0 ¼ 0, and d1 ¼ 1, in which case g in equation (5.45) can be identiﬁed. Assumption (5.47) is very di¤erent from the proxy variable assumption. Assuming that d1 0 0—otherwise q1 is not correlated with q—we can rearrange equation (5.46) as q ¼ \u0002ðd0=d1Þ þ ð1=d1Þq1 \u0002 ð1=d1Þa1 ð5:48Þ where the error in this equation, \u0002ð1=d1Þa1, is necessarily correlated with q1; the OLS–proxy variable solution would be inconsistent. To use the indicator assumption (5.47), we need some additional information. One possibility is to have a second indicator of q: q2 ¼ r0 þ r1q þ a2 ð5:49Þ where a2 satisﬁes the same assumptions as a1 and r1 0 0. We still need one more assumption: Covða1; a2Þ ¼ 0 ð5:50Þ This implies that any correlation between q1 and q2 arises through their common dependence on q. Plugging q1 in for q and rearranging gives y ¼ a0 þ xb þ g1q1 þ ðv \u0002 g1a1Þ ð5:51Þ where g1 ¼ g=d1. Now, q2 is uncorrelated with v because it is redundant in equation (5.45). Further, by assumption, q2 is uncorrelated with a1 (a1 is uncorrelated with q and a2). Since q1 and q2 are correlated, q2 can be used as an IV for q1 in equation (5.51). Of course the roles of q2 and q1 can be reversed. This solution to the omitted variables problem is sometimes called the multiple indicator solution. It is important to see that the multiple indicator IV solution is very di¤erent from the IV solution that leaves q in the error term. When we leave q as part of the error, we must decide which elements of x are correlated with q, and then ﬁnd IVs for those elements of x. With multiple indicators for q, we need not know which elements of x are correlated with q; they all might be. In equation (5.51) the elements of x serve as their own instruments. Under the assumptions we have made, we only need an in- strument for q1, and q2 serves that purpose. Example 5.5 (IQ and KWW as Indicators of Ability): We apply the indicator method to the model of Example 4.3, using the 935 observations in NLS80.RAW. In addition to IQ, we have a knowledge of the working world (KWW ) test score. If we Chapter 5 106", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 122, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p123::c0", "text": "write IQ ¼ d0 þ d1abil þ a1, KWW ¼ r0 þ r1abil þ a2, and the previous assumptions are satisﬁed in equation (4.29), then we can add IQ to the wage equation and use KWW as an instrument for IQ. We get logð^wageÞ ¼ 4:59 ð0:33Þ þ :014 ð:003Þ exper þ :010 ð:003Þ tenure þ :201 ð:041Þ married \u0002 :051 ð:031Þ south þ :177 ð:028Þ urban \u0002 :023 ð:074Þ black þ :025 ð:017Þ educ þ :013 ð:005Þ IQ The estimated return to education is about 2.5 percent, and it is not statistically sig- niﬁcant at the 5 percent level even with a one-sided alternative. If we reverse the roles of KWW and IQ, we get an even smaller return to education: about 1.7 percent with a t statistic of about 1.07. The statistical insigniﬁcance is perhaps not too surprising given that we are using IV, but the magnitudes of the estimates are surprisingly small. Perhaps a1 and a2 are correlated with each other, or with some elements of x. In the case of the CEV measurement error model, q1 and q2 are measures of q assumed to have uncorrelated measurement errors. Since d0 ¼ r0 ¼ 0 and d1 ¼ r1 ¼ 1, g1 ¼ g. Therefore, having two measures, where we plug one into the equation and use the other as its instrument, provides consistent estimators of all parameters in the CEV setup. There are other ways to use indicators of an omitted variable (or a single mea- surement in the context of measurement error) in an IV approach. Suppose that only one indicator of q is available. Without further information, the parameters in the structural model are not identiﬁed. However, suppose we have additional variables that are redundant in the structural equation (uncorrelated with v), are uncorrelated with the error a1 in the indicator equation, and are correlated with q. Then, as you are asked to show in Problem 5.7, estimating equation (5.51) using this additional set of variables as instruments for q1 produces consistent estimators. This is the method proposed by Griliches and Mason (1972) and also used by Blackburn and Neumark (1992). Problems 5.1. In this problem you are to establish the algebraic equivalence between 2SLS and OLS estimation of an equation containing an additional regressor. Although the result is completely general, for simplicity consider a model with a single (suspected) endogenous variable: Instrumental Variables Estimation of Single-Equation Linear Models 107", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 123, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p124::c0", "text": "y1 ¼ z1d1 þ a1y2 þ u1 y2 ¼ zp2 þ v2 For notational clarity, we use y2 as the suspected endogenous variable and z as the vector of all exogenous variables. The second equation is the reduced form for y2. Assume that z has at least one more element than z1. We know that one estimator of ðd1; a1Þ is the 2SLS estimator using instruments x. Consider an alternative estimator of ðd1; a1Þ: (a) estimate the reduced form by OLS, and save the residuals ^v2; (b) estimate the following equation by OLS: y1 ¼ z1d1 þ a1y2 þ r1^v2 þ error ð5:52Þ Show that the OLS estimates of d1 and a1 from this regression are identical to the 2SLS estimators. [Hint: Use the partitioned regression algebra of OLS. In particular, if ^y ¼ x1 ^b1 þ x2 ^b2 is an OLS regression, ^b1 can be obtained by ﬁrst regressing x1 on x2, getting the residuals, say €x1, and then regressing y on €x1; see, for example, Davidson and MacKinnon (1993, Section 1.4). You must also use the fact that z1 and ^v2 are orthogonal in the sample.] 5.2. Consider a model for the health of an individual: health ¼ b0 þ b1age þ b2weight þ b3height þ b4male þ b5work þ b6exercise þ u1 ð5:53Þ where health is some quantitative measure of the person’s health, age, weight, height, and male are self-explanatory, work is weekly hours worked, and exercise is the hours of exercise per week. a. Why might you be concerned about exercise being correlated with the error term u1? b. Suppose you can collect data on two additional variables, disthome and distwork, the distances from home and from work to the nearest health club or gym. Discuss whether these are likely to be uncorrelated with u1. c. Now assume that disthome and distwork are in fact uncorrelated with u1, as are all variables in equation (5.53) with the exception of exercise. Write down the reduced form for exercise, and state the conditions under which the parameters of equation (5.53) are identiﬁed. d. How can the identiﬁcation assumption in part c be tested? 5.3. Consider the following model to estimate the e¤ects of several variables, in- cluding cigarette smoking, on the weight of newborns: Chapter 5 108", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 124, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p125::c0", "text": "logðbwghtÞ ¼ b0 þ b1male þ b2 parity þ b3 logð famincÞ þ b4 packs þ u ð5:54Þ where male is a binary indicator equal to one if the child is male; parity is the birth order of this child; faminc is family income; and packs is the average number of packs of cigarettes smoked per day during pregnancy. a. Why might you expect packs to be correlated with u? b. Suppose that you have data on average cigarette price in each woman’s state of residence. Discuss whether this information is likely to satisfy the properties of a good instrumental variable for packs. c. Use the data in BWGHT.RAW to estimate equation (5.54). First, use OLS. Then, use 2SLS, where cigprice is an instrument for packs. Discuss any important di¤er- ences in the OLS and 2SLS estimates. d. Estimate the reduced form for packs. What do you conclude about identiﬁcation of equation (5.54) using cigprice as an instrument for packs? What bearing does this conclusion have on your answer from part c? 5.4. Use the data in CARD.RAW for this problem. a. Estimate a logðwageÞ equation by OLS with educ, exper, exper2, black, south, smsa, reg661 through reg668, and smsa66 as explanatory variables. Compare your results with Table 2, Column (2) in Card (1995). b. Estimate a reduced form equation for educ containing all explanatory variables from part a and the dummy variable nearc4. Do educ and nearc4 have a practically and statistically signiﬁcant partial correlation? [See also Table 3, Column (1) in Card (1995).] c. Estimate the logðwageÞ equation by IV, using nearc4 as an instrument for educ. Compare the 95 percent conﬁdence interval for the return to education with that obtained from part a. [See also Table 3, Column (5) in Card (1995).] d. Now use nearc2 along with nearc4 as instruments for educ. First estimate the reduced form for educ, and comment on whether nearc2 or nearc4 is more strongly related to educ. How do the 2SLS estimates compare with the earlier estimates? e. For a subset of the men in the sample, IQ score is available. Regress iq on nearc4. Is IQ score uncorrelated with nearc4? f. Now regress iq on nearc4 along with smsa66, reg661, reg662, and reg669. Are iq and nearc4 partially correlated? What do you conclude about the importance of controlling for the 1966 location and regional dummies in the logðwageÞ equation when using nearc4 as an IV for educ? Instrumental Variables Estimation of Single-Equation Linear Models 109", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 125, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p126::c0", "text": "5.5. One occasionally sees the following reasoning used in applied work for choos- ing instrumental variables in the context of omitted variables. The model is y1 ¼ z1d1 þ a1y2 þ gq þ a1 where q is the omitted factor. We assume that a1 satisﬁes the structural error as- sumption Eða1 j z1; y2; qÞ ¼ 0, that z1 is exogenous in the sense that Eðq j z1Þ ¼ 0, but that y2 and q may be correlated. Let z2 be a vector of instrumental variable candi- dates for y2. Suppose it is known that z2 appears in the linear projection of y2 onto ðz1; z2Þ, and so the requirement that z2 be partially correlated with y2 is satisﬁed. Also, we are willing to assume that z2 is redundant in the structural equation, so that a1 is uncorrelated with z2. What we are unsure of is whether z2 is correlated with the omitted variable q, in which case z2 would not contain valid IVs. To ‘‘test’’ whether z2 is in fact uncorrelated with q, it has been suggested to use OLS on the equation y1 ¼ z1d1 þ a1y2 þ z2c1 þ u1 ð5:55Þ where u1 ¼ gq þ a1, and test H0: c1 ¼ 0. Why does this method not work? 5.6. Refer to the multiple indicator model in Section 5.3.2. a. Show that if q2 is uncorrelated with xj, j ¼ 1; 2; . . . ; K, then the reduced form of q1 depends only on q2. [Hint: Use the fact that the reduced form of q1 is the linear projection of q1 onto ð1; x1; x2; . . . ; xK; q2Þ and ﬁnd the coe‰cient vector on x using Property LP.7 from Chapter 2.] b. What happens if q2 and x are correlated? In this setting, is it realistic to assume that q2 and x are uncorrelated? Explain. 5.7. Consider model (5.45) where v has zero mean and is uncorrelated with x1; . . . ; xK and q. The unobservable q is thought to be correlated with at least some of the xj. Assume without loss of generality that EðqÞ ¼ 0. You have a single indicator of q, written as q1 ¼ d1q þ a1, d1 0 0, where a1 has zero mean and is uncorrelated with each of xj, q, and v. In addition, z1; z2; . . . ; zM is a set of variables that are (1) redundant in the structural equation (5.45) and (2) uncorrelated with a1. a. Suggest an IV method for consistently estimating the bj. Be sure to discuss what is needed for identiﬁcation. b. If equation (5.45) is a logðwageÞ equation, q is ability, q1 is IQ or some other test score, and z1; . . . ; zM are family background variables, such as parents’ education and Chapter 5 110", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 126, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p127::c0", "text": "number of siblings, describe the economic assumptions needed for consistency of the the IV procedure in part a. c. Carry out this procedure using the data in NLS80.RAW. Include among the ex- planatory variables exper, tenure, educ, married, south, urban, and black. First use IQ as q1 and then KWW. Include in the zh the variables meduc, feduc, and sibs. Discuss the results. 5.8. Consider a model with unobserved heterogeneity (q) and measurement error in an explanatory variable: y ¼ b0 þ b1x1 þ \u0001 \u0001 \u0001 þ bKx\u0005 K þ q þ v where eK ¼ xK \u0002 x\u0005 K is the measurement error and we set the coe‰cient on q equal to one without loss of generality. The variable q might be correlated with any of the explanatory variables, but an indicator, q1 ¼ d0 þ d1q þ a1, is available. The mea- surement error eK might be correlated with the observed measure, xK. In addition to q1, you also have variables z1, z2; . . . ; zM, M b 2, that are uncorrelated with v, a1, and eK. a. Suggest an IV procedure for consistently estimating the bj. Why is M b 2 required? (Hint: Plug in q1 for q and xK for x\u0005 K, and go from there.) b. Apply this method to the model estimated in Example 5.5, where actual educa- tion, say educ\u0005, plays the role of x\u0005 K. Use IQ as the indicator of q ¼ ability, and KWW, meduc, feduc, and sibs as the elements of z. 5.9. Suppose that the following wage equation is for working high school graduates: logðwageÞ ¼ b0 þ b1exper þ b2exper2 þ b3twoyr þ b4 fouryr þ u where twoyr is years of junior college attended and fouryr is years completed at a four-year college. You have distances from each person’s home at the time of high school graduation to the nearest two-year and four-year colleges as instruments for twoyr and fouryr. Show how to rewrite this equation to test H0: b3 ¼ b4 against H0: b4 > b3, and explain how to estimate the equation. See Kane and Rouse (1995) and Rouse (1995), who implement a very similar procedure. 5.10. Consider IV estimation of the simple linear model with a single, possibly endogenous, explanatory variable, and a single instrument: y ¼ b0 þ b1x þ u EðuÞ ¼ 0; Covðz; uÞ ¼ 0; Covðz; xÞ 0 0; Eðu2 j zÞ ¼ s2 Instrumental Variables Estimation of Single-Equation Linear Models 111", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 127, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p128::c0", "text": "a. Under the preceding (standard) assumptions, show that Avar ﬃﬃﬃﬃ N p ð ^b1 \u0002 b1Þ can be expressed as s2=ðr2 zxs2 xÞ, where s2 x ¼ VarðxÞ and rzx ¼ Corrðz; xÞ. Compare this result with the asymptotic variance of the OLS estimator under Assumptions OLS.1–OLS.3. b. Comment on how each factor a¤ects the asymptotic variance of the IV estimator. What happens as rzx ! 0? 5.11. A model with a single endogenous explanatory variable can be written as y1 ¼ z1d1 þ a1y2 þ u1; Eðz0u1Þ ¼ 0 where z ¼ ðz1; z2Þ. Consider the following two-step method, intended to mimic 2SLS: a. Regress y2 on z2, and obtain ﬁtted values, ~y2. (That is, z1 is omitted from the ﬁrst- stage regression.) b. Regress y1 on z1, ~y2 to obtain ~d1 and ~a1. Show that ~d1 and ~a1 are generally in- consistent. When would ~d1 and ~a1 be consistent? [Hint: Let y0 2 be the population linear projection of y2 on z2, and let a2 be the projection error: y0 2 ¼ z2l2 þ a2, Eðz0 2a2Þ ¼ 0. For simplicity, pretend that l2 is known, rather than estimated; that is, assume that ~y2 is actually y0 2. Then, write y1 ¼ z1d1 þ a1y0 2 þ a1a2 þ u1 and check whether the composite error a1a2 þ u1 is uncorrelated with the explanatory variables.] 5.12. In the setup of Section 5.1.2 with x ¼ ðx1; . . . ; xKÞ and z 1 ðx1; x2; . . . ; xK\u00021; z1; . . . ; zMÞ (let x1 ¼ 1 to allow an intercept), assume that Eðz0zÞ is nonsingular. Prove that rank Eðz0xÞ ¼ K if and only if at least one yj in equation (5.15) is di¤erent from zero. [Hint: Write x\u0005 ¼ ðx1; . . . ; xK\u00021; x\u0005 KÞ as the linear projection of each ele- ment of x on z, where x\u0005 K ¼ d1x1 þ \u0001 \u0001 \u0001 þ dK\u00021xK\u00021 þ y1z1 þ \u0001 \u0001 \u0001 þ yMzM. Then x ¼ x\u0005 þ r, where Eðz0rÞ ¼ 0, so that Eðz0xÞ ¼ Eðz0x\u0005Þ. Now x\u0005 ¼ zP, where P is the L \u0003 K matrix whose ﬁrst K \u0002 1 columns are the ﬁrst K \u0002 1 unit vectors in RL— ð1; 0; 0; . . . ; 0Þ0, ð0; 1; 0; . . . ; 0Þ0; . . . ; ð0; 0; . . . ; 1; 0; . . . ; 0Þ0—and whose last column is ðd1; d2; . . . ; dK\u00021; y1; . . . ; yMÞ. Write Eðz0x\u0005Þ ¼ Eðz0zÞP, so that, because Eðz0zÞ is nonsingular, Eðz0x\u0005Þ has rank K if and only if P has rank K.] 5.13. Consider the simple regression model y ¼ b0 þ b1x þ u and let z be a binary instrumental variable for x. a. Show that the IV estimator ^b1 can be written as Chapter 5 112", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 128, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p129::c0", "text": "^b1 ¼ ðy1 \u0002 y0Þ=ðx1 \u0002 x0Þ where y0 and x0 are the sample averages of yi and xi over the part of the sample with zi ¼ 0, and y1 and x1 are the sample averages of yi and xi over the part of the sample with zi ¼ 1. This estimator, known as a grouping estimator, was ﬁrst suggested by Wald (1940). b. What is the intepretation of ^b1 if x is also binary, for example, representing par- ticipation in a social program? 5.14. Consider the model in (5.1) and (5.2), where we have additional exogenous variables z1; . . . ; zM. Let z ¼ ð1; x1; . . . ; xK\u00021; z1; . . . ; zMÞ be the vector of all exoge- nous variables. This problem essentially asks you to obtain the 2SLS estimator using linear projections. Assume that Eðz0zÞ is nonsingular. a. Find Lðy j zÞ in terms of the bj, x1; . . . ; xK\u00021, and x\u0005 K ¼ LðxK j zÞ. b. Argue that, provided x1; . . . ; xK\u00021; x\u0005 K are not perfectly collinear, an OLS regres- sion of y on 1, x1; . . . ; xK\u00021; x\u0005 K —using a random sample—consistently estimates all bj. c. State a necessary and su‰cient condition for x\u0005 K not to be a perfect linear combi- nation of x1; . . . ; xK\u00021. What 2SLS assumption is this identical to? 5.15. Consider the model y ¼ xb þ u, where x1, x2; . . . ; xK1, K1 a K, are the (potentially) endogenous explanatory variables. (We assume a zero intercept just to simplify the notation; the following results carry over to models with an unknown intercept.) Let z1; . . . ; zL1 be the instrumental variables available from outside the model. Let z ¼ ðz1; . . . ; zL1; xK1þ1; . . . ; xKÞ and assume that Eðz0zÞ is nonsingular, so that Assumption 2SLS.2a holds. a. Show that a necessary condition for the rank condition, Assumption 2SLS.2b, is that for each j ¼ 1; . . . ; K1, at least one zh must appear in the reduced form of xj. b. With K1 ¼ 2, give a simple example showing that the condition from part a is not su‰cient for the rank condition. c. If L1 ¼ K1, show that a su‰cient condition for the rank condition is that only zj appears in the reduced form for xj, j ¼ 1; . . . ; K1. [As in Problem 5.12, it su‰ces to study the rank of the L \u0003 K matrix P in Lðx j zÞ ¼ zP.] Instrumental Variables Estimation of Single-Equation Linear Models 113", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 129, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p130::c0", "text": "6 Additional Single-Equation Topics 6.1 Estimation with Generated Regressors and Instruments 6.1.1 OLS with Generated Regressors We often need to draw on results for OLS estimation when one or more of the regressors have been estimated from a ﬁrst-stage procedure. To illustrate the issues, consider the model y ¼ b0 þ b1x1 þ \u0001 \u0001 \u0001 þ bKxK þ gq þ u ð6:1Þ We observe x1; . . . ; xK, but q is unobserved. However, suppose that q is related to observable data through the function q ¼ f ðw; dÞ, where f is a known function and w is a vector of observed variables, but the vector of parameters d is unknown (which is why q is not observed). Often, but not always, q will be a linear function of w and d. Suppose that we can consistently estimate d, and let ^d be the estimator. For each observation i, ^qi ¼ f ðwi; ^dÞ e¤ectively estimates qi. Pagan (1984) calls ^qi a generated regressor. It seems reasonable that, replacing qi with ^qi in running the OLS regression yi on 1; xi1; xi2; . . . ; xik; ^qi; i ¼ 1; . . . ; N ð6:2Þ should produce consistent estimates of all parameters, including g. The question is, What assumptions are su‰cient? While we do not cover the asymptotic theory needed for a careful proof until Chapter 12 (which treats nonlinear estimation), we can provide some intuition here. Because plim ^d ¼ d, by the law of large numbers it is reasonable that N\u00021 X N i¼1 ^qiui ! p EðqiuiÞ; N\u00021 X N i¼1 xij^qi ! p EðxijqiÞ From this relation it is easily shown that the usual OLS assumption in the population— that u is uncorrelated with ðx1; x2; . . . ; xK; qÞ—su‰ces for the two-step procedure to be consistent (along with the rank condition of Assumption OLS.2 applied to the expanded vector of explanatory variables). In other words, for consistency, replacing qi with ^qi in an OLS regression causes no problems. Things are not so simple when it comes to inference: the standard errors and test statistics obtained from regression (6.2) are generally invalid because they ignore the sampling variation in ^d. Since ^d is also obtained using data—usually the same sample of data—uncertainty in the estimate should be accounted for in the second step. Nevertheless, there is at least one important case where the sampling variation of ^d can be ignored, at least asymptotically: if", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 130, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p131::c0", "text": "E½‘d f ðw; dÞ0u\u0003 ¼ 0 ð6:3Þ g ¼ 0 ð6:4Þ then the ﬃﬃﬃﬃ N p -limiting distribution of the OLS estimators from regression (6.2) is the same as the OLS estimators when q replaces ^q. Condition (6.3) is implied by the zero conditional mean condition Eðu j x; wÞ ¼ 0 ð6:5Þ which usually holds in generated regressor contexts. We often want to test the null hypothesis H0: g ¼ 0 before including ^q in the ﬁnal regression. Fortunately, the usual t statistic on ^q has a limiting standard normal dis- tribution under H0, so it can be used to test H0. It simply requires the usual homo- skedasticity assumption, Eðu2 j x; qÞ ¼ s2. The heteroskedasticity-robust statistic works if heteroskedasticity is present in u under H0. Even if condition (6.3) holds, if g 0 0, then an adjustment is needed for the asymptotic variances of all OLS estimators that are due to estimation of d. Thus, standard t statistics, F statistics, and LM statistics will not be asymptotically valid when g 0 0. Using the methods of Chapter 3, it is not di‰cult to derive an ad- justment to the usual variance matrix estimate that accounts for the variability in ^d (and also allows for heteroskedasticity). It is not true that replacing qi with ^qi simply introduces heteroskedasticity into the error term; this is not the correct way to think about the generated regressors issue. Accounting for the fact that ^d depends on the same random sample used in the second-stage estimation is much di¤erent from having heteroskedasticity in the error. Of course, we might want to use a heteroskedasticity-robust standard error for testing H0: g ¼ 0 because heteroskedasticity in the population error u can always be a problem. However, just as with the usual OLS standard error, this is generally justiﬁed only under H0: g ¼ 0. A general formula for the asymptotic variance of 2SLS in the presence of gen- erated regressors is given in the appendix to this chapter; this covers OLS with gen- erated regressors as a special case. A general framework for handling these problems is given in Newey (1984) and Newey and McFadden (1994), but we must hold o¤ until Chapter 14 to give a careful treatment. 6.1.2 2SLS with Generated Instruments In later chapters we will need results on 2SLS estimation when the instruments have been estimated in a preliminary stage. Write the population model as Chapter 6 116", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 131, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p132::c0", "text": "y ¼ xb þ u ð6:6Þ Eðz0uÞ ¼ 0 ð6:7Þ where x is a 1 \u0004 K vector of explanatory variables and z is a 1 \u0004 L ðL b KÞ vector of intrumental variables. Assume that z ¼ gðw; lÞ, where gð\u0001 ; lÞ is a known function but l needs to be estimated. For each i, deﬁne the generated instruments ^zi 1 gðwi; ^lÞ. What can we say about the 2SLS estimator when the ^zi are used as instruments? By the same reasoning for OLS with generated regressors, consistency follows under weak conditions. Further, under conditions that are met in many applications, we can ignore the fact that the instruments were estimated in using 2SLS for infer- ence. Su‰cient are the assumptions that ^l is ﬃﬃﬃﬃ N p -consistent for l and that E½‘lgðw; lÞ0u\u0003 ¼ 0 ð6:8Þ Under condition (6.8), which holds when Eðu j wÞ ¼ 0, the ﬃﬃﬃﬃ N p -asymptotic distribu- tion of ^b is the same whether we use l or ^l in constructing the instruments. This fact greatly simpliﬁes calculation of asymptotic standard errors and test statistics. There- fore, if we have a choice, there are practical reasons for using 2SLS with generated instruments rather than OLS with generated regressors. We will see some examples in Part IV. One consequence of this discussion is that, if we add the 2SLS homoskedasticity assumption (2SLS.3), the usual 2SLS standard errors and test statistics are asymp- totically valid. If Assumption 2SLS.3 is violated, we simply use the heteroskedasticity- robust standard errors and test statistics. Of course, the ﬁnite sample properties of the estimator using ^zi as instruments could be notably di¤erent from those using zi as instruments, especially for small sample sizes. Determining whether this is the case requires either more sophisticated asymptotic approximations or simulations on a case-by-case basis. 6.1.3 Generated Instruments and Regressors We will encounter examples later where some instruments and some regressors are estimated in a ﬁrst stage. Generally, the asymptotic variance needs to be adjusted because of the generated regressors, although there are some special cases where the usual variance matrix estimators are valid. As a general example, consider the model y ¼ xb þ gf ðw; dÞ þ u; Eðu j z; wÞ ¼ 0 and we estimate d in a ﬁrst stage. If g ¼ 0, then the 2SLS estimator of ðb 0; gÞ0 in the equation Additional Single-Equation Topics 117", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 132, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p133::c0", "text": "yi ¼ xib þ g^fi þ errori using instruments ðzi; ^fiÞ, has a limiting distribution that does not depend on the limiting distribution of ﬃﬃﬃﬃ N p ð^d \u0002 dÞ under conditions (6.3) and (6.8). Therefore, the usual 2SLS t statistic for ^g, or its heteroskedsticity-robust version, can be used to test H0: g ¼ 0. 6.2 Some Speciﬁcation Tests In Chapters 4 and 5 we covered what is usually called classical hypothesis testing for OLS and 2SLS. In this section we cover some tests of the assumptions underlying either OLS or 2SLS. These are easy to compute and should be routinely reported in applications. 6.2.1 Testing for Endogeneity We start with the linear model and a single possibly endogenous variable. For nota- tional clarity we now denote the dependent variable by y1 and the potentially endog- enous explanatory variable by y2. As in all 2SLS contexts, y2 can be continuous or binary, or it may have continuous and discrete characteristics; there are no restric- tions. The population model is y1 ¼ z1d1 þ a1y2 þ u1 ð6:9Þ where z1 is 1 \u0004 L1 (including a constant), d1 is L1 \u0004 1, and u1 is the unobserved dis- turbance. The set of all exogenous variables is denoted by the 1 \u0004 L vector z, where z1 is a strict subset of z. The maintained exogeneity assumption is Eðz0u1Þ ¼ 0 ð6:10Þ It is important to keep in mind that condition (6.10) is assumed throughout this section. We also assume that equation (6.9) is identiﬁed when Eðy2u1Þ 0 0, which requires that z have at least one element not in z1 (the order condition); the rank condition is that at least one element of z not in z1 is partially correlated with y2 (after netting out z1). Under these assumptions, we now wish to test the null hypothesis that y2 is actually exogenous. Hausman (1978) suggested comparing the OLS and 2SLS estimators of b1 1 ðd0 1; a1Þ0 as a formal test of endogeneity: if y2 is uncorrelated with u1, the OLS and 2SLS estimators should di¤er only by sampling error. This reasoning leads to the Hausman test for endogeneity. Chapter 6 118", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 133, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p134::c0", "text": "The original form of the statistic turns out to be cumbersome to compute because the matrix appearing in the quadratic form is singular, except when no exogenous variables are present in equation (6.9). As pointed out by Hausman (1978, 1983), there is a regression-based form of the test that turns out to be asymptotically equivalent to the original form of the Hausman test. In addition, it extends easily to other situations, including some nonlinear models that we cover in Chapters 15, 16, and 19. To derive the regression-based test, write the linear projection of y2 on z in error form as y2 ¼ zp2 þ v2 ð6:11Þ Eðz0v2Þ ¼ 0 ð6:12Þ where p2 is L \u0004 1. Since u1 is uncorrelated with z, it follows from equations (6.11) and (6.12) that y2 is endogenous if and only if Eðu1v2Þ 0 0. Thus we can test whether the structural error, u1, is correlated with the reduced form error, v2. Write the linear projection of u1 onto v2 in error form as u1 ¼ r1v2 þ e1 ð6:13Þ where r1 ¼ Eðv2u1Þ=Eðv2 2Þ, Eðv2e1Þ ¼ 0, and Eðz0e1Þ ¼ 0 (since u1 and v2 are each orthogonal to z). Thus, y2 is exogenous if and only if r1 ¼ 0. Plugging equation (6.13) into equation (6.9) gives the equation y1 ¼ z1d1 þ a1y2 þ r1v2 þ e1 ð6:14Þ The key is that e1 is uncorrelated with z1, y2, and v2 by construction. Therefore, a test of H0: r1 ¼ 0 can be done using a standard t test on the variable v2 in an OLS re- gression that includes z1 and y2. The problem is that v2 is not observed. Nevertheless, the reduced form parameters p2 are easily estimated by OLS. Let ^v2 denote the OLS residuals from the ﬁrst-stage reduced form regression of y2 on z—remember that z contains all exogenous variables. If we replace v2 with ^v2 we have the equation y1 ¼ z1d1 þ a1y2 þ r1^v2 þ error ð6:15Þ and d1, a1, and r1 can be consistently estimated by OLS. Now we can use the results on generated regressors in Section 6.1.1: the usual OLS t statistic for ^r1 is a valid test of H0: r1 ¼ 0, provided the homoskedasticity assumption Eðu2 1 j z; y2Þ ¼ s2 1 is sat- isﬁed under H0. (Remember, y2 is exogenous under H0.) A heteroskedasticity-robust t statistic can be used if heteroskedasticity is suspected under H0. Additional Single-Equation Topics 119", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 134, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p135::c0", "text": "As shown in Problem 5.1, the OLS estimates of d1 and a1 from equation (6.15) are in fact identical to the 2SLS estimates. This fact is convenient because, along with being computationally simple, regression (6.15) allows us to compare the magnitudes of the OLS and 2SLS estimates in order to determine whether the di¤erences are practically signiﬁcant, rather than just ﬁnding statistically signiﬁcant evidence of endogeneity of y2. It also provides a way to verify that we have computed the statistic correctly. We should remember that the OLS standard errors that would be reported from equation (6.15) are not valid unless r1 ¼ 0, because ^v2 is a generated regressor. In practice, if we reject H0: r1 ¼ 0, then, to get the appropriate standard errors and other test statistics, we estimate equation (6.9) by 2SLS. Example 6.1 (Testing for Endogeneity of Education in a Wage Equation): Consider the wage equation logðwageÞ ¼ d0 þ d1exper þ d2exper2 þ a1educ þ u1 ð6:16Þ for working women, where we believe that educ and u1 may be correlated. The instruments for educ are parents’ education and husband’s education. So, we ﬁrst regress educ on 1, exper, exper2, motheduc, fatheduc, and huseduc and obtain the residuals, ^v2. Then we simply include ^v2 along with unity, exper, exper2, and educ in an OLS regression and obtain the t statistic on ^v2. Using the data in MROZ.RAW gives the result ^r1 ¼ :047 and t^r1 ¼ 1:65. We ﬁnd evidence of endogeneity of educ at the 10 percent signiﬁcance level against a two-sided alternative, and so 2SLS is probably a good idea (assuming that we trust the instruments). The correct 2SLS standard errors are given in Example 5.3. Rather than comparing the OLS and 2SLS estimates of a particular linear combi- nation of the parameters—as the original Hausman test does—it often makes sense to compare just the estimates of the parameter of interest, which is usually a1. If, under H0, Assumptions 2SLS.1–2SLS.3 hold with w replacing z, where w includes all nonredundant elements in x and z, obtaining the test is straightforward. Under these assumptions it can be shown that Avarð^a1;2SLS \u0002 ^a1;OLSÞ ¼ Avarð^a1;2SLSÞ \u0002 Avarð^a1;OLSÞ. [This conclusion essentially holds because of Theorem 5.3; Problem 6.12 asks you to show this result formally. Hausman (1978), Newey and McFadden (1994, Section 5.3), and Section 14.5.1 contain more general treatments.] Therefore, the Hausman t statistic is simply ð^a1;2SLS \u0002 ^a1;OLSÞ=f½seð^a1;2SLSÞ\u00032 \u0002 ½seð^a1;OLSÞ\u00032g1=2, where the standard errors are the usual ones computed under homoskedasticity. The denominator in the t statistic is the standard error of ð^a1;2SLS \u0002 ^a1;OLSÞ. If there is Chapter 6 120", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 135, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p136::c0", "text": "heteroskedasticity under H0, this standard error is invalid because the asymptotic variance of the di¤erence is no longer the di¤erence in asymptotic variances. Extending the regression-based Hausman test to several potentially endogenous explanatory variables is straightforward. Let y2 denote a 1 \u0004 G1 vector of possible endogenous variables in the population model y1 ¼ z1d1 þ y2a1 þ u1; Eðz0u1Þ ¼ 0 ð6:17Þ where a1 is now G1 \u0004 1. Again, we assume the rank condition for 2SLS. Write the reduced form as y2 ¼ zP2 þ v2, where P2 is L \u0004 G1 and v2 is the 1 \u0004 G1 vector of population reduced form errors. For a generic observation let ^v2 denote the 1 \u0004 G1 vector of OLS residuals obtained from each reduced form. (In other words, take each element of y2 and regress it on z to obtain the RF residuals; then collect these in the row vector ^v2.) Now, estimate the model y1 ¼ z1d1 þ y2a1 þ ^v2r1 þ error ð6:18Þ and do a standard F test of H0: r1 ¼ 0, which tests G1 restrictions in the unrestricted model (6.18). The restricted model is obtained by setting r1 ¼ 0, which means we estimate the original model (6.17) by OLS. The test can be made robust to hetero- skedasticity in u1 (since u1 ¼ e1 under H0) by applying the heteroskedasticity-robust Wald statistic in Chapter 4. In some regression packages, such as Stata=, the robust test is implemented as an F-type test. An alternative to the F test is an LM-type test. Let ^u1 be the OLS residuals from the regression y1 on z1; y2 (the residuals obtained under the null that y2 is exogenous). Then, obtain the usual R-squared (assuming that z1 contains a constant), say R2 u, from the regression ^u1 on z1; y2;^v2 ð6:19Þ and use NR2 u as asymptotically w2 G1. This test again maintains homoskedasticity under H0. The test can be made heteroskedasticity-robust using the method described in equation (4.17): take x1 ¼ ðz1; y2Þ and x2 ¼ ^v2. See also Wooldridge (1995b). Example 6.2 (Endogeneity of Education in a Wage Equation, continued): We add the interaction term black\u0001educ to the log(wage) equation estimated by Card (1995); see also Problem 5.4. Write the model as logðwageÞ ¼ a1educ þ a2 black\u0001educ þ z1d1 þ u1 ð6:20Þ where z1 contains a constant, exper, exper2, black, smsa, 1966 regional dummy vari- ables, and a 1966 SMSA indicator. If educ is correlated with u1, then we also expect Additional Single-Equation Topics 121", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 136, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p137::c0", "text": "black\u0001educ to be correlated with u1. If nearc4, a binary indicator for whether a worker grew up near a four-year college, is valid as an instrumental variable for educ, then a natural instrumental variable for black\u0001educ is black\u0001nearc4. Note that black\u0001nearc4 is uncorrelated with u1 under the conditional mean assumption Eðu1 j zÞ ¼ 0, where z contains all exogenous variables. The equation estimated by OLS is log ^ðwageÞ ¼ 4:81 ð0:75Þ þ :071 ð:004Þ educ þ :018 ð:006Þ black\u0001educ \u0002 :419 ð:079Þ black þ \u0001 \u0001 \u0001 Therefore, the return to education is estimated to be about 1.8 percentage points higher for blacks than for nonblacks, even though wages are substantially lower for blacks at all but unrealistically high levels of education. (It takes an estimated 23.3 years of education before a black worker earns as much as a nonblack worker.) To test whether educ is exogenous we must test whether educ and black\u0001educ are uncorrelated with u1. We do so by ﬁrst regressing educ on all instrumental variables: those elements in z1 plus nearc4 and black\u0001nearc4. (The interaction black\u0001nearc4 should be included because it might be partially correlated with educ.) Let ^v21 be the OLS residuals from this regression. Similarly, regress black\u0001educ on z1, nearc4, and black\u0001nearc4, and save the residuals ^v22. By the way, the fact that the dependent variable in the second reduced form regression, black\u0001educ, is zero for a large fraction of the sample has no bearing on how we test for endogeneity. Adding ^v21 and ^v22 to the OLS regression and computing the joint F test yields F ¼ 0:54 and p-value ¼ 0.581; thus we do not reject exogeneity of educ and black\u0001educ. Incidentally, the reduced form regressions conﬁrm that educ is partially corre- lated with nearc4 (but not black\u0001nearc4) and black\u0001educ is partially correlated with black\u0001nearc4 (but not nearc4). It is easily seen that these ﬁndings mean that the rank condition for 2SLS is satisﬁed—see Problem 5.15c. Even though educ does not ap- pear to be endogenous in equation (6.20), we estimate the equation by 2SLS: log ^ðwageÞ ¼ 3:84 ð0:97Þ þ :127 ð:057Þ educ þ :011 ð:040Þ black\u0001educ \u0002 :283 ð:506Þ black þ \u0001 \u0001 \u0001 The 2SLS point estimates certainly di¤er from the OLS estimates, but the standard errors are so large that the 2SLS and OLS estimates are not statistically di¤erent. 6.2.2 Testing Overidentifying Restrictions When we have more instruments than we need to identify an equation, we can test whether the additional instruments are valid in the sense that they are uncorrelated with u1. To explain the various procedures, write the equation in the form Chapter 6 122", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 137, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p138::c0", "text": "y1 ¼ z1d1 þ y2a1 þ u1 ð6:21Þ where z1 is 1 \u0004 L1 and y2 is 1 \u0004 G1. The 1 \u0004 L vector of all exogenous variables is again z; partition this as z ¼ ðz1; z2Þ where z2 is 1 \u0004 L2 and L ¼ L1 þ L2. Because the model is overidentiﬁed, L2 > G1. Under the usual identiﬁcation conditions we could use any 1 \u0004 G1 subset of z2 as instruments for y2 in estimating equation (6.21) (re- member the elements of z1 act as their own instruments). Following his general principle, Hausman (1978) suggested comparing the 2SLS estimator using all instru- ments to 2SLS using a subset that just identiﬁes equation (6.21). If all instruments are valid, the estimates should di¤er only as a result of sampling error. As with testing for endogeneity, constructing the original Hausman statistic is computationally cumber- some. Instead, a simple regression-based procedure is available. It turns out that, under homoskedasticity, a test for validity of the overidentiﬁ- cation restrictions is obtained as NR2 u from the OLS regression ^u1 on z ð6:22Þ where ^u1 are the 2SLS residuals using all of the instruments z and R2 u is the usual R- squared (assuming that z1 and z contain a constant; otherwise it is the uncentered R- squared). In other words, simply estimate regression (6.21) by 2SLS and obtain the 2SLS residuals, ^u1. Then regress these on all exogenous variables (including a con- stant). Under the null that Eðz0u1Þ ¼ 0 and Assumption 2SLS.3, NR2 u @ a w2 Q1, where Q1 1 L2 \u0002 G1 is the number of overidentifying restrictions. The usefulness of the Hausman test is that, if we reject the null hypothesis, then our logic for choosing the IVs must be reexamined. If we fail to reject the null, then we can have some conﬁdence in the overall set of instruments used. Of course, it could also be that the test has low power for detecting endogeneity of some of the instruments. A heteroskedasticity-robust version is a little more complicated but is still easy to obtain. Let ^y2 denote the ﬁtted values from the ﬁrst-stage regressions (each element of y2 onto z). Now, let h2 be any 1 \u0004 Q1 subset of z2. (It does not matter which elements of z2 we choose, as long as we choose Q1 of them.) Regress each element of h2 onto ðz1; ^y2Þ and collect the residuals, ^r2 ð1 \u0004 Q1Þ. Then an asymptotic w2 Q1 test statistic is obtained as N \u0002 SSR0 from the regression 1 on ^u1^r2. The proof that this method works is very similar to that for the heteroskedasticity-robust test for exclusion restrictions. See Wooldridge (1995b) for details. Example 6.3 (Overidentifying Restrictions in the Wage Equation): In estimating equation (6.16) by 2SLS, we used (motheduc, fatheduc, huseduc) as instruments for educ. Therefore, there are two overidentifying restrictions. Letting ^u1 be the 2SLS residuals from equation (6.16) using all instruments, the test statistic is N times the R- squared from the OLS regression Additional Single-Equation Topics 123", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 138, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p139::c0", "text": "^u1 on 1; exper; exper2; motheduc; fatheduc; huseduc Under H0 and homoskedasticity, NR2 u @ a w2 2. Using the data on working women in MROZ.RAW gives R2 u ¼ :0026, and so the overidentiﬁcation test statistic is about 1.11. The p-value is about .574, so the overidentifying restrictions are not rejected at any reasonable level. For the heteroskedasticity-robust version, one approach is to obtain the residuals, ^r1 and ^r2, from the OLS regressions motheduc on 1, exper, exper2, and e ^duc and fatheduc on 1, exper, exper2, and e ^duc, where e ^duc are the ﬁrst-stage ﬁtted values from the regression educ on 1, exper, exper2, motheduc, fatheduc, and huseduc. Then obtain N \u0002 SSR from the OLS regression 1 on ^u1 \u0001 ^r1, ^u1\u0001 ^r2. Using only the 428 observations on working women to obtain ^r1 and ^r2, the value of the robust test sta- tistic is about 1.04 with p-value ¼ :595, which is similar to the p-value for the non- robust test. 6.2.3 Testing Functional Form Sometimes we need a test with power for detecting neglected nonlinearities in models estimated by OLS or 2SLS. A useful approach is to add nonlinear functions, such as squares and cross products, to the original model. This approach is easy when all explanatory variables are exogenous: F statistics and LM statistics for exclusion restrictions are easily obtained. It is a little tricky for models with endogenous ex- planatory variables because we need to choose instruments for the additional non- linear functions of the endogenous variables. We postpone this topic until Chapter 9 when we discuss simultaneous equation models. See also Wooldridge (1995b). Putting in squares and cross products of all exogenous variables can consume many degrees of freedom. An alternative is Ramsey’s (1969) RESET, which has degrees of freedom that do not depend on K. Write the model as y ¼ xb þ u ð6:23Þ Eðu j xÞ ¼ 0 ð6:24Þ [You should convince yourself that it makes no sense to test for functional form if we only assume that Eðx0uÞ ¼ 0. If equation (6.23) deﬁnes a linear projection, then, by deﬁnition, functional form is not an issue.] Under condition (6.24) we know that any function of x is uncorrelated with u (hence the previous suggestion of putting squares and cross products of x as additional regressors). In particular, if condition (6.24) holds, then ðxbÞp is uncorrelated with u for any integer p. Since b is not observed, we replace it with the OLS estimator, ^b. Deﬁne ^yi ¼ xi ^b as the OLS ﬁtted values and ^ui as the OLS residuals. By deﬁnition of OLS, the sample covariance between ^ui and ^yi is zero. But we can test whether the ^ui are su‰ciently correlated with low-order poly- Chapter 6 124", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 139, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p140::c0", "text": "nomials in ^yi, say ^y2 i , ^y3 i , and ^y4 i , as a test for neglected nonlinearity. There are a couple of ways to do so. Ramsey suggests adding these terms to equation (6.23) and doing a standard F test [which would have an approximate F3;N\u0002K\u00023 distribution under equation (6.23) and the homoskedasticity assumption Eðu2 j xÞ ¼ s2]. Another possibility is to use an LM test: Regress ^ui onto xi, ^y2 i , ^y3 i , and ^y4 i and use N times the R-squared from this regression as w2 3. The methods discussed in Chapter 4 for obtaining heteroskedasticity-robust statistics can be applied here as well. Ramsey’s test uses generated regressors, but the null is that each generated regressor has zero population coe‰cient, and so the usual limit theory applies. (See Section 6.1.1.) There is some misunderstanding in the testing literature about the merits of RESET. It has been claimed that RESET can be used to test for a multitude of speciﬁcation problems, including omitted variables and heteroskedasticity. In fact, RESET is generally a poor test for either of these problems. It is easy to write down models where an omitted variable, say q, is highly correlated with each x, but RESET has the same distribution that it has under H0. A leading case is seen when Eðq j xÞ is linear in x. Then Eðy j xÞ is linear in x [even though Eðy j xÞ 0 Eðy j x; qÞ\u0003, and the asymptotic power of RESET equals its asymptotic size. See Wooldridge (1995b) and Problem 6.4a. The following is an empirical illustration. Example 6.4 (Testing for Neglected Nonlinearities in a Wage Equation): We use OLS and the data in NLS80.RAW to estimate the equation from Example 4.3: logðwageÞ ¼ b0 þ b1exper þ b2tenure þ b3married þ b4south þ b5urban þ b6black þ b7educ þ u The null hypothesis is that the expected value of u given the explanatory variables in the equation is zero. The R-squared from the regression ^u on x, ^y2, and ^y3 yields R2 u ¼ :0004, so the chi-square statistic is .374 with p-valueA:83. (Adding ^y4 only increases the p-value.) Therefore, RESET provides no evidence of functional form misspeciﬁcation. Even though we already know IQ shows up very signiﬁcantly in the equation (t statistic ¼ 3.60—see Example 4.3), RESET does not, and should not be expected to, detect the omitted variable problem. It can only test whether the expected value of y given the variables actually in the regression is linear in those variables. 6.2.4 Testing for Heteroskedasticity As we have seen for both OLS and 2SLS, heteroskedasticity does not a¤ect the con- sistency of the estimators, and it is only a minor nuisance for inference. Nevertheless, sometimes we want to test for the presence of heteroskedasticity in order to justify use Additional Single-Equation Topics 125", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 140, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p141::c0", "text": "of the usual OLS or 2SLS statistics. If heteroskedasticity is present, more e‰cient estimation is possible. We begin with the case where the explanatory variables are exogenous in the sense that u has zero mean given x: y ¼ b0 þ xb þ u; Eðu j xÞ ¼ 0 The reason we do not assume the weaker assumption Eðx0uÞ ¼ 0 is that the fol- lowing class of tests we derive—which encompasses all of the widely used tests for heteroskedasticity—are not valid unless Eðu j xÞ ¼ 0 is maintained under H0. Thus we maintain that the mean Eðy j xÞ is correctly speciﬁed, and then we test the con- stant conditional variance assumption. If we do not assume correct speciﬁcation of Eðy j xÞ, a signiﬁcant heteroskedasticity test might just be detecting misspeciﬁed functional form in Eðy j xÞ; see Problem 6.4c. Because Eðu j xÞ ¼ 0, the null hypothesis can be stated as H0: Eðu2 j xÞ ¼ s2. Under the alternative, Eðu2 j xÞ depends on x in some way. Thus it makes sense to test H0 by looking at covariances Cov½hðxÞ; u2\u0003 ð6:25Þ for some 1 \u0004 Q vector function hðxÞ. Under H0, the covariance in expression (6.25) should be zero for any choice of hð\u0001Þ. Of course a general way to test zero correlation is to use a regression. Putting i subscripts on the variables, write the model u2 i ¼ d0 þ hid þ vi ð6:26Þ where hi 1 hðxiÞ; we make the standard rank assumption that VarðhiÞ has rank Q, so that there is no perfect collinearity in hi. Under H0, Eðvi j hiÞ ¼ Eðvi j xiÞ ¼ 0, d ¼ 0, and d0 ¼ s2. Thus we can apply an F test or an LM test for the null H0: d ¼ 0 in equation (6.26). One thing to notice is that vi cannot have a normal distribution under H0: because vi ¼ u2 i \u0002 s2; vi b \u0002s2. This does not matter for asymptotic anal- ysis; the OLS regression from equation (6.26) gives a consistent, ﬃﬃﬃﬃ N p -asymptotically normal estimator of d whether or not H0 is true. But to apply a standard F or LM test, we must assume that, under H0, Eðv2 i j xiÞ is constant: that is, the errors in equation (6.26) are homoskedastic. In terms of the original error ui, this assumption implies that Eðu4 i j xiÞ ¼ constant 1 k2 ð6:27Þ under H0. This is called the homokurtosis (constant conditional fourth moment) as- sumption. Homokurtosis always holds when u is independent of x, but there are Chapter 6 126", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 141, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p142::c0", "text": "conditional distributions for which Eðu j xÞ ¼ 0 and Varðu j xÞ ¼ s2 but Eðu4 j xÞ depends on x. As a practical matter, we cannot test d ¼ 0 in equation (6.26) directly because ui is not observed. Since ui ¼ yi \u0002 xib and we have a consistent estimator of b, it is natu- ral to replace u2 i with ^u2 i , where the ^ui are the OLS residuals for observation i. Doing this step and applying, say, the LM principle, we obtain NR2 c from the regression ^u2 i on 1; hi; i ¼ 1; 2; . . . ; N ð6:28Þ where R2 c is just the usual centered R-squared. Now, if the u2 i were used in place of the ^u2 i , we know that, under H0 and condition (6.27), NR2 c @ a w2 Q, where Q is the di- mension of hi. What adjustment is needed because we have estimated u2 i ? It turns out that, be- cause of the structure of these tests, no adjustment is needed to the asymptotics. (This statement is not generally true for regressions where the dependent variable has been estimated in a ﬁrst stage; the current setup is special in that regard.) After tedious algebra, it can be shown that N\u00021=2 X N i¼1 h0 ið^u2 i \u0002 ^s2Þ ¼ N\u00021=2 X N i¼1 ðhi \u0002 mhÞ0ðu2 i \u0002 s2Þ þ opð1Þ ð6:29Þ see Problem 6.5. Along with condition (6.27), this equation can be shown to justify the NR2 c test from regression (6.28). Two popular tests are special cases. Koenker’s (1981) version of the Breusch and Pagan (1979) test is obtained by taking hi 1 xi, so that Q ¼ K. [The original version of the Breusch-Pagan test relies heavily on normality of the ui, in particular k2 ¼ 3s2, so that Koenker’s version based on NR2 c in regression (6.28) is preferred.] White’s (1980b) test is obtained by taking hi to be all nonconstant, unique elements of xi and x0 ixi: the levels, squares, and cross products of the regressors in the conditional mean. The Breusch-Pagan and White tests have degrees of freedom that depend on the number of regressors in Eðy j xÞ. Sometimes we want to conserve on degrees of free- dom. A test that combines features of the Breusch-Pagan and White tests, but which has only two dfs, takes ^hi 1 ð ^yi; ^y2 i Þ, where the ^yi are the OLS ﬁtted values. (Recall that these are linear functions of the xi.) To justify this test, we must be able to re- place hðxiÞ with hðxi; ^bÞ. We discussed the generated regressors problem for OLS in Section 6.1.1 and concluded that, for testing purposes, using estimates from earlier stages causes no complications. This is the case here as well: NR2 c from ^u2 i on 1, ^yi, ^y2 i , i ¼ 1; 2; . . . ; N has a limiting w2 2 distribution under the null, along with condition (6.27). This is easily seen to be a special case of the White test because ð^yi; ^y2 i Þ con- tains two linear combinations of the squares and cross products of all elements in xi. Additional Single-Equation Topics 127", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 142, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p143::c0", "text": "A simple modiﬁcation is available for relaxing the auxiliary homokurtosis as- sumption (6.27). Following the work of Wooldridge (1990)—or, working directly from the representation in equation (6.29), as in Problem 6.5—it can be shown that N \u0002 SSR0 from the regression (without a constant) 1 on ðhi \u0002 hÞð^u2 i \u0002 ^s2Þ; i ¼ 1; 2; . . . ; N ð6:30Þ is distributed asymptotically as w2 Q under H0 [there are Q regressors in regression (6.30)]. This test is very similar to the heteroskedasticity-robust LM statistics derived in Chapter 4. It is sometimes called a heterokurtosis-robust test for heteroskedasticity. If we allow some elements of xi to be endogenous but assume we have instruments zi such that Eðui j ziÞ ¼ 0 and the rank condition holds, then we can test H0: Eðu2 i j ziÞ ¼ s2 (which implies Assumption 2SLS.3). Let hi 1 hðziÞ be a 1 \u0004 Q function of the exogenous variables. The statistics are computed as in either regression (6.28) or (6.30), depending on whether the homokurtosis is maintained, where the ^ui are the 2SLS residuals. There is, however, one caveat. For the validity of the asymptotic variances that these regressions implicitly use, an additional assumption is needed under H0: Covðxi; ui j ziÞ must be constant. This covariance is zero when zi ¼ xi, so there is no additional assumption when the regressors are exogenous. Without the assumption of constant conditional covariance, the tests for heteroskedasticity are more complicated. For details, see Wooldridge (1990). You should remember that hi (or ^hi) must only be a function of exogenous vari- ables and estimated parameters; it should not depend on endogenous elements of xi. Therefore, when xi contains endogenous variables, it is not valid to use xi ^b and ðxi ^bÞ2 as elements of ^hi. It is valid to use, say, ^xi ^b and ð^xi ^bÞ2, where the ^xi are the ﬁrst-stage ﬁtted values from regressing xi on zi. 6.3 Single-Equation Methods under Other Sampling Schemes So far our treatment of OLS and 2SLS has been explicitly for the case of random samples. In this section we brieﬂy discuss some issues that arise for other sampling schemes that are sometimes assumed for cross section data. 6.3.1 Pooled Cross Sections over Time A data structure that is useful for a variety of purposes, including policy analysis, is what we will call pooled cross sections over time. The idea is that during each year a new random sample is taken from the relevant population. Since distributions of variables tend to change over time, the identical distribution assumption is not usu- ally valid, but the independence assumption is. This approach gives rise to indepen- Chapter 6 128", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 143, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p144::c0", "text": "dent, not identically distributed (i.n.i.d.) observations. It is important not to confuse a pooling of independent cross sections with a di¤erent data structure, panel data, which we treat starting in Chapter 7. Brieﬂy, in a panel data set we follow the same group of individuals, ﬁrms, cities, and so on over time. In a pooling of cross sections over time, there is no replicability over time. (Or, if units appear in more than one time period, their recurrence is treated as coincidental and ignored.) Every method we have learned for pure cross section analysis can be applied to pooled cross sections, including corrections for heteroskedasticity, speciﬁcation test- ing, instrumental variables, and so on. But in using pooled cross sections, we should usually include year (or other time period) dummies to account for aggregate changes over time. If year dummies appear in a model, and it is estimated by 2SLS, the year dummies are their own instruments, as the passage of time is exogenous. For an ex- ample, see Problem 6.8. Time dummies can also appear in tests for heteroskedasticity to determine whether the unconditional error variance has changed over time. In some cases we interact some explanatory variables with the time dummies to allow partial e¤ects to change over time. This procedure can be very useful for policy analysis. In fact, much of the recent literature in policy analyis using natural experi- ments can be cast as a pooled cross section analysis with appropriately chosen dummy variables and interactions. In the simplest case, we have two time periods, say year 1 and year 2. There are also two groups, which we will call a control group and an experimental group or treatment group. In the natural experiment literature, people (or ﬁrms, or cities, and so on) ﬁnd themselves in the treatment group essentially by accident. For example, to study the e¤ects of an unexpected change in unemployment insurance on unemploy- ment duration, we choose the treatment group to be unemployed individuals from a state that has a change in unemployment compensation. The control group could be unemployed workers from a neighboring state. The two time periods chosen would straddle the policy change. As another example, the treatment group might consist of houses in a city under- going unexpected property tax reform, and the control group would be houses in a nearby, similar town that is not subject to a property tax change. Again, the two (or more) years of data would include the period of the policy change. Treatment means that a house is in the city undergoing the regime change. To formalize the discussion, call A the control group, and let B denote the treat- ment group; the dummy variable dB equals unity for those in the treatment group and is zero otherwise. Letting d2 denote a dummy variable for the second (post-policy- change) time period, the simplest equation for analyzing the impact of the policy change is Additional Single-Equation Topics 129", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 144, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p145::c0", "text": "y ¼ b0 þ d0d2 þ b1dB þ d1d2 \u0001 dB þ u ð6:31Þ where y is the outcome variable of interest. The period dummy d2 captures aggregate factors that a¤ect y over time in the same way for both groups. The presence of dB by itself captures possible di¤erences between the treatment and control groups be- fore the policy change occurs. The coe‰cient of interest, d1, multiplies the interaction term, d2 \u0001 dB (which is simply a dummy variable equal to unity for those observations in the treatment group in the second year). The OLS estimator, ^d1, has a very interesting interpretation. Let yA;1 denote the sample average of y for the control group in the ﬁrst year, and let yA;2 be the average of y for the control group in the second year. Deﬁne yB;1 and yB;2 similarly. Then ^d1 can be expressed as ^d1 ¼ ðyB;2 \u0002 yB;1Þ \u0002 ðyA;2 \u0002 yA;1Þ ð6:32Þ This estimator has been labeled the di¤erence-in-di¤erences (DID) estimator in the recent program evaluation literature, although it has a long history in analysis of variance. To see how e¤ective ^d1 is for estimating policy e¤ects, we can compare it with some alternative estimators. One possibility is to ignore the control group completely and use the change in the mean over time for the treatment group, yB;2 \u0002 yB;1, to measure the policy e¤ect. The problem with this estimator is that the mean response can change over time for reasons unrelated to the policy change. Another possibility is to ignore the ﬁrst time period and compute the di¤erence in means for the treatment and control groups in the second time period, yB;2 \u0002 yA;2. The problem with this pure cross section approach is that there might be systematic, unmeasured di¤erences in the treatment and control groups that have nothing to do with the treatment; attrib- uting the di¤erence in averages to a particular policy might be misleading. By comparing the time changes in the means for the treatment and control groups, both group-speciﬁc and time-speciﬁc e¤ects are allowed for. Nevertheless, unbiased- ness of the DID estimator still requires that the policy change not be systematically related to other factors that a¤ect y (and are hidden in u). In most applications, additional covariates appear in equation (6.31); for example, characteristics of unemployed people or housing characteristics. These account for the possibility that the random samples within a group have systematically di¤er- ent characteristics in the two time periods. The OLS estimator of d1 no longer has the simple representation in equation (6.32), but its interpretation is essentially unchanged. Chapter 6 130", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 145, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p146::c0", "text": "Example 6.5 (Length of Time on Workers’ Compensation): Meyer, Viscusi, and Durbin (1995) (hereafter, MVD) study the length of time (in weeks) that an injured worker receives workers’ compensation. On July 15, 1980, Kentucky raised the cap on weekly earnings that were covered by workers’ compensation. An increase in the cap has no e¤ect on the beneﬁt for low-income workers, but it makes it less costly for a high-income worker to stay on workers’ comp. Therefore, the control group is low-income workers, and the treatment group is high-income workers; high-income workers are deﬁned as those for whom the pre-policy-change cap on beneﬁts is binding. Using random samples both before and after the policy change, MVD are able to test whether more generous workers’ compensation causes people to stay out of work longer (everything else ﬁxed). MVD start with a di¤erence-in-di¤erences analysis, using log(durat) as the dependent variable. The variable afchnge is the dummy variable for observations after the policy change, and highearn is the dummy variable for high earners. The estimated equation is logð ^duratÞ ¼ 1:126 ð0:031Þ þ :0077 ð:0447Þ afchnge þ :256 ð:047Þ highearn þ :191 ð:069Þ afchnge\u0001highearn ð6:33Þ N ¼ 5; 626; R2 ¼ :021 Therefore, ^d1 ¼ :191 ðt ¼ 2:77Þ, which implies that the average duration on workers’ compensation increased by about 19 percent due to the higher earnings cap. The co- e‰cient on afchnge is small and statistically insigniﬁcant: as is expected, the increase in the earnings cap had no e¤ect on duration for low-earnings workers. The coe‰- cient on highearn shows that, even in the absence of any change in the earnings cap, high earners spent much more time—on the order of 100 \u0001 ½expð:256Þ \u0002 1\u0003 ¼ 29:2 percent—on workers’ compensation. MVD also add a variety of controls for gender, marital status, age, industry, and type of injury. These allow for the fact that the kind of people and type of injuries di¤er systematically in the two years. Perhaps not surprisingly, controlling for these factors has little e¤ect on the estimate of d1; see the MVD article and Problem 6.9. Sometimes the two groups consist of people or cities in di¤erent states in the United States, often close geographically. For example, to assess the impact of changing alcohol taxes on alcohol consumption, we can obtain random samples on individuals from two states for two years. In state A, the control group, there was no Additional Single-Equation Topics 131", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 146, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p147::c0", "text": "change in alcohol taxes. In state B, taxes increased between the two years. The out- come variable would be a measure of alcohol consumption, and equation (6.31) can be estimated to determine the e¤ect of the tax on alcohol consumption. Other factors, such as age, education, and gender can be controlled for, although this procedure is not necessary for consistency if sampling is random in both years and in both states. The basic equation (6.31) can be easily modiﬁed to allow for continuous, or at least nonbinary, ‘‘treatments.’’ An example is given in Problem 6.7, where the ‘‘treatment’’ for a particular home is its distance from a garbage incinerator site. In other words, there is not really a control group: each unit is put somewhere on a continuum of possible treatments. The analysis is similar because the treatment dummy, dB, is simply replaced with the nonbinary treatment. For a survey on the natural experiment methodology, as well as several additional examples, see Meyer (1995). 6.3.2 Geographically Stratiﬁed Samples Various kinds of stratiﬁed sampling, where units in the sample are represented with di¤erent frequencies than they are in the population, are also common in the social sciences. We treat general kinds of stratiﬁcation in Chapter 17. Here, we discuss some issues that arise with geographical stratiﬁcation, where random samples are taken from separate geographical units. If the geographically stratiﬁed sample can be treated as being independent but not identically distributed, no substantive modiﬁcations are needed to apply the previous econometric methods. However, it is prudent to allow di¤erent intercepts across strata, and even di¤erent slopes in some cases. For example, if people are sampled from states in the United States, it is often important to include state dummy vari- ables to allow for systematic di¤erences in the response and explanatory variables across states. If we are interested in the e¤ects of variables measured at the strata level, and the individual observations are correlated because of unobserved strata e¤ects, estima- tion and inference are much more complicated. A model with strata-level covariates and within-strata correlation is yis ¼ xisb þ zsg þ qs þ eis ð6:34Þ where i is for individual and s is for stratum. The covariates in xis change with the individual, while zs changes only at the strata level. That is, there is correlation in the covariates across individuals within the same stratum. The variable qs is an unob- served stratum e¤ect. We would typically assume that the observations are inde- pendently distributed across strata, that the eis are independent across i, and that Chapter 6 132", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 147, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p148::c0", "text": "Eðeis j Xs; zs; qsÞ ¼ 0 for all i and s—where Xs is the set of explanatory variables for all units in stratum s—and qs is an unobserved stratum e¤ect. The presence of the unobservable qs induces correlation in the composite error uis ¼ qs þ eis within each stratum. If we are interested in the coe‰cients on the individual-speciﬁc variables, that is, b, then there is a simple solution: include stra- tum dummies along with xis. That is, we estimate the model yis ¼ as þ xisb þ eis by OLS, where as is the stratum-speciﬁc intercept. Things are more interesting when we want to estimate g. The OLS estimators of b and g in the regression of yis on xis, zs are still unbiased if Eðqs j Xs; zsÞ ¼ 0, but consistency and asymptotic normality are tricky, because, with a small number of strata and many observations within each stratum, the asymptotic analysis makes sense only if the number of observations within each stratum grows, usually with the number of strata ﬁxed. Because the observations within a stratum are correlated, the usual law of large numbers and central limit theorem cannot be applied. By means of a simulation study, Moulton (1990) shows that ignoring the within-group correlation when obtaining standard errors for ^g can be very misleading. Moulton also gives some corrections to the OLS standard errors, but it is not clear what kind of asymp- totic analysis justiﬁes them. If the strata are, say, states in the United States, and we are interested in the e¤ect of state-level policy variables on economic behavior, one way to proceed is to use state-level data on all variables. This avoids the within-stratum correlation in the composite error in equation (6.34). A drawback is that state policies that can be taken as exogenous at the individual level are often endogenous at the aggregate level. However, if zs in equation (6.34) contains policy variables, perhaps we should question whether these would be uncorrelated with qs. If qs and zs are correlated, OLS using individual-level data would be biased and inconsistent. Related issues arise when aggregate-level variables are used as instruments in equations describing individual behavior. For example, in a birth weight equation, Currie and Cole (1993) use measures of state-level AFDC beneﬁts as instruments for individual women’s participation in AFDC. (Therefore, the binary endogenous ex- planatory variable is at the individual level, while the instruments are at the state level.) If state-level AFDC beneﬁts are exogenous in the birth weight equation, and AFDC participation is su‰ciently correlated with state beneﬁt levels—a question that can be checked using the ﬁrst-stage regression—then the IV approach will yield a consistent estimator of the e¤ect of AFDC participation on birth weight. Mo‰tt (1996) discusses assumptions under which using aggregate-level IVs yields consistent estimators. He gives the example of using observations on workers from two cities to estimate the impact of job training programs. In each city, some people Additional Single-Equation Topics 133", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 148, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p149::c0", "text": "received some job training while others did not. The key element in xis is a job training indicator. If, say, city A exogenously o¤ered more job training slots than city B, a city dummy variable can be used as an IV for whether each worker received training. See Mo‰tt (1996) and Problem 5.13b for an interpretation of such estimators. If there are unobserved group e¤ects in the error term, then at a minimum, the usual 2SLS standard errors will be inappropriate. More problematic is that aggregate- level variables might be correlated with qs. In the birth weight example, the level of AFDC beneﬁts might be correlated with unobserved health care quality variables that are in qs. In the job training example, city A may have spent more on job train- ing because its workers are, on average, less productive than the workers in city B. Unfortunately, controlling for qs by putting in strata dummies and applying 2SLS does not work: by deﬁnition, the instruments only vary across strata—not within strata—and so b in equation (6.34) would be unidentiﬁed. In the job training exam- ple, we would put in a dummy variable for city of residence as an explanatory vari- able, and therefore we could not use this dummy variable as an IV for job training participation: we would be short one instrument. 6.3.3 Spatial Dependence As the previous subsection suggests, cross section data that are not the result of independent sampling can be di‰cult to handle. Spatial correlation, or, more gen- erally, spatial dependence, typically occurs when cross section units are large relative to the population, such as when data are collected at the county, state, province, or country level. Outcomes from adjacent units are likely to be correlated. If the corre- lation arises mainly through the explanatory variables (as opposed to unobservables), then, practically speaking, nothing needs to be done (although the asymptotic anal- ysis can be complicated). In fact, sometimes covariates for one county or state appear as explanatory variables in the equation for neighboring units, as a way of capturing spillover e¤ects. This fact in itself causes no real di‰culties. When the unobservables are correlated across nearby geographical units, OLS can still have desirable properties—often unbiasedness, consistency, and asymptotic nor- mality can be established—but the asymptotic arguments are not nearly as uniﬁed as in the random sampling case, and estimating asymptotic variances becomes di‰cult. 6.3.4 Cluster Samples Cluster sampling is another case where cross section observations are correlated, but it is somewhat easier to handle. The key is that we randomly sample a large number of clusters, and each cluster consists of relatively few units (compared with the overall sample size). While we allow the units within each cluster to be correlated, we assume Chapter 6 134", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 149, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p150::c0", "text": "independence across clusters. An example is studying teenage peer e¤ects using a large sample of neighborhoods (the clusters) with relatively few teenagers per neigh- borhood. Or, using siblings in a large sample of families. The asymptotic analysis is with ﬁxed cluster sizes with the number of clusters getting large. As we will see in Section 11.5, handling within-cluster correlation in this context is relatively straight- forward. In fact, when the explanatory variables are exogenous, OLS is consistent and asymptotically normal, but the asymptotic variance matrix needs to be adjusted. The same holds for 2SLS. Problems 6.1. a. In Problem 5.4d, test the null hypothesis that educ is exogenous. b. Test the the single overidentifying restriction in this example. 6.2. In Problem 5.8b, test the null hypothesis that educ and IQ are exogenous in the equation estimated by 2SLS. 6.3. Consider a model for individual data to test whether nutrition a¤ects produc- tivity (in a developing country): logðproducÞ ¼ d0 þ d1exper þ d2exper2 þ d3educ þ a1calories þ a2 protein þ u1 ð6:35Þ where produc is some measure of worker productivity, calories is caloric intake per day, and protein is a measure of protein intake per day. Assume here that exper, exper2, and educ are all exogenous. The variables calories and protein are possibly correlated with u1 (see Strauss and Thomas, 1995, for discussion). Possible instru- mental variables for calories and protein are regional prices of various goods such as grains, meats, breads, dairy products, and so on. a. Under what circumstances do prices make good IVs for calories and proteins? What if prices reﬂect quality of food? b. How many prices are needed to identify equation (6.35)? c. Suppose we have M prices, p1; . . . ; pM. Explain how to test the null hypothesis that calories and protein are exogenous in equation (6.35). 6.4. Consider a structural linear model with unobserved variable q: y ¼ xb þ q þ v; Eðv j x; qÞ ¼ 0 Additional Single-Equation Topics 135", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 150, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p151::c0", "text": "Suppose, in addition, that Eðq j xÞ ¼ xd for some K \u0004 1 vector d; thus, q and x are possibly correlated. a. Show that Eðy j xÞ is linear in x. What consequences does this fact have for tests of functional form to detect the presence of q? Does it matter how strongly q and x are correlated? Explain. b. Now add the assumptions Varðv j x; qÞ ¼ s2 v and Varðq j xÞ ¼ s2 q . Show that Varðy j xÞ is constant. [Hint: Eðqv j xÞ ¼ 0 by iterated expectations.] What does this fact imply about using tests for heteroskedasticity to detect omitted variables? c. Now write the equation as y ¼ xb þ u, where Eðx0uÞ ¼ 0 and Varðu j xÞ ¼ s2. If Eðu j xÞ 0 EðuÞ, argue that an LM test of the form (6.28) will detect ‘‘hetero- skedasticity’’ in u, at least in large samples. 6.5. a. Verify equation (6.29) under the assumptions Eðu j xÞ ¼ 0 and Eðu2 j xÞ ¼ s2. b. Show that, under the additional assumption (6.27), E½ðu2 i \u0002 s2Þ2ðhi \u0002 mhÞ0ðhi \u0002 mhÞ\u0003 ¼ h2E½ðhi \u0002 mhÞ0ðhi \u0002 mhÞ\u0003 where h2 ¼ E½ðu2 \u0002 s2Þ2\u0003. c. Explain why parts a and b imply that the LM statistic from regression (6.28) has a limiting w2 Q distribution. d. If condition (6.27) does not hold, obtain a consistent estimator of E½ðu2 i \u0002 s2Þ2ðhi \u0002 mhÞ0ðhi \u0002 mhÞ\u0003. Show how this leads to the heterokurtosis-robust test for heteroskedasticity. 6.6. Using the test for heteroskedasticity based on the auxiliary regression ^u2 on ^y, ^y2, test the log(wage) equation in Example 6.4 for heteroskedasticity. Do you detect heteroskedasticity at the 5 percent level? 6.7. For this problem use the data in HPRICE.RAW, which is a subset of the data used by Kiel and McClain (1995). The ﬁle contains housing prices and characteristics for two years, 1978 and 1981, for homes sold in North Andover, Massachusetts. In 1981 construction on a garbage incinerator began. Rumors about the incinerator being built were circulating in 1979, and it is for this reason that 1978 is used as the base year. By 1981 it was very clear that the incinerator would be operating soon. a. Using the 1981 cross section, estimate a bivariate, constant elasticity model relat- ing housing price to distance from the incinerator. Is this regression appropriate for determining the causal e¤ects of incinerator on housing prices? Explain. b. Pooling the two years of data, consider the model Chapter 6 136", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 151, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p152::c0", "text": "logðpriceÞ ¼ d0 þ d1y81 þ d2 logðdistÞ þ d3y81 \u0001 logðdistÞ þ u If the incinerator has a negative e¤ect on housing prices for homes closer to the incinerator, what sign is d3? Estimate this model and test the null hypothesis that building the incinerator had no e¤ect on housing prices. c. Add the variables log(intst), ½logðintstÞ\u00032, log(area), log(land ), age, age2, rooms, baths to the model in part b, and test for an incinerator e¤ect. What do you conclude? 6.8. The data in FERTIL1.RAW are a pooled cross section on more than a thou- sand U.S. women for the even years between 1972 and 1984, inclusive; the data set is similar to the one used by Sander (1992). These data can be used to study the rela- tionship between women’s education and fertility. a. Use OLS to estimate a model relating number of children ever born to a woman (kids) to years of education, age, region, race, and type of environment reared in. You should use a quadratic in age and should include year dummies. What is the estimated relationship between fertility and education? Holding other factors ﬁxed, has there been any notable secular change in fertility over the time period? b. Reestimate the model in part a, but use motheduc and fatheduc as instruments for educ. First check that these instruments are su‰ciently partially correlated with educ. Test whether educ is in fact exogenous in the fertility equation. c. Now allow the e¤ect of education to change over time by including interaction terms such as y74\u0001educ, y76\u0001educ, and so on in the model. Use interactions of time dummies and parents’ education as instruments for the interaction terms. Test that there has been no change in the relationship between fertility and education over time. 6.9. Use the data in INJURY.RAW for this question. a. Using the data for Kentucky, reestimate equation (6.33) adding as explanatory variables male, married, and a full set of industry- and injury-type dummy variables. How does the estimate on afchnge\u0001highearn change when these other factors are controlled for? Is the estimate still statistically signiﬁcant? b. What do you make of the small R-squared from part a? Does this mean the equation is useless? c. Estimate equation (6.33) using the data for Michigan. Compare the estimate on the interaction term for Michigan and Kentucky, as well as their statistical signiﬁcance. 6.10. Consider a regression model with interactions and squares of some explana- tory variables: Eðy j xÞ ¼ zb, where z contains a constant, the elements of x, and quadratics and interactions of terms in x. Additional Single-Equation Topics 137", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 152, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p153::c0", "text": "a. Let m ¼ EðxÞ be the population mean of x, and let x be the sample average based on the N available observations. Let ^b be the OLS estimator of b using the N obser- vations on y and z. Show that ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ and ﬃﬃﬃﬃ N p ðx \u0002 mÞ are asymptotically un- correlated. [Hint: Write ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ as in equation (4.8), and ignore the op(1) term. You will need to use the fact that Eðu j xÞ ¼ 0:] b. In the model of Problem 4.8, use part a to argue that Avarð^a1Þ ¼ Avarð~a1Þ þ b2 3 Avarðx2Þ ¼ Avarð~a1Þ þ b2 3ðs2 2=NÞ where a1 ¼ b1 þ b3m2, ~a1 is the estimator of a1 if we knew m2, and s2 2 ¼ Varðx2Þ. c. How would you obtain the correct asymptotic standard error of ^a1, having run the regression in Problem 4.8d? [Hint: The standard error you get from the regression is really seð~a1Þ. Thus you can square this to estimate Avarð~a1Þ, then use the preceding formula. You need to estimate s2 2, too.] d. Apply the result from part c to the model in Problem 4.8; in particular, ﬁnd the corrected asymptotic standard error for ^a1, and compare it with the uncorrected one from Problem 4.8d. (Both can be nonrobust to heteroskedasticity.) What do you conclude? 6.11. The following wage equation represents the populations of working people in 1978 and 1985: logðwageÞ ¼ b0 þ d0y85 þ b1educ þ d1y85\u0001educ þ b2exper þ b3exper2 þ b4union þ b5 female þ d5y85\u0001 female þ u where the explanatory variables are standard. The variable union is a dummy vari- able equal to one if the person belongs to a union and zero otherwise. The variable y85 is a dummy variable equal to one if the observation comes from 1985 and zero if it comes from 1978. In the ﬁle CPS78_85.RAW there are 550 workers in the sample in 1978 and a di¤erent set of 534 people in 1985. a. Estimate this equation and test whether the return to education has changed over the seven-year period. b. What has happened to the gender gap over the period? c. Wages are measured in nominal dollars. What coe‰cients would change if we measure wage in 1978 dollars in both years? [Hint: Use the fact that for all 1985 observations, logðwagei=P85Þ ¼ logðwageiÞ \u0002 logðP85Þ, where P85 is the common deﬂator; P85 ¼ 1:65 according to the Consumer Price Index.] d. Is there evidence that the variance of the error has changed over time? Chapter 6 138", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 153, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p154::c0", "text": "e. With wages measured nominally, and holding other factors ﬁxed, what is the estimated increase in nominal wage for a male with 12 years of education? Propose a regression to obtain a conﬁdence interval for this estimate. (Hint: You must replace y85\u0001educ with something else.) 6.12. In the linear model y ¼ xb þ u, assume that Assumptions 2SLS.1 and 2SLS.3 hold with w in place of z, where w contains all nonredundant elements of x and z. Further, assume that the rank conditions hold for OLS and 2SLS. Show that Avar½ ﬃﬃﬃﬃ N p ð ^b2SLS \u0002 ^bOLSÞ\u0003 ¼ Avar½ ﬃﬃﬃﬃ N p ð ^b2SLS \u0002 bÞ\u0003 \u0002 Avar½ ﬃﬃﬃﬃ N p ð ^bOLS \u0002 bÞ\u0003 [Hint: First, Avar½ ﬃﬃﬃﬃ N p ð ^b2SLS \u0002 ^bOLSÞ\u0003 ¼ V1 þ V2 \u0002 ðC þ C0Þ, where V1 ¼ Avar \u0001 ½ ﬃﬃﬃﬃ N p ð ^b2SLS \u0002 bÞ\u0003, V2 ¼ Avar½ ﬃﬃﬃﬃ N p ð ^bOLS \u0002 bÞ\u0003, and C is the asymptotic covariance between ﬃﬃﬃﬃ N p ð ^b2SLS \u0002 bÞ and ﬃﬃﬃﬃ N p ð ^bOLS \u0002 bÞ. You can stack the formulas for the 2SLS and OLS estimators and show that C ¼ s2½Eðx\u00050x\u0005Þ\u0003\u00021Eðx\u00050xÞ½Eðx0xÞ\u0003\u00021 ¼ s2½Eðx0xÞ\u0003\u00021 ¼ V2. To show the second equality, it will be helpful to use Eðx\u00050xÞ ¼ Eðx\u00050x\u0005Þ:] Appendix 6A We derive the asymptotic distribution of the 2SLS estimator in an equation with generated regressors and generated instruments. The tools needed to make the proof rigorous are introduced in Chapter 12, but the key components of the proof can be given here in the context of the linear model. Write the model as y ¼ xb þ u; Eðu j vÞ ¼ 0 where x ¼ fðw; dÞ, d is a Q \u0004 1 vector, and b is K \u0004 1. Let ^d be a ﬃﬃﬃﬃ N p -consistent es- timator of d. The instruments for each i are ^zi ¼ gðvi; ^lÞ where gðv; lÞ is a 1 \u0004 L vector, l is an S \u0004 1 vector of parameters, and ^l is ﬃﬃﬃﬃ N p -consistent for l. Let ^b be the 2SLS estimator from the equation yi ¼ ^xib þ errori where ^xi ¼ fðwi; ^dÞ, using instruments ^zi: ^b ¼ X N i¼1 ^x0 i^zi ! X N i¼1 ^z0 i^zi !\u00021 X N i¼1 ^z0 i^xi ! 2 4 3 5 \u00021 X N i¼1 ^x0 i^zi ! X N i¼1 ^z0 i^zi !\u00021 X N i¼1 ^z0 i yi ! Write yi ¼ ^xib þ ðxi \u0002 ^xiÞb þ ui, where xi ¼ fðwi; dÞ. Plugging this in and multi- plying through by ﬃﬃﬃﬃ N p gives Additional Single-Equation Topics 139", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 154, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p155::c0", "text": "ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ ¼ ð ^C0 ^D\u00021 ^CÞ\u00021 ^C0 ^D\u00021 N\u00021=2 X N i¼1 ^z0 i½ðxi \u0002 ^xiÞb þ ui\u0003 ( ) where ^C 1 N\u00021 X N i¼1 ^z0 i^xi and ^D ¼ N\u00021 X N i¼1 ^z0 i^zi Now, using Lemma 12.1 in Chapter 12, ^C ! p Eðz0xÞ and ^D ! p Eðz0zÞ. Further, a mean value expansion of the kind used in Theorem 12.3 gives N\u00021=2 X N i¼1 ^z0 iui ¼ N\u00021=2 X N i¼1 z0 iui þ N\u00021 X N i¼1 ‘l gðvi; lÞui \" # ﬃﬃﬃﬃ N p ð^l \u0002 lÞ þ opð1Þ where ‘lgðvi; lÞ is the L \u0004 S Jacobian of gðvi; lÞ0. Because Eðui j viÞ ¼ 0, E½‘lgðvi; lÞ0ui\u0003 ¼ 0. It follows that N\u00021 PN i¼1 ‘lgðvi; lÞui ¼ opð1Þ and, since ﬃﬃﬃﬃ N p ð^l \u0002 lÞ ¼ Opð1Þ, it follows that N\u00021=2 X N i¼1 ^z0 iui ¼ N\u00021=2 X N i¼1 z0 iui þ opð1Þ Next, using similar reasoning, N\u00021=2 X N i¼1 ^z0 iðxi \u0002 ^xiÞb ¼ \u0002 N\u00021 X N i¼1 ðb n ziÞ0‘dfðwi; dÞ \" # ﬃﬃﬃﬃ N p ð^d \u0002 dÞ þ opð1Þ ¼ \u0002G ﬃﬃﬃﬃ N p ð^d \u0002 dÞ þ opð1Þ where G ¼ E½ðb n ziÞ0‘dfðwi; dÞ\u0003 and ‘dfðwi; dÞ is the K \u0004 Q Jacobian of fðwi; dÞ0. We have used a mean value expansion and ^z0 iðxi \u0002 ^xiÞb ¼ ðb n^ziÞ0ðxi \u0002 ^xiÞ0. Now, assume that ﬃﬃﬃﬃ N p ð^d \u0002 dÞ ¼ N\u00021=2 X N i¼1 riðdÞ þ opð1Þ where E½riðdÞ\u0003 ¼ 0. This assumption holds for all estimators discussed so far, and it also holds for most estimators in nonlinear models; see Chapter 12. Collecting all terms gives ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ ¼ ðC0D\u00021CÞ\u00021C0D\u00021 N\u00021=2 X N i¼1 ½z0 iui \u0002 GriðdÞ\u0003 ( ) þ opð1Þ Chapter 6 140", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 155, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p156::c0", "text": "By the central limit theorem, ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ @ a Normal½0; ðC0D\u00021CÞ\u00021C0D\u00021MD\u00021CðC0D\u00021CÞ\u00021\u0003 where M ¼ Var½z0 iui \u0002 GriðdÞ\u0003 The asymptotic variance of ^b is estimated as ð ^C0 ^D\u00021 ^CÞ\u00021 ^C0 ^D\u00021 ^M^D\u00021 ^Cð ^C0 ^D\u00021 ^CÞ\u00021=N; ð6:36Þ where ^M ¼ N\u00021 X N i¼1 ð^z0 i ^ui \u0002 ^G^riÞð^z0 i ^ui \u0002 ^G^riÞ0 ð6:37Þ ^G ¼ N\u00021 X N i¼1 ð ^b n^ziÞ0‘dfðwi; ^dÞ ð6:38Þ and ^ri ¼ rið^dÞ; ^ui ¼ yi \u0002 ^xi ^b ð6:39Þ A few comments are in order. First, estimation of l does not a¤ect the asymptotic distribution of ^b. Therefore, if there are no generated regressors, the usual 2SLS in- ference procedures are valid [G ¼ 0 in this case and so M ¼ Eðu2 i z0 iziÞ]. If G ¼ 0 and Eðu2z0zÞ ¼ s2Eðz0zÞ, then the usual 2SLS standard errors and test statistics are valid. If Assumption 2SLS.3 fails, then the heteroskedasticity-robust statistics are valid. If G 0 0, then the asymptotic variance of ^b depends on that of ^d [through the presence of riðdÞ]. Neither the usual 2SLS variance matrix estimator nor the heteroskedasticity-robust form is valid in this case. The matrix ^M should be com- puted as in equation (6.37). In some cases, G ¼ 0 under the null hypothesis that we wish to test. The jth row of G can be written as E½zijb 0‘dfðwi; dÞ\u0003. Now, suppose that ^xih is the only generated regressor, so that only the hth row of ‘dfðwi; dÞ is nonzero. But then if bh ¼ 0, b 0‘dfðwi; dÞ ¼ 0. It follows that G ¼ 0 and M ¼ Eðu2 i z0 iziÞ, so that no adjustment for the preliminary estimation of d is needed. This observation is very useful for a variety of speciﬁcation tests, including the test for endogeneity in Section 6.2.1. We will also use it in sample selection contexts later on. Additional Single-Equation Topics 141", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 156, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p157::c0", "text": "7 Estimating Systems of Equations by OLS and GLS 7.1 Introduction This chapter begins our analysis of linear systems of equations. The ﬁrst method of estimation we cover is system ordinary least squares, which is a direct extension of OLS for single equations. In some important special cases the system OLS estimator turns out to have a straightforward interpretation in terms of single-equation OLS estimators. But the method is applicable to very general linear systems of equations. We then turn to a generalized least squares (GLS) analysis. Under certain as- sumptions, GLS—or its operationalized version, feasible GLS—will turn out to be asymptotically more e‰cient than system OLS. However, we emphasize in this chapter that the e‰ciency of GLS comes at a price: it requires stronger assumptions than system OLS in order to be consistent. This is a practically important point that is often overlooked in traditional treatments of linear systems, particularly those which assume that explanatory variables are nonrandom. As with our single-equation analysis, we assume that a random sample is available from the population. Usually the unit of observation is obvious—such as a worker, a household, a ﬁrm, or a city. For example, if we collect consumption data on various commodities for a sample of families, the unit of observation is the family (not a commodity). The framework of this chapter is general enough to apply to panel data models. Because the asymptotic analysis is done as the cross section dimension tends to in- ﬁnity, the results are explicitly for the case where the cross section dimension is large relative to the time series dimension. (For example, we may have observations on N ﬁrms over the same T time periods for each ﬁrm. Then, we assume we have a random sample of ﬁrms that have data in each of the T years.) The panel data model covered here, while having many useful applications, does not fully exploit the replicability over time. In Chapters 10 and 11 we explicitly consider panel data models that con- tain time-invariant, unobserved e¤ects in the error term. 7.2 Some Examples We begin with two examples of systems of equations. These examples are fairly gen- eral, and we will see later that variants of them can also be cast as a general linear system of equations. Example 7.1 (Seemingly Unrelated Regressions): The population model is a set of G linear equations,", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 157, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p158::c0", "text": "y1 ¼ x1b1 þ u1 y2 ¼ x2b2 þ u2 .. . yG ¼ xGbG þ uG ð7:1Þ where xg is 1 \u0001 Kg and bg is Kg \u0001 1, g ¼ 1; 2; . . . ; G. In many applications xg is the same for all g (in which case the bg necessarily have the same dimension), but the general model allows the elements and the dimension of xg to vary across equations. Remember, the system (7.1) represents a generic person, ﬁrm, city, or whatever from the population. The system (7.1) is often called Zellner’s (1962) seemingly unrelated regressions (SUR) model (for cross section data in this case). The name comes from the fact that, since each equation in the system (7.1) has its own vector bg, it appears that the equations are unrelated. Nevertheless, correlation across the errors in di¤er- ent equations can provide links that can be exploited in estimation; we will see this point later. As a speciﬁc example, the system (7.1) might represent a set of demand functions for the population of families in a country: housing ¼ b10 þ b11houseprc þ b12 foodprc þ b13clothprc þ b14income þ b15size þ b16age þ u1 food ¼ b20 þ b21houseprc þ b22 foodprc þ b23clothprc þ b24income þ b25size þ b26age þ u2 clothing ¼ b30 þ b31houseprc þ b32 foodprc þ b33clothprc þ b34income þ b35size þ b36age þ u3 In this example, G ¼ 3 and xg (a 1 \u0001 7 vector) is the same for g ¼ 1; 2; 3. When we need to write the equations for a particular random draw from the pop- ulation, yg, xg, and ug will also contain an i subscript: equation g becomes yig ¼ xigbg þ uig. For the purposes of stating assumptions, it does not matter whether or not we include the i subscript. The system (7.1) has the advantage of being less clut- tered while focusing attention on the population, as is appropriate for applications. But for derivations we will often need to indicate the equation for a generic cross section unit i. When we study the asymptotic properties of various estimators of the bg, the asymptotics is done with G ﬁxed and N tending to inﬁnity. In the household demand example, we are interested in a set of three demand functions, and the unit of obser- Chapter 7 144", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 158, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p159::c0", "text": "vation is the family. Therefore, inference is done as the number of families in the sample tends to inﬁnity. The assumptions that we make about how the unobservables ug are related to the explanatory variables ðx1; x2; . . . ; xGÞ are crucial for determining which estimators of the bg have acceptable properties. Often, when system (7.1) represents a structural model (without omitted variables, errors-in-variables, or simultaneity), we can as- sume that Eðug j x1; x2; . . . ; xGÞ ¼ 0; g ¼ 1; . . . ; G ð7:2Þ One important implication of assumption (7.2) is that ug is uncorrelated with the explanatory variables in all equations, as well as all functions of these explanatory variables. When system (7.1) is a system of equations derived from economic theory, assumption (7.2) is often very natural. For example, in the set of demand functions that we have presented, xg 1 x is the same for all g, and so assumption (7.2) is the same as Eðug j xgÞ ¼ Eðug j xÞ ¼ 0. If assumption (7.2) is maintained, and if the xg are not the same across g, then any explanatory variables excluded from equation g are assumed to have no e¤ect on expected yg once xg has been controlled for. That is, Eðyg j x1; x2; . . . xGÞ ¼ Eðyg j xgÞ ¼ xgbg; g ¼ 1; 2; . . . ; G ð7:3Þ There are examples of SUR systems where assumption (7.3) is too strong, but stan- dard SUR analysis either explicitly or implicitly makes this assumption. Our next example involves panel data. Example 7.2 (Panel Data Model): Suppose that for each cross section unit we ob- serve data on the same set of variables for T time periods. Let xt be a 1 \u0001 K vector for t ¼ 1; 2; . . . ; T, and let b be a K \u0001 1 vector. The model in the population is yt ¼ xtb þ ut; t ¼ 1; 2; . . . ; T ð7:4Þ where yt is a scalar. For example, a simple equation to explain annual family saving over a ﬁve-year span is savt ¼ b0 þ b1inct þ b2aget þ b3educt þ ut; t ¼ 1; 2; . . . ; 5 where inct is annual income, educt is years of education of the household head, and aget is age of the household head. This is an example of a linear panel data model. It is a static model because all explanatory variables are dated contemporaneously with savt. The panel data setup is conceptually very di¤erent from the SUR example. In Ex- ample 7.1, each equation explains a di¤erent dependent variable for the same cross Estimating Systems of Equations by OLS and GLS 145", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 159, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p160::c0", "text": "section unit. Here we only have one dependent variable we are trying to explain— sav—but we observe sav, and the explanatory variables, over a ﬁve-year period. (Therefore, the label ‘‘system of equations’’ is really a misnomer for panel data applications. At this point, we are using the phrase to denote more than one equation in any context.) As we will see in the next section, the statistical properties of esti- mators in SUR and panel data models can be analyzed within the same structure. When we need to indicate that an equation is for a particular cross section unit i during a particular time period t, we write yit ¼ xitb þ uit. We will omit the i sub- script whenever its omission does not cause confusion. What kinds of exogeneity assumptions do we use for panel data analysis? One possibility is to assume that ut and xt are orthogonal in the conditional mean sense: Eðut j xtÞ ¼ 0; t ¼ 1; . . . ; T ð7:5Þ We call this contemporaneous exogeneity of xt because it only restricts the relation- ship between the disturbance and explanatory variables in the same time period. It is very important to distinguish assumption (7.5) from the stronger assumption Eðut j x1; x2; . . . ; xTÞ ¼ 0; t ¼ 1; . . . ; T ð7:6Þ which, combined with model (7.4), is identical to Eðyt j x1; x2; . . . ; xTÞ ¼ Eðyt j xtÞ. Assumption (7.5) places no restrictions on the relationship between xs and ut for s 0 t, while assumption (7.6) implies that each ut is uncorrelated with the explanatory variables in all time periods. When assumption (7.6) holds, we say that the explana- tory variables fx1; x2; . . . ; xt; . . . ; xTg are strictly exogenous. To illustrate the di¤erence between assumptions (7.5) and (7.6), let xt 1 ð1; yt\u00021Þ. Then assumption (7.5) holds if Eðyt j yt\u00021; yt\u00022; . . . ; y0Þ ¼ b0 þ b1 yt\u00021, which imposes ﬁrst-order dynamics in the conditional mean. However, assumption (7.6) must fail since xtþ1 ¼ ð1; ytÞ, and therefore Eðut j x1; x2; . . . ; xTÞ ¼ Eðut j y0; y1; . . . ; yT\u00021Þ ¼ ut for t ¼ 1; 2; . . . ; T \u0002 1 (because ut ¼ yt \u0002 b0 \u0002 b1 yt\u00021Þ. Assumption (7.6) can fail even if xt does not contain a lagged dependent variable. Consider a model relating poverty rates to welfare spending per capita, at the city level. A ﬁnite distributed lag (FDL) model is povertyt ¼ yt þ d0welfaret þ d1welfaret\u00021 þ d2welfaret\u00022 þ ut ð7:7Þ where we assume a two-year e¤ect. The parameter yt simply denotes a di¤erent ag- gregate time e¤ect in each year. It is reasonable to think that welfare spending reacts to lagged poverty rates. An equation that captures this feedback is welfaret ¼ ht þ r1povertyt\u00021 þ rt ð7:8Þ Chapter 7 146", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 160, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p161::c0", "text": "Even if equation (7.7) contains enough lags of welfare spending, assumption (7.6) would be violated if r1 0 0 in equation (7.8) because welfaretþ1 depends on ut and xtþ1 includes welfaretþ1. How we go about consistently estimating b depends crucially on whether we maintain assumption (7.5) or the stronger assumption (7.6). Assuming that the xit are ﬁxed in repeated samples is e¤ectively the same as making assumption (7.6). 7.3 System OLS Estimation of a Multivariate Linear System 7.3.1 Preliminaries We now analyze a general multivariate model that contains the examples in Section 7.2, and many others, as special cases. Assume that we have independent, identically distributed cross section observations fðXi; yiÞ: i ¼ 1; 2; . . . ; Ng, where Xi is a G \u0001 K matrix and yi is a G \u0001 1 vector. Thus, yi contains the dependent variables for all G equations (or time periods, in the panel data case). The matrix Xi contains the ex- planatory variables appearing anywhere in the system. For notational clarity we in- clude the i subscript for stating the general model and the assumptions. The multivariate linear model for a random draw from the population can be expressed as yi ¼ Xib þ ui ð7:9Þ where b is the K \u0001 1 parameter vector of interest and ui is a G \u0001 1 vector of un- observables. Equation (7.9) explains the G variables yi1; . . . ; yiG in terms of Xi and the unobservables ui. Because of the random sampling assumption, we can state all assumptions in terms of a generic observation; in examples, we will often omit the i subscript. Before stating any assumptions, we show how the two examples introduced in Section 7.2 ﬁt into this framework. Example 7.1 (SUR, continued): The SUR model (7.1) can be expressed as in equation (7.9) by deﬁning yi ¼ ðyi1; yi2; . . . ; yiGÞ0, ui ¼ ðui1; ui2; . . . ; uiGÞ0, and Xi ¼ xi1 0 0 \u0003 \u0003 \u0003 0 0 xi2 0 0 0 ... ... 0 0 0 0 \u0003 \u0003 \u0003 xiG 0 B B B B B B B @ 1 C C C C C C C A ; b ¼ b1 b2 ... bG 0 B B B B @ 1 C C C C A ð7:10Þ Estimating Systems of Equations by OLS and GLS 147", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 161, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p162::c0", "text": "Note that the dimension of Xi is G \u0001 ðK1 þ K2 þ \u0003 \u0003 \u0003 þ KGÞ, so we deﬁne K 1 K1 þ \u0003 \u0003 \u0003 þ KG. Example 7.2 (Panel Data, continued): The panel data model (7.6) can be expressed as in equation (7.9) by choosing Xi to be the T \u0001 K matrix Xi ¼ ðx0 i1; x0 i2; . . . ; x0 iTÞ0. 7.3.2 Asymptotic Properties of System OLS Given the model in equation (7.9), we can state the key orthogonality condition for consistent estimation of b by system ordinary least squares (SOLS). assumption SOLS.1: EðX0 iuiÞ ¼ 0. Assumption SOLS.1 appears similar to the orthogonality condition for OLS analysis of single equations. What it implies di¤ers across examples because of the multiple- equation nature of equation (7.9). For most applications, Xi has a su‰cient number of elements equal to unity so that Assumption SOLS.1 implies that EðuiÞ ¼ 0, and we assume zero mean for the sake of discussion. It is informative to see what Assumption SOLS.1 entails in the previous examples. Example 7.1 (SUR, continued): In the SUR case, X0 iui ¼ ðxi1ui1; . . . ; xiGuiGÞ0, and so Assumption SOLS.1 holds if and only if Eðx0 iguigÞ ¼ 0; g ¼ 1; 2; . . . ; G ð7:11Þ Thus, Assumption SOLS.1 does not require xih and uig to be uncorrelated when h 0 g. Example 7.2 (Panel Data, continued): For the panel data setup, X0 iui ¼ PT t¼1 x0 ituit; therefore, a su‰cient, and very natural, condition for Assumption SOLS.1 is Eðx0 ituitÞ ¼ 0; t ¼ 1; 2; . . . ; T ð7:12Þ Like assumption (7.5), assumption (7.12) allows xis and uit to be correlated when s 0 t; in fact, assumption (7.12) is weaker than assumption (7.5). Therefore, As- sumption SOLS.1 does not impose strict exogeneity in panel data contexts. Assumption SOLS.1 is the weakest assumption we can impose in a regression framework to get consistent estimators of b. As the previous examples show, As- sumption SOLS.1 allows some elements of Xi to be correlated with elements of ui. Much stronger is the zero conditional mean assumption Eðui j XiÞ ¼ 0 ð7:13Þ Chapter 7 148", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 162, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p163::c0", "text": "which implies, among other things, that every element of Xi and every element of ui are uncorrelated. [Of course, assumption (7.13) is not as strong as assuming that ui and Xi are actually independent.] Even though assumption (7.13) is stronger than Assumption SOLS.1, it is, nevertheless, reasonable in some applications. Under Assumption SOLS.1 the vector b satisﬁes E½X0 iðyi \u0002 XibÞ\u0004 ¼ 0 ð7:14Þ or EðX0 iXiÞb ¼ EðX0 iyiÞ. For each i, X0 iyi is a K \u0001 1 random vector and X0 iXi is a K \u0001 K symmetric, positive semideﬁnite random matrix. Therefore, EðX0 iXiÞ is always a K \u0001 K symmetric, positive semideﬁnite nonrandom matrix (the expectation here is deﬁned over the population distribution of Xi). To be able to estimate b we need to assume that it is the only K \u0001 1 vector that satisﬁes assumption (7.14). assumption SOLS.2: A 1 EðX0 iXiÞ is nonsingular (has rank K ). Under Assumptions SOLS.1 and SOLS.2 we can write b as b ¼ ½EðX0 iXiÞ\u0004\u00021EðX0 iyiÞ ð7:15Þ which shows that Assumptions SOLS.1 and SOLS.2 identify the vector b. The anal- ogy principle suggests that we estimate b by the sample analogue of assumption (7.15). Deﬁne the system ordinary least squares (SOLS) estimator of b as ^b ¼ N\u00021 X N i¼1 X0 iXi !\u00021 N\u00021 X N i¼1 X0 iyi ! ð7:16Þ For computing ^b using matrix language programming, it is sometimes useful to write ^b ¼ ðX0XÞ\u00021X0Y, where X 1 ðX0 1; X0 2; . . . ; X0 NÞ0 is the NG \u0001 K matrix of stacked X and Y 1 ðy0 1; y0 2; . . . ; y0 NÞ0 is the NG \u0001 1 vector of stacked observations on the yi. For asymptotic derivations, equation (7.16) is much more convenient. In fact, the con- sistency of ^b can be read o¤ of equation (7.16) by taking probability limits. We summarize with a theorem: theorem 7.1 (Consistency of System OLS): Under Assumptions SOLS.1 and SOLS.2, ^b ! p b. It is useful to see what the system OLS estimator looks like for the SUR and panel data examples. Example 7.1 (SUR, continued): For the SUR model, Estimating Systems of Equations by OLS and GLS 149", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 163, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p164::c0", "text": "X N i¼1 X0 iXi ¼ X N i¼1 x0 i1xi1 0 0 \u0003 \u0003 \u0003 0 0 x0 i2xi2 0 0 0 .. . .. . 0 0 0 0 \u0003 \u0003 \u0003 x0 iGxiG 0 B B B B B B B @ 1 C C C C C C C A ; X N i¼1 X0 iyi ¼ X N i¼1 x0 i1 yi1 x0 i2 yi2 ... x0 iG yiG 0 B B B B @ 1 C C C C A Straightforward inversion of a block diagonal matrix shows that the OLS estimator from equation (7.16) can be written as ^b ¼ ð ^b 0 1; ^b 0 2; . . . ; ^b 0 GÞ0, where each ^bg is just the single-equation OLS estimator from the gth equation. In other words, system OLS estimation of a SUR model (without restrictions on the parameter vectors bg) is equivalent to OLS equation by equation. Assumption SOLS.2 is easily seen to hold if Eðx0 igxigÞ is nonsingular for all g. Example 7.2 (Panel Data, continued): In the panel data case, X N i¼1 X0 iXi ¼ X N i¼1 X T t¼1 x0 itxit; X N i¼1 X0 iyi ¼ X N i¼1 X T t¼1 x0 ityit Therefore, we can write ^b as ^b ¼ X N i¼1 X T t¼1 x0 itxit !\u00021 X N i¼1 X T t¼1 x0 it yit ! ð7:17Þ This estimator is called the pooled ordinary least squares (POLS) estimator because it corresponds to running OLS on the observations pooled across i and t. We men- tioned this estimator in the context of independent cross sections in Section 6.3. The estimator in equation (7.17) is for the same cross section units sampled at di¤erent points in time. Theorem 7.1 shows that the POLS estimator is consistent under the orthogonality conditions in assumption (7.12) and the mild condition rank EðPT t¼1 x0 itxitÞ ¼ K. In the general system (7.9), the system OLS estimator does not necessarily have an interpretation as OLS equation by equation or as pooled OLS. As we will see in Section 7.7 for the SUR setup, sometimes we want to impose cross equation restric- tions on the bg, in which case the system OLS estimator has no simple interpretation. While OLS is consistent under Assumptions SOLS.1 and SOLS.2, it is not neces- sarily unbiased. Assumption (7.13), and the ﬁnite sample assumption rankðX0XÞ ¼ K, do ensure unbiasedness of OLS conditional on X. [This conclusion follows be- cause, under independent sampling, Eðui j X1; X2; . . . ; XNÞ ¼ Eðui j XiÞ ¼ 0 under as- Chapter 7 150", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 164, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p165::c0", "text": "sumption (7.13).] We focus on the weaker Assumption SOLS.1 because assumption (7.13) is often violated in economic applications, something we will see especially in our panel data analysis. For inference, we need to ﬁnd the asymptotic variance of the OLS estimator under essentially the same two assumptions; technically, the following derivation requires the elements of X0 iuiu0 iXi to have ﬁnite expected absolute value. From (7.16) and (7.9) write ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ ¼ N\u00021 X N i¼1 X0 iXi !\u00021 N\u00021=2 X N i¼1 X0 iui ! Because EðX0 iuiÞ ¼ 0 under Assumption SOLS.1, the CLT implies that N\u00021=2 X N i¼1 X0 iui ! d Normalð0; BÞ ð7:18Þ where B 1 EðX0 iuiu0 iXiÞ 1 VarðX0 iuiÞ ð7:19Þ In particular, N\u00021=2 PN i¼1 X0 iui ¼ Opð1Þ. But ðX0X=NÞ\u00021 ¼ A\u00021 þ opð1Þ, so ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ ¼ A\u00021 N\u00021=2 X N i¼1 X0 iui ! þ ½ðX0X=NÞ\u00021 \u0002 A\u00021\u0004 N\u00021=2 X N i¼1 X0 iui ! ¼ A\u00021 N\u00021=2 X N i¼1 X0 iui ! þ opð1Þ \u0003 Opð1Þ ¼ A\u00021 N\u00021=2 X N i¼1 X0 iui ! þ opð1Þ ð7:20Þ Therefore, just as with single-equation OLS and 2SLS, we have obtained an asymp- totic representation for ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ that is a nonrandom linear combination of a par- tial sum that satisﬁes the CLT. Equations (7.18) and (7.20) and the asymptotic equivalence lemma imply ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ ! d Normalð0; A\u00021BA\u00021Þ ð7:21Þ We summarize with a theorem. theorem 7.2 (Asymptotic Normality of SOLS): Under Assumptions SOLS.1 and SOLS.2, equation (7.21) holds. Estimating Systems of Equations by OLS and GLS 151", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 165, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p166::c0", "text": "The asymptotic variance of ^b is Avarð ^bÞ ¼ A\u00021BA\u00021=N ð7:22Þ so that Avarð ^bÞ shrinks to zero at the rate 1=N, as expected. Consistent estimation of A is simple: ^A 1 X0X=N ¼ N\u00021 X N i¼1 X0 iXi ð7:23Þ A consistent estimator of B can be found using the analogy principle. First, because B ¼ EðX0 iuiu0 iXiÞ, N\u00021 PN i¼1 X0 iuiu0 iXi ! p B. Since the ui are not observed, we replace them with the SOLS residuals: ^ui 1 yi \u0002 Xi ^b ¼ ui \u0002 Xið ^b \u0002 bÞ ð7:24Þ Using matrix algebra and the law of large numbers, it can be shown that ^B 1 N\u00021 X N i¼1 X0 i^ui^u0 iXi ! p B ð7:25Þ [To establish equation (7.25), we need to assume that certain moments involving Xi and ui are ﬁnite.] Therefore, Avar ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ is consistently estimated by ^A\u00021^B^A\u00021, and Avarð ^bÞ is estimated as ^V 1 X N i¼1 X0 iXi !\u00021 X N i¼1 X0 i^ui^u0 iXi ! X N i¼1 X0 iXi !\u00021 ð7:26Þ Under Assumptions SOLS.1 and SOLS.2, we perform inference on b as if ^b is nor- mally distributed with mean b and variance matrix (7.26). The square roots of the diagonal elements of the matrix (7.26) are reported as the asymptotic standard errors. The t ratio, ^bj=seð ^bjÞ, has a limiting normal distribution under the null hypothesis H0: bj ¼ 0. Sometimes the t statistics are treated as being distributed as tNG\u0002K, which is asymptotically valid because NG \u0002 K should be large. The estimator in matrix (7.26) is another example of a robust variance matrix esti- mator because it is valid without any second-moment assumptions on the errors ui (except, as usual, that the second moments are well deﬁned). In a multivariate setting it is important to know what this robustness allows. First, the G \u0001 G unconditional variance matrix, W 1 Eðuiu0 iÞ, is entirely unrestricted. This fact allows cross equation correlation in an SUR system as well as di¤erent error variances in each equation. In panel data models, an unrestricted W allows for arbitrary serial correlation and Chapter 7 152", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 166, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p167::c0", "text": "time-varying variances in the disturbances. A second kind of robustness is that the conditional variance matrix, Varðui j XiÞ, can depend on Xi in an arbitrary, unknown fashion. The generality a¤orded by formula (7.26) is possible because of the N ! y asymptotics. In special cases it is useful to impose more structure on the conditional and un- conditional variance matrix of ui in order to simplify estimation of the asymptotic variance. We will cover an important case in Section 7.5.2. Essentially, the key re- striction will be that the conditional and unconditional variances of ui are the same. There are also some special assumptions that greatly simplify the analysis of the pooled OLS estimator for panel data; see Section 7.8. 7.3.3 Testing Multiple Hypotheses Testing multiple hypotheses in a very robust manner is easy once ^V in matrix (7.26) has been obtained. The robust Wald statistic for testing H0: Rb ¼ r, where R is Q \u0001 K with rank Q and r is Q \u0001 1, has its usual form, W ¼ ðR^b \u0002 rÞ0ðR^VR0Þ\u00021ðR^b \u0002 rÞ. Under H0, W @ a w2 Q. In the SUR case this is the easiest and most robust way of testing cross equation restrictions on the parameters in di¤erent equations using sys- tem OLS. In the panel data setting, the robust Wald test provides a way of testing multiple hypotheses about b without assuming homoskedasticity or serial indepen- dence of the errors. 7.4 Consistency and Asymptotic Normality of Generalized Least Squares 7.4.1 Consistency System OLS is consistent under fairly weak assumptions, and we have seen how to perform robust inference using OLS. If we strengthen Assumption SOLS.1 and add assumptions on the conditional variance matrix of ui, we can do better using a gen- eralized least squares procedure. As we will see, GLS is not usually feasible because it requires knowing the variance matrix of the errors up to a multiplicative constant. Nevertheless, deriving the consistency and asymptotic distribution of the GLS esti- mator is worthwhile because it turns out that the feasible GLS estimator is asymp- totically equivalent to GLS. We start with the model (7.9), but consistency of GLS generally requires a stronger assumption than Assumption SOLS.1. We replace Assumption SOLS.1 with the as- sumption that each element of ui is uncorrelated with each element of Xi. We can state this succinctly using the Kronecker product: Estimating Systems of Equations by OLS and GLS 153", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 167, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p168::c0", "text": "assumption SGLS.1: EðXi n uiÞ ¼ 0. Typically, at least one element of Xi is unity, so in practice Assumption SGLS.1 implies that EðuiÞ ¼ 0. We will assume ui has a zero mean for our discussion but not in proving any results. Assumption SGLS.1 plays a crucial role in establishing consistency of the GLS estimator, so it is important to recognize that it puts more restrictions on the ex- planatory variables than does Assumption SOLS.1. In other words, when we allow the explanatory variables to be random, GLS requires a stronger assumption than system OLS in order to be consistent. Su‰cient for Assumption SGLS.1, but not necessary, is the zero conditional mean assumption (7.13). This conclusion follows from a standard iterated expectations argument. For GLS estimation of multivariate equations with i.i.d. observations, the second- moment matrix of ui plays a key role. Deﬁne the G \u0001 G symmetric, positive semi- deﬁnite matrix W 1 Eðuiu0 iÞ ð7:27Þ As mentioned in Section 7.3.2, we call W the unconditional variance matrix of ui. [In the rare case that EðuiÞ 0 0, W is not the variance matrix of ui, but it is always the appropriate matrix for GLS estimation.] It is important to remember that expression (7.27) is deﬁnitional: because we are using random sampling, the unconditional vari- ance matrix is necessarily the same for all i. In place of Assumption SOLS.2, we assume that a weighted version of the expected outer product of Xi is nonsingular. assumption SGLS.2: W is positive deﬁnite and EðX0 iW\u00021XiÞ is nonsingular. For the general treatment we assume that W is positive deﬁnite, rather than just positive semideﬁnite. In applications where the dependent variables across equations satisfy an adding up constraint—such as expenditure shares summing to unity—an equation must be dropped to ensure that W is nonsingular, a topic we return to in Section 7.7.3. As a practical matter, Assumption SGLS.2 is not very restrictive. The assumption that the K \u0001 K matrix EðX0 iW\u00021XiÞ has rank K is the analogue of As- sumption SOLS.2. The usual motivation for the GLS estimator is to transform a system of equations where the error has nonscalar variance-covariance matrix into a system where the error vector has a scalar variance-covariance matrix. We obtain this by multiplying equation (7.9) by W\u00021=2: Chapter 7 154", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 168, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p169::c0", "text": "W\u00021=2yi ¼ ðW\u00021=2XiÞb þ W\u00021=2ui; or y\u0005 i ¼ X\u0005 i b þ u\u0005 i ð7:28Þ Simple algebra shows that Eðu\u0005 i u\u00050 i Þ ¼ IG. Now we estimate equation (7.28) by system OLS. (As yet, we have no real justiﬁ- cation for this step, but we know SOLS is consistent under some assumptions.) Call this estimator b \u0005. Then b \u0005 1 X N i¼1 X\u00050 i X\u0005 i !\u00021 X N i¼1 X\u00050 i y\u0005 i ! ¼ X N i¼1 X0 iW\u00021Xi !\u00021 X N i¼1 X0 iW\u00021yi ! ð7:29Þ This is the generalized least squares (GLS) estimator of b. Under Assumption SGLS.2, b \u0005 exists with probability approaching one as N ! y. We can write b \u0005 using full matrix notation as b \u0005 ¼ ½X0ðIN n W\u00021ÞX\u0004\u00021 \u0003 ½X0ðIN n W\u00021ÞY\u0004, where X and Y are the data matrices deﬁned in Section 7.3.2 and IN is the N \u0001 N identity matrix. But for establishing the asymptotic properties of b \u0005, it is most convenient to work with equation (7.29). We can establish consistency of b \u0005 under Assumptions SGLS.1 and SGLS.2 by writing b \u0005 ¼ b þ N\u00021 X N i¼1 X0 iW\u00021Xi !\u00021 N\u00021 X N i¼1 X0 iW\u00021ui ! ð7:30Þ By the weak law of large numbers (WLLN), N\u00021 PN i¼1 X0 iW\u00021Xi ! p EðX0 iW\u00021XiÞ. By Assumption SGLS.2 and Slutsky’s theorem (Lemma 3.4), N\u00021 PN i¼1 X0 iW\u00021Xi \u0002 \u0003\u00021 ! p A\u00021, where A is now deﬁned as A 1 EðX0 iW\u00021XiÞ ð7:31Þ Now we must show that plim N\u00021 PN i¼1 X0 iW\u00021ui ¼ 0. By the WLLN, it is su‰cient that EðX0 iW\u00021uiÞ ¼ 0. This is where Assumption SGLS.1 comes in. We can argue this point informally because W\u00021Xi is a linear combination of Xi, and since each element of Xi is uncorrelated with each element of ui, any linear combination of Xi is uncor- related with ui. We can also show this directly using the algebra of Kronecker prod- ucts and vectorization. For conformable matrices D, E, and F, recall that vecðDEFÞ ¼ ðF0 n DÞ vecðEÞ, where vecðCÞ is the vectorization of the matrix C. [That is, vecðCÞ is the column vector obtained by stacking the columns of C from ﬁrst to last; see Theil (1983).] Therefore, under Assumption SGLS.1, vec EðX0 iW\u00021uiÞ ¼ E½ðu0 i n X0 iÞ\u0004 vecðW\u00021Þ ¼ E½ðui n XiÞ0\u0004 vecðW\u00021Þ ¼ 0 Estimating Systems of Equations by OLS and GLS 155", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 169, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p170::c0", "text": "where we have also used the fact that the expectation and vec operators can be interchanged. We can now read the consistency of the GLS estimator o¤ of equation (7.30). We do not state this conclusion as a theorem because the GLS estimator itself is rarely available. The proof of consistency that we have sketched fails if we only make Assumption SOLS.1: EðX0 iuiÞ ¼ 0 does not imply EðX0 iW\u00021uiÞ ¼ 0, except when W and Xi have special structures. If Assumption SOLS.1 holds but Assumption SGLS.1 fails, the transformation in equation (7.28) generally induces correlation between X\u0005 i and u\u0005 i . This can be an important point, especially for certain panel data applications. If we are willing to make the zero conditional mean assumption (7.13), b \u0005 can be shown to be unbiased conditional on X. 7.4.2 Asymptotic Normality We now sketch the asymptotic normality of the GLS estimator under Assumptions SGLS.1 and SGLS.2 and some weak moment conditions. The ﬁrst step is familiar: ﬃﬃﬃﬃ N p ðb \u0005 \u0002 bÞ ¼ N\u00021 X N i¼1 X0 iW\u00021Xi !\u00021 N\u00021=2 X N i¼1 X0 iW\u00021ui ! ð7:32Þ By the CLT, N\u00021=2 PN i¼1 X0 iW\u00021ui ! d Normalð0; BÞ, where B 1 EðX0 iW\u00021uiu0 iW\u00021XiÞ ð7:33Þ Further, since N\u00021=2 PN i¼1 X0 iW\u00021ui ¼ Opð1Þ and ðN\u00021 PN i¼1 X0 iW\u00021XiÞ\u00021 \u0002 A\u00021 ¼ opð1Þ, we can write ﬃﬃﬃﬃ N p ðb \u0005 \u0002 bÞ ¼ A\u00021ðN\u00021=2 PN i¼1 x0 iW\u00021uiÞ þ opð1Þ. It follows from the asymptotic equivalence lemma that ﬃﬃﬃﬃ N p ðb \u0005 \u0002 bÞ @ a Normalð0; A\u00021BA\u00021Þ ð7:34Þ Thus, Avarð ^bÞ ¼ A\u00021BA\u00021=N ð7:35Þ The asymptotic variance in equation (7.35) is not the asymptotic variance usually derived for GLS estimation of systems of equations. Usually the formula is reported as A\u00021=N. But equation (7.35) is the appropriate expression under the assumptions made so far. The simpler form, which results when B ¼ A, is not generally valid under Assumptions SGLS.1 and SGLS.2, because we have assumed nothing about the variance matrix of ui conditional on Xi. In Section 7.5.2 we make an assumption that simpliﬁes equation (7.35). Chapter 7 156", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 170, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p171::c0", "text": "7.5 Feasible GLS 7.5.1 Asymptotic Properties Obtaining the GLS estimator b \u0005 requires knowing W up to scale. That is, we must be able to write W ¼ s2C where C is a known G \u0001 G positive deﬁnite matrix and s2 is allowed to be an unknown constant. Sometimes C is known (one case is C ¼ IG), but much more often it is unknown. Therefore, we now turn to the analysis of feasible GLS (FGLS) estimation. In FGLS estimation we replace the unknown matrix W with a consistent estimator. Because the estimator of W appears highly nonlinearly in the expression for the FGLS estimator, deriving ﬁnite sample properties of FGLS is generally di‰cult. [However, under essentially assumption (7.13) and some additional assumptions, including symmetry of the distribution of ui, Kakwani (1967) showed that the distri- bution of the FGLS is symmetric about b, a property which means that the FGLS is unbiased if its expected value exists; see also Schmidt (1976, Section 2.5).] The asymptotic properties of the FGLS estimator are easily established as N ! y be- cause, as we will show, its ﬁrst-order asymptotic properties are identical to those of the GLS estimator under Assumptions SGLS.1 and SGLS.2. It is for this purpose that we spent some time on GLS. After establishing the asymptotic equivalence, we can easily obtain the limiting distribution of the FGLS estimator. Of course, GLS is trivially a special case of FGLS, where there is no ﬁrst-stage estimation error. We assume we have a consistent estimator, ^W, of W: plim N!y ^W ¼ W ð7:36Þ [Because the dimension of ^W does not depend on N, equation (7.36) makes sense when deﬁned element by element.] When W is allowed to be a general positive deﬁnite matrix, the following estimation approach can be used. First, obtain the system OLS estimator of b, which we denote ^^b^b in this section to avoid confusion. We already showed that ^^b^b is consistent for b under Assumptions SOLS.1 and SOLS.2, and therefore under Assumptions SGLS.1 and SOLS.2. (In what follows, we assume that Assumptions SOLS.2 and SGLS.2 both hold.) By the WLLN, plimðN\u00021 PN i¼1 uiu0 iÞ ¼ W, and so a natural estimator of W is ^W 1 N\u00021 X N i¼1 ^^u^ui^^u^u0 i ð7:37Þ Estimating Systems of Equations by OLS and GLS 157", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 171, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p172::c0", "text": "where ^^u^ui 1 yi \u0002 Xi ^^b^b are the SOLS residuals. We can show that this estimator is con- sistent for W under Assumptions SGLS.1 and SOLS.2 and standard moment con- ditions. First, write ^^u^ui ¼ ui \u0002 Xið ^^b^b \u0002 bÞ ð7:38Þ so that ^^u^ui^^u^u0 i ¼ uiu0 i \u0002 uið ^^b^b \u0002 bÞ0X0 i \u0002 Xið ^^b^b \u0002 bÞu0 i þ Xið ^^b^b \u0002 bÞð ^^b^b \u0002 bÞ0X0 i ð7:39Þ Therefore, it su‰ces to show that the averages of the last three terms converge in probability to zero. Write the average of the vec of the ﬁrst term as N\u00021 PN i¼1ðXi n uiÞ \u0003 ð ^^b^b \u0002 bÞ, which is opð1Þ because plimð ^^b^b \u0002 bÞ ¼ 0 and N\u00021 PN i¼1ðXi n uiÞ ! p 0. The third term is the transpose of the second. For the last term in equation (7.39), note that the average of its vec can be written as N\u00021 X N i¼1 ðXi n XiÞ \u0003 vecfð ^^b^b \u0002 bÞð ^^b^b \u0002 bÞ0g ð7:40Þ Now vecfð ^^b^b \u0002 bÞð ^^b^b \u0002 bÞ0g ¼ opð1Þ. Further, assuming that each element of Xi has ﬁnite second moment, N\u00021 PN i¼1ðXi n XiÞ ¼ Opð1Þ by the WLLN. This step takes care of the last term, since Opð1Þ \u0003 opð1Þ ¼ opð1Þ. We have shown that ^W ¼ N\u00021 X N i¼1 uiu0 i þ opð1Þ ð7:41Þ and so equation (7.36) follows immediately. [In fact, a more careful analysis shows that the opð1Þ in equation (7.41) can be replaced by opðN\u00021=2Þ; see Problem 7.4.] Sometimes the elements of W are restricted in some way (an important example is the random e¤ects panel data model that we will cover in Chapter 10). In such cases a di¤erent estimator of W is often used that exploits these restrictions. As with ^W in equation (7.37), such estimators typically use the system OLS residuals in some fashion and lead to consistent estimators assuming the structure of W is correctly speciﬁed. The advantage of equation (7.37) is that it is consistent for W quite gener- ally. However, if N is not very large relative to G, equation (7.37) can have poor ﬁnite sample properties. Given ^W, the feasible GLS (FGLS) estimator of b is ^b ¼ X N i¼1 X0 i ^W\u00021Xi !\u00021 X N i¼1 X0 i ^W\u00021yi ! ð7:42Þ or, in full matrix notation, ^b ¼ ½X0ðIN n ^W\u00021ÞX\u0004\u00021½X0ðIN n ^W\u00021ÞY\u0004. Chapter 7 158", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 172, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p173::c0", "text": "We have already shown that the (infeasible) GLS estimator is consistent under Assumptions SGLS.1 and SGLS.2. Because ^W converges to W, it is not surprising that FGLS is also consistent. Rather than show this result separately, we verify the stronger result that FGLS has the same limiting distribution as GLS. The limiting distribution of FGLS is obtained by writing ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ ¼ N\u00021 X N i¼1 X0 i ^W\u00021Xi !\u00021 N\u00021=2 X N i¼1 X0 i ^W\u00021ui ! ð7:43Þ Now N\u00021=2 X N i¼1 X0 i ^W\u00021ui \u0002 N\u00021=2 X N i¼1 X0 iW\u00021ui ¼ N\u00021=2 X N i¼1 ðui n XiÞ0 \" # vecð ^W\u00021 \u0002 W\u00021Þ Under Assumption SGLS.1, the CLT implies that N\u00021=2 PN i¼1ðui n XiÞ ¼ Opð1Þ. Because Opð1Þ \u0003 opð1Þ ¼ opð1Þ, it follows that N\u00021=2 X N i¼1 X0 i ^W\u00021ui ¼ N\u00021=2 X N i¼1 X0 iW\u00021ui þ opð1Þ A similar argument shows that N\u00021 PN i¼1 X0 i ^W\u00021Xi ¼ N\u00021 PN i¼1 X0 iW\u00021Xi þ opð1Þ. Therefore, we have shown that ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ ¼ N\u00021 X N i¼1 X0 iW\u00021Xi !\u00021 N\u00021=2 X N i¼1 X0 iW\u00021ui ! þ opð1Þ ð7:44Þ The ﬁrst term in equation (7.44) is just ﬃﬃﬃﬃ N p ðb \u0005 \u0002 bÞ, where b \u0005 is the GLS estimator. We can write equation (7.44) as ﬃﬃﬃﬃ N p ð ^b \u0002 b \u0005Þ ¼ opð1Þ ð7:45Þ which shows that ^b and b \u0005 are ﬃﬃﬃﬃ N p -equivalent. Recall from Chapter 3 that this statement is much stronger than simply saying that b \u0005 and ^b are both consistent for b. There are many estimators, such as system OLS, that are consistent for b but are not ﬃﬃﬃﬃ N p -equivalent to b \u0005. The asymptotic equivalence of ^b and b \u0005 has practically important consequences. The most important of these is that, for performing asymptotic inference about b using ^b, we do not have to worry that ^W is an estimator of W. Of course, whether the asymptotic approximation gives a reasonable approximation to the actual distribu- tion of ^b is di‰cult to tell. With large N, the approximation is usually pretty good. Estimating Systems of Equations by OLS and GLS 159", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 173, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p174::c0", "text": "But if N is small relative to G, ignoring estimation of W in performing inference about b can be misleading. We summarize the limiting distribution of FGLS with a theorem. theorem 7.3 (Asymptotic Normality of FGLS): Under Assumptions SGLS.1 and SGLS.2, ﬃﬃﬃﬃ N p ð ^b \u0002 bÞ @ a Normalð0; A\u00021BA\u00021Þ ð7:46Þ where A is deﬁned in equation (7.31) and B is deﬁned in equation (7.33). In the FGLS context a consistent estimator of A is ^A 1 N\u00021 X N i¼1 X0 i ^W\u00021Xi ð7:47Þ A consistent estimator of B is also readily available after FGLS estimation. Deﬁne the FGLS residuals by ^ui 1 yi \u0002 Xi ^b; i ¼ 1; 2; . . . ; N ð7:48Þ [The only di¤erence between the FGLS and SOLS residuals is that the FGLS esti- mator is inserted in place of the SOLS estimator; in particular, the FGLS residuals are not from the transformed equation (7.28).] Using standard arguments, a consis- tent estimator of B is ^B 1 N\u00021 X N i¼1 X0 i ^W\u00021^ui^u0 i ^W\u00021Xi The estimator of Avarð ^bÞ can be written as ^A\u00021^B^A\u00021=N ¼ X N i¼1 X0 i ^W\u00021Xi !\u00021 X N i¼1 X0 i ^W\u00021^ui^u0 i ^W\u00021Xi ! X N i¼1 X0 i ^W\u00021Xi !\u00021 ð7:49Þ This is the extension of the White (1980b) heteroskedasticity-robust asymptotic vari- ance estimator to the case of systems of equations; see also White (1984). This esti- mator is valid under Assumptions SGLS.1 and SGLS.2; that is, it is completely robust. 7.5.2 Asymptotic Variance of FGLS under a Standard Assumption Under the assumptions so far, FGLS really has nothing to o¤er over SOLS. In ad- dition to being computationally more di‰cult, FGLS is less robust than SOLS. So why is FGLS used? The answer is that, under an additional assumption, FGLS is Chapter 7 160", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 174, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p175::c0", "text": "asymptotically more e‰cient than SOLS (and other estimators). First, we state the weakest condition that simpliﬁes estimation of the asymptotic variance for FGLS. For reasons to be seen shortly, we call this a system homoskedasticity assumption. assumption SGLS.3: EðX0 iW\u00021uiu0 iW\u00021XiÞ ¼ EðX0 iW\u00021XiÞ, where W 1 Eðuiu0 iÞ. Another way to state this assumption is, B ¼ A, which, from expression (7.46), sim- pliﬁes the asymptotic variance. As stated, Assumption SGLS.3 is somewhat di‰cult to interpret. When G ¼ 1, it reduces to Assumption OLS.3. When W is diagonal and Xi has either the SUR or panel data structure, Assumption SGLS.3 implies a kind of conditional homoskedasticity in each equation (or time period). Generally, Assump- tion SGLS.3 puts restrictions on the conditional variances and covariances of ele- ments of ui. A su‰cient (though certainly not necessary) condition for Assumption SGLS.3 is easier to interpret: Eðuiu0 i j XiÞ ¼ Eðuiu0 iÞ ð7:50Þ If Eðui j XiÞ ¼ 0, then assumption (7.50) is the same as assuming Varðui j XiÞ ¼ VarðuiÞ ¼ W, which means that each variance and each covariance of elements involving ui must be constant conditional on all of Xi. This is a very natural way of stating a system homoskedasticity assumption, but it is sometimes too strong. When G ¼ 2, W contains three distinct elements, s2 1 ¼ Eðu2 i1Þ, s2 2 ¼ Eðu2 i2Þ, and s12 ¼ Eðui1ui2Þ. These elements are not restricted by the assumptions we have made. (The inequality js12j < s1s2 must always hold for W to be a nonsingular covariance matrix.) However, assumption (7.50) requires Eðu2 i1 j XiÞ ¼ s2 1, Eðu2 i2 j XiÞ ¼ s2 2, and Eðui1ui2 j XiÞ ¼ s12: the conditional variances and covariance must not depend on Xi. That assumption (7.50) implies Assumption SGLS.3 is a consequence of iterated expectations: EðX0 iW\u00021uiu0 iW\u00021XiÞ ¼ E½EðX0 iW\u00021uiu0 iW\u00021Xi j XiÞ\u0004 ¼ E½X0 iW\u00021Eðuiu0 i j XiÞW\u00021Xi\u0004 ¼ EðX0 iW\u00021WW\u00021XiÞ ¼ EðX0 iW\u00021XiÞ While assumption (7.50) is easier to intepret, we use Assumption SGLS.3 for stating the next theorem because there are cases, including some dynamic panel data models, where Assumption SGLS.3 holds but assumption (7.50) does not. theorem 7.4 (Usual Variance Matrix for FGLS): Under Assumptions SGLS.1– SGLS.3, the asymptotic variance of the FGLS estimator is Avarð ^bÞ ¼ A\u00021=N 1 ½EðX0 iW\u00021XiÞ\u0004\u00021=N. Estimating Systems of Equations by OLS and GLS 161", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 175, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p176::c0", "text": "We obtain an estimator of Avarð ^bÞ by using our consistent estimator of A: Av^arð ^bÞ ¼ ^A\u00021=N ¼ X N i¼1 X0 i ^W\u00021Xi !\u00021 ð7:51Þ Equation (7.51) is the ‘‘usual’’ formula for the asymptotic variance of FGLS. It is nonrobust in the sense that it relies on Assumption SGLS.3 in addition to Assump- tions SGLS.1 and SGLS.2. If heteroskedasticity in ui is suspected, then the robust estimator (7.49) should be used. Assumption (7.50) also has important e‰ciency implications. One consequence of Problem 7.2 is that, under Assumptions SGLS.1, SOLS.2, SGLS.2, and (7.50), the FGLS estimator is more e‰cient than the system OLS estimator. We can actually say much more: FGLS is more e‰cient than any other estimator that uses the ortho- gonality conditions EðXi n uiÞ ¼ 0. This conclusion will follow as a special case of Theorem 8.4 in Chapter 8, where we deﬁne the class of competing estimators. If we replace Assumption SGLS.1 with the zero conditional mean assumption (7.13), then an even stronger e‰ciency result holds for FGLS, something we treat in Section 8.6. 7.6 Testing Using FGLS Asymptotic standard errors are obtained in the usual fashion from the asymptotic variance estimates. We can use the nonrobust version in equation (7.51) or, even better, the robust version in equation (7.49), to construct t statistics and conﬁdence intervals. Testing multiple restrictions is fairly easy using the Wald test, which always has the same general form. The important consideration lies in choosing the asymp- totic variance estimate, ^V. Standard Wald statistics use equation (7.51), and this approach produces limiting chi-square statistics under the homoskedasticity assump- tion SGLS.3. Completely robust Wald statistics are obtained by choosing ^V as in equation (7.49). If Assumption SGLS.3 holds under H0, we can deﬁne a statistic based on the weighted sums of squared residuals. To obtain the statistic, we estimate the model with and without the restrictions imposed on b, where the same estimator of W, usu- ally based on the unrestricted SOLS residuals, is used in obtaining the restricted and unrestricted FGLS estimators. Let ~ui denote the residuals from constrained FGLS (with Q restrictions imposed on ~b) using variance matrix ^W. It can be shown that, under H0 and Assumptions SGLS.1–SGLS.3, Chapter 7 162", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 176, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p177::c0", "text": "X N i¼1 ~u0 i ^W\u00021~ui \u0002 X N i¼1 ^u0 i ^W\u00021^ui ! @ a w2 Q ð7:52Þ Gallant (1987) shows expression (7.52) for nonlinear models with ﬁxed regressors; essentially the same proof works here under Assumptions SGLS.1–SGLS.3, as we will show more generally in Chapter 12. The statistic in expression (7.52) is the di¤erence between the transformed sum of squared residuals from the restricted and unrestricted models, but it is just as easy to calculate expression (7.52) directly. Gallant (1987, Chapter 5) has found that an F statistic has better ﬁnite sample properties. The F statistic in this context is deﬁned as F ¼ X N i¼1 ~u0 i ^W\u00021~ui \u0002 X N i¼1 ^u0 i ^W\u00021^ui !\u0004 X N i¼1 ^u0 i ^W\u00021^ui ! \" # ½ðNG \u0002 KÞ\u0004=Q ð7:53Þ Why can we treat this equation as having an approximate F distribution? First, for NG \u0002 K large, FQ;NG\u0002K @ a w2 Q=Q. Therefore, dividing expression (7.52) by Q gives us an approximate FQ;NG\u0002K distribution. The presence of the other two terms in equation (7.53) is to improve the F-approximation. Since Eðu0 iW\u00021uiÞ ¼ trfEðW\u00021uiu0 iÞg ¼ trfEðW\u00021WÞg ¼ G, it follows that ðNGÞ\u00021 PN i¼1 u0 iW\u00021ui ! p 1; re- placing u0 iW\u00021ui with ^u0 i ^W\u00021^ui does not a¤ect this consistency result. Subtracting o¤ K as a degrees-of-freedom adjustment changes nothing asymptotically, and so ðNG \u0002 KÞ\u00021 PN i¼1 ^u0 i ^W\u00021^ui ! p 1. Multiplying expression (7.52) by the inverse of this quantity does not a¤ect its asymptotic distribution. 7.7 Seemingly Unrelated Regressions, Revisited We now return to the SUR system in assumption (7.2). We saw in Section 7.3 how to write this system in the form (7.9) if there are no cross equation restrictions on the bg. We also showed that the system OLS estimator corresponds to estimating each equation separately by OLS. As mentioned earlier, in most applications of SUR it is reasonable to assume that Eðx0 iguihÞ ¼ 0, g; h ¼ 1; 2; . . . ; G, which is just Assumption SGLS.1 for the SUR structure. Under this assumption, FGLS will consistently estimate the bg. OLS equation by equation is simple to use and leads to standard inference for each bg under the OLS homoskedasticity assumption Eðu2 ig j xigÞ ¼ s2 g, which is standard in SUR contexts. So why bother using FGLS in such applications? There are two answers. First, as mentioned in Section 7.5.2, if we can maintain assumption (7.50) in addition to Assumption SGLS.1 (and SGLS.2), FGLS is asymptotically at least as Estimating Systems of Equations by OLS and GLS 163", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 177, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p178::c0", "text": "e‰cient as system OLS. Second, while OLS equation by equation allows us to easily test hypotheses about the coe‰cients within an equation, it does not provide a con- venient way for testing cross equation restrictions. It is possible to use OLS for testing cross equation restrictions by using the variance matrix (7.26), but if we are willing to go through that much trouble, we should just use FGLS. 7.7.1 Comparison between OLS and FGLS for SUR Systems There are two cases where OLS equation by equation is algebraically equivalent to FGLS. The ﬁrst case is fairly straightforward to analyze in our setting. theorem 7.5 (Equivalence of FGLS and OLS, I): If ^W is a diagonal matrix, then OLS equation by equation is identical to FGLS. Proof: If ^W is diagonal, then ^W\u00021 ¼ diagð^s\u00022 1 ; . . . ; ^s\u00022 G Þ. With Xi deﬁned as in the matrix (7.10), straightforward algebra shows that X0 i ^W\u00021Xi ¼ ^C\u00021X0 iXi and X0 i ^W\u00021yi ¼ ^C\u00021X0 iyi where ^C is the block diagonal matrix with ^s2 gIkg as its gth block. It follows that the FGLS estimator can be written as ^b ¼ X N i¼1 ^C\u00021X0 iXi !\u00021 X N i¼1 ^C\u00021X0 iyi ! ¼ X N i¼1 X0 iXi !\u00021 X N i¼1 X0 iyi ! which is the system OLS estimator. In applications, ^W would not be diagonal unless we impose a diagonal structure. Nevertheless, we can use Theorem 7.5 to obtain an asymptotic equivalance result when W is diagonal. If W is diagonal, then the GLS and OLS are algebraically iden- tical (because GLS uses W). We know that FGLS and GLS are ﬃﬃﬃﬃ N p -asymptotically equivalent for any W. Therefore, OLS and FGLS are ﬃﬃﬃﬃ N p -asymptotically equivalent if W is diagonal, even though they are not algebraically equivalent (because ^W is not diagonal). The second algebraic equivalence result holds without any restrictions on ^W. It is special in that it assumes that the same regressors appear in each equation. theorem 7.6 (Equivalence of FGLS and OLS, II): If xi1 ¼ xi2 ¼ \u0003 \u0003 \u0003 ¼ xiG for all i, that is, if the same regressors show up in each equation (for all observations), then OLS equation by equation and FGLS are identical. In practice, Theorem 7.6 holds when the population model has the same explanatory variables in each equation. The usual proof of this result groups all N observations Chapter 7 164", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 178, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p179::c0", "text": "for the ﬁrst equation followed by the N observations for the second equation, and so on (see, for example, Greene, 1997, Chapter 17). Problem 7.5 asks you to prove Theorem 7.6 in the current setup, where we have ordered the observations to be amenable to asymptotic analysis. It is important to know that when every equation contains the same regressors in an SUR system, there is still a good reason to use a SUR software routine in obtaining the estimates: we may be interested in testing joint hypotheses involving parameters in di¤erent equations. In order to do so we need to estimate the variance matrix of ^b (not just the variance matrix of each ^bg, which only allows tests of the coe‰cients within an equation). Estimating each equation by OLS does not directly yield the covariances between the estimators from di¤erent equations. Any SUR routine will perform this operation automatically, then compute F statistics as in equation (7.53) (or the chi-square alternative, the Wald statistic). Example 7.3 (SUR System for Wages and Fringe Beneﬁts): We use the data on wages and fringe beneﬁts in FRINGE.RAW to estimate a two-equation system for hourly wage and hourly beneﬁts. There are 616 workers in the data set. The FGLS estimates are given in Table 7.1, with asymptotic standard errors in parentheses below estimated coe‰cients. The estimated coe‰cients generally have the signs we expect. Other things equal, people with more education have higher hourly wage and beneﬁts, males have higher predicted wages and beneﬁts ($1.79 and 27 cents higher, respectively), and people with more tenure have higher earnings and beneﬁts, although the e¤ect is diminishing in both cases. (The turning point for hrearn is at about 10.8 years, while for hrbens it is 22.5 years.) The coe‰cients on experience are interesting. Experience is estimated to have a dimininshing e¤ect for beneﬁts but an increasing e¤ect for earnings, although the estimated upturn for earnings is not until 9.5 years. Belonging to a union implies higher wages and beneﬁts, with the beneﬁts coe‰cient being especially statistically signiﬁcant ðtA7:5Þ. The errors across the two equations appear to be positively correlated, with an estimated correlation of about .32. This result is not surprising: the same unobserv- ables, such as ability, that lead to higher earnings, also lead to higher beneﬁts. Clearly there are signiﬁcant di¤erences between males and females in both earn- ings and beneﬁts. But what about between whites and nonwhites, and married and unmarried people? The F-type statistic for joint signiﬁcance of married and white in both equations is F ¼ 1:83. We are testing four restrictions ðQ ¼ 4Þ, N ¼ 616, G ¼ 2, and K ¼ 2ð13Þ ¼ 26, so the degrees of freedom in the F distribution are 4 and 1,206. The p-value is about .121, so these variables are jointly insigniﬁcant at the 10 per- cent level. Estimating Systems of Equations by OLS and GLS 165", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 179, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p180::c0", "text": "If the regressors are di¤erent in di¤erent equations, W is not diagonal, and the conditions in Section 7.5.2 hold, then FGLS is generally asymptotically more e‰cient than OLS equation by equation. One thing to remember is that the e‰ciency of FGLS comes at the price of assuming that the regressors in each equation are uncorrelated with the errors in each equation. For SOLS and FGLS to be di¤erent, the xg must vary across g. If xg varies across g, certain explanatory variables have been intentionally omitted from some equations. If we are interested in, say, the ﬁrst equation, but we make a mistake in specifying the second equation, FGLS will gen- erally produce inconsistent estimators of the parameters in all equations. However, OLS estimation of the ﬁrst equation is consistent if Eðx0 1u1Þ ¼ 0. The previous discussion reﬂects the trade-o¤ between e‰ciency and robustness that we often encounter in estimation problems. Table 7.1 An Estimated SUR Model for Hourly Wages and Hourly Beneﬁts Explanatory Variables hrearn hrbens educ .459 (.069) .077 (.008) exper \u0002.076 (.057) .023 (.007) exper2 .0040 (.0012) \u0002.0005 (.0001) tenure .110 (.084) .054 (.010) tenure2 \u0002.0051 (.0033) \u0002.0012 (.0004) union .808 (.408) .366 (.049) south \u0002.457 (.552) \u0002.023 (.066) nrtheast \u00021.151 (0.606) \u0002.057 (.072) nrthcen \u0002.636 (.556) \u0002.038 (.066) married .642 (.418) .058 (.050) white 1.141 (0.612) .090 (.073) male 1.785 (0.398) .268 (.048) intercept \u00022.632 (1.228) \u0002.890 (.147) Chapter 7 166", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 180, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p181::c0", "text": "7.7.2 Systems with Cross Equation Restrictions So far we have studied SUR under the assumption that the bg are unrelated across equations. When systems of equations are used in economics, especially for modeling consumer and producer theory, there are often cross equation restrictions on the parameters. Such models can still be written in the general form we have covered, and so they can be estimated by system OLS and FGLS. We still refer to such sys- tems as SUR systems, even though the equations are now obviously related, and system OLS is no longer OLS equation by equation. Example 7.4 (SUR with Cross Equation Restrictions): Consider the two-equation population model y1 ¼ g10 þ g11x11 þ g12x12 þ a1x13 þ a2x14 þ u1 ð7:54Þ y2 ¼ g20 þ g21x21 þ a1x22 þ a2x23 þ g24x24 þ u2 ð7:55Þ where we have imposed cross equation restrictions on the parameters in the two equations because a1 and a2 show up in each equation. We can put this model into the form of equation (7.9) by appropriately deﬁning Xi and b. For example, deﬁne b ¼ ðg10; g11; g12; a1; a2; g20; g21; g24Þ0, which we know must be an 8 \u0001 1 vector because there are 8 parameters in this system. The order in which these elements appear in b is up to us, but once b is deﬁned, Xi must be chosen accordingly. For each observa- tion i, deﬁne the 2 \u0001 8 matrix Xi ¼ 1 xi11 xi12 xi13 xi14 0 0 0 0 0 0 xi22 xi23 1 xi21 xi24 \u0005 \u0006 Multiplying Xi by b gives the equations (7.54) and (7.55). In applications such as the previous example, it is fairly straightforward to test the cross equation restrictions, especially using the sum of squared residuals statistics [equation (7.52) or (7.53)]. The unrestricted model simply allows each explanatory variable in each equation to have its own coe‰cient. We would use the unrestricted estimates to obtain ^W, and then obtain the restricted estimates using ^W. 7.7.3 Singular Variance Matrices in SUR Systems In our treatment so far we have assumed that the variance matrix W of ui is non- singular. In consumer and producer theory applications this assumption is not always true in the original structural equations, because of additivity constraints. Example 7.5 (Cost Share Equations): Suppose that, for a given year, each ﬁrm in a particular industry uses three inputs, capital (K ), labor (L), and materials (M ). Estimating Systems of Equations by OLS and GLS 167", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 181, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p182::c0", "text": "Because of regional variation and di¤erential tax concessions, ﬁrms across the United States face possibly di¤erent prices for these inputs: let piK denote the price of capital to ﬁrm i, piL be the price of labor for ﬁrm i, and siM denote the price of materials for ﬁrm i. For each ﬁrm i, let siK be the cost share for capital, let siL be the cost share for labor, and let siM be the cost share for materials. By deﬁnition, siK þ siL þ siM ¼ 1. One popular set of cost share equations is siK ¼ g10 þ g11 logðpiKÞ þ g12 logðpiLÞ þ g13 logðpiMÞ þ uiK ð7:56Þ siL ¼ g20 þ g12 logðpiKÞ þ g22 logðpiLÞ þ g23 logðpiMÞ þ uiL ð7:57Þ siM ¼ g30 þ g13 logðpiKÞ þ g23 logðpiLÞ þ g33 logðpiMÞ þ uiM ð7:58Þ where the symmetry restrictions from production theory have been imposed. The errors uig can be viewed as unobservables a¤ecting production that the economist cannot observe. For an SUR analysis we would assume that Eðui j piÞ ¼ 0 ð7:59Þ where ui 1 ðuiK; uiL; uiMÞ0 and pi 1 ðpiK; piL; piMÞ. Because the cost shares must sum to unity for each i, g10 þ g20 þ g30 ¼ 1, g11 þ g12 þ g13 ¼ 0, g12 þ g22 þ g23 ¼ 0, g13 þ g23 þ g33 ¼ 0, and uiK þ uiL þ uiM ¼ 0. This last restriction implies that W 1 VarðuiÞ has rank two. Therefore, we can drop one of the equations—say, the equation for materials—and analyze the equations for labor and capital. We can express the restrictions on the gammas in these ﬁrst two equations as g13 ¼ \u0002g11 \u0002 g12 ð7:60Þ g23 ¼ \u0002g12 \u0002 g22 ð7:61Þ Using the fact that logða=bÞ ¼ logðaÞ \u0002 logðbÞ, we can plug equations (7.60) and (7.61) into equations (7.56) and (7.57) to get siK ¼ g10 þ g11 logðpiK=piMÞ þ g12 logðpiL=piMÞ þ uiK siL ¼ g20 þ g12 logðpiK=piMÞ þ g22 logðpiL=piMÞ þ uiL We now have a two-equation system with variance matrix of full rank, with unknown parameters g10; g20; g11; g12, and g22. To write this in the form (7.9), redeﬁne ui ¼ ðuiK; uiLÞ0 and yi 1 ðsiK; siLÞ0. Take b 1 ðg10; g11; g12; g20; g22Þ0 and then Xi must be Xi 1 1 logðpiK=piMÞ logðpiL=piMÞ 0 0 0 0 logðpiK=piMÞ 1 logðpiL=piMÞ \u0005 \u0006 ð7:62Þ This formulation imposes all the conditions implied by production theory. Chapter 7 168", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 182, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p183::c0", "text": "This model could be extended in several ways. The simplest would be to allow the intercepts to depend on ﬁrm characteristics. For each ﬁrm i, let zi be a 1 \u0001 J vector of observable ﬁrm characteristics, where zi1 1 1. Then we can extend the model to siK ¼ zid1 þ g11 logðpiK=piMÞ þ g12 logðpiL=piMÞ þ uiK ð7:63Þ siL ¼ zid2 þ g12 logðpiK=piMÞ þ g22 logðpiL=piMÞ þ uiL ð7:64Þ where Eðuig j zi; piK; piL; piMÞ ¼ 0; g ¼ K; L ð7:65Þ Because we have already reduced the system to two equations, theory implies no restrictions on d1 and d2. As an exercise, you should write this system in the form (7.9). For example, if b 1 ðd0 1; g11; g12; d0 2; g22Þ0 is ð2J þ 3Þ \u0001 1, how should Xi be deﬁned? Under condition (7.65), system OLS and FGLS estimators are both consistent. (In this setup system OLS is not OLS equation by equation because g12 shows up in both equations). FGLS is asymptotically e‰cient if Varðui j zi; piÞ is constant. If Varðui j zi; piÞ depends on ðzi; piÞ—see Brown and Walker (1995) for a discussion of why we should expect it to—then we should at least use the robust variance matrix estimator for FGLS. We can easily test the symmetry assumption imposed in equations (7.63) and (7.64). One approach is to ﬁrst estimate the system without any restrictions on the parameters, in which case FGLS reduces to OLS estimation of each equation. Then, compute the t statistic of the di¤erence in the estimates on logðpiL=piMÞ in equation (7.63) and logðpiK=piMÞ in equation (7.64). Or, the F statistic from equation (7.53) can be used; ^W would be obtained from the unrestricted OLS estimation of each equation. System OLS has no robustness advantages over FGLS in this setup because we cannot relax assumption (7.65) in any useful way. 7.8 The Linear Panel Data Model, Revisited We now study the linear panel data model in more detail. Having data over time for the same cross section units is useful for several reasons. For one, it allows us to look at dynamic relationships, something we cannot do with a single cross section. A panel data set also allows us to control for unobserved cross section heterogeneity, but we will not exploit this feature of panel data until Chapter 10. Estimating Systems of Equations by OLS and GLS 169", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 183, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p184::c0", "text": "7.8.1 Assumptions for Pooled OLS We now summarize the properties of pooled OLS and feasible GLS for the linear panel data model yt ¼ xtb þ ut; t ¼ 1; 2; . . . ; T ð7:66Þ As always, when we need to indicate a particular cross section observation we include an i subscript, such as yit. This model may appear overly restrictive because b is the same in each time period. However, by appropriately choosing xit, we can allow for parameters changing over time. Also, even though we write xit, some of the elements of xit may not be time- varying, such as gender dummies when i indexes individuals, or industry dummies when i indexes ﬁrms, or state dummies when i indexes cities. Example 7.6 (Wage Equation with Panel Data): Suppose we have data for the years 1990, 1991, and 1992 on a cross section of individuals, and we would like to estimate the e¤ect of computer usage on individual wages. One possible static model is logðwageitÞ ¼ y0 þ y1d91t þ y2d92t þ d1computerit þ d2educit þ d3experit þ d4 femalei þ uit ð7:67Þ where d91t and d92t are dummy indicators for the years 1991 and 1992 and com- puterit is a measure of how much person i used a computer during year t. The inclu- sion of the year dummies allows for aggregate time e¤ects of the kind discussed in the Section 7.2 examples. This equation contains a variable that is constant across t, femalei, as well as variables that can change across i and t, such as educit and experit. The variable educit is given a t subscript, which indicates that years of education could change from year to year for at least some people. It could also be the case that educit is the same for all three years for every person in the sample, in which case we could remove the time subscript. The distinction between variables that are time- constant is not very important here; it becomes much more important in Chapter 10. As a general rule, with large N and small T it is a good idea to allow for separate intercepts for each time period. Doing so allows for aggregate time e¤ects that have the same inﬂuence on yit for all i. Anything that can be done in a cross section context can also be done in a panel data setting. For example, in equation (7.67) we can interact femalei with the time dummy variables to see whether productivity of females has changed over time, or we Chapter 7 170", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 184, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p185::c0", "text": "can interact educit and computerit to allow the return to computer usage to depend on level of education. The two assumptions su‰cient for pooled OLS to consistently estimate b are as follows: assumption POLS.1: Eðx0 tutÞ ¼ 0, t ¼ 1; 2; . . . ; T. assumption POLS.2: rank½PT t¼1 Eðx0 txtÞ\u0004 ¼ K. Remember, Assumption POLS.1 says nothing about the relationship between xs and ut for s 0 t. Assumption POLS.2 essentially rules out perfect linear dependencies among the explanatory variables. To apply the usual OLS statistics from the pooled OLS regression across i and t, we need to add homoskedasticity and no serial correlation assumptions. The weakest forms of these assumptions are the following: assumption POLS.3: (a) Eðu2 t x0 txtÞ ¼ s2Eðx0 txtÞ, t ¼ 1; 2; . . . ; T, where s2 ¼ Eðu2 t Þ for all t; (b) Eðutusx0 txsÞ ¼ 0, t 0 s, t; s ¼ 1; . . . ; T. The ﬁrst part of Assumption POLS.3 is a fairly strong homoskedasticity assumption; su‰cient is Eðu2 t j xtÞ ¼ s2 for all t. This means not only that the conditional variance does not depend on xt, but also that the unconditional variance is the same in every time period. Assumption POLS.3b essentially restricts the conditional covariances of the errors across di¤erent time periods to be zero. In fact, since xt almost always contains a constant, POLS.3b requires at a minimum that EðutusÞ ¼ 0, t 0 s. Su‰- cient for POLS.3b is Eðutus j xt; xsÞ ¼ 0, t 0 s, t; s ¼ 1; . . . ; T. It is important to remember that Assumption POLS.3 implies more than just a certain form of the unconditional variance matrix of u 1 ðu1; . . . ; uTÞ0. Assumption POLS.3 implies Eðuiu0 iÞ ¼ s2IT, which means that the unconditional variances are constant and the unconditional covariances are zero, but it also e¤ectively restricts the conditional variances and covariances. theorem 7.7 (Large Sample Properties of Pooled OLS): Under Assumptions POLS.1 and POLS.2, the pooled OLS estimator is consistent and asymptotically normal. If Assumption POLS.3 holds in addition, then Avarð ^bÞ ¼ s2½EðX0 iXiÞ\u0004\u00021=N, so that the appropriate estimator of Avarð ^bÞ is ^s2ðX0XÞ\u00021 ¼ ^s2 X N i¼1 X T t¼1 x0 itxit !\u00021 ð7:68Þ where ^s2 is the usual OLS variance estimator from the pooled regression Estimating Systems of Equations by OLS and GLS 171", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 185, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p186::c0", "text": "yit on xit; t ¼ 1; 2; . . . ; T; i ¼ 1; . . . ; N ð7:69Þ It follows that the usual t statistics and F statistics from regression (7.69) are ap- proximately valid. Therefore, the F statistic for testing Q linear restrictions on the K \u0001 1 vector b is F ¼ ðSSRr \u0002 SSRurÞ SSRur \u0003 ðNT \u0002 KÞ Q ð7:70Þ where SSRur is the sum of squared residuals from regression (7.69), and SSRr is the regression using the NT observations with the restrictions imposed. Why is a simple pooled OLS analysis valid under Assumption POLS.3? It is easy to show that Assumption POLS.3 implies that B ¼ s2A, where B 1 PT t¼1 PT s¼1 Eðutusx0 txsÞ, and A 1 PT t¼1 Eðx0 txtÞ. For the panel data case, these are the matrices that appear in expression (7.21). For computing the pooled OLS estimates and standard statistics, it does not matter how the data are ordered. However, if we put lags of any variables in the equation, it is easiest to order the data in the same way as is natural for studying asymptotic properties: the ﬁrst T observations should be for the ﬁrst cross section unit (ordered chronologically), the next T observations are for the next cross section unit, and so on. This procedure gives NT rows in the data set ordered in a very speciﬁc way. Example 7.7 (E¤ects of Job Training Grants on Firm Scrap Rates): Using the data from JTRAIN1.RAW (Holzer, Block, Cheatham, and Knott, 1993), we estimate a model explaining the ﬁrm scrap rate in terms of grant receipt. We can estimate the equation for 54 ﬁrms and three years of data (1987, 1988, and 1989). The ﬁrst grants were given in 1988. Some ﬁrms in the sample in 1989 received a grant only in 1988, so we allow for a one-year-lagged e¤ect: logð^scrapitÞ ¼ :597 ð:203Þ \u0002 :239 ð:311Þ d88t \u0002 :497 ð:338Þ d89t þ :200 ð:338Þ grantit þ :049 ð:436Þ granti;t\u00021 N ¼ 54; T ¼ 3; R2 ¼ :0173 where we have put i and t subscripts on the variables to emphasize which ones change across ﬁrm or time. The R-squared is just the usual one computed from the pooled OLS regression. In this equation, the estimated grant e¤ect has the wrong sign, and neither the current nor lagged grant variable is statistically signiﬁcant. When a lag of logðscrapitÞ is added to the equation, the estimates are notably di¤erent. See Problem 7.9. Chapter 7 172", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 186, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p187::c0", "text": "7.8.2 Dynamic Completeness While the homoskedasticity assumption, Assumption POLS.3a, can never be guar- anteed to hold, there is one important case where Assumption POLS.3b must hold. Suppose that the explanatory variables xt are such that, for all t, Eðyt j xt; yt\u00021; xt\u00021; . . . ; y1; x1Þ ¼ Eðyt j xtÞ ð7:71Þ This assumption means that xt contains su‰cient lags of all variables such that additional lagged values have no partial e¤ect on yt. The inclusion of lagged y in equation (7.71) is important. For example, if zt is a vector of contemporaneous vari- ables such that Eðyt j zt; zt\u00021; . . . ; z1Þ ¼ Eðyt j zt; zt\u00021; . . . ; zt\u0002LÞ and we choose xt ¼ ðzt; zt\u00021; . . . ; zt\u0002LÞ, then Eðyt j xt; xt\u00021; . . . ; x1Þ ¼ Eðyt j xtÞ. But equation (7.71) need not hold. Generally, in static and FDL models, there is no rea- son to expect equation (7.71) to hold, even in the absence of speciﬁcation problems such as omitted variables. We call equation (7.71) dynamic completeness of the conditional mean. Often, we can ensure that equation (7.71) is at least approximately true by putting su‰cient lags of zt and yt into xt. In terms of the disturbances, equation (7.71) is equivalent to Eðut j xt; ut\u00021; xt\u00021; . . . ; u1; x1Þ ¼ 0 ð7:72Þ and, by iterated expectations, equation (7.72) implies Eðutus j xt; xsÞ ¼ 0, s 0 t. Therefore, equation (7.71) implies Assumption POLS.3b as well as Assumption POLS.1. If equation (7.71) holds along with the homoskedasticity assumption Varðyt j xtÞ ¼ s2, then Assumptions POLS.1 and POLS.3 both hold, and standard OLS statistics can be used for inference. The following example is similar in spirit to an analysis of Maloney and McCormick (1993), who use a large random sample of students (including nonathletes) from Clemson University in a cross section analysis. Example 7.8 (E¤ect of Being in Season on Grade Point Average): The data in GPA.RAW are on 366 student-athletes at a large university. There are two semesters of data (fall and spring) for each student. Of primary interest is the ‘‘in-season’’ e¤ect on athletes’ GPAs. The model—with i, t subscripts—is trmgpait ¼ b0 þb1springt þb2cumgpait þb3crsgpait þb4 frstsemit þb5seasonit þb6SATi þb7verbmathi þb8hsperci þb9hssizei þb10blacki þb11 femalei þuit Estimating Systems of Equations by OLS and GLS 173", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 187, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p188::c0", "text": "The variable cumgpait is cumulative GPA at the beginning of the term, and this clearly depends on past-term GPAs. In other words, this model has something akin to a lagged dependent variable. In addition, it contains other variables that change over time (such as seasonit) and several variables that do not (such as SATi). We as- sume that the right-hand side (without uit) represents a conditional expectation, so that uit is necessarily uncorrelated with all explanatory variables and any functions of them. It may or may not be that the model is also dynamically complete in the sense of equation (7.71); we will show one way to test this assumption in Section 7.8.5. The estimated equation is tr^mgpait ¼ \u00022:07 ð0:34Þ \u0002 :012 ð:046Þ springt þ :315 ð:040Þ cumgpait þ :984 ð:096Þ crsgpait þ :769 ð:120Þ frstsemit \u0002 :046 ð:047Þ seasonit þ :00141 ð:00015Þ SATi \u0002 :113 ð:131Þ verbmathi \u0002 :0066 ð:0010Þ hsperci \u0002 :000058 ð:000099Þ hssizei \u0002 :231 ð:054Þ blacki þ :286 ð:051Þ femalei N ¼ 366; T ¼ 2; R2 ¼ :519 The in-season e¤ect is small—an athlete’s GPA is estimated to be .046 points lower when the sport is in season—and it is statistically insigniﬁcant as well. The other coe‰cients have reasonable signs and magnitudes. Often, once we start putting any lagged values of yt into xt, then equation (7.71) is an intended assumption. But this generalization is not always true. In the previous example, we can think of the variable cumgpa as another control we are using to hold other factors ﬁxed when looking at an in-season e¤ect on GPA for college athletes: cumgpa can proxy for omitted factors that make someone successful in college. We may not care that serial correlation is still present in the error, except that, if equation (7.71) fails, we need to estimate the asymptotic variance of the pooled OLS estimator to be robust to serial correlation (and perhaps heteroskedasticity as well). In introductory econometrics, students are often warned that having serial corre- lation in a model with a lagged dependent variable causes the OLS estimators to be inconsistent. While this statement is true in the context of a speciﬁc model of serial correlation, it is not true in general, and therefore it is very misleading. [See Wool- dridge (2000a, Chapter 12) for more discussion in the context of the AR(1) model.] Our analysis shows that, whatever is included in xt, pooled OLS provides consis- tent estimators of b whenever Eðyt j xtÞ ¼ xtb; it does not matter that the ut might be serially correlated. Chapter 7 174", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 188, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p189::c0", "text": "7.8.3 A Note on Time Series Persistence Theorem 7.7 imposes no restrictions on the time series persistence in the data fðxit; yitÞ: t ¼ 1; 2; . . . ; Tg. In light of the explosion of work in time series economet- rics on asymptotic theory with persistent processes [often called unit root processes— see, for example, Hamilton (1994)], it may appear that we have not been careful in stating our assumptions. However, we do not need to restrict the dynamic behavior of our data in any way because we are doing ﬁxed-T, large-N asymptotics. It is for this reason that the mechanics of the asymptotic analysis is the same for the SUR case and the panel data case. If T is large relative to N, the asymptotics here may be misleading. Fixing N while T grows or letting N and T both grow takes us into the realm of multiple time series analysis: we would have to know about the temporal dependence in the data, and, to have a general treatment, we would have to assume some form of weak dependence (see Wooldridge, 1994, for a discussion of weak de- pendence). Recently, progress has been made on asymptotics in panel data with large T and N when the data have unit roots; see, for example, Pesaran and Smith (1995) and Phillips and Moon (1999). As an example, consider the simple AR(1) model yt ¼ b0 þ b1 yt\u00021 þ ut; Eðut j yt\u00021; . . . ; y0Þ ¼ 0 Assumption POLS.1 holds (provided the appropriate moments exist). Also, As- sumption POLS.2 can be maintained. Since this model is dynamically complete, the only potential nuisance is heteroskedasticity in ut that changes over time or depends on yt\u00021. In any case, the pooled OLS estimator from the regression yit on 1, yi;t\u00021, t ¼ 1; . . . ; T, i ¼ 1; . . . ; N, produces consistent, ﬃﬃﬃﬃ N p -asymptotically normal estima- tors for ﬁxed T as N ! y, for any values of b0 and b1. In a pure time series case, or in a panel data case with T ! y and N ﬁxed, we would have to assume jb1j < 1, which is the stability condition for an AR(1) model. Cases where jb1j b 1 cause considerable complications when the asymptotics is done along the time series dimension (see Hamilton, 1994, Chapter 19). Here, a large cross section and relatively short time series allow us to be agnostic about the amount of temporal persistence. 7.8.4 Robust Asymptotic Variance Matrix Because Assumption POLS.3 can be restrictive, it is often useful to obtain a ro- bust estimate of Avarð ^bÞ that is valid without Assumption POLS.3. We have already seen the general form of the estimator, given in matrix (7.26). In the case of panel data, this estimator is fully robust to arbitrary heteroskedasticity—conditional or unconditional—and arbitrary serial correlation across time (again, conditional or Estimating Systems of Equations by OLS and GLS 175", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 189, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p190::c0", "text": "unconditional). The residuals ^ui are the T \u0001 1 pooled OLS residuals for cross sec- tion observation i. Some statistical packages compute these very easily, although the command may be disguised. Whether a software package has this capability or whether it must be programmed by you, the data must be stored as described earlier: The ðyi; XiÞ should be stacked on top of one another for i ¼ 1; . . . ; N. 7.8.5 Testing for Serial Correlation and Heteroskedasticity after Pooled OLS Testing for Serial Correlation It is often useful to have a simple way to detect serial correlation after estimation by pooled OLS. One reason to test for serial correlation is that it should not be present if the model is supposed to be dynamically complete in the conditional mean. A second reason to test for serial correlation is to see whether we should compute a robust variance matrix estimator for the pooled OLS estimator. One interpretation of serial correlation in the errors of a panel data model is that the error in each time period contains a time-constant omitted factor, a case we cover explicitly in Chapter 10. For now, we are simply interested in knowing whether or not the errors are serially correlated. We focus on the alternative that the error is a ﬁrst-order autoregressive process; this will have power against fairly general kinds of serial correlation. Write the AR(1) model as ut ¼ r1ut\u00021 þ et ð7:73Þ where Eðet j xt; ut\u00021; xt\u00021; ut\u00022; . . .Þ ¼ 0 ð7:74Þ Under the null hypothesis of no serial correlation, r1 ¼ 0. One way to proceed is to write the dynamic model under AR(1) serial correlation as yt ¼ xtb þ r1ut\u00021 þ et; t ¼ 2; . . . ; T ð7:75Þ where we lose the ﬁrst time period due to the presence of ut\u00021. If we can observe the ut, it is clear how we should proceed: simply estimate equation (7.75) by pooled OLS (losing the ﬁrst time period) and perform a t test on ^r1. To operationalize this proce- dure, we replace the ut with the pooled OLS residuals. Therefore, we run the regression yit on xit; ^ui;t\u00021; t ¼ 2; . . . ; T; i ¼ 1; . . . ; N ð7:76Þ and do a standard t test on the coe‰cient of ^ui;t\u00021. A statistic that is robust to arbi- trary heteroskedasticity in Varðyt j xt; ut\u00021Þ is obtained by the usual heteroskedasticity- robust t statistic in the pooled regression. This includes Engle’s (1982) ARCH model and any other form of static or dynamic heteroskedasticity. Chapter 7 176", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 190, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p191::c0", "text": "Why is a t test from regression (7.76) valid? Under dynamic completeness, equation (7.75) satisﬁes Assumptions POLS.1–POLS.3 if we also assume that Varðyt j xt; ut\u00021Þ is constant. Further, the presence of the generated regressor ^ui;t\u00021 does not a¤ect the limiting distribution of ^r1 under the null because r1 ¼ 0. Verifying this claim is sim- ilar to the pure cross section case in Section 6.1.1. A nice feature of the statistic computed from regression (7.76) is that it works whether or not xt is strictly exogenous. A di¤erent form of the test is valid if we as- sume strict exogeneity: use the t statistic on ^ui;t\u00021 in the regression ^uit on ^ui;t\u00021; t ¼ 2; . . . ; T; i ¼ 1; . . . ; N ð7:77Þ or its heteroskedasticity-robust form. That this test is valid follows by applying Problem 7.4 and the assumptions for pooled OLS with a lagged dependent variable. Example 7.9 (Athletes’ Grade Point Averages, continued): We apply the test from regression (7.76) because cumgpa cannot be strictly exogenous (GPA this term a¤ects cumulative GPA after this term). We drop the variables spring and frstsem from re- gression (7.76), since these are identically unity and zero, respectively, in the spring semester. We obtain ^r1 ¼ :194 and t^r1 ¼ 3:18, and so the null hypothesis is rejected. Thus there is still some work to do to capture the full dynamics. But, if we assume that we are interested in the conditional expectation implicit in the estimation, we are getting consistent estimators. This result is useful to know because we are primarily interested in the in-season e¤ect, and the other variables are simply acting as controls. The presence of serial correlation means that we should compute standard errors robust to arbitrary serial correlation (and heteroskedasticity); see Problem 7.10. Testing for Heteroskedasticity The primary reason to test for heteroskedasticity after running pooled OLS is to detect violation of Assumption POLS.3a, which is one of the assumptions needed for the usual statistics accompanying a pooled OLS regression to be valid. We assume throughout this section that Eðut j xtÞ ¼ 0, t ¼ 1; 2; . . . ; T, which strengthens Assumption POLS.1 but does not require strict exoge- neity. Then the null hypothesis of homoskedasticity can be stated as Eðu2 t j xtÞ ¼ s2, t ¼ 1; 2; . . . ; T. Under H0, u2 it is uncorrelated with any function of xit; let hit denote a 1 \u0001 Q vector of nonconstant functions of xit. In particular, hit can, and often should, contain dummy variables for the di¤erent time periods. From the tests for heteroskedasticity in Section 6.2.4. the following procedure is natural. Let ^u2 it denote the squared pooled OLS residuals. Then obtain the usual R- squared, R2 c, from the regression ^u2 it on 1; hit; t ¼ 1; . . . ; T; i ¼ 1; . . . ; N ð7:78Þ Estimating Systems of Equations by OLS and GLS 177", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 191, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p192::c0", "text": "The test statistic is NTR2 c, which is treated as asymptotically w2 Q under H0. (Alter- natively, we can use the usual F test of joint signiﬁcance of hit from the pooled OLS regression. The degrees of freedom are Q and NT \u0002 K.) When is this procedure valid? Using arguments very similar to the cross sectional tests from Chapter 6, it can be shown that the statistic has the same distribution if u2 it replaces ^u2 it; this fact is very convenient because it allows us to focus on the other features of the test. E¤ectively, we are performing a standard LM test of H0: d ¼ 0 in the model u2 it ¼ d0 þ hitd þ ait; t ¼ 1; 2; . . . ; T ð7:79Þ This test requires that the errors faitg be appropriately serially uncorrelated and requires homoskedasticity; that is, Assumption POLS.3 must hold in equation (7.79). Therefore, the tests based on nonrobust statistics from regression (7.78) essentially re- quire that Eða2 it j xitÞ be constant—meaning that Eðu4 it j xitÞ must be constant under H0. We also need a stronger homoskedasticity assumption; Eðu2 it j xit; ui;t\u00021; xi;t\u00021; . . .Þ ¼ s2 is su‰cient for the faitg in equation (7.79) to be appropriately serially uncorrelated. A fully robust test for heteroskedasticity can be computed from the pooled regres- sion (7.78) by obtaining a fully robust variance matrix estimator for ^d [see equation (7.26)]; this can be used to form a robust Wald statistic. Since violation of Assumption POLS.3a is of primary interest, it makes sense to include elements of xit in hit, and possibly squares and cross products of elements of xit. Another useful choice, covered in Chapter 6, is ^hit ¼ ð^yit; ^y2 itÞ, the pooled OLS ﬁtted values and their squares. Also, Assumption POLS.3a requires the uncondi- tional variances Eðu2 itÞ to be the same across t. Whether they are can be tested directly by choosing hit to have T \u0002 1 time dummies. If heteroskedasticity is detected but serial correlation is not, then the usual heteroskedasticity-robust standard errors and test statistics from the pooled OLS re- gression (7.69) can be used. 7.8.6 Feasible GLS Estimation under Strict Exogeneity When Eðuiu0 iÞ 0 s2IT, it is reasonable to consider a feasible GLS analysis rather than a pooled OLS analysis. In Chapter 10 we will cover a particular FGLS analysis after we introduce unobserved components panel data models. With large N and small T, nothing precludes an FGLS analysis in the current setting. However, we must remember that FGLS is not even guaranteed to produce consistent, let alone e‰cient, estimators under Assumptions POLS.1 and POLS.2. Unless W ¼ Eðuiu0 iÞ is a diago- nal matrix, Assumption POLS.1 should be replaced with the strict exogeneity as- sumption (7.6). (Problem 7.7 covers the case when W is diagonal.) Sometimes we are Chapter 7 178", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 192, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p193::c0", "text": "willing to assume strict exogeneity in static and ﬁnite distributed lag models. As we saw earlier, it cannot hold in models with lagged yit, and it can fail in static models or distributed lag models if there is feedback from yit to future zit. Problems 7.1. Provide the details for a proof of Theorem 7.1. 7.2. In model (7.9), maintain Assumptions SOLS.1 and SOLS.2, and assume EðX0 iuiu0 iXiÞ ¼ EðX0 iWXiÞ, where W 1 Eðuiu0 iÞ. [The last assumption is a di¤erent way of stating the homoskesdasticity assumption for systems of equations; it always holds if assumption (7.50) holds.] Let ^bSOLS denote the system OLS estimator. a. Show that Avarð ^bSOLSÞ ¼ ½EðX0 iXiÞ\u0004\u00021½EðX0 iWXiÞ\u0004½EðX0 iXiÞ\u0004\u00021=N. b. How would you estimate the asymptotic variance in part a? c. Now add Assumptions SGLS.1–SGLS.3. Show that Avarð ^bSOLSÞ \u0002 Avarð ^bFGLSÞ is positive semideﬁnite. {Hint: Show that ½Avarð ^bFGLSÞ\u0004\u00021 \u0002 ½Avarð ^bSOLSÞ\u0004\u00021 is p.s.d.} d. If, in addition to the previous assumptions, W ¼ s2IG, show that SOLS and FGLS have the same asymptotic variance. e. Evaluate the following statement: ‘‘Under the assumptions of part c, FGLS is never asymptotically worse than SOLS, even if W ¼ s2IG.’’ 7.3. Consider the SUR model (7.2) under Assumptions SOLS.1, SOLS.2, and SGLS.3, with W 1 diagðs2 1; . . . ; s2 GÞ; thus, GLS and OLS estimation equation by equation are the same. (In the SUR model with diagonal W, Assumption SOLS.1 is the same as Assumption SGLS.1, and Assumption SOLS.2 is the same as Assump- tion SGLS.2.) a. Show that single-equation OLS estimators from any two equations, say, ^bg and ^bh, are asymptotically uncorrelated. (That is, show that the asymptotic variance of the system OLS estimator ^b is block diagonal.) b. Under the conditions of part a, assume that b1 and b2 (the parameter vectors in the ﬁrst two equations) have the same dimension. Explain how you would test H0: b1 ¼ b2 against H1: b1 0 b2. c. Now drop Assumption SGLS.3, maintaining Assumptions SOLS.1 and SOLS.2 and diagonality of W. Suppose that ^W is estimated in an unrestricted manner, so that FGLS and OLS are not algebraically equivalent. Show that OLS and FGLS are ﬃﬃﬃﬃ N p -asymptotically equivalent, that is, ﬃﬃﬃﬃ N p ð ^bSOLS \u0002 ^bFGLSÞ ¼ opð1Þ. This is one case where FGLS is consistent under Assumption SOLS.1. Estimating Systems of Equations by OLS and GLS 179", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 193, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p194::c0", "text": "7.4. Using the ﬃﬃﬃﬃ N p -consistency of the system OLS estimator ^^b^b for b, for ^W in equation (7.37) show that vec½ ﬃﬃﬃﬃ N p ð ^W \u0002 WÞ\u0004 ¼ vec N\u00021=2 X N i¼1 ðuiu0 i \u0002 WÞ \" # þ opð1Þ under Assumptions SGLS.1 and SOLS.2. (Note: This result does not hold when As- sumption SGLS.1 is replaced with the weaker Assumption SOLS.1.) Assume that all moment conditions needed to apply the WLLN and CLT are satisﬁed. The impor- tant conclusion is that the asymptotic distribution of vec ﬃﬃﬃﬃ N p ð ^W \u0002 WÞ does not depend on that of ﬃﬃﬃﬃ N p ð ^^b^b \u0002 bÞ, and so any asymptotic tests on the elements of W can ignore the estimation of b. [Hint: Start from equation (7.39) and use the fact that ﬃﬃﬃﬃ N p ð ^^b^b \u0002 bÞ ¼ Opð1Þ.] 7.5. Prove Theorem 7.6, using the fact that when Xi ¼ IG n xi, X N i¼1 X0 i ^W\u00021Xi ¼ ^W\u00021 n X N i¼1 x0 ixi ! and X N i¼1 X0 i ^W\u00021yi ¼ ð ^W\u00021 n IKÞ X N i¼1 x0 i yi1 ... X N i¼1 x0 i yiG 0 B B B B B B B @ 1 C C C C C C C A 7.6. Start with model (7.9). Suppose you wish to impose Q linear restrictions of the form Rb ¼ r, where R is a Q \u0001 K matrix and r is a Q \u0001 1 vector. Assume that R is partitioned as R 1 ½R1 j R2\u0004, where R1 is a Q \u0001 Q nonsingular matrix and R2 is a Q \u0001 ðK \u0002 QÞ matrix. Partition Xi as Xi 1 ½Xi1 j Xi2\u0004, where Xi1 is G \u0001 Q and Xi2 is G \u0001 ðK \u0002 QÞ, and partition b as b 1 ðb 0 1; b 0 2Þ0. The restrictions Rb ¼ r can be expressed as R1b1 þ R2b2 ¼ r, or b1 ¼ R\u00021 1 ðr \u0002 R2b2Þ. Show that the restricted model can be written as ~yi ¼ ~Xi2b2 þ ui where ~yi ¼ yi \u0002 Xi1R\u00021 1 r and ~Xi2 ¼ Xi2 \u0002 Xi1R\u00021 1 R2. 7.7. Consider the panel data model yit ¼ xitb þ uit; t ¼ 1; 2; . . . ; T Eðuit j xit; ui;t\u00021; xi;t\u00021; . . . ; Þ ¼ 0 Eðu2 it j xitÞ ¼ Eðu2 itÞ ¼ s2 t ; t ¼ 1; . . . ; T ð7:80Þ Chapter 7 180", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 194, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p195::c0", "text": "[Note that Eðu2 it j xitÞ does not depend on xit, but it is allowed to be a di¤erent con- stant in each time period.] a. Show that W ¼ Eðuiu0 iÞ is a diagonal matrix. [Hint: The zero conditional mean assumption (7.80) implies that uit is uncorrelated with uis for s < t.] b. Write down the GLS estimator assuming that W is known. c. Argue that Assumption SGLS.1 does not necessarily hold under the assumptions made. (Setting xit ¼ yi;t\u00021 might help in answering this part.) Nevertheless, show that the GLS estimator from part b is consistent for b by showing that EðX0 iW\u00021uiÞ ¼ 0. [This proof shows that Assumption SGLS.1 is su‰cient, but not necessary, for con- sistency. Sometimes EðX0 iW\u00021uiÞ ¼ 0 even though Assumption SGLS.1 does not hold.] d. Show that Assumption SGLS.3 holds under the given assumptions. e. Explain how to consistently estimate each s2 t (as N ! y). f. Argue that, under the assumptions made, valid inference is obtained by weighting each observation ðyit; xitÞ by 1=^st and then running pooled OLS. g. What happens if we assume that s2 t ¼ s2 for all t ¼ 1; . . . ; T? 7.8. Redo Example 7.3, disaggregating the beneﬁts categories into value of vacation days, value of sick leave, value of employer-provided insurance, and value of pen- sion. Use hourly measures of these along with hrearn, and estimate an SUR model. Does marital status appear to a¤ect any form of compensation? Test whether another year of education increases expected pension value and expected insurance by the same amount. 7.9. Redo Example 7.7 but include a single lag of logðscrapÞ in the equation to proxy for omitted variables that may determine grant receipt. Test for AR(1) serial correlation. If you ﬁnd it, you should also compute the fully robust standard errors that allow for abitrary serial correlation across time and heteroskedasticity. 7.10. In Example 7.9, compute standard errors fully robust to serial correlation and heteroskedasticity. Discuss any important di¤erences between the robust standard errors and the usual standard errors. 7.11. Use the data in CORNWELL.RAW for this question; see Problem 4.13. a. Using the data for all seven years, and using the logarithms of all variables, esti- mate a model relating the crime rate to prbarr, prbconv, prbpris, avgsen, and polpc. Use pooled OLS and include a full set of year dummies. Test for serial correlation assuming that the explanatory variables are strictly exogenous. If there is serial cor- relation, obtain the fully robust standard errors. Estimating Systems of Equations by OLS and GLS 181", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 195, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p196::c0", "text": "b. Add a one-year lag of logðcrmrteÞ to the equation from part a, and compare with the estimates from part a. c. Test for ﬁrst-order serial correlation in the errors in the model from part b. If serial correlation is present, compute the fully robust standard errors. d. Add all of the wage variables (in logarithmic form) to the equation from part c. Which ones are statistically and economically signiﬁcant? Are they jointly signiﬁcant? Test for joint signiﬁcance of the wage variables allowing arbitrary serial correlation and heteroskedasticity. 7.12. If you add wealth at the beginning of year t to the saving equation in Example 7.2, is the strict exogeneity assumption likely to hold? Explain. Chapter 7 182", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 196, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p197::c0", "text": "8 System Estimation by Instrumental Variables 8.1 Introduction and Examples In Chapter 7 we covered system estimation of linear equations when the explana- tory variables satisfy certain exogeneity conditions. For many applications, even the weakest of these assumptions, Assumption SOLS.1, is violated, in which case instru- mental variables procedures are indispensable. The modern approach to system instrumental variables (SIV) estimation is based on the principle of generalized method of moments (GMM). Method of moments estimation has a long history in statistics for obtaining simple parameter estimates when maximum likelihood estimation requires nonlinear optimization. Hansen (1982) and White (1982b) showed how the method of moments can be generalized to apply to a variety of econometric models, and they derived the asymptotic properties of GMM. Hansen (1982), who coined the name ‘‘generalized method of moments,’’ treated time series data, and White (1982b) assumed independently sampled observations. Though the models considered in this chapter are more general than those treated in Chapter 5, the derivations of asymptotic properties of system IV estimators are mechanically similar to the derivations in Chapters 5 and 7. Therefore, the proofs in this chapter will be terse, or omitted altogether. In econometrics, the most familar application of SIV estimation is to a simultane- ous equations model (SEM). We will cover SEMs speciﬁcally in Chapter 9, but it is useful to begin with a typical SEM example. System estimation procedures have applications beyond the classical simultaneous equations methods. We will also use the results in this chapter for the analysis of panel data models in Chapter 11. Example 8.1 (Labor Supply and Wage O¤er Functions): Consider the following labor supply function representing the hours of labor supply, hs, at any wage, w, faced by an individual. As usual, we express this in population form: hsðwÞ ¼ g1w þ z1d1 þ u1 ð8:1Þ where z1 is a vector of observed labor supply shifters—including such things as education, past experience, age, marital status, number of children, and nonlabor income—and u1 contains unobservables a¤ecting labor supply. The labor supply function can be derived from individual utility-maximizing behavior, and the nota- tion in equation (8.1) is intended to emphasize that, for given z1 and u1, a labor supply function gives the desired hours worked at any possible wage ðwÞ facing the worker. As a practical matter, we can only observe equilibrium values of hours worked and hourly wage. But the counterfactual reasoning underlying equation (8.1) is the proper way to view labor supply.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 197, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p198::c0", "text": "A wage o¤er function gives the hourly wage that the market will o¤er as a function of hours worked. (It could be that the wage o¤er does not depend on hours worked, but in general it might.) For observed productivity attributes z2 (for example, edu- cation, experience, and amount of job training) and unobserved attributes u2, we write the wage o¤er function as woðhÞ ¼ g2h þ z2d2 þ u2 ð8:2Þ Again, for given z2 and u2, woðhÞ gives the wage o¤er for an individual agreeing to work h hours. Equations (8.1) and (8.2) explain di¤erent sides of the labor market. However, rarely can we assume that an individual is given an exogenous wage o¤er and then, at that wage, decides how much to work based on equation (8.1). A reasonable approach is to assume that observed hours and wage are such that equations (8.1) and (8.2) both hold. In other words, letting ðh; wÞ denote the equilibrium values, we have h ¼ g1w þ z1d1 þ u1 ð8:3Þ w ¼ g2h þ z2d2 þ u2 ð8:4Þ Under weak restrictions on the parameters, these equations can be solved uniquely for ðh; wÞ as functions of z1, z2, u1, u2, and the parameters; we consider this topic generally in Chapter 9. Further, if z1 and z2 are exogenous in the sense that Eðu1 j z1; z2Þ ¼ Eðu2 j z1; z2Þ ¼ 0 then, under identiﬁcation assumptions, we can consistently estimate the parameters of the labor supply and wage o¤er functions. We consider identiﬁcation of SEMs in detail in Chapter 9. We also ignore what is sometimes a practically important issue: the equilibrium hours for an individual might be zero, in which case w is not observed for such people. We deal with missing data issues in Chapter 17. For a random draw from the population we can write hi ¼ g1wi þ zi1d1 þ ui1 ð8:5Þ wi ¼ g2hi þ zi2d2 þ ui2 ð8:6Þ Except under very special assumptions, ui1 will be correlated with wi, and ui2 will be correlated with hi. In other words, wi is probably endogenous in equation (8.5), and hi is probably endogenous in equation (8.6). It is for this reason that we study system instrumental variables methods. Chapter 8 184", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 198, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p199::c0", "text": "An example with the same statistical structure as Example 8.1, but with an omitted variables interpretation, is motivated by Currie and Thomas (1995). Example 8.2 (Student Performance and Head Start): Consider an equation to test the e¤ect of Head Start participation on subsequent student performance: scorei ¼ g1HeadStarti þ zi1d1 þ ui1 ð8:7Þ where scorei is the outcome on a test when the child is enrolled in school and HeadStarti is a binary indicator equal to one if child i participated in Head Start at an early age. The vector zi1 contains other observed factors, such as income, educa- tion, and family background variables. The error term ui1 contains unobserved fac- tors that a¤ect score—such as child’s ability—that may also be correlated with HeadStart. To capture the possible endogeneity of HeadStart, we write a linear reduced form (linear projection) for HeadStarti: HeadStarti ¼ zid2 þ ui2 ð8:8Þ Remember, this projection always exists even though HeadStarti is a binary variable. The vector zi contains zi1 and at least one factor a¤ecting Head Start participation that does not have a direct e¤ect on score. One possibility is distance to the nearest Head Start center. In this example we would probably be willing to assume that Eðui1 j ziÞ ¼ 0—since the test score equation is structural—but we would only want to assume Eðz0 iui2Þ ¼ 0, since the Head Start equation is a linear projection involving a binary dependent variable. Correlation between u1 and u2 means HeadStart is endogenous in equation (8.7). Both of the previous examples can be written for observation i as yi1 ¼ xi1b1 þ ui1 ð8:9Þ yi2 ¼ xi2b2 þ ui2 ð8:10Þ which looks just like a two-equation SUR system but where xi1 and xi2 can contain endogenous as well as exogenous variables. Because xi1 and xi2 are generally corre- lated with ui1 and ui2, estimation of these equations by OLS or FGLS, as we studied in Chapter 7, will generally produce inconsistent estimators. We already know one method for estimating an equation such as equation (8.9): if we have su‰cient instruments, apply 2SLS. Often 2SLS produces acceptable results, so why should we go beyond single-equation analysis? Not surprisingly, our interest in system methods with endogenous explanatory variables has to do with e‰ciency. In many cases we can obtain more e‰cient estimators by estimating b1 and b2 jointly, System Estimation by Instrumental Variables 185", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 199, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p200::c0", "text": "that is, by using a system procedure. The e‰ciency gains are analogous to the gains that can be realized by using feasible GLS rather than OLS in a SUR system. 8.2 A General Linear System of Equations We now discuss estimation of a general linear model of the form yi ¼ Xib þ ui ð8:11Þ where yi is a G \u0001 1 vector, Xi is a G \u0001 K matrix, and ui is the G \u0001 1 vector of errors. This model is identical to equation (7.9), except that we will use di¤erent assump- tions. In writing out examples, we will often omit the observation subscript i, but for the general analysis carrying it along is a useful notational device. As in Chapter 7, the rows of yi, Xi, and ui can represent di¤erent time periods for the same cross- sectional unit (so G ¼ T, the total number of time periods). Therefore, the following analysis applies to panel data models where T is small relative to the cross section sample size, N; for an example, see Problem 8.8. We cover general panel data appli- cations in Chapter 11. (As in Chapter 7, the label ‘‘systems of equations’’ is not es- pecially accurate for basic panel data models because we have only one behavioral equation over T di¤erent time periods.) The following orthogonality condition is the basis for estimating b: assumption SIV.1: EðZ0 iuiÞ ¼ 0, where Zi is a G \u0001 L matrix of observable instru- mental variables. (The acronym SIV stands for ‘‘system instrumental variables.’’) For the purposes of discussion, we assume that EðuiÞ ¼ 0; this assumption is almost always true in prac- tice anyway. From what we know about IV and 2SLS for single equations, Assumption SIV.1 cannot be enough to identify the vector b. An assumption su‰cient for identiﬁcation is the rank condition: assumption SIV.2: rank EðZ0 iXiÞ ¼ K. Assumption SIV.2 generalizes the rank condition from the single-equation case. (When G ¼ 1, Assumption SIV.2 is the same as Assumption 2SLS.2b.) Since EðZ0 iXiÞ is an L \u0001 K matrix, Assumption SIV.2 requires the columns of this matrix to be lin- early independent. Necessary for the rank condition is the order condition: L b K. We will investigate the rank condition in detail for a broad class of models in Chapter 9. For now, we just assume that it holds. Chapter 8 186", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 200, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p201::c0", "text": "In what follows, it is useful to carry along a particular example that applies to simultaneous equations models and other models with potentially endogenous ex- planatory variables. Write a G equation system for the population as y1 ¼ x1b1 þ u1 .. . yG ¼ xGbG þ uG ð8:12Þ where, for each equation g, xg is a 1 \u0001 Kg vector that can contain both exogenous and endogenous variables. For each g, bg is Kg \u0001 1. Because this looks just like the SUR system from Chapter 7, we will refer to it as a SUR system, keeping in mind the crucial fact that some elements of xg are thought to be correlated with ug for at least some g. For each equation we assume that we have a set of instrumental variables, a 1 \u0001 Lg vector zg, that are exogenous in the sense that Eðz0 gugÞ ¼ 0; g ¼ 1; 2; . . . ; G ð8:13Þ In most applications unity is an element of zg for each g, so that EðugÞ ¼ 0, all g. As we will see, and as we already know from single-equation analysis, if xg contains some elements correlated with ug, then zg must contain more than just the exogenous variables appearing in equation g. Much of the time the same instruments, which consist of all exogenous variables appearing anywhere in the system, are valid for every equation, so that zg ¼ z, g ¼ 1; 2; . . . ; G. Some applications require us to have di¤erent instruments for di¤erent equations, so we allow that possibility here. Putting an i subscript on the variables in equations (8.12), and deﬁning yi G\u00011 1 yi1 yi2 ... yiG 0 B B B B @ 1 C C C C A ; Xi G\u0001K 1 xi1 0 0 \u0002 \u0002 \u0002 0 0 xi2 0 \u0002 \u0002 \u0002 0 .. . .. . 0 0 0 \u0002 \u0002 \u0002 xiG 0 B B B B @ 1 C C C C A ; ui G\u00011 1 ui1 ui2 .. . uiG 0 B B B B @ 1 C C C C A ð8:14Þ and b ¼ ðb 0 1; b 0 2; . . . ; b 0 GÞ0, we can write equation (8.12) in the form (8.11). Note that K ¼ K1 þ K2 þ \u0002 \u0002 \u0002 þ KG is the total number of parameters in the system. The matrix of instruments has a structure similar to Xi: Zi 1 zi1 0 0 \u0002 \u0002 \u0002 0 0 zi2 0 \u0002 \u0002 \u0002 0 .. . .. . 0 0 0 \u0002 \u0002 \u0002 ziG 0 B B B B @ 1 C C C C A ð8:15Þ System Estimation by Instrumental Variables 187", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 201, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p202::c0", "text": "which has dimension G \u0001 L, where L ¼ L1 þ L2 þ \u0002 \u0002 \u0002 þ LG. Then, for each i, Z0 iui ¼ ðzi1ui1; zi2ui2; . . . ; ziGuiGÞ0 ð8:16Þ and so EðZ0 iuiÞ ¼ 0 reproduces the orthogonality conditions (8.13). Also, EðZ0 iXiÞ ¼ Eðz0 i1xi1Þ 0 0 \u0002 \u0002 \u0002 0 0 Eðz0 i2xi2Þ 0 \u0002 \u0002 \u0002 0 ... ... 0 0 0 \u0002 \u0002 \u0002 Eðz0 iGxiGÞ 0 B B B B @ 1 C C C C A ð8:17Þ where Eðz0 igxigÞ is Lg \u0001 Kg. Assumption SIV.2 requires that this matrix have full col- umn rank, where the number of columns is K ¼ K1 þ K2 þ \u0002 \u0002 \u0002 þ KG. A well-known result from linear algebra says that a block diagonal matrix has full column rank if and only if each block in the matrix has full column rank. In other words, Assump- tion SIV.2 holds in this example if and only if rank Eðz0 igxigÞ ¼ Kg; g ¼ 1; 2; . . . ; G ð8:18Þ This is exactly the rank condition needed for estimating each equation by 2SLS, which we know is possible under conditions (8.13) and (8.18). Therefore, identiﬁca- tion of the SUR system is equivalent to identiﬁcation equation by equation. This reasoning assumes that the bg are unrestricted across equations. If some prior restrictions are known, then identiﬁcation is more complicated, something we cover explicitly in Chapter 9. In the important special case where the same instruments, zi, can be used for every equation, we can write deﬁnition (8.15) as Zi ¼ IG n zi. 8.3 Generalized Method of Moments Estimation 8.3.1 A General Weighting Matrix The orthogonality conditions in Assumption SIV.1 suggest an estimation strategy. Under Assumptions SIV.1 and SIV.2, b is the unique K \u0001 1 vector solving the linear set population moment conditions E½Z0 iðyi \u0003 XibÞ\u0004 ¼ 0 ð8:19Þ (That b is a solution follows from Assumption SIV.1; that it is unique follows by Assumption SIV.2.) In other words, if b is any other K \u0001 1 vector (so that at least one element of b is di¤erent from the corresponding element in b), then Chapter 8 188", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 202, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p203::c0", "text": "E½Z0 iðyi \u0003 XibÞ\u0004 0 0 ð8:20Þ This formula shows that b is identiﬁed. Because sample averages are consistent esti- mators of population moments, the analogy principle applied to condition (8.19) suggests choosing the estimator ^b to solve N\u00031 X N i¼1 Z0 iðyi \u0003 Xi ^bÞ ¼ 0 ð8:21Þ Equation (8.21) is a set of L linear equations in the K unknowns in ^b. First consider the case L ¼ K, so that we have exactly enough IVs for the explanatory variables in the system. Then, if the K \u0001 K matrix PN i¼1 Z0 iXi is nonsingular, we can solve for ^b as ^b ¼ N\u00031 X N i¼1 Z0 iXi !\u00031 N\u00031 X N i¼1 Z0 iyi ! ð8:22Þ We can write ^b using full matrix notation as ^b ¼ ðZ0XÞ\u00031Z0Y, where Z is the NG \u0001 L matrix obtained by stacking Zi from i ¼ 1; 2; . . . ; N; X is the NG \u0001 K matrix obtained by stacking Xi from i ¼ 1; 2; . . . ; N, and Y is the NG \u0001 1 vector obtained from stacking yi; i ¼ 1; 2; . . . ; N. We call equation (8.22) the system IV (SIV) esti- mator. Application of the law of large numbers shows that the SIV estimator is con- sistent under Assumptions SIV.1 and SIV.2. When L > K—so that we have more columns in the IV matrix Zi than we need for identiﬁcation—choosing ^b is more complicated. Except in special cases, equation (8.21) will not have a solution. Instead, we choose ^b to make the vector in equation (8.21) as ‘‘small’’ as possible in the sample. One idea is to minimize the squared Euclidean length of the L \u0001 1 vector in equation (8.21). Dropping the 1=N, this approach suggests choosing ^b to make X N i¼1 Z0 iðyi \u0003 Xi ^bÞ \" #0 X N i¼1 Z0 iðyi \u0003 Xi ^bÞ \" # as small as possible. While this method produces a consistent estimator under Assumptions SIV.1 and SIV.2, it rarely produces the best estimator, for reasons we will see in Section 8.3.3. A more general class of estimators is obtained by using a weighting matrix in the quadratic form. Let ^W be an L \u0001 L symmetric, positive semideﬁnite matrix, where the ‘‘5’’ is included to emphasize that ^W is generally an estimator. A generalized method of moments (GMM) estimator of b is a vector ^b that solves the problem System Estimation by Instrumental Variables 189", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 203, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p204::c0", "text": "min b X N i¼1 Z0 iðyi \u0003 XibÞ \" #0 ^W X N i¼1 Z0 iðyi \u0003 XibÞ \" # ð8:23Þ Because expression (8.23) is a quadratic function of b, the solution to it has a closed form. Using multivariable calculus or direct substitution, we can show that the unique solution is ^b ¼ ðX0Z ^WZ0XÞ\u00031ðX0Z ^WZ0YÞ ð8:24Þ assuming that X0Z ^WZ0X is nonsingular. To show that this estimator is consistent, we assume that ^W has a nonsingular probability limit. assumption SIV.3: ^W ! p W as N ! y, where W is a nonrandom, symmetric, L \u0001 L positive deﬁnite matrix. In applications, the convergence in Assumption SIV.3 will follow from the law of large numbers because ^W will be a function of sample averages. The fact that W is assumed to be positive deﬁnite means that ^W is positive deﬁnite with probability approaching one (see Chapter 3). We could relax the assumption of positive deﬁ- niteness to positive semideﬁniteness at the cost of complicating the assumptions. In most applications, we can assume that W is positive deﬁnite. theorem 8.1 (Consistency of GMM): Under Assumptions SIV.1–SIV.3, ^b ! p b as N ! y. Proof: Write ^b ¼ N\u00031 X N i¼1 X0 iZi ! ^W N\u00031 X N i¼1 Z0 iXi ! \" #\u00031 N\u00031 X N i¼1 X0 iZi ! ^W N\u00031 X N i¼1 Z0 iyi ! Plugging in yi ¼ Xib þ ui and doing a little algebra gives ^b ¼ b þ N\u00031 X N i¼1 X0 iZi ! ^W N\u00031 X N i¼1 Z0 iXi ! \" #\u00031 N\u00031 X N i¼1 X0 iZi ! ^W N\u00031 X N i¼1 Z0 iui ! Under Assumption SIV.2, C 1 EðZ0 iXiÞ has rank K, and combining this with As- sumption SIV.3, C0WC has rank K and is therefore nonsingular. It follows by the law of large numbers that plim ^b ¼ b þ ðC0WCÞ\u00031C0Wðplim N\u00031 PN i¼1 Z0 iuiÞ ¼ b þ ðC0WCÞ\u00031C0W \u0002 0 ¼ b: Theorem 8.1 shows that a large class of estimators is consistent for b under Assumptions SIV.1 and SIV.2, provided that we choose ^W to satisfy modest restric- Chapter 8 190", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 204, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p205::c0", "text": "tions. When L ¼ K, the GMM estimator in equation (8.24) becomes equation (8.22), no matter how we choose ^W, because X0Z is a K \u0001 K nonsingular matrix. We can also show that ^b is asymptotically normally distributed under these ﬁrst three assumptions. theorem 8.2 (Asymptotic Normality of GMM): Under Assumptions SIV.1–SIV.3, ﬃﬃﬃﬃ N p ð ^b \u0003 bÞ is asymptotically normally distributed with mean zero and Avar ﬃﬃﬃﬃ N p ð ^b \u0003 bÞ ¼ ðC0WCÞ\u00031C0WLWCðC0WCÞ\u00031 ð8:25Þ where L 1 EðZ0 iuiu0 iZiÞ ¼ VarðZ0 iuiÞ ð8:26Þ We will not prove this theorem in detail as it can be reasoned from ﬃﬃﬃﬃ N p ð ^b \u0003 bÞ ¼ N\u00031 X N i¼1 X0 iZi ! ^W N\u00031 X N i¼1 Z0 iXi ! \" #\u00031 N\u00031 X N i¼1 X0 iZi ! ^W N\u00031=2 X N i¼1 Z0 iui ! where we use the fact that N\u00031=2 PN i¼1 Z0 iui ! d Normalð0; LÞ. The asymptotic vari- ance matrix in equation (8.25) looks complicated, but it can be consistently esti- mated. If ^L is a consistent estimator of L—more on this later—then equation (8.25) is consistently estimated by ½ðX0Z=NÞ ^WðZ0X=NÞ\u0004\u00031ðX0Z=NÞ ^W^L ^WðZ0X=NÞ½ðX0Z=NÞ ^WðZ0X=NÞ\u0004\u00031 ð8:27Þ As usual, we estimate Avarð ^bÞ by dividing expression (8.27) by N. While the general formula (8.27) is occasionally useful, it turns out that it is greatly simpliﬁed by choosing ^W appropriately. Since this choice also (and not coinciden- tally) gives the asymptotically e‰cient estimator, we hold o¤ discussing asymptotic variances further until we cover the optimal choice of ^W in Section 8.3.3. 8.3.2 The System 2SLS Estimator A choice of ^W that leads to a useful and familiar-looking estimator is ^W ¼ N\u00031 X N i¼1 Z0 iZi !\u00031 ¼ ðZ0Z=NÞ\u00031 ð8:28Þ which is a consistent estimator of ½EðZ0 iZiÞ\u0004\u00031. Assumption SIV.3 simply requires that EðZ0 iZiÞ exist and be nonsingular, and these requirements are not very restrictive. System Estimation by Instrumental Variables 191", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 205, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p206::c0", "text": "When we plug equation (8.28) into equation (8.24) and cancel N everywhere, we get ^b ¼ ½X0ZðZ0ZÞ\u00031Z0X\u0004\u00031X0ZðZ0ZÞ\u00031Z0Y ð8:29Þ This looks just like the single-equation 2SLS estimator, and so we call it the system 2SLS estimator. When we apply equation (8.29) to the system of equations (8.12), with deﬁnitions (8.14) and (8.15), we get something very familiar. As an exercise, you should show that ^b produces 2SLS equation by equation. (The proof relies on the block diagonal structures of Z0 iZi and Z0 iXi for each i.) In other words, we estimate the ﬁrst equation by 2SLS using instruments zi1, the second equation by 2SLS using instruments zi2, and so on. When we stack these into one long vector, we get equation (8.29). Problem 8.8 asks you to show that, in panel data applications, a natural choice of Zi makes the system 2SLS estimator a pooled 2SLS estimator. In the next subsection we will see that the system 2SLS estimator is not necessarily the asymptotically e‰cient estimator. Still, it is ﬃﬃﬃﬃ N p -consistent and easy to compute given the data matrices X, Y, and Z. This latter feature is important because we need a preliminary estimator of b to obtain the asymptotically e‰cient estimator. 8.3.3 The Optimal Weighting Matrix Given that a GMM estimator exists for any positive deﬁnite weighting matrix, it is important to have a way of choosing among all of the possibilities. It turns out that there is a choice of W that produces the GMM estimator with the smallest asymp- totic variance. We can appeal to expression (8.25) for a hint as to the optimal choice of W. It is this expression we are trying to make as small as possible, in the matrix sense. (See Deﬁnition 3.11 for the deﬁnition of relative asymptotic e‰ciency.) The expression (8.25) simpliﬁes to ðC0L\u00031CÞ\u00031 if we set W 1 L\u00031. Using standard arguments from matrix algebra, it can be shown that ðC0WCÞ\u00031C0WLWCðC0WCÞ\u00031 \u0003 ðC0L\u00031CÞ\u00031 is positive semideﬁnite for any L \u0001 L positive deﬁnite matrix W. The easiest way to prove this point is to show that ðC0L\u00031CÞ \u0003 ðC0WCÞðC0WLWCÞ\u00031ðC0WCÞ ð8:30Þ is positive semideﬁnite, and we leave this proof as an exercise (see Problem 8.5). This discussion motivates the following assumption and theorem. assumption SIV.4: W ¼ L\u00031, where L is deﬁned by expression (8.26). theorem 8.3 (Optimal Weighting Matrix): Under Assumptions SIV.1–SIV.4, the resulting GMM estimator is e‰cient among all GMM estimators of the form (8.24). Chapter 8 192", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 206, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p207::c0", "text": "Provided that we can consistently estimate L, we can obtain the asymptotically e‰- cient GMM estimator. Any consistent estimator of L delivers the e‰cient GMM es- timator, but one estimator is commonly used that imposes no structure on L. Procedure 8.1 (GMM with Optimal Weighting Matrix): a. Let ^^b^b be an initial consistent estimator of b. In most cases this is the system 2SLS estimator. b. Obtain the G \u0001 1 residual vectors ^^u^ui ¼ yi \u0003 Xi ^^b^b; i ¼ 1; 2; . . . ; N ð8:31Þ c. A generally consistent estimator of L is ^L ¼ N\u00031 PN i¼1 Z0 i^^u^ui^^u^u0 iZi. d. Choose ^W 1 ^L\u00031 ¼ N\u00031 X N i¼1 Z0 i^^u^ui^^u^u0 iZi !\u00031 ð8:32Þ and use this matrix to obtain the asymptotically optimal GMM estimator. The estimator of L in part c of Procedure 8.1 is consistent for EðZ0 iuiu0 iZiÞ under general conditions. When each row of Zi and ui represent di¤erent time periods—so that we have a single-equation panel data model—the estimator ^L allows for arbi- trary heteroskedasticity (conditional or unconditional) as well as arbitrary serial de- pendence (conditional or unconditional). The reason we can allow this generality is that we ﬁx the row dimension of Zi and ui and let N ! y. Therefore, we are assuming that N, the size of the cross section, is large enough relative to T to make ﬁxed T asymptotics sensible. (This is the same approach we took in Chapter 7.) With N very large relative to T, there is no need to downweight correlations between time periods that are far apart, as in the Newey and West (1987) estimator applied to time series problems. Ziliak and Kniesner (1998) do use a Newey-West type procedure in a panel data application with large N. Theoretically, this is not required, and it is not completely general because it assumes that the underlying time series are weakly de- pendent. (See Wooldridge, 1994, for discussion of weak dependence in time series contexts.) A Newey-West type estimator might improve the ﬁnite-sample perfor- mance of the GMM estimator. The asymptotic variance of the optimal GMM estimator is estimated as ðX0ZÞ X N i¼1 Z0 i^ui^u0 iZi !\u00031 ðZ0XÞ 2 4 3 5 \u00031 ð8:33Þ System Estimation by Instrumental Variables 193", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 207, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p208::c0", "text": "where ^ui 1 yi \u0003 Xi ^b; asymptotically, it makes no di¤erence whether the ﬁrst-stage residuals ^^u^ui are used in place of ^ui. The square roots of diagonal elements of this matrix are the asymptotic standard errors of the optimal GMM estimator. This esti- mator is called a minimum chi-square estimator, for reasons that will become clear in Section 8.5.2. When Zi ¼ Xi and the ^ui are the system OLS residuals, expression (8.33) becomes the robust variance matrix estimator for SOLS [see expression (7.26)]. This expres- sion reduces to the robust variance matrix estimator for FGLS when Zi ¼ ^W\u00031Xi and the ^ui are the FGLS residuals [see equation (7.49)]. 8.3.4 The Three-Stage Least Squares Estimator The GMM estimator using weighting matrix (8.32) places no restrictions on either the unconditional or conditional (on Zi) variance matrix of ui: we can obtain the asymptotically e‰cient estimator without making additional assumptions. Neverthe- less, it is still common, especially in traditional simultaneous equations analysis, to assume that the conditional variance matrix of ui given Zi is constant. This assump- tion leads to a system estimator that is a middle ground between system 2SLS and the always-e‰cient minimum chi-square estimator. The three-stage least squares (3SLS) estimator is a GMM estimator that uses a particular weighting matrix. To deﬁne the 3SLS estimator, let ^^u^ui ¼ yi \u0003 Xi ^^b^b be the residuals from an initial estimation, usually system 2SLS. Deﬁne the G \u0001 G matrix ^W 1 N\u00031 X N i¼1 ^^u^ui^^u^u0 i ð8:34Þ Using the same arguments as in the FGLS case in Section 7.5.1, ^W ! p W ¼ Eðuiu0 iÞ. The weighting matrix used by 3SLS is ^W ¼ N\u00031 X N i¼1 Z0 i ^WZi !\u00031 ¼ ½Z0ðIN n ^WÞZ=N\u0004\u00031 ð8:35Þ where IN is the N \u0001 N identity matrix. Plugging this into equation (8.24) gives the 3SLS estimator ^b ¼ ½X0ZfZ0ðIN n ^WÞZg\u00031Z0X\u0004\u00031X0ZfZ0ðIN n ^WÞZg\u00031Z0Y ð8:36Þ By Theorems 8.1 and 8.2, ^b is consistent and asymptotically normal under Assump- tions SIV.1–SIV.3. Assumption SIV.3 requires EðZ0 iWZiÞ to be nonsingular, a stan- dard assumption. Chapter 8 194", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 208, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p209::c0", "text": "When is 3SLS asymptotically e‰cient? First, note that equation (8.35) always consistently estimates ½EðZ0 iWZiÞ\u0004\u00031. Therefore, from Theorem 8.3, equation (8.35) is an e‰cient weighting matrix provided EðZ0 iWZiÞ ¼ L ¼ EðZ0 iuiu0 iZiÞ. assumption SIV.5: EðZ0 iuiu0 iZiÞ ¼ EðZ0 iWZiÞ, where W 1 Eðuiu0 iÞ. Assumption SIV.5 is the system extension of the homoskedasticity assumption for 2SLS estimation of a single equation. A su‰cient condition for Assumption SIV.5, and one that is easier to interpret, is Eðuiu0 i j ZiÞ ¼ Eðuiu0 iÞ ð8:37Þ We do not take equation (8.37) as the homoskedasticity assumption because there are interesting applications where Assumption SIV.5 holds but equation (8.37) does not (more on this topic in Chapters 9 and 11). When Eðui j ZiÞ ¼ 0 ð8:38Þ is assumed in place of Assumption SIV.1, then equation (8.37) is equivalent to Varðui j ZiÞ ¼ VarðuiÞ. Whether we state the assumption as in equation (8.37) or use the weaker form, Assumption SIV.5, it is important to see that the elements of the unconditional variance matrix W are not restricted: s2 g ¼ VarðugÞ can change across g, and sgh ¼ Covðug; uhÞ can di¤er across g and h. The system homoskedasticity assumption (8.37) necessarily holds when the instru- ments Zi are treated as nonrandom and VarðuiÞ is constant across i. Because we are assuming random sampling, we are forced to properly focus attention on the variance of ui conditional on Zi. For the system of equations (8.12) with instruments deﬁned in the matrix (8.15), Assumption SIV.5 reduces to (without the i subscript) Eðuguhz0 gzhÞ ¼ EðuguhÞEðz0 gzhÞ; g; h ¼ 1; 2; . . . ; G ð8:39Þ Therefore, uguh must be uncorrelated with each of the elements of z0 gzh. When g ¼ h, assumption (8.39) becomes Eðu2 gz0 gzgÞ ¼ Eðu2 gÞEðz0 gzgÞ ð8:40Þ so that u2 g is uncorrelated with each element of zg along with the squares and cross products of the zg elements. This is exactly the homoskedasticity assumption for single-equation IV analysis (Assumption 2SLS.3). For g 0 h, assumption (8.39) is new because it involves covariances across di¤erent equations. Assumption SIV.5 implies that Assumption SIV.4 holds [because the matrix (8.35) consistently estimates L\u00031 under Assumption SIV.5]. Therefore, we have the follow- ing theorem: System Estimation by Instrumental Variables 195", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 209, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p210::c0", "text": "theorem 8.4 (Optimality of 3SLS): Under Assumptions SIV.1, SIV.2, SIV.3, and SIV.5, the 3SLS estimator is an optimal GMM estimator. Further, the appropriate estimator of Avarð ^bÞ is ðX0ZÞ X N i¼1 Z0 i ^WZi !\u00031 ðZ0XÞ 2 4 3 5 \u00031 ¼ ½X0ZfZ0ðIN n ^WÞZg\u00031Z0X\u0004\u00031 ð8:41Þ It is important to understand the implications of this theorem. First, without As- sumption SIV.5, the 3SLS estimator is generally less e‰cient, asymptotically, than the minimum chi-square estimator, and the asymptotic variance estimator for 3SLS in equation (8.41) is inappropriate. Second, even with Assumption SIV.5, the 3SLS estimator is no more asymptotically e‰cient than the minimum chi-square estimator: expressions (8.32) and (8.35) are both consistent estimators of L\u00031 under Assumption SIV.5. In other words, the estimators based on these two di¤erent choices for ^W are ﬃﬃﬃﬃ N p -equivalent under Assumption SIV.5. Given the fact that the GMM estimator using expression (8.32) as the weighting matrix is never worse, asymptotically, than 3SLS, and in some important cases is strictly better, why is 3SLS ever used? There are at least two reasons. First, 3SLS has a long history in simultaneous equations models, whereas the GMM approach has been around only since the early 1980s, starting with the work of Hansen (1982) and White (1982b). Second, the 3SLS estimator might have better ﬁnite sample properties than the optimal GMM estimator when Assumption SIV.5 holds. However, whether it does or not must be determined on a case-by-case basis. There is an interesting corollary to Theorem 8.4. Suppose that in the system (8.11) we can assume EðXi n uiÞ ¼ 0, which is Assumption SGLS.1 from Chapter 7. We can use a method of moments approach to estimating b, where the instruments for each equation, xo i , is the row vector containing every row of Xi. As shown by Im, Ahn, Schmidt, and Wooldridge (1999), the 3SLS estimator using instruments Zi 1 IG n xo i is equal to the feasible GLS estimator that uses the same ^W. Therefore, if Assumption SIV.5 holds with Zi 1 IG n xo i , FGLS is asymptotically e‰cient in the class of GMM estimators that use the orthogonality condition in Assumption SGLS.1. Su‰cient for Assumption SIV.5 in the GLS context is the homoskedasticity assumption Eðuiu0 i j XiÞ ¼ W. 8.3.5 Comparison between GMM 3SLS and Traditional 3SLS The deﬁnition of the GMM 3SLS estimator in equation (8.36) di¤ers from the deﬁ- nition of the 3SLS estimator in most textbooks. Using our notation, the expression for the traditional 3SLS estimator is Chapter 8 196", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 210, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p211::c0", "text": "^b ¼ X N i¼1 ^X0 i ^W\u00031 ^Xi !\u00031 X N i¼1 ^X0 i ^W\u00031yi ! ¼ ½^X0ðIN n ^W\u00031Þ^X\u0004\u00031 ^X0ðIN n ^W\u00031ÞY ð8:42Þ where ^W is given in expression (8.34), ^Xi 1 Zi ^P, and ^P ¼ ðZ0ZÞ\u00031Z0X. Comparing equations (8.36) and (8.42) shows that, in general, these are di¤erent estimators. To study equation (8.42) more closely, write it as ^b ¼ b þ N\u00031 X N i¼1 ^X0 i ^W\u00031 ^Xi !\u00031 N\u00031 X N i¼1 ^X0 i ^W\u00031ui ! Because ^P ! p P 1 ½EðZ0 iZiÞ\u0004\u00031EðZ0 iXiÞ and ^W ! p W, the probability limit of the sec- ond term is the same as plim N\u00031 X N i¼1 ðZiPÞ0W\u00031ðZiPÞ \" #\u00031 N\u00031 X N i¼1 ðZiPÞ0W\u00031ui \" # ð8:43Þ The ﬁrst factor in expression (8.43) generally converges to a positive deﬁnite matrix. Therefore, if equation (8.42) is to be consistent for b, we need E½ðZiPÞ0W\u00031ui\u0004 ¼ P0E½ðW\u00031ZiÞ0ui\u0004 ¼ 0 Without assuming a special structure for P, we should have that W\u00031Zi is uncorre- lated with ui, an assumption that is not generally implied by Assumption SIV.1. In other words, the traditional 3SLS estimator generally uses a di¤erent set of ortho- gonality conditions than the GMM 3SLS estimator. The GMM 3SLS estimator is guaranteed to be consistent under Assumptions SIV.1–SIV.3, while the traditional 3SLS estimator is not. The best way to illustrate this point is with model (8.12) where Zi is given in matrix (8.15) and we assume Eðz0 iguigÞ ¼ 0, g ¼ 1; 2; . . . ; G. Now, unless W is diagonal, E½ðW\u00031ZiÞ0ui\u0004 0 0 unless zig is uncorrelated with each uih for all g; h ¼ 1; 2; . . . ; G. If zig is correlated with uih for some g 0 h, the transformation of the instruments in equation (8.42) results in inconsistency. The GMM 3SLS estimator is based on the original orthogonality conditions, while the traditional 3SLS estimator is not. See Problem 8.6 for the G ¼ 2 case. Why, then, does equation (8.42) usually appear as the deﬁnition of the 3SLS esti- mator? The reason is that the 3SLS estimator is typically introduced in simultaneous equations models where any variable exogenous in one equation is assumed to be System Estimation by Instrumental Variables 197", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 211, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p212::c0", "text": "exogenous in all equations. Consider the model (8.12) again, but assume that the in- strument matrix is Zi ¼ IG n zi, where zi contains the exogenous variables appearing anywhere in the system. With this choice of Zi, Assumption SIV.1 is equivalent to Eðz0 iuigÞ ¼ 0, g ¼ 1; 2; . . . ; G. It follows that any linear combination of Zi is orthog- onal to ui, including W\u00031Zi. In this important special case, traditional 3SLS is a consistent estimator. In fact, as shown by Schmidt (1990), the GMM 3SLS estimator and the traditional 3SLS estimator are algebraically identical. Because we will encounter cases where we need di¤erent instruments for di¤erent equations, the GMM deﬁnition of 3SLS in equation (8.36) is preferred: it is more generally valid, and it reduces to the standard deﬁnition in the traditional simulta- neous equations setting. 8.4 Some Considerations When Choosing an Estimator We have already discussed the assumptions under which the 3SLS estimator is an e‰cient GMM estimator. It follows that, under the assumptions of Theorem 8.4, 3SLS is as e‰cient asymptotically as the system 2SLS estimator. Nevertheless, it is useful to know that there are some situations where the system 2SLS and 3SLS esti- mators are equivalent. First, when the general system (8.11) is just identiﬁed, that is, L ¼ K, all GMM estimators reduce to the instrumental variables estimator in equa- tion (8.22). In the special (but still fairly general) case of the SUR system (8.12), the system is just identiﬁed if and only if each equation is just identiﬁed: Lg ¼ Kg, g ¼ 1; 2; . . . ; G and the rank condition holds for each equation. When each equation is just identiﬁed, the system IV estimator is IV equation by equation. For the remaining discussion, we consider model (8.12) when at least one equation is overidentiﬁed. When ^W is a diagonal matrix, that is, ^W ¼ diagð^s2 1; . . . ; ^s2 GÞ, 2SLS equation by equation is algebraically equivalent to 3SLS, regardless of the degree of overidentiﬁcation (see Problem 8.7). Therefore, if we force our estimator ^W to be diagonal, we obtain 2SLS equation by equation. The algebraic equivalance between system 2SLS and 3SLS when ^W is diagonal allows us to conclude that 2SLS and 3SLS are asymptotically equivalent if W is di- agonal. The reason is simple. If we could use W in the 3SLS estimator, 3SLS would be identical to 2SLS. The actual 3SLS estimator, which uses ^W, is ﬃﬃﬃﬃ N p -equivalent to the hypothetical 3SLS estimator that uses W. Therefore, 3SLS and 2SLS are ﬃﬃﬃﬃ N p - equivalent. Even in cases where the 2SLS estimator is not algebraically or asympotically equivalent to 3SLS, it is not necessarily true that we should prefer 3SLS (or the minimum chi-square estimator more generally). Why? Suppose that primary interest Chapter 8 198", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 212, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p213::c0", "text": "lies in estimating the parameters in the ﬁrst equation, b1. On the one hand, we know that 2SLS estimation of this equation produces consistent estimators under the orthogonality condition Eðz0 1u1Þ ¼ 0 and the condition rank Eðz0 1x1Þ ¼ K1. We do not care what is happening elsewhere in the system as long as these two assumptions hold. On the other hand, the system-based 3SLS and minimum chi-square estimators of b1 are generally inconsistent unless Eðz0 gugÞ ¼ 0 for all g. Therefore, in using a system method to consistently estimate b1, all equations in the system must be prop- erly speciﬁed, which means their instruments must be exogenous. Such is the nature of system estimation procedures. As with system OLS and FGLS, there is a trade-o¤ between robustness and e‰ciency. 8.5 Testing Using GMM 8.5.1 Testing Classical Hypotheses Testing hypotheses after GMM estimation is straightforward. Let ^b denote a GMM estimator, and let ^V denote its estimated asymptotic variance. Although the following analysis can be made more general, in most applications we use an optimal GMM estimator. Without Assumption SIV.5, the weighting matrix would be expression (8.32) and ^V would be as in expression (8.33). This can be used for computing t sta- tistics by obtaining the asymptotic standard errors (square roots of the diagonal elements of ^V). Wald statistics of linear hypotheses of the form H0: Rb ¼ r, where R is a Q \u0001 K matrix with rank Q, are obtained using the same statistic we have already seen several times. Under Assumption SIV.5 we can use the 3SLS estimator and its asymptotic variance estimate in equation (8.41). For testing general system hypoth- eses we would probably not use the 2SLS estimator because its asymptotic variance is more complicated unless we make very restrictive assumptions. An alternative method for testing linear restrictions uses a statistic based on the dif- ference in the GMM objective function with and without the restrictions imposed. To apply this statistic, we must assume that the GMM estimator uses the optimal weighting matrix, so that ^W consistently estimates ½VarðZ0 iuiÞ\u0004\u00031. Then, from Lemma 3.8, N\u00031=2 X N i¼1 Z0 iui !0 ^W N\u00031=2 X N i¼1 Z0 iui ! @ a w2 L ð8:44Þ since Z0 iui is an L \u0001 1 vector with zero mean and variance L. If ^W does not con- sistently estimate ½VarðZ0 iuiÞ\u0004\u00031, then result (8.44) is false, and the following method does not produce an asymptotically chi-square statistic. System Estimation by Instrumental Variables 199", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 213, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p214::c0", "text": "Let ^b again be the GMM estimator, using optimal weighting matrix ^W, obtained without imposing the restrictions. Let ~b be the GMM estimator using the same weighting matrix ^W but obtained with the Q linear restrictions imposed. The restricted estimator can always be obtained by estimating a linear model with K \u0003 Q rather than K parameters. Deﬁne the unrestricted and restricted residuals as ^ui 1 yi \u0003 Xi ^b and ~ui 1 yi \u0003 Xi ~b, respectively. It can be shown that, under H0, the GMM distance statistic has a limiting chi-square distribution: X N i¼1 Z0 i~ui !0 ^W X N i¼1 Z0 i~ui ! \u0003 X N i¼1 Z0 i^ui !0 ^W X N i¼1 Z0 i^ui ! \" # =N @ a w2 Q ð8:45Þ See, for example, Hansen (1982) and Gallant (1987). The GMM distance statistic is simply the di¤erence in the criterion function (8.23) evaluated at the restricted and unrestricted estimates, divided by the sample size, N. For this reason, expression (8.45) is called a criterion function statistic. Because constrained minimization cannot result in a smaller objective function than unconstrained minimization, expression (8.45) is always nonnegative and usually strictly positive. Under Assumption SIV.5 we can use the 3SLS estimator, in which case expression (8.45) becomes X N i¼1 Z0 i~ui !0 X N i¼1 Z0 i ^WZi !\u00031 X N i¼1 Z0 i~ui ! \u0003 X N i¼1 Z0 i^ui !0 X N i¼1 Z0 i ^WZi !\u00031 X N i¼1 Z0 i^ui ! ð8:46Þ where ^W would probably be computed using the 2SLS residuals from estimating the unrestricted model. The division by N has disappeared because of the deﬁnition of ^W; see equation (8.35). Testing nonlinear hypotheses is easy once the unrestricted estimator ^b has been obtained. Write the null hypothesis as H0: cðbÞ ¼ 0 ð8:47Þ where cðbÞ 1 ½c1ðbÞ; c2ðbÞ; . . . ; cQðbÞ\u00040 is a Q \u0001 1 vector of functions. Let CðbÞ de- note the Q \u0001 K Jacobian of cðbÞ. Assuming that rank CðbÞ ¼ Q, the Wald statistic is W ¼ cð ^bÞ0ð^C^V^C0Þ\u00031cð ^bÞ ð8:48Þ where ^C 1 Cð ^bÞ is the Jacobian evaluated at the GMM estimate ^b. Under H0, the Wald statistic has an asymptotic w2 Q distribution. Chapter 8 200", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 214, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p215::c0", "text": "8.5.2 Testing Overidentiﬁcation Restrictions Just as in the case of single-equation analysis with more exogenous variables than explanatory variables, we can test whether overidentifying restrictions are valid in a system context. In the model (8.11) with instrument matrix Zi, where Xi is G \u0001 K and Zi is G \u0001 L, there are overidentifying restrictions if L > K. Assuming that ^W is an optimal weighting matrix, it can be shown that N\u00031=2 X N i¼1 Z0 i^ui !0 ^W N\u00031=2 X N i¼1 Z0 i^ui ! @ a w2 L\u0003K ð8:49Þ under the null hypothesis H0: EðZ0 iuiÞ ¼ 0. The asymptotic w2 L\u0003K distribution is sim- ilar to result (8.44), but expression (8.44) contains the unobserved errors, ui, whereas expression (8.49) contains the residuals, ^ui. Replacing ui with ^ui causes the degrees of freedom to fall from L to L \u0003 K: in e¤ect, K orthogonality conditions have been used to compute ^b, and L \u0003 K are left over for testing. The overidentiﬁcation test statistic in expression (8.49) is just the objective function (8.23) evaluated at the solution ^b and divided by N. It is because of expression (8.49) that the GMM estimator using the optimal weighting matrix is called the minimum chi-square estimator: ^b is chosen to make the minimum of the objective function have an asymptotic chi-square distribution. If ^W is not optimal, expression (8.49) fails to hold, making it much more di‰cult to test the overidentifying restrictions. When L ¼ K, the left-hand side of expression (8.49) is identically zero; there are no over- identifying restrictions to be tested. Under Assumption SIV.5, the 3SLS estimator is a minimum chi-square estimator, and the overidentiﬁcation statistic in equation (8.49) can be written as X N i¼1 Z0 i^ui !0 X N i¼1 Z0 i ^WZi !\u00031 X N i¼1 Z0 i^ui ! ð8:50Þ Without Assumption SIV.5, the limiting distribution of this statistic is not chi square. In the case where the model has the form (8.12), overidentiﬁcation test statistics can be used to choose between a systems and a single-equation method. For example, if the test statistic (8.50) rejects the overidentifying restrictions in the entire system, then the 3SLS estimators of the ﬁrst equation are generally inconsistent. Assuming that the single-equation 2SLS estimation passes the overidentiﬁcation test discussed in Chapter 6, 2SLS would be preferred. However, in making this judgment it is, as always, important to compare the magnitudes of the two sets of estimates in addition System Estimation by Instrumental Variables 201", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 215, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p216::c0", "text": "to the statistical signiﬁcance of test statistics. Hausman (1983, p. 435) shows how to construct a statistic based directly on the 3SLS and 2SLS estimates of a particular equation (assuming that 3SLS is asymptotically more e‰cient under the null), and this discussion can be extended to allow for the more general minimum chi-square estimator. 8.6 More E‰cient Estimation and Optimal Instruments In Section 8.3.3 we characterized the optimal weighting matrix given the matrix Zi of instruments. But this discussion begs the question of how we can best choose Zi. In this section we brieﬂy discuss two e‰ciency results. The ﬁrst has to do with adding valid instruments. To be precise, let Zi1 be a G \u0001 L1 submatrix of the G \u0001 L matrix Zi, where Zi satisﬁes Assumptions SIV.1 and SIV.2. We also assume that Zi1 satisﬁes Assumption SIV.2; that is, EðZ0 i1XiÞ has rank K. This assumption ensures that b is identiﬁed using the smaller set of instruments. (Necessary is L1 b K.) Given Zi1, we know that the e‰cient GMM estimator uses a weighting matrix that is consistent for L\u00031 1 , where L1 ¼ EðZ0 i1uiu0 iZi1Þ. When we use the full set of instruments Zi ¼ ðZi1; Zi2Þ, the op- timal weighting matrix is a consistent estimator of L given in expression (8.26). The question is, Can we say that using the full set of instruments (with the optimal weighting matrix) is better than using the reduced set of instruments (with the opti- mal weighting matrix)? The answer is that, asymptotically, we can do no worse, and often we can do better, using a larger set of valid instruments. The proof that adding orthogonality conditions generally improves e‰ciency pro- ceeds by comparing the asymptotic variances of ﬃﬃﬃﬃ N p ð ~b \u0003 bÞ and ﬃﬃﬃﬃ N p ð ^b \u0003 bÞ, where the former estimator uses the restricted set of IVs and the latter uses the full set. Then Avar ﬃﬃﬃﬃ N p ð ~b \u0003 bÞ \u0003 Avar ﬃﬃﬃﬃ N p ð ^b \u0003 bÞ ¼ ðC0 1L\u00031 1 C1Þ\u00031 \u0003 ðC0L\u00031CÞ\u00031 ð8:51Þ where C1 ¼ EðZ0 i1XiÞ. The di¤erence in equation (8.51) is positive semideﬁnite if and only if C0L\u00031C \u0003 C0 1L\u00031 1 C1 is p.s.d. The latter result is shown by White (1984, Prop- osition 4.49) using the formula for partitioned inverse; we will not reproduce it here. The previous argument shows that we can never do worse asymptotically by add- ing instruments and computing the minimum chi-square estimator. But we need not always do better. The proof in White (1984) shows that the asymptotic variances of ~b and ^b are identical if and only if C2 ¼ EðZ0 i2uiu0 iZi1ÞL\u00031 1 C1 ð8:52Þ Chapter 8 202", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 216, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p217::c0", "text": "where C2 ¼ EðZ0 i2XiÞ. Generally, this condition is di‰cult to check. However, if we assume that EðZ0 iuiu0 iZiÞ ¼ s2EðZ0 iZiÞ—the ideal assumption for system 2SLS—then condition (8.52) becomes EðZ0 i2XiÞ ¼ EðZ0 i2Zi1Þ½EðZ0 i1Zi1Þ\u0004\u00031EðZ0 i1XiÞ Straightforward algebra shows that this condition is equivalent to E½ðZi2 \u0003 Zi1D1Þ0Xi\u0004 ¼ 0 ð8:53Þ where D1 ¼ ½EðZ0 i1Zi1Þ\u0004\u00031EðZ0 i1Zi2Þ is the L1 \u0001 L2 matrix of coe‰cients from the population regression of Zi1 on Zi2. Therefore, condition (8.53) has a simple inter- pretation: Xi is orthogonal to the part of Zi2 that is left after netting out Zi1. This statement means that Zi2 is not partially correlated with Xi, and so it is not useful as instruments once Zi1 has been included. Condition (8.53) is very intuitive in the context of 2SLS estimation of a single equation. Under Eðu2 i z0 iziÞ ¼ s2Eðz0 iziÞ, 2SLS is the minimum chi-square estimator. The elements of zi would include all exogenous elements of xi, and then some. If, say, xiK is the only endogenous element of xi, condition (8.53) becomes LðxiK j zi1; zi2Þ ¼ LðxiK j zi1Þ ð8:54Þ so that the linear projection of xiK onto zi depends only on zi1. If you recall how the IVs for 2SLS are obtained—by estimating the linear projection of xiK on zi in the ﬁrst stage—it makes perfectly good sense that zi2 can be omitted under condition (8.54) without a¤ecting e‰ciency of 2SLS. In the general case, if the error vector ui contains conditional heteroskedasticity, or correlation across its elements (conditional or otherwise), condition (8.52) is unlikely to be true. As a result, we can keep improving asymptotic e‰ciency by adding more valid instruments. Whenever the error term satisﬁes a zero conditional mean assumption, unlimited IVs are available. For example, consider the linear model Eðy j xÞ ¼ xb, so that the error u ¼ y \u0003 xb has a zero mean given x. The OLS esti- mator is the IV estimator using IVs z1 ¼ x. The preceding e‰ciency result implies that, if Varðu j xÞ 0 VarðuÞ, there are unlimited minimum chi-square estimators that are asymptotically more e‰cient than OLS. Because Eðu j xÞ ¼ 0, hðxÞ is a valid set of IVs for any vector function hð\u0002Þ. (Assuming, as always, that the appropriate moments exist.) Then, the minimum chi-square estimate using IVs z ¼ ½x; hðxÞ\u0004 is generally more asymptotically e‰cient than OLS. (Chamberlain, 1982, and Cragg, 1983, independently obtained this result.) If Varðy j xÞ is constant, adding functions of x to the IV list results in no asymptotic improvement because the linear projection of x onto x and hðxÞ obviously does not depend on hðxÞ. System Estimation by Instrumental Variables 203", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 217, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p218::c0", "text": "Under homoskedasticity, adding moment conditions does not reduce the asymp- totic e‰ciency of the minimum chi-square estimator. Therefore, it may seem that, when we have a linear model that represents a conditional expectation, we cannot lose by adding IVs and performing minimum chi-square. [Plus, we can then test the functional form Eðy j xÞ ¼ xb by testing the overidentifying restrictions.] Unfortu- nately, as shown by several authors, including Tauchen (1986), Altonji and Segal (1996), and Ziliak (1997), GMM estimators that use many overidentifying restric- tions can have very poor ﬁnite sample properties. The previous discussion raises the following possibility: rather than adding more and more orthogonality conditions to improve on ine‰cient estimators, can we ﬁnd a small set of optimal IVs? The answer is yes, provided we replace Assumption SIV.1 with a zero conditional mean assumption. assumption SIV.10: Eðuig j ziÞ ¼ 0, g ¼ 1; . . . ; G for some vector zi. Assumption SIV.10 implies that zi is exogenous in every equation, and each element of the instrument matrix Zi can be any function of zi. theorem 8.5 (Optimal Instruments): Under Assumption SIV.10 (and su‰cient reg- ularity conditions), the optimal choice of instruments is Z\u0005 i ¼ WðziÞ\u00031EðXi j ziÞ, where WðziÞ 1 Eðu0 iui j ziÞ, provided that rank EðZ\u00050 i XiÞ ¼ K. We will not prove Theorem 8.5 here. We discuss a more general case in Section 14.5; see also Newey and McFadden (1994, Section 5.4). Theorem 8.5 implies that, if the G \u0001 K matrix Z\u0005 i were available, we would use it in equation (8.22) in place of Zi to obtain the SIV estimator with the smallest asymptotic variance. This would take the arbitrariness out of choosing additional functions of zi to add to the IV list: once we have Z\u0005 i , all other functions of zi are redundant. Theorem 8.5 implies that, if the errors in the system satisfy SIV.10, the homo- skedasticity assumption (8.37), and EðXi j ziÞ ¼ ZiP for some G \u0001 L matrix Zi and an L \u0001 K unknown matrix P, then the 3SLS estimator is the e‰cient estimator based on the orthogonality conditions SIV.10. Showing this result is easy given the traditional form of the 3SLS estimator in equation (8.41). If Eðui j XiÞ ¼ 0 and Eðuiu0 i j XiÞ ¼ W, then the optimal instruments are W\u00031Xi, which gives the GLS estimator. Replacing W by ^W has no e¤ect asymptotically, and so the FGLS is the SIV estimator with optimal choice of instruments. Without further assumptions, both WðziÞ and EðXi j ziÞ can be arbitrary functions of zi, in which case the optimal SIV estimator is not easily obtainable. It is possible to ﬁnd an estimator that is asymptotically e‰cient using nonparametric estimation Chapter 8 204", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 218, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p219::c0", "text": "methods to estimate WðziÞ and EðXi j ziÞ, but there are many practical hurdles to overcome in applying such procedures. See Newey (1990) for an approach that approximates EðXi j ziÞ by parametric functional forms, where the approximation gets better as the sample size grows. Problems 8.1. Show that the GMM estimator that solves the problem (8.23) satisﬁes the ﬁrst- order condition X N i¼1 Z0 iXi !0 ^W X N i¼1 Z0 iðyi \u0003 Xi ^bÞ ! ¼ 0 Use this expression to obtain formula (8.24). 8.2. Consider the system of equations yi ¼ Xib þ ui where i indexes the cross section observation, yi and ui are G \u0001 1, Xi is G \u0001 K, Zi is the G \u0001 L matrix of instruments, and b is K \u0001 1. Let W ¼ Eðuiu0 iÞ. Make the follow- ing four assumptions: (1) EðZ0 iuiÞ ¼ 0; (2) rank EðZ0 iXiÞ ¼ K; (3) EðZ0 iZiÞ is non- singular; and (4) EðZ0 iWZiÞ is nonsingular. a. What are the properties of the 3SLS estimator? b. Find the asymptotic variance matrix of ﬃﬃﬃﬃ N p ð ^b3SLS \u0003 bÞ. c. How would you estimate Avarð ^b3SLSÞ? 8.3. Let x be a 1 \u0001 K random vector and let z be a 1 \u0001 M random vector. Suppose that Eðx j zÞ ¼ Lðx j zÞ ¼ zP, where P is an M \u0001 K matrix; in other words, the ex- pectation of x given z is linear in z. Let hðzÞ be any 1 \u0001 Q nonlinear function of z, and deﬁne an expanded instrument list as w 1 ½z; hðzÞ\u0004. Show that rank Eðz0xÞ ¼ rank Eðw0xÞ. fHint: First show that rank Eðz0xÞ ¼ rank Eðz0x\u0005Þ, where x\u0005 is the linear projection of x onto z; the same holds with z replaced by w. Next, show that when Eðx j zÞ ¼ Lðx j zÞ, L½x j z; hðzÞ\u0004 ¼ Lðx j zÞ for any function hðzÞ of z.g 8.4. Consider the system of equations (8.12), and let z be a row vector of vari- ables exogenous in every equation. Assume that the exogeneity assumption takes the stronger form Eðug j zÞ ¼ 0, g ¼ 1; 2; . . . ; G. This assumption means that z and non- linear functions of z are valid instruments in every equation. System Estimation by Instrumental Variables 205", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 219, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p220::c0", "text": "a. Suppose that Eðxg j zÞ is linear in z for all g. Show that adding nonlinear functions of z to the instrument list cannot help in satisfying the rank condition. (Hint: Apply Problem 8.3.) b. What happens if Eðxg j zÞ is a nonlinear function of z for some g? 8.5. Verify that the di¤erence ðC0L\u00031CÞ \u0003 ðC0WCÞðC0WLWCÞ\u00031ðC0WCÞ in ex- pression (8.30) is positive semideﬁnite for any symmetric positive deﬁnite matrices W and L. fHint: Show that the di¤erence can be expressed as C0L\u00031=2½IL \u0003 DðD0DÞ\u00031D0\u0004L\u00031=2C where D 1 L1=2WC. Then, note that for any L \u0001 K matrix D, IL \u0003 DðD0DÞ\u00031D0 is a symmetric, idempotent matrix, and therefore positive semideﬁnite.g 8.6. Consider the system (8.12) in the G ¼ 2 case, with an i subscript added: yi1 ¼ xi1b1 þ ui1 yi2 ¼ xi2b2 þ ui2 The instrument matrix is Zi ¼ zi1 0 0 zi2 \u0002 \u0003 Let W be the 2 \u0001 2 variance matrix of ui 1 ðui1; ui2Þ0, and write W\u00031 ¼ s11 s12 s12 s22 \u0002 \u0003 a. Find EðZ0 iW\u00031uiÞ and show that it is not necessarily zero under the orthogonality conditions Eðz0 i1ui1Þ ¼ 0 and Eðz0 i2ui2Þ ¼ 0. b. What happens if W is diagonal (so that W\u00031 is diagonal)? c. What if zi1 ¼ zi2 (without restrictions on W)? 8.7. With deﬁnitions (8.14) and (8.15), show that system 2SLS and 3SLS are numerically identical whenever ^W is a diagonal matrix. 8.8. Consider the standard panel data model introduced in Chapter 7: yit ¼ xitb þ uit ð8:55Þ where the 1 \u0001 K vector xit might have some elements correlated with uit. Let zit be a 1 \u0001 L vector of instruments, L b K, such that Eðz0 ituitÞ ¼ 0, t ¼ 1; 2; . . . ; T. (In prac- Chapter 8 206", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 220, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p221::c0", "text": "tice, zit would contain some elements of xit, including a constant and possibly time dummies.) a. Write down the system 2SLS estimator if the instrument matrix is Zi ¼ ðz0 i1; z0 i2; . . . ; z0 iTÞ0 (a T \u0001 L matrix). Show that this estimator is a pooled 2SLS esti- mator. That is, it is the estimator obtained by 2SLS estimation of equation (8.55) using instruments zit, pooled across all i and t. b. What is the rank condition for the pooled 2SLS estimator? c. Without further assumptions, show how to estimate the asymptotic variance of the pooled 2SLS estimator. d. Show that the assumptions Eðuit j zit; ui;t\u00031; zi;t\u00031; . . . ; ui1; zi1Þ ¼ 0; t ¼ 1; . . . ; T ð8:56Þ Eðu2 it j zitÞ ¼ s2; t ¼ 1; . . . ; T ð8:57Þ imply that the usual standard errors and test statistics reported from the pooled 2SLS estimation are valid. These assumptions make implementing 2SLS for panel data very simple. e. What estimator would you use under condition (8.56) but where we relax condi- tion (8.57) to Eðu2 it j zitÞ ¼ Eðu2 itÞ 1 s2 t , t ¼ 1; . . . ; T? This approach will involve an initial pooled 2SLS estimation. 8.9. Consider the single-equation linear model from Chapter 5: y ¼ xb þ u. Strengthen Assumption 2SLS.1 to Eðu j zÞ ¼ 0 and Assumption 2SLS.3 to Eðu2 j zÞ ¼ s2, and keep the rank condition 2SLS.2. Show that if Eðx j zÞ ¼ zP for some L \u0001 K matrix P, the 2SLS estimator uses the optimal instruments based on the orthogon- ality condition Eðu j zÞ ¼ 0. What does this result imply about OLS if Eðu j xÞ ¼ 0 and Varðu j xÞ ¼ s2? 8.10. In the model from Problem 8.8, let ^uit 1 yit \u0003 xit ^b be the residuals after pooled 2SLS estimation. a. Consider the following test for AR(1) serial correlation in fuit: t ¼ 1; . . . ; Tg: es- timate the auxiliary equation yit ¼ xitb þ r^ui;t\u00031 þ errorit; t ¼ 2; . . . ; T; i ¼ 1; . . . ; N by 2SLS using instruments ðzit; ^ui;t\u00031Þ, and use the t statistic on ^r. Argue that, if we strengthen (8.56) to Eðuit j zit; xi;t\u00031; ui;t\u00031; zi;t\u00031; xi;t\u00032; . . . ; xi1; ui1; zi1Þ ¼ 0, then the heteroskedasticity-robust t statistic for ^r is asymptotically valid as a test for serial correlation. [Hint: Under the dynamic completeness assumption (8.56), which is System Estimation by Instrumental Variables 207", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 221, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p222::c0", "text": "e¤ectively the null hypothesis, the fact that ^ui;t\u00031 is used in place of ui;t\u00031 does not a¤ect the limiting distribution of ^r; see Section 6.1.3.] What is the homoskedasticity assumption that justiﬁes the usual t statistic? b. What should be done to obtain a heteroskedasticity-robust test? 8.11. a. Use Theorem 8.5 to show that, in the single-equation model y1 ¼ z1d1 þ a1y2 þ u1 with Eðu1 j zÞ ¼ 0—where z1 is a strict subset of z—and Varðu1 j zÞ ¼ s2 1, the optimal instrumental variables are ½z1; Eðy2 j zÞ\u0004. b. If y2 is a binary variable with Pðy2 ¼ 1 j zÞ ¼ FðzÞ for some known function Fð\u0002Þ, 0 a FðzÞ a 1, what are the optimal IVs? Chapter 8 208", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 222, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p223::c0", "text": "9 Simultaneous Equations Models 9.1 The Scope of Simultaneous Equations Models The emphasis in this chapter is on situations where two or more variables are jointly determined by a system of equations. Nevertheless, the population model, the iden- tiﬁcation analysis, and the estimation methods apply to a much broader range of problems. In Chapter 8, we saw that the omitted variables problem described in Ex- ample 8.2 has the same statistical structure as the true simultaneous equations model in Example 8.1. In fact, any or all of simultaneity, omitted variables, and measure- ment error can be present in a system of equations. Because the omitted variable and measurement error problems are conceptually easier—and it was for this reason that we discussed them in single-equation contexts in Chapters 4 and 5—our examples and discussion in this chapter are geared mostly toward true simultaneous equations models (SEMs). For e¤ective application of true SEMs, we must understand the kinds of situations suitable for SEM analysis. The labor supply and wage o¤er example, Example 8.1, is a legitimate SEM application. The labor supply function describes individual be- havior, and it is derivable from basic economic principles of individual utility max- imization. Holding other factors ﬁxed, the labor supply function gives the hours of labor supply at any potential wage facing the individual. The wage o¤er function describes ﬁrm behavior, and, like the labor supply function, the wage o¤er function is self-contained. When an equation in an SEM has economic meaning in isolation from the other equations in the system, we say that the equation is autonomous. One way to think about autonomy is in terms of counterfactual reasoning, as in Example 8.1. If we know the parameters of the labor supply function, then, for any individual, we can ﬁnd labor hours given any value of the potential wage (and values of the other observed and unobserved factors a¤ecting labor supply). In other words, we could, in principle, trace out the individual labor supply function for given levels of the other observed and unobserved variables. Causality is closely tied to the autonomy requirement. An equation in an SEM should represent a causal relationship; therefore, we should be interested in varying each of the explanatory variables—including any that are endogenous—while hold- ing all the others ﬁxed. Put another way, each equation in an SEM should represent some underlying conditional expectation that has a causal structure. What compli- cates matters is that the conditional expectations are in terms of counterfactual vari- ables. In the labor supply example, if we could run a controlled experiment, where we exogenously vary the wage o¤er across individuals, then the labor supply function could be estimated without ever considering the wage o¤er function. In fact, in the", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 223, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p224::c0", "text": "absence of omitted variables or measurement error, ordinary least squares would be an appropriate estimation method. Generally, supply and demand examples satisfy the autonomy requirement, re- gardless of the level of aggregation (individual, household, ﬁrm, city, and so on), and simultaneous equations systems were originally developed for such applications. [See, for example, Haavelmo (1943) and Kiefer’s (1989) interview of Arthur S. Goldberger.] Unfortunately, many recent applications of simultaneous equations methods fail the autonomy requirement; as a result, it is di‰cult to interpret what has actually been estimated. Examples that fail the autonomy requirement often have the same feature: the endogenous variables in the system are all choice variables of the same economic unit. As an example, consider an individual’s choice of weekly hours spent in legal market activities and hours spent in criminal behavior. An economic model of crime can be derived from utility maximization; for simplicity, suppose the choice is only between hours working legally (work) and hours involved in crime (crime). The fac- tors assumed to be exogenous to the individual’s choice are things like wage in legal activities, other income sources, probability of arrest, expected punishment, and so on. The utility function can depend on education, work experience, gender, race, and other demographic variables. Two structural equations fall out of the individual’s optimization problem: one has work as a function of the exogenous factors, demographics, and unobservables; the other has crime as a function of these same factors. Of course, it is always possible that factors treated as exogenous by the individual cannot be treated as exogenous by the econometrician: unobservables that a¤ect the choice of work and crime could be correlated with the observable factors. But this possibility is an omitted variables problem. (Measurement error could also be an important issue in this example.) Whether or not omitted variables or measurement error are problems, each equation has a causal interpretation. In the crime example, and many similar examples, it may be tempting to stop be- fore completely solving the model—or to circumvent economic theory altogether— and specify a simultaneous equations system consisting of two equations. The ﬁrst equation would describe work in terms of crime, while the second would have crime as a function of work (with other factors appearing in both equations). While it is often possible to write the ﬁrst-order conditions for an optimization problem in this way, these equations are not the structural equations of interest. Neither equation can stand on its own, and neither has a causal interpretation. For example, what would it mean to study the e¤ect of changing the market wage on hours spent in criminal Chapter 9 210", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 224, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p225::c0", "text": "activity, holding hours spent in legal employment ﬁxed? An individual will generally adjust the time spent in both activities to a change in the market wage. Often it is useful to determine how one endogenous choice variable trades o¤ against another, but in such cases the goal is not—and should not be—to infer causality. For example, Biddle and Hamermesh (1990) present OLS regressions of minutes spent per week sleeping on minutes per week working (controlling for education, age, and other demographic and health factors). Biddle and Hamermesh recognize that there is nothing ‘‘structural’’ about such an analysis. (In fact, the choice of the dependent variable is largely arbitrary.) Biddle and Hamermesh (1990) do derive a structural model of the demand for sleep (along with a labor supply function) where a key ex- planatory variable is the wage o¤er. The demand for sleep has a causal interpreta- tion, and it does not include labor supply on the right-hand side. Why are SEM applications that do not satisfy the autonomy requirement so prev- alent in applied work? One possibility is that there appears to be a general misper- ception that ‘‘structural’’ and ‘‘simultaneous’’ are synonymous. However, we already know that structural models need not be systems of simultaneous equations. And, as the crime/work example shows, a simultaneous system is not necessarily structural. 9.2 Identiﬁcation in a Linear System 9.2.1 Exclusion Restrictions and Reduced Forms Write a system of linear simultaneous equations for the population as y1 ¼ yð1Þgð1Þ þ zð1Þdð1Þ þ u1 .. . yG ¼ yðGÞgðGÞ þ zðGÞdðGÞ þ uG ð9:1Þ where yðhÞ is 1 \u0001 Gh, gðhÞ is Gh \u0001 1, zðhÞ is 1 \u0001 Mh, and dðhÞ is Mh \u0001 1, h ¼ 1; 2; . . . ; G. These are structural equations for the endogenous variables y1; y2; . . . ; yG. We will assume that, if the system (9.1) represents a true simultaneous equations model, then equilibrium conditions have been imposed. Hopefully, each equation is autonomous, but, of course, they do not need to be for the statistical analysis. The vector yðhÞ denotes endogenous variables that appear on the right-hand side of the hth structural equation. By convention, yðhÞ can contain any of the endogenous variables y1; y2; . . . ; yG except for yh. The variables in zðhÞ are the exogenous variables appearing in equation h. Usually there is some overlap in the exogenous variables Simultaneous Equations Models 211", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 225, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p226::c0", "text": "across di¤erent equations; for example, except in special circumstances each zðhÞ would contain unity to allow for nonzero intercepts. The restrictions imposed in sys- tem (9.1) are called exclusion restrictions because certain endogenous and exogenous variables are excluded from some equations. The 1 \u0001 M vector of all exogenous variables z is assumed to satisfy Eðz0ugÞ ¼ 0; g ¼ 1; 2; . . . ; G ð9:2Þ When all of the equations in system (9.1) are truly structural, we are usually willing to assume Eðug j zÞ ¼ 0; g ¼ 1; 2; . . . ; G ð9:3Þ However, we know from Chapters 5 and 8 that assumption (9.2) is su‰cient for consistent estimation. Sometimes, especially in omitted variables and measurement error applications, one or more of the equations in system (9.1) will simply represent a linear projection onto exogenous variables, as in Example 8.2. It is for this reason that we use assumption (9.2) for most of our identiﬁcation and estimation analysis. We assume throughout that Eðz0zÞ is nonsingular, so that there are no exact linear dependencies among the exogenous variables in the population. Assumption (9.2) implies that the exogenous variables appearing anywhere in the system are orthogonal to all the structural errors. If some elements in, say, zð1Þ, do not appear in the second equation, then we are explicitly assuming that they do not enter the structural equation for y2. If there are no reasonable exclusion restrictions in an SEM, it may be that the system fails the autonomy requirement. Generally, in the system (9.1), the error ug in equation g will be correlated with yðgÞ (we show this correlation explicitly later), and so OLS and GLS will be inconsistent. Nevertheless, under certain identiﬁcation assumptions, we can estimate this system using the instrumental variables procedures covered in Chapter 8. In addition to the exclusion restrictions in system (9.1), another possible source of identifying information is on the G \u0001 G variance matrix S 1 VarðuÞ. For now, S is unrestricted and therefore contains no identifying information. To motivate the general analysis, consider speciﬁc labor supply and demand func- tions for some population: hsðwÞ ¼ g1 logðwÞ þ zð1Þdð1Þ þ u1 hdðwÞ ¼ g2 logðwÞ þ zð2Þdð2Þ þ u2 where w is the dummy argument in the labor supply and labor demand functions. We assume that observed hours, h, and observed wage, w, equate supply and demand: Chapter 9 212", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 226, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p227::c0", "text": "h ¼ hsðwÞ ¼ hdðwÞ The variables in zð1Þ shift the labor supply curve, and zð2Þ contains labor demand shifters. By deﬁning y1 ¼ h and y2 ¼ logðwÞ we can write the equations in equilib- rium as a linear simultaneous equations model: y1 ¼ g1y2 þ zð1Þdð1Þ þ u1 ð9:4Þ y1 ¼ g2y2 þ zð2Þdð2Þ þ u2 ð9:5Þ Nothing about the general system (9.1) rules out having the same variable on the left- hand side of more than one equation. What is needed to identify the parameters in, say, the supply curve? Intuitively, since we observe only the equilibrium quantities of hours and wages, we cannot dis- tinguish the supply function from the demand function if zð1Þ and zð2Þ contain exactly the same elements. If, however, zð2Þ contains an element not in zð1Þ—that is, if there is some factor that exogenously shifts the demand curve but not the supply curve—then we can hope to estimate the parameters of the supply curve. To identify the demand curve, we need at least one element in zð1Þ that is not also in zð2Þ. To formally study identiﬁcation, assume that g1 0 g2; this assumption just means that the supply and demand curves have di¤erent slopes. Subtracting equation (9.5) from equation (9.4), dividing by g2 \u0002 g1, and rearranging gives y2 ¼ zð1Þp21 þ zð2Þp22 þ v2 ð9:6Þ where p21 1 dð1Þ=ðg2 \u0002 g1Þ, p22 ¼ \u0002dð2Þ=ðg2 \u0002 g1Þ, and v2 1 ðu1 \u0002 u2Þ=ðg2 \u0002 g1Þ. This is the reduced form for y2 because it expresses y2 as a linear function of all of the exogenous variables and an error v2 which, by assumption (9.2), is orthogonal to all exogenous variables: Eðz0v2Þ ¼ 0. Importantly, the reduced form for y2 is obtained from the two structural equations (9.4) and (9.5). Given equation (9.4) and the reduced form (9.6), we can now use the identiﬁcation condition from Chapter 5 for a linear model with a single right-hand-side endogenous variable. This condition is easy to state: the reduced form for y2 must contain at least one exogenous variable not also in equation (9.4). This means there must be at least one element of zð2Þ not in zð1Þ with coe‰cient in equation (9.6) di¤erent from zero. Now we use the structural equations. Because p22 is proportional to dð2Þ, the condi- tion is easily restated in terms of the structural parameters: in equation (9.5) at least one element of zð2Þ not in zð1Þ must have nonzero coe‰cient. In the supply and de- mand example, identiﬁcation of the supply function requires at least one exogenous variable appearing in the demand function that does not also appear in the supply function; this conclusion corresponds exactly with our earlier intuition. Simultaneous Equations Models 213", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 227, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p228::c0", "text": "The condition for identifying equation (9.5) is just the mirror image: there must be at least one element of zð1Þ actually appearing in equation (9.4) that is not also an element of zð2Þ. Example 9.1 (Labor Supply for Married Women): Consider labor supply and de- mand equations for married women, with the equilibrium condition imposed: hours ¼ g1 logðwageÞ þ d10 þ d11educ þ d12age þ d13kids þ d14othinc þ u1 hours ¼ g2 logðwageÞ þ d20 þ d21educ þ d22exper þ u2 The supply equation is identiﬁed because, by assumption, exper appears in the de- mand function (assuming d22 0 0) but not in the supply equation. The assumption that past experience has no direct a¤ect on labor supply can be questioned, but it has been used by labor economists. The demand equation is identiﬁed provided that at least one of the three variables age, kids, and othinc actually appears in the supply equation. We now extend this analysis to the general system (9.1). For concreteness, we study identiﬁcation of the ﬁrst equation: y1 ¼ yð1Þgð1Þ þ zð1Þdð1Þ þ u1 ¼ xð1Þbð1Þ þ u1 ð9:7Þ where the notation used for the subscripts is needed to distinguish an equation with exclusion restrictions from a general equation that we will study in Section 9.2.2. Assuming that the reduced forms exist, write the reduced form for yð1Þ as yð1Þ ¼ zPð1Þ þ vð1Þ ð9:8Þ where E½z0vð1Þ\u0003 ¼ 0. Further, deﬁne the M \u0001 M1 matrix selection matrix Sð1Þ, which consists of zeros and ones, such that zð1Þ ¼ zSð1Þ. The rank condition from Chapter 5, Assumption 2SLS.2b, can be stated as rank E½z0xð1Þ\u0003 ¼ K1 ð9:9Þ where K1 1 G1 þ M1. But E½z0xð1Þ\u0003 ¼ E½z0ðzPð1Þ; zSð1ÞÞ\u0003 ¼ Eðz0zÞ½Pð1Þ j Sð1Þ\u0003. Since we always assume that Eðz0zÞ has full rank M, assumption (9.9) is the same as rank½Pð1Þ j Sð1Þ\u0003 ¼ G1 þ M1 ð9:10Þ In other words, ½Pð1Þ j Sð1Þ\u0003 must have full column rank. If the reduced form for yð1Þ has been found, this condition can be checked directly. But there is one thing we can conclude immediately: because ½Pð1Þ j Sð1Þ\u0003 is an M \u0001 ðG1 þ M1Þ matrix, a necessary Chapter 9 214", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 228, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p229::c0", "text": "condition for assumption (9.10) is M b G1 þ M1, or M \u0002 M1 b G1 ð9:11Þ We have already encountered condition (9.11) in Chapter 5: the number of exoge- nous variables not appearing in the ﬁrst equation, M \u0002 M1, must be at least as great as the number of endogenous variables appearing on the right-hand side of the ﬁrst equation, G1. This is the order condition for identiﬁcation of equation one. We have proven the following theorem: theorem 9.1 (Order Condition with Exclusion Restrictions): In a linear system of equations with exclusion restrictions, a necessary condition for identifying any par- ticular equation is that the number of excluded exogenous variables from the equa- tion must be at least as large as the number of included right-hand-side endogenous variables in the equation. It is important to remember that the order condition is only necessary, not su‰cient, for identiﬁcation. If the order condition fails for a particular equation, there is no hope of estimating the parameters in that equation. If the order condition is met, the equation might be identiﬁed. 9.2.2 General Linear Restrictions and Structural Equations The identiﬁcation analysis of the preceding subsection is useful when reduced forms are appended to structural equations. When an entire structural system has been speciﬁed, it is best to study identiﬁcation entirely in terms of the structural parameters. To this end, we now write the G equations in the population as yg1 þ zd1 þ u1 ¼ 0 .. . ygG þ zdG þ uG ¼ 0 ð9:12Þ where y 1 ðy1; y2; . . . ; yGÞ is the 1 \u0001 G vector of all endogenous variables and z 1 ðz1; . . . ; zMÞ is still the 1 \u0001 M vector of all exogenous variables, and probably con- tains unity. We maintain assumption (9.2) throughout this section and also assume that Eðz0zÞ is nonsingular. The notation here di¤ers from that in Section 9.2.1. Here, gg is G \u0001 1 and dg is M \u0001 1 for all g ¼ 1; 2; . . . ; G, so that the system (9.12) is the general linear system without any restrictions on the structural parameters. We can write this system compactly as yG þ zD þ u ¼ 0 ð9:13Þ Simultaneous Equations Models 215", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 229, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p230::c0", "text": "where u 1 ðu1; . . . ; uGÞ is the 1 \u0001 G vector of structural errors, G is the G \u0001 G matrix with gth column gg, and D is the M \u0001 G matrix with gth column dg. So that a reduced form exists, we assume that G is nonsingular. Let S 1 Eðu0uÞ denote the G \u0001 G variance matrix of u, which we assume to be nonsingular. At this point, we have placed no other restrictions on G, D, or S. The reduced form is easily expressed as y ¼ zð\u0002DG\u00021Þ þ uð\u0002G\u00021Þ 1 zP þ v ð9:14Þ where P 1 ð\u0002DG\u00021Þ and v 1 uð\u0002G\u00021Þ. Deﬁne L 1 Eðv0vÞ ¼ G\u000210SG\u00021 as the re- duced form variance matrix. Because Eðz0vÞ ¼ 0 and Eðz0zÞ is nonsingular, P and L are identiﬁed because they can be consistently estimated given a random sample on y and z by OLS equation by equation. The question is, Under what assumptions can we recover the structural parameters G, D, and S from the reduced form parameters? It is easy to see that, without some restrictions, we will not be able to identify any of the parameters in the structural system. Let F be any G \u0001 G nonsingular matrix, and postmultiply equation (9.13) by F: yGF þ zDF þ uF ¼ 0 or yG\u0004 þ zD\u0004 þ u\u0004 ¼ 0 ð9:15Þ where G\u0004 1 GF, D\u0004 1 DF, and u\u0004 1 uF; note that Varðu\u0004Þ ¼ F0SF. Simple algebra shows that equations (9.15) and (9.13) have identical reduced forms. This result means that, without restrictions on the structural parameters, there are many equiv- alent structures in the sense that they lead to the same reduced form. In fact, there is an equivalent structure for each nonsingular F. Let B 1 G D \u0001 \u0002 be the ðG þ MÞ \u0001 G matrix of structural parameters in equation (9.13). If F is any nonsingular G \u0001 G matrix, then F represents an admissible linear transformation if 1. BF satisﬁes all restrictions on B. 2. F0SF satisﬁes all restrictions on S. To identify the system, we need enough prior information on the structural param- eters ðB; SÞ so that F ¼ IG is the only admissible linear transformation. In most applications identiﬁcation of B is of primary interest, and this identiﬁca- tion is achieved by putting restrictions directly on B. As we will touch on in Section 9.4.2, it is possible to put restrictions on S in order to identify B, but this approach is somewhat rare in practice. Until we come to Section 9.4.2, S is an unrestricted G \u0001 G positive deﬁnite matrix. Chapter 9 216", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 230, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p231::c0", "text": "As before, we consider identiﬁcation of the ﬁrst equation: yg1 þ zd1 þ u1 ¼ 0 ð9:16Þ or g11y1 þ g12y2 þ \u0005 \u0005 \u0005 þ g1GyG þ d11z1 þ d12z2 þ \u0005 \u0005 \u0005 þ d1MzM þ u1 ¼ 0. The ﬁrst re- striction we make on the parameters in equation (9.16) is the normalization restriction that one element of g1 is \u00021. Each equation in the system (9.1) has a normalization restriction because one variable is taken to be the left-hand-side explained variable. In applications, there is usually a natural normalization for each equation. If there is not, we should ask whether the system satisﬁes the autonomy requirement discussed in Section 9.1. (Even in models that satisfy the autonomy requirement, we often have to choose between reasonable normalization conditions. For example, in Example 9.1, we could have speciﬁed the second equation to be a wage o¤er equation rather than a labor demand equation.) Let b1 1 ðg0 1; d0 1Þ0 be the ðG þ MÞ \u0001 1 vector of structural parameters in the ﬁrst equation. With a normalization restriction there are ðG þ MÞ \u0002 1 unknown elements in b1. Assume that prior knowledge about b1 can be expressed as R1b1 ¼ 0 ð9:17Þ where R1 is a J1 \u0001 ðG þ MÞ matrix of known constants, and J1 is the number of restrictions on b1 (in addition to the normalization restriction). We assume that rank R1 ¼ J1, so that there are no redundant restrictions. The restrictions in assumption (9.17) are sometimes called homogeneous linear restrictions, but, when coupled with a normalization assumption, equation (9.17) actually allows for nonhomogeneous restrictions. Example 9.2 (A Three-Equation System): Consider the ﬁrst equation in a system with G ¼ 3 and M ¼ 4: y1 ¼ g12y2 þ g13y3 þ d11z1 þ d12z2 þ d13z3 þ d14z4 þ u1 so that g1 ¼ ð\u00021; g12; g13Þ0, d1 ¼ ðd11; d12; d13; d14Þ0, and b1 ¼ ð\u00021; g12; g13; d11; d12; d13; d14Þ0. (We can set z1 ¼ 1 to allow an intercept.) Suppose the restrictions on the structural parameters are g12 ¼ 0 and d13 þ d14 ¼ 3. Then J1 ¼ 2 and R1 ¼ 0 1 0 0 0 0 0 3 0 0 0 0 1 1 \u0001 \u0002 Straightforward multiplication gives R1b1 ¼ ðg12; d13 þ d14 \u0002 3Þ0, and setting this vector to zero as in equation (9.17) incorporates the restrictions on b1. Simultaneous Equations Models 217", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 231, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p232::c0", "text": "Given the linear restrictions in equation (9.17), when are these and the normaliza- tion restriction enough to identify b1? Let F again be any G \u0001 G nonsingular matrix, and write it in terms of its columns as F ¼ ðf1; f2; . . . ; fGÞ. Deﬁne a linear transfor- mation of B as B\u0004 ¼ BF, so that the ﬁrst column of B\u0004 is b \u0004 1 1 Bf1. We need to ﬁnd a condition so that equation (9.17) allows us to distinguish b1 from any other b \u0004 1. For the moment, ignore the normalization condition. The vector b \u0004 1 satisﬁes the linear restrictions embodied by R1 if and only if R1b \u0004 1 ¼ R1ðBf1Þ ¼ ðR1BÞf1 ¼ 0 ð9:18Þ Naturally, ðR1BÞf1 ¼ 0 is true for f1 ¼ e1 1 ð1; 0; 0; . . . ; 0Þ0, since then b \u0004 1 ¼ Bf1 ¼ b1. Since assumption (9.18) holds for f1 ¼ e1 it clearly holds for any scalar multiple of e1. The key to identiﬁcation is that vectors of the form c1e1, for some constant c1, are the only vectors f1 satisfying condition (9.18). If condition (9.18) holds for vectors f1 other than scalar multiples of e1 then we have no hope of identifying b1. Stating that condition (9.18) holds only for vectors of the form c1e1 just means that the null space of R1B has dimension unity. Equivalently, because R1B has G columns, rank R1B ¼ G \u0002 1 ð9:19Þ This is the rank condition for identiﬁcation of b1 in the ﬁrst structural equation under general linear restrictions. Once condition (9.19) is known to hold, the normalization restriction allows us to distinguish b1 from any other scalar multiple of b1. theorem 9.2 (Rank Condition for Identiﬁcation): Let b1 be the ðG þ MÞ \u0001 1 vector of structural parameters in the ﬁrst equation, with the normalization restriction that one of the coe‰cients on an endogenous variable is \u00021. Let the additional informa- tion on b1 be given by restriction (9.17). Then b1 is identiﬁed if and only if the rank condition (9.19) holds. As promised earlier, the rank condition in this subsection depends on the structural parameters, B. We can determine whether the ﬁrst equation is identiﬁed by studying the matrix R1B. Since this matrix can depend on all structural parameters, we must generally specify the entire structural model. The J1 \u0001 G matrix R1B can be written as R1B ¼ ½R1b1; R1b2; . . . ; R1bG\u0003, where bg is the ðG þ MÞ \u0001 1 vector of structural parameters in equation g. By assumption (9.17), the ﬁrst column of R1B is the zero vector. Therefore, R1B cannot have rank larger than G \u0002 1. What we must check is whether the columns of R1B other than the ﬁrst form a linearly independent set. Using condition (9.19) we can get a more general form of the order condition. Because G is nonsingular, B necessarily has rank G (full column rank). Therefore, for Chapter 9 218", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 232, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p233::c0", "text": "condition (9.19) to hold, we must have rank R1 b G \u0002 1. But we have assumed that rank R1 ¼ J1, which is the row dimension of R1. theorem 9.3 (Order Condition for Identiﬁcation): In system (9.12) under assump- tion (9.17), a necessary condition for the ﬁrst equation to be identiﬁed is J1 b G \u0002 1 ð9:20Þ where J1 is the row dimension of R1. Equation (9.20) is the general form of the order condition. We can summarize the steps for checking whether the ﬁrst equation in the system is identiﬁed. 1. Set one element of g1 to \u00021 as a normalization. 2. Deﬁne the J1 \u0001 ðG þ MÞ matrix R1 such that equation (9.17) captures all restric- tions on b1. 3. If J1 < G \u0002 1, the ﬁrst equation is not identiﬁed. 4. If J1 b G \u0002 1, the equation might be identiﬁed. Let B be the matrix of all struc- tural parameters with only the normalization restrictions imposed, and compute R1B. Now impose the restrictions in the entire system and check the rank condition (9.19). The simplicity of the order condition makes it attractive as a tool for studying identiﬁcation. Nevertheless, it is not di‰cult to write down examples where the order condition is satisﬁed but the rank condition fails. Example 9.3 (Failure of the Rank Condition): Consider the following three-equation structural model in the population ðG ¼ 3; M ¼ 4Þ: y1 ¼ g12y2 þ g13y3 þ d11z1 þ d13z3 þ u1 ð9:21Þ y2 ¼ g21y1 þ d21z1 þ u2 ð9:22Þ y3 ¼ d31z1 þ d32z2 þ d33z3 þ d34z4 þ u3 ð9:23Þ where z1 1 1, EðugÞ ¼ 0, g ¼ 1; 2; 3, and each zj is uncorrelated with each ug. Note that the third equation is already a reduced form equation (although it may also have a structural interpretation). In equation (9.21) we have set g11 ¼ \u00021, d12 ¼ 0, and d14 ¼ 0. Since this equation contains two right-hand-side endogenous variables and there are two excluded exogenous variables, it passes the order condition. To check the rank condition, let b1 denote the 7 \u0001 1 vector of parameters in the ﬁrst equation with only the normalization restriction imposed: b1 ¼ ð\u00021; g12; g13; d11; d12; d13; d14Þ0. The restrictions d12 ¼ 0 and d14 ¼ 0 are obtained by choosing Simultaneous Equations Models 219", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 233, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p234::c0", "text": "R1 ¼ 0 0 0 0 1 0 0 0 0 0 0 0 0 1 \u0001 \u0002 Let B be the full 7 \u0001 3 matrix of parameters with only the three normalizations imposed [so that b2 ¼ ðg21; \u00021; g23; d21; d22; d23; d24Þ0 and b3 ¼ ðg31; g32; \u00021; d31; d32; d33; d34Þ0]. Matrix multiplication gives R1B ¼ d12 d22 d32 d14 d24 d34 \u0001 \u0002 Now we impose all of the restrictions in the system. In addition to the restrictions d12 ¼ 0 and d14 ¼ 0 from equation (9.21), we also have d22 ¼ 0 and d24 ¼ 0 from equation (9.22). Therefore, with all restrictions imposed, R1B ¼ 0 0 d32 0 0 d34 \u0001 \u0002 ð9:24Þ The rank of this matrix is at most unity, and so the rank condition fails because G \u0002 1 ¼ 2. Equation (9.22) easily passes the order condition. It is left to you to show that the rank condition holds if and only if d13 0 0 and at least one of d32 and d34 is di¤erent from zero. The third equation is identiﬁed because it contains no endogenous ex- planatory variables. When the restrictions on b1 consist entirely of normalization and exclusion re- strictions, the order condition (9.20) reduces to the order condition (9.11), as can be seen by the following argument. When all restrictions are exclusion restrictions, the matrix R1 consists only of zeros and ones, and the number of rows in R1 equals the number of excluded right-hand-side endogenous variables, G \u0002 G1 \u0002 1, plus the number of excluded exogenous variables, M \u0002 M1. In other words, J1 ¼ ðG \u0002 G1 \u0002 1Þ þ ðM \u0002 M1Þ, and so the order condition (9.20) becomes ðG \u0002 G1 \u0002 1Þ þ ðM \u0002 M1Þ b G \u0002 1, which, upon rearrangement, becomes condition (9.11). 9.2.3 Unidentiﬁed, Just Identiﬁed, and Overidentiﬁed Equations We have seen that, for identifying a single equation the rank condition (9.19) is neces- sary and su‰cient. When condition (9.19) fails, we say that the equation is unidentiﬁed. When the rank condition holds, it is useful to reﬁne the sense in which the equation is identiﬁed. If J1 ¼ G \u0002 1, then we have just enough identifying information. If we were to drop one restriction in R1, we would necessarily lose identiﬁcation of the ﬁrst equation because the order condition would fail. Therefore, when J1 ¼ G \u0002 1, we say that the equation is just identiﬁed. Chapter 9 220", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 234, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p235::c0", "text": "If J1 > G \u0002 1, it is often possible to drop one or more restrictions on the param- eters of the ﬁrst equation and still achieve identiﬁcation. In this case we say the equa- tion is overidentiﬁed. Necessary but not su‰cient for overidentiﬁcation is J1 > G \u0002 1. It is possible that J1 is strictly greater than G \u0002 1 but the restrictions are such that drop- ping one restriction loses identiﬁcation, in which case the equation is not overidentiﬁed. In practice, we often appeal to the order condition to determine the degree of overidentiﬁcation. While in special circumstances this approach can fail to be accu- rate, for most applications it is reasonable. Thus, for the ﬁrst equation, J1 \u0002 ðG \u0002 1Þ is usually intepreted as the number of overidentifying restrictions. Example 9.4 (Overidentifying Restrictions): Consider the two-equation system y1 ¼ g12y2 þ d11z1 þ d12z2 þ d13z3 þ d14z4 þ u1 ð9:25Þ y2 ¼ g21y1 þ d21z1 þ d22z2 þ u2 ð9:26Þ where EðzjugÞ ¼ 0, all j and g. Without further restrictions, equation (9.25) fails the order condition because every exogenous variable appears on the right-hand side, and the equation contains an endogenous variable. Using the order condition, equa- tion (9.26) is overidentiﬁed, with one overidentifying restriction. If z3 does not actu- ally appear in equation (9.25), then equation (9.26) is just identiﬁed, assuming that d14 0 0. 9.3 Estimation after Identiﬁcation 9.3.1 The Robustness-E‰ciency Trade-o¤ All SEMs with linearly homogeneous restrictions within each equation can be written with exclusion restrictions as in the system (9.1); doing so may require redeﬁning some of the variables. If we let xðgÞ ¼ ðyðgÞ; zðgÞÞ and bðgÞ ¼ ðg0 ðgÞ; d0 ðgÞÞ0, then the sys- tem (9.1) is in the general form (8.11) with the slight change in notation. Under as- sumption (9.2) the matrix of instruments for observation i is the G \u0001 GM matrix Zi 1 IG n zi ð9:27Þ If every equation in the system passes the rank condition, a system estimation procedure—such as 3SLS or the more general minimum chi-square estimator—can be used. Alternatively, the equations of interest can be estimated by 2SLS. The bot- tom line is that the methods studied in Chapters 5 and 8 are directly applicable. All of the tests we have covered apply, including the tests of overidentifying restrictions in Chapters 6 and 8, and the single-equation tests for endogeneity in Chapter 6. Simultaneous Equations Models 221", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 235, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p236::c0", "text": "When estimating a simultaneous equations system, it is important to remember the pros and cons of full system estimation. If all equations are correctly speciﬁed, system procedures are asymptotically more e‰cient than a single-equation procedure such as 2SLS. But single-equation methods are more robust. If interest lies, say, in the ﬁrst equation of a system, 2SLS is consistent and asymptotically normal provided the ﬁrst equation is correctly speciﬁed and the instruments are exogenous. However, if one equation in a system is misspeciﬁed, the 3SLS or GMM estimates of all the pa- rameters are generally inconsistent. Example 9.5 (Labor Supply for Married, Working Women): Using the data in MROZ.RAW, we estimate a labor supply function for working, married women. Rather than specify a demand function, we specify the second equation as a wage o¤er function and impose the equilibrium condition: hours ¼ g12 logðwageÞ þ d10 þ d11educ þ d12age þ d13kidslt6 þ d14kidsge6 þ d15nwifeinc þ u1 ð9:28Þ logðwageÞ ¼ g21hours þ d20 þ d21educ þ d22exper þ d23exper2 þ u2 ð9:29Þ where kidslt6 is number of children less than 6, kidsge6 is number of children between 6 and 18, and nwifeinc is income other than the woman’s labor income. We assume that u1 and u2 have zero mean conditional on educ, age, kidslt6, kidsge6, nwifeinc, and exper. The key restriction on the labor supply function is that exper (and exper2) have no direct e¤ect on current annual hours. This identiﬁes the labor supply function with one overidentifying restriction, as used by Mroz (1987). We estimate the labor supply function ﬁrst by OLS [to see what ignoring the endogeneity of logðwageÞ does] and then by 2SLS, using as instruments all exogenous variables in equations (9.28) and (9.29). There are 428 women who worked at some time during the survey year, 1975. The average annual hours are about 1,303 with a minimum of 12 and a maximum of 4,950. We ﬁrst estimate the labor supply function by OLS: ho^urs ¼ 2;114:7 ð340:1Þ \u0002 17:41 ð54:22Þ logðwageÞ \u0002 14:44 ð17:97Þ educ \u0002 7:73 ð5:53Þ age \u0002 342:50 ð100:01Þ kidslt6 \u0002 115:02 ð30:83Þ kidsge6 \u0002 4:35 ð3:66Þ nwifeinc Chapter 9 222", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 236, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p237::c0", "text": "The OLS estimates indicate a downward-sloping labor supply function, although the estimate on logðwageÞ is statistically insigniﬁcant. The estimates are much di¤erent when we use 2SLS: ho^urs ¼ 2;432:2 ð594:2Þ þ 1;544:82 ð480:74Þ logðwageÞ \u0002 177:45 ð58:14Þ educ \u0002 10:78 ð9:58Þ age \u0002 210:83 ð176:93Þ kidslt6 \u0002 47:56 ð56:92Þ kidsge6 \u0002 9:25 ð6:48Þ nwifeinc The estimated labor supply elasticity is 1;544:82=hours. At the mean hours for work- ing women, 1,303, the estimated elasticity is about 1.2, which is quite large. The supply equation has a single overidentifying restriction. The regression of the 2SLS residuals ^u1 on all exogenous variables produces R2 u ¼ :002, and so the test statistic is 428ð:002ÞA:856 with p-valueA:355; the overidentifying restriction is not rejected. Under the exclusion restrictions we have imposed, the wage o¤er function (9.29) is also identiﬁed. Before estimating the equation by 2SLS, we ﬁrst estimate the reduced form for hours to ensure that the exogenous variables excluded from equation (9.29) are jointly signiﬁcant. The p-value for the F test of joint signiﬁcance of age, kidslt6, kidsge6, and nwifeinc is about .0009. Therefore, we can proceed with 2SLS estimation of the wage o¤er equation. The coe‰cient on hours is about .00016 (standard errorA:00022), and so the wage o¤er does not appear to di¤er by hours worked. The remaining coe‰cients are similar to what is obtained by dropping hours from equa- tion (9.29) and estimating the equation by OLS. (For example, the 2SLS coe‰cient on education is about .111 with seA:015.) Interestingly, while the wage o¤er function (9.29) is identiﬁed, the analogous labor demand function is apparently unidentiﬁed. (This ﬁnding shows that choosing the normalization—that is, choosing between a labor demand function and a wage o¤er function—is not innocuous.) The labor demand function, written in equilibrium, would look like this: hours ¼ g22 logðwageÞ þ d20 þ d21educ þ d22exper þ d23exper2 þ u2 ð9:30Þ Estimating the reduced form for logðwageÞ and testing for joint signiﬁcance of age, kidslt6, kidsge6, and nwifeinc yields a p-value of about .46, and so the exogenous variables excluded from equation (9.30) would not seem to appear in the reduced form for logðwageÞ. Estimation of equation (9.30) by 2SLS would be pointless. [You are invited to estimate equation (9.30) by 2SLS to see what happens.] Simultaneous Equations Models 223", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 237, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p238::c0", "text": "It would be more e‰cient to estimate equations (9.28) and (9.29) by 3SLS, since each equation is overidentiﬁed (assuming the homoskedasticity assumption SIV.5). If heteroskedasticity is suspected, we could use the general minimum chi-square esti- mator. A system procedure is more e‰cient for estimating the labor supply function because it uses the information that age, kidslt6, kidsge6, and nwifeinc do not appear in the logðwageÞ equation. If these exclusion restrictions are wrong, the 3SLS esti- mators of parameters in both equations are generally inconsistent. Problem 9.9 asks you to obtain the 3SLS estimates for this example. 9.3.2 When Are 2SLS and 3SLS Equivalent? In Section 8.4 we discussed the relationship between 2SLS and 3SLS for a general linear system. Applying that discussion to linear SEMs, we can immediately draw the following conclusions: (1) if each equation is just identiﬁed, 2SLS equation by equa- tion is algebraically identical to 3SLS, which is the same as the IV estimator in equation (8.22); (2) regardless of the degree of overidentiﬁcation, 2SLS equation by equation and 3SLS are identical if ^S is diagonal. Another useful equivalence result in the context of linear SEMs is as follows. Suppose that the ﬁrst equation in a system is overidentiﬁed but every other equation is just identiﬁed. (A special case occurs when the ﬁrst equation is a structural equa- tion and all remaining equations are unrestricted reduced forms.) Then the 2SLS es- timator of the ﬁrst equation is the same as the 3SLS estimator. This result follows as a special case of Schmidt (1976, Theorem 5.2.13). 9.3.3 Estimating the Reduced Form Parameters So far, we have discussed estimation of the structural parameters. The usual justiﬁ- cations for focusing on the structural parameters are as follows: (1) we are interested in estimates of ‘‘economic parameters’’ (such as labor supply elasticities) for curi- osity’s sake; (2) estimates of structural parameters allow us to obtain the e¤ects of a variety of policy interventions (such as changes in tax rates); and (3) even if we want to estimate the reduced form parameters, we often can do so more e‰ciently by ﬁrst estimating the structural parameters. Concerning the second reason, if the goal is to estimate, say, the equilibrium change in hours worked given an exogenous change in a marginal tax rate, we must ultimately estimate the reduced form. As another example, we might want to estimate the e¤ect on county-level alcohol consumption due to an increase in exogenous alcohol taxes. In other words, we are interested in qEðyg j zÞ=qzj ¼ pgj, where yg is alcohol consumption and zj is the tax on alcohol. Under weak assumptions, reduced form equations exist, and each equa- tion of the reduced form can be estimated by ordinary least squares. Without placing any restrictions on the reduced form, OLS equation by equation is identical to SUR Chapter 9 224", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 238, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p239::c0", "text": "estimation (see Section 7.7). In other words, we do not need to analyze the structural equations at all in order to consistently estimate the reduced form parameters. Ordi- nary least squares estimates of the reduced form parameters are robust in the sense that they do not rely on any identiﬁcation assumptions imposed on the structural system. If the structural model is correctly speciﬁed and at least one equation is over- identiﬁed, we obtain asymptotically more e‰cient estimators of the reduced form parameters by deriving the estimates from the structural parameter estimates. In particular, given the structural parameter estimates ^D and ^G, we can obtain the re- duced form estimates as ^P ¼ \u0002^D^G\u00021 [see equation (9.14)]. These are consistent, ﬃﬃﬃﬃ N p - asymptotically normal estimators (although the asymptotic variance matrix is some- what complicated). From Problem 3.9, we obtain the most e‰cient estimator of P by using the most e‰cient estimators of D and G (minimum chi-square or, under system homoskedasticity, 3SLS). Just as in estimating the structural parameters, there is a robustness-e‰ciency trade-o¤ in estimating the pgj. As mentioned earlier, the OLS estimators of each reduced form are robust to misspeciﬁcation of any restrictions on the structural equations (although, as always, each element of z should be exogenous for OLS to be consistent). The estimators of the pgj derived from estimators of D and G—whether the latter are 2SLS or system estimators—are generally nonrobust to incorrect restrictions on the structural system. See Problem 9.11 for a simple illustration. 9.4 Additional Topics in Linear SEMs 9.4.1 Using Cross Equation Restrictions to Achieve Identiﬁcation So far we have discussed identiﬁcation of a single equation using only within-equation parameter restrictions [see assumption (9.17)]. This is by far the leading case, espe- cially when the system represents a simultaneous equations model with truly auton- omous equations. Nevertheless, occasionally economic theory implies parameter restrictions across di¤erent equations in a system that contains endogenous variables. Not surprisingly, such cross equation restrictions are generally useful for identifying equations. A general treatment is beyond the scope of our analysis. Here we just give an example to show how identiﬁcation and estimation work. Consider the two-equation system y1 ¼ g12y2 þ d11z1 þ d12z2 þ d13z3 þ u1 ð9:31Þ y2 ¼ g21y1 þ d21z1 þ d22z2 þ u2 ð9:32Þ Simultaneous Equations Models 225", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 239, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p240::c0", "text": "where each zj is uncorrelated with u1 and u2 (z1 can be unity to allow for an inter- cept). Without further information, equation (9.31) is unidentiﬁed, and equation (9.32) is just identiﬁed if and only if d13 0 0. We maintain these assumptions in what follows. Now suppose that d12 ¼ d22. Because d22 is identiﬁed in equation (9.32) we can treat it as known for studying identiﬁcation of equation (9.31). But d12 ¼ d22, and so we can write y1 \u0002 d12z2 ¼ g12y2 þ d11z1 þ d13z3 þ u1 ð9:33Þ where y1 \u0002 d12z2 is e¤ectively known. Now the right-hand side of equation (9.33) has one endogenous variable, y2, and the two exogenous variables z1 and z3. Because z2 is excluded from the right-hand side, we can use z2 as an instrument for y2, as long as z2 appears in the reduced form for y2. This is the case provided d12 ¼ d22 0 0. This approach to showing that equation (9.31) is identiﬁed also suggests a consis- tent estimation procedure: ﬁrst, estimate equation (9.32) by 2SLS using ðz1; z2; z3Þ as instruments, and let ^d22 be the estimator of d22. Then, estimate y1 \u0002 ^d22z2 ¼ g12y2 þ d11z1 þ d13z3 þ error by 2SLS using ðz1; z2; z3Þ as instruments. Since ^d22 ! p d12 when d12 ¼ d22 0 0, this last step produces consistent estimators of g12, d11, and d13. Unfortunately, the usual 2SLS standard errors obtained from the ﬁnal estimation would not be valid because of the preliminary estimation of d22. It is easier to use a system procedure when cross equation restrictions are present because the asymptotic variance can be obtained directly. We can always rewrite the system in a linear form with the restrictions imposed. For this example, one way to do so is to write the system as y1 y2 \u0001 \u0002 ¼ y2 z1 z2 z3 0 0 0 0 z2 0 y1 z1 \u0001 \u0002 b þ u1 u2 \u0001 \u0002 ð9:34Þ where b ¼ ðg12; d11; d12; d13; g21; d21Þ0. The parameter d22 does not show up in b be- cause we have imposed the restriction d12 ¼ d22 by appropriate choice of the matrix of explanatory variables. The matrix of instruments is I2 n z, meaning that we just use all exogenous vari- ables as instruments in each equation. Since I2 n z has six columns, the order condi- tion is exactly satisﬁed (there are six elements of b), and we have already seen when the rank condition holds. The system can be consistently estimated using GMM or 3SLS. Chapter 9 226", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 240, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p241::c0", "text": "9.4.2 Using Covariance Restrictions to Achieve Identiﬁcation In most applications of linear SEMs, identiﬁcation is obtained by putting restrictions on the matrix of structural parameters B. Occasionally, we are willing to put restric- tions on the variance matrix S of the structural errors. Such restrictions, which are almost always zero covariance assumptions, can help identify the structural param- eters in some equations. For general treatments see Hausman (1983) and Hausman, Newey, and Taylor (1987). We give a couple of examples to show how identiﬁcation with covariance restrictions works. The ﬁrst example is the two-equation system y1 ¼ g12y2 þ d11z1 þ d13z3 þ u1 ð9:35Þ y2 ¼ g21y1 þ d21z1 þ d22z2 þ d23z3 þ u2 ð9:36Þ Equation (9.35) is just identiﬁed if d22 0 0, which we assume, while equation (9.36) is unidentiﬁed without more information. Suppose that we have one piece of additional information in terms of a covariance restriction: Covðu1; u2Þ ¼ Eðu1u2Þ ¼ 0 ð9:37Þ In other words, if S is the 2 \u0001 2 structural variance matrix, we are assuming that S is diagonal. Assumption (9.37), along with d22 0 0, is enough to identify equation (9.36). Here is a simple way to see how assumption (9.37) identiﬁes equation (9.36). First, because g12, d11, and d13 are identiﬁed, we can treat them as known when studying identiﬁcation of equation (9.36). But if the parameters in equation (9.35) are known, u1 is e¤ectively known. By assumption (9.37), u1 is uncorrelated with u2, and u1 is certainly partially correlated with y1. Thus, we e¤ectively have ðz1; z2; z3; u1Þ as in- struments available for estimating equation (9.36), and this result shows that equa- tion (9.36) is identiﬁed. We can use this method for verifying identiﬁcation to obtain consistent estimators. First, estimate equation (9.35) by 2SLS using instruments ðz1; z2; z3Þ and save the 2SLS residuals, ^u1. Then estimate equation (9.36) by 2SLS using instruments ðz1; z2; z3; ^u1Þ. The fact that ^u1 depends on estimates from a prior stage does not a¤ect consistency. But inference is complicated because of the estimation of u1: condition (6.8) does not hold because u1 depends on y2, which is correlated with u2. The most e‰cient way to use covariance restrictions is to write the entire set of orthogonality conditions as E½z0u1ðb1Þ\u0003 ¼ 0, E½z0u2ðb2Þ\u0003 ¼ 0, and E½u1ðb1Þu2ðb2Þ\u0003 ¼ 0 ð9:38Þ Simultaneous Equations Models 227", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 241, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p242::c0", "text": "where the notation u1ðb1Þ emphasizes that the errors are functions of the structural parameters b1—with normalization and exclusion restrictions imposed—and simi- larly for u2ðb2Þ. For example, from equation (9.35), u1ðb1Þ ¼ y1 \u0002 g12 y2 \u0002 d11z1 \u0002 d13z3. Equation (9.38), because it is nonlinear in b1 and b2, takes us outside the realm of linear moment restrictions. In Chapter 14 we will use nonlinear moment con- ditions in GMM estimation. A general example with covariance restrictions is a fully recursive system. First, a recursive system can be written as y1 ¼ zd1 þ u1 y2 ¼ g21y1 þ zd2 þ u2 y3 ¼ g31y1 þ g32y2 þ zd3 þ u3 .. . yG ¼ gG1y1 þ \u0005 \u0005 \u0005 þ gG;G\u00021yG\u00021 þ zdG þ uG ð9:39Þ so that in each equation only endogenous variables from previous equations appear on the right-hand side. We have allowed all exogenous variables to appear in each equation, and we maintain assumption (9.2). The ﬁrst equation in the system (9.39) is clearly identiﬁed and can be estimated by OLS. Without further exclusion restrictions none of the remaining equations is iden- tiﬁed, but each is identiﬁed if we assume that the structural errors are pairwise uncorrelated: Covðug; uhÞ ¼ 0; g 0 h ð9:40Þ This assumption means that S is a G \u0001 G diagonal matrix. Equations (9.39) and (9.40) deﬁne a fully recursive system. Under these assumptions, the right-hand-side variables in equation g are each uncorrelated with ug; this fact is easily seen by starting with the ﬁrst equation and noting that y1 is a linear function of z and u1. Then, in the second equation, y1 is uncorrelated with u2 under assumption (9.40). But y2 is a linear function of z, u1, and u2, and so y2 and y1 are both uncorrelated with u3 in the third equation. And so on. It follows that each equation in the system is con- sistently estimated by ordinary least squares. It turns out that OLS equation by equation is not necessarily the most e‰cient estimator in fully recursive systems, even though S is a diagonal matrix. Generally, e‰ciency can be improved by adding the zero covariance restrictions to the ortho- gonality conditions, as in equation (9.38), and applying nonlinear GMM estimation. See Lahiri and Schmidt (1978) and Hausman, Newey, and Taylor (1987). Chapter 9 228", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 242, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p243::c0", "text": "9.4.3 Subtleties Concerning Identiﬁcation and E‰ciency in Linear Systems So far we have discussed identiﬁcation and estimation under the assumption that each exogenous variable appearing in the system, zj, is uncorrelated with each struc- tural error, ug. It is important to assume only zero correlation in the general treat- ment because we often add a reduced form equation for an endogenous variable to a structural system, and zero correlation is all we should impose in linear reduced forms. For entirely structural systems, it is often natural to assume that the structural errors satisfy the zero conditional mean assumption Eðug j zÞ ¼ 0; g ¼ 1; 2; . . . ; G ð9:41Þ In addition to giving the parameters in the structural equations the appropriate par- tial e¤ect interpretations, assumption (9.41) has some interesting statistical impli- cations: any function of z is uncorrelated with each error ug. Therefore, in the labor supply example (9.28), age2, logðageÞ, educ\u0005exper, and so on (there are too many functions to list) are all uncorrelated with u1 and u2. Realizing this fact, we might ask, Why not use nonlinear functions of z as additional instruments in estimation? We need to break the answer to this question into two parts. The ﬁrst concerns identiﬁcation, and the second concerns e‰ciency. For identiﬁcation, the bottom line is this: adding nonlinear functions of z to the instrument list cannot help with identi- ﬁcation in linear systems. You were asked to show this generally in Problem 8.4, but the main points can be illustrated with a simple model: y1 ¼ g12y2 þ d11z1 þ d12z2 þ u1 ð9:42Þ y2 ¼ g21y1 þ d21z1 þ u2 ð9:43Þ Eðu1j zÞ ¼ Eðu2 j zÞ ¼ 0 ð9:44Þ From the order condition in Section 9.2.2, equation (9.42) is not identiﬁed, and equation (9.43) is identiﬁed if and only if d12 0 0. Knowing properties of conditional expectations, we might try something clever to identify equation (9.42): since, say, z2 1 is uncorrelated with u1 under assumption (9.41), and z2 1 would appear to be corre- lated with y2, we can use it as an instrument for y2 in equation (9.42). Under this reasoning, we would have enough instruments—z1; z2; z2 1—to identify equation (9.42). In fact, any number of functions of z1 and z2 can be added to the instrument list. The fact that this argument is faulty is fortunate because our identiﬁcation analysis in Section 9.2.2 says that equation (9.42) is not identiﬁed. In this example it is clear that z2 1 cannot appear in the reduced form for y2 because z2 1 appears nowhere in the Simultaneous Equations Models 229", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 243, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p244::c0", "text": "system. Technically, because Eðy2 j zÞ is linear in z1 and z2 under assumption (9.44), the linear projection of y2 onto ðz1; z2; z2 1Þ does not depend on z2 1: Lðy2 j z1; z2; z2 1Þ ¼ Lðy2 j z1; z2Þ ¼ p21z1 þ p22z2 ð9:45Þ In other words, there is no partial correlation between y2 and z2 1 once z1 and z2 are included in the projection. The zero conditional mean assumptions (9.41) can have some relevance for choosing an e‰cient estimator, although not always. If assumption (9.41) holds and Varðu j zÞ ¼ VarðuÞ ¼ S, 3SLS using instruments z for each equation is the asymp- totically e‰cient estimator that uses the orthogonality conditions in assumption (9.41); this conclusion follows from Theorem 8.5. In other words, if Varðu j zÞ is constant, it does not help to expand the instrument list beyond the functions of the exogenous variables actually appearing in the system. However, if assumption (9.41) holds but Varðu j zÞ is not constant, we can do better (asymptotically) than 3SLS. If hðzÞ is some additional functions of the exogenous variables, the minimum chi-square estimator using ½z; hðzÞ\u0003 as instruments in each equation is, generally, more e‰cient than 3SLS or minimum chi-square using only z as IVs. This result was discovered independently by Hansen (1982) and White (1982b), and it follows from the discussion in Section 8.6. Expanding the IV list to arbitrary functions of z and applying full GMM is not used very much in practice: it is usually not clear how to choose hðzÞ, and, if we use too many additional instru- ments, the ﬁnite sample properties of the GMM estimator can be poor, as we dis- cussed in Section 8.6. For SEMs linear in the parameters but nonlinear in endogenous variables (in a sense to be made precise), adding nonlinear functions of the exogenous variables to the instruments not only is desirable, but is often needed to achieve identiﬁcation. We turn to this topic next. 9.5 SEMs Nonlinear in Endogenous Variables We now study models that are nonlinear in some endogenous variables. While the general estimation methods we have covered are still applicable, identiﬁcation and choice of instruments require special attention. 9.5.1 Identiﬁcation The issues that arise in identifying models nonlinear in endogenous variables are most easily illustrated with a simple example. Suppose that supply and demand are Chapter 9 230", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 244, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p245::c0", "text": "given by logðqÞ ¼ g12 logðpÞ þ g13½logðpÞ\u00032 þ d11z1 þ u1 ð9:46Þ logðqÞ ¼ g22 logðpÞ þ d22z2 þ u2 ð9:47Þ Eðu1 j zÞ ¼ Eðu2 j zÞ ¼ 0 ð9:48Þ where the ﬁrst equation is the supply equation, the second equation is the demand equation, and the equilibrium condition that supply equals demand has been imposed. For simplicity, we do not include an intercept in either equation, but no important conclusions hinge on this omission. The exogenous variable z1 shifts the supply function but not the demand function; z2 shifts the demand function but not the supply function. The vector of exogenous variables appearing somewhere in the sys- tem is z ¼ ðz1; z2Þ. It is important to understand why equations (9.46) and (9.47) constitute a ‘‘non- linear’’ system. This system is still linear in parameters, which is important because it means that the IV procedures we have learned up to this point are still applicable. Further, it is not the presence of the logarithmic transformations of q and p that makes the system nonlinear. In fact, if we set g13 ¼ 0, then the model is linear for the purposes of identiﬁcation and estimation: deﬁning y1 1 logðqÞ and y2 1 logðpÞ, we can write equations (9.46) and (9.47) as a standard two-equation system. When we include ½logðpÞ\u00032 we have the model y1 ¼ g12y2 þ g13y2 2 þ d11z1 þ u1 ð9:49Þ y1 ¼ g22y2 þ d22z2 þ u2 ð9:50Þ With this system there is no way to deﬁne two endogenous variables such that the system is a two-equation system in two endogenous variables. The presence of y2 2 in equation (9.49) makes this model di¤erent from those we have studied up until now. We say that this is a system nonlinear in endogenous variables. What this statement really means is that, while the system is still linear in parameters, identiﬁcation needs to be treated di¤erently. If we used equations (9.49) and (9.50) to obtain y2 as a function of the z1; z2; u1; u2, and the parameters, the result would not be linear in z and u. In this particular case we can ﬁnd the solution for y2 using the quadratic formula (assuming a real solution exists). However, Eðy2 j zÞ would not be linear in z unless g13 ¼ 0, and Eðy2 2 j zÞ would not be linear in z regardless of the value of g13. These observations have important implications for identiﬁcation of equation (9.49) and for choosing instruments. Simultaneous Equations Models 231", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 245, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p246::c0", "text": "Before considering equations (9.49) and (9.50) further, consider a second example where closed form expressions for the endogenous variables in terms of the exoge- nous variables and structural errors do not even exist. Suppose that a system de- scribing crime rates in terms of law enforcement spending is crime ¼ g12 logðspendingÞ þ zð1Þdð1Þ þ u1 ð9:51Þ spending ¼ g21crime þ g22crime2 þ zð2Þdð2Þ þ u2 ð9:52Þ where the errors have zero mean given z. Here, we cannot solve for either crime or spending (or any other transformation of them) in terms of z, u1, u2, and the parameters. And there is no way to deﬁne y1 and y2 to yield a linear SEM in two endogenous variables. The model is still linear in parameters, but Eðcrime j zÞ, E½logðspendingÞ j z\u0003, and Eðspending j zÞ are not linear in z (nor can we ﬁnd closed forms for these expectations). One possible approach to identiﬁcation in nonlinear SEMs is to ignore the fact that the same endogenous variables show up di¤erently in di¤erent equations. In the supply and demand example, deﬁne y3 1 y2 2 and rewrite equation (9.49) as y1 ¼ g12 y2 þ g13y3 þ d11z1 þ u1 ð9:53Þ Or, in equations (9.51) and (9.52) deﬁne y1 ¼ crime, y2 ¼ spending, y3 ¼ logðspendingÞ, and y4 ¼ crime2, and write y1 ¼ g12 y3 þ zð1Þdð1Þ þ u1 ð9:54Þ y2 ¼ g21y1 þ g22y4 þ zð2Þdð2Þ þ u2 ð9:55Þ Deﬁning nonlinear functions of endogenous variables as new endogenous variables turns out to work fairly generally, provided we apply the rank and order conditions properly. The key question is, What kinds of equations do we add to the system for the newly deﬁned endogenous variables? If we add linear projections of the newly deﬁned endogenous variables in terms of the original exogenous variables appearing somewhere in the system—that is, the linear projection onto z—then we are being much too restrictive. For example, sup- pose to equations (9.53) and (9.50) we add the linear equation y3 ¼ p31z1 þ p32z2 þ v3 ð9:56Þ where, by deﬁnition, Eðz1v3Þ ¼ Eðz2v3Þ ¼ 0. With equation (9.56) to round out the system, the order condition for identiﬁcation of equation (9.53) clearly fails: we have two endogenous variables in equation (9.53) but only one excluded exogenous vari- able, z2. Chapter 9 232", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 246, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p247::c0", "text": "The conclusion that equation (9.53) is not identiﬁed is too pessimistic. There are many other possible instruments available for y2 2. Because Eðy2 2 j zÞ is not linear in z1 and z2 (even if g13 ¼ 0), other functions of z1 and z2 will appear in a linear projection involving y2 2 as the dependent variable. To see what the most useful of these are likely to be, suppose that the structural system actually is linear, so that g13 ¼ 0. Then y2 ¼ p21z1 þ p22z2 þ v2, where v2 is a linear combination of u1 and u2. Squaring this reduced form and using Eðv2 j zÞ ¼ 0 gives Eðy2 2 j zÞ ¼ p2 21z2 1 þ p2 22z2 2 þ 2p21p22z1z2 þ Eðv2 2 j zÞ ð9:57Þ If Eðv2 2 j zÞ is constant, an assumption that holds under homoskedasticity of the structural errors, then equation (9.57) shows that y2 2 is correlated with z2 1, z2 2, and z1z2, which makes these functions natural instruments for y2 2. The only case where no functions of z are correlated with y2 2 occurs when both p21 and p22 equal zero, in which case the linear version of equation (9.49) (with g13 ¼ 0) is also unidentiﬁed. Because we derived equation (9.57) under the restrictive assumptions g13 ¼ 0 and homoskedasticity of v2, we would not want our linear projection for y2 2 to omit the exogenous variables that originally appear in the system. In practice, we would aug- ment equations (9.53) and (9.50) with the linear projection y3 ¼ p31z1 þ p32z2 þ p33z2 1 þ p34z2 2 þ p35z1z2 þ v3 ð9:58Þ where v3 is, by deﬁnition, uncorrelated with z1, z2, z2 1, z2 2, and z1z2. The system (9.53), (9.50), and (9.58) can now be studied using the usual rank condition. Adding equation (9.58) to the original system and then studying the rank condition of the ﬁrst two equations is equivalent to studying the rank condition in the smaller system (9.53) and (9.50). What we mean by this statement is that we do not explicitly add an equation for y3 ¼ y2 2, but we do include y3 in equation (9.53). Therefore, when applying the rank condition to equation (9.53), we use G ¼ 2 (not G ¼ 3). The reason this approach is the same as studying the rank condition in the three-equation system (9.53), (9.50), and (9.58) is that adding the third equation increases the rank of R1B by one whenever at least one additional nonlinear function of z appears in equation (9.58). (The functions z2 1, z2 2, and z1z2 appear nowhere else in the system.) As a general approach to identiﬁcation in models where the nonlinear functions of the endogenous variables depend only on a single endogenous variable—such as the two examples that we have already covered—Fisher (1965) argues that the following method is su‰cient for identiﬁcation: 1. Relabel the nonredundant functions of the endogenous variables to be new endogenous variables, as in equation (9.53) or (9.54) and equation (9.55). Simultaneous Equations Models 233", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 247, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p248::c0", "text": "2. Apply the rank condition to the original system without increasing the number of equations. If the equation of interest satisﬁes the rank condition, then it is identiﬁed. The proof that this method works is complicated, and it requires more assumptions than we have made (such as u being independent of z). Intuitively, we can expect each additional nonlinear function of the endogenous variables to have a linear projection that depends on new functions of the exogenous variables. Each time we add another function of an endogenous variable, it e¤ectively comes with its own instruments. Fisher’s method can be expected to work in all but the most pathological cases. One case where it does not work is if Eðv2 2 j zÞ in equation (9.57) is heteroskedastic in such a way as to cancel out the squares and cross product terms in z1 and z2; then Eðy2 2 j zÞ would be constant. Such unfortunate coincidences are not practically important. It is tempting to think that Fisher’s rank condition is also necessary for identiﬁca- tion, but this is not the case. To see why, consider the two-equation system y1 ¼ g12 y2 þ g13y2 2 þ d11z1 þ d12z2 þ u1 ð9:59Þ y2 ¼ g21y1 þ d21z1 þ u2 ð9:60Þ The ﬁrst equation cleary fails the modiﬁed rank condition because it fails the order condition: there are no restrictions on the ﬁrst equation except the normalization re- striction. However, if g13 0 0 and g21 0 0, then Eðy2 j zÞ is a nonlinear function of z (which we cannot obtain in closed form). The result is that functions such as z2 1, z2 2, and z1z2 (and others) will appear in the linear projections of y2 and y2 2 even after z1 and z2 have been included, and these can then be used as instruments for y2 and y2 2. But if g13 ¼ 0, the ﬁrst equation cannot be identiﬁed by adding nonlinear functions of z1 and z2 to the instrument list: the linear projection of y2 on z1, z2, and any function of ðz1; z2Þ will only depend on z1 and z2. Equation (9.59) is an example of a poorly identiﬁed model because, when it is identiﬁed, it is identiﬁed due to a nonlinearity (g13 0 0 in this case). Such identiﬁcation is especially tenuous because the hypothesis H0: g13 ¼ 0 cannot be tested by estimating the structural equation (since the structural equation is not identiﬁed when H0 holds). There are other models where identiﬁcation can be veriﬁed using reasoning similar to that used in the labor supply example. Models with interactions between exoge- nous variables and endogenous variables can be shown to be identiﬁed when the model without the interactions is identiﬁed (see Example 6.2 and Problem 9.6). Models with interactions among endogenous variables are also fairly easy to handle. Generally, it is good practice to check whether the most general linear version of the model would be identiﬁed. If it is, then the nonlinear version of the model is probably Chapter 9 234", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 248, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p249::c0", "text": "identiﬁed. We saw this result in equation (9.46): if this equation is identiﬁed when g13 ¼ 0, then it is identiﬁed for any value of g13. If the most general linear version of a nonlinear model is not identiﬁed, we should be very wary about proceeding, since identiﬁcation hinges on the presence of nonlinearities that we usually will not be able to test. 9.5.2 Estimation In practice, it is di‰cult to know which additional functions we should add to the instrument list for nonlinear SEMs. Naturally, we must always include the exogenous variables appearing somewhere in the system instruments in every equation. After that, the choice is somewhat arbitrary, although the functional forms appearing in the structural equations can be helpful. A general approach is to always use some squares and cross products of the exog- enous variables appearing somewhere in the system. If something like exper2 appears in the system, additional terms such as exper3 and exper4 would be added to the in- strument list. Once we decide on a set of instruments, any equation in a nonlinear SEM can be estimated by 2SLS. Because each equation satisﬁes the assumptions of single-equation analysis, we can use everything we have learned up to now for inference and speciﬁ- cation testing for 2SLS. A system method can also be used, where linear projections for the functions of endogenous variables are explicitly added to the system. Then, all exogenous variables included in these linear projections can be used as the instru- ments for every equation. The minimum chi-square estimator is generally more ap- propriate than 3SLS because the homoskedasticity assumption will rarely be satisﬁed in the linear projections. It is important to apply the instrumental variables procedures directly to the structural equation or equations. In other words, we should directly use the formulas for 2SLS, 3SLS, or GMM. Trying to mimic 2SLS or 3SLS by substituting ﬁtted values for some of the endogenous variables inside the nonlinear functions is usually a mistake: neither the conditional expectation nor the linear projection operator passes through nonlinear functions, and so such attempts rarely produce consistent estimators in nonlinear systems. Example 9.6 (Nonlinear Labor Supply Function): We add ½logðwageÞ\u00032 to the labor supply function in Example 9.5: hours ¼ g12 logðwageÞ þ g13½logðwageÞ\u00032 þ d10 þ d11educ þ d12age þ d13kidslt6 þ d14kidsge6 þ d15nwifeinc þ u1 ð9:61Þ logðwageÞ ¼ d20 þ d21educ þ d22exper þ d23exper2 þ u2 ð9:62Þ Simultaneous Equations Models 235", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 249, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p250::c0", "text": "where we have dropped hours from the wage o¤er function because it was insig- niﬁcant in Example 9.5. The natural assumptions in this system are Eðu1j zÞ ¼ Eðu2 j zÞ ¼ 0, where z contains all variables other than hours and logðwageÞ. There are many possibilities as additional instruments for ½logðwageÞ\u00032. Here, we add three quadratic terms to the list—age2, educ2, and nwifeinc2—and we estimate equation (9.61) by 2SLS. We obtain ^g12 ¼ 1;873:62 (se ¼ 635:99) and ^g13 ¼ \u0002437:29 (se ¼ 350:08). The t statistic on ½logðwageÞ\u00032 is about \u00021:25, so we would be justiﬁed in dropping it from the labor supply function. Regressing the 2SLS residuals ^u1 on all variables used as instruments in the supply equation gives R-squared ¼ :0061, and so the N-R-squared statistic is 2.61. With a w2 3 distribution this gives p-value ¼ :456. Thus, we fail to reject the overidentifying restrictions. In the previous example we may be tempted to estimate the labor supply function using a two-step procedure that appears to mimic 2SLS: 1. Regress logðwageÞ on all exogenous variables appearing in the system and obtain the predicted values. For emphasis, call these ^y2. 2. Estimate the labor supply function from the OLS regression hours on 1, ^y2, ð^y2Þ2, educ; . . . ; nwifeinc. This two-step procedure is not the same as estimating equation (9.61) by 2SLS, and, except in special circumstances, it does not produce consistent estimators of the structural parameters. The regression in step 2 is an example of what is sometimes called a forbidden regression, a phrase that describes replacing a nonlinear function of an endogenous explanatory variable with the same nonlinear function of ﬁtted values from a ﬁrst-stage estimation. In plugging ﬁtted values into equation (9.61), our mis- take is in thinking that the linear projection of the square is the square of the linear projection. What the 2SLS estimator does in the ﬁrst stage is project each of y2 and y2 2 onto the original exogenous variables and the additional nonlinear functions of these that we have chosen. The ﬁtted values from the reduced form regression for y2 2, say ^y3, are not the same as the squared ﬁtted values from the reduced form regression for y2, ð^y2Þ2. This distinction is the di¤erence between a consistent estimator and an inconsistent estimator. If we apply the forbidden regression to equation (9.61), some of the estimates are very di¤erent from the 2SLS estimates. For example, the coe‰cient on educ, when equation (9.61) is properly estimated by 2SLS, is about \u000287:85 with a t statistic of \u00021:32. The forbidden regression gives a coe‰cient on educ of about \u0002176:68 with a t statistic of \u00025:36. Unfortunately, the t statistic from the forbidden regression is gen- erally invalid, even asymptotically. (The forbidden regression will produce consistent estimators in the special case g13 ¼ 0, if Eðu1j zÞ ¼ 0; see Problem 9.12.) Chapter 9 236", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 250, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p251::c0", "text": "Many more functions of the exogenous variables could be added to the instrument list in estimating the labor supply function. From Chapter 8, we know that e‰ciency of GMM never falls by adding more nonlinear functions of the exogenous variables to the instrument list (even under the homoskedasticity assumption). This statement is true whether we use a single-equation or system method. Unfortunately, the fact that we do no worse asymptotically by adding instruments is of limited practical help, since we do not want to use too many instruments for a given data set. In Example 9.6, rather than using a long list of additional nonlinear functions, we might use ð^y2Þ2 as a single IV for y2 2. (This method is not the same as the forbidden regression!) If it happens that g13 ¼ 0 and the structural errors are homoskedastic, this would be the optimal IV. (See Problem 9.12.) A general system linear in parameters can be written as y1 ¼ q1ðy; zÞb1 þ u1 .. . yG ¼ qGðy; zÞbG þ uG ð9:63Þ where Eðug j zÞ ¼ 0, g ¼ 1; 2; . . . ; G. Among other things this system allows for com- plicated interactions among endogenous and exogenous variables. We will not give a general analysis of such systems because identiﬁcation and choice of instruments are too abstract to be very useful. Either single-equation or system methods can be used for estimation. 9.6 Di¤erent Instruments for Di¤erent Equations There are general classes of SEMs where the same instruments cannot be used for every equation. We already encountered one such example, the fully recursive sys- tem. Another general class of models is SEMs where, in addition to simultaneous determination of some variables, some equations contain variables that are endoge- nous as a result of omitted variables or measurement error. As an example, reconsider the labor supply and wage o¤er equations (9.28) and (9.62), respectively. On the one hand, in the supply function it is not unreasonable to assume that variables other than logðwageÞ are uncorrelated with u1. On the other hand, ability is a variable omitted from the logðwageÞ equation, and so educ might be correlated with u2. This is an omitted variable, not a simultaneity, issue, but the statistical problem is the same: correlation between the error and an explanatory variable. Simultaneous Equations Models 237", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 251, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p252::c0", "text": "Equation (9.28) is still identiﬁed as it was before, because educ is exogenous in equation (9.28). What about equation (9.62)? It satisﬁes the order condition because we have excluded four exogenous variables from equation (9.62): age, kidslt6, kidsge6, and nwifeinc. How can we analyze the rank condition for this equation? We need to add to the system the linear projection of educ on all exogenous variables: educ ¼ d30 þ d31exper þ d32exper2 þ d33age þ d34kidslt6 þ d35kidsge6 þ d36nwifeinc þ u3 ð9:64Þ Provided the variables other than exper and exper2 are su‰ciently partially corre- lated with educ, the logðwageÞ equation is identiﬁed. However, the 2SLS estimators might be poorly behaved if the instruments are not very good. If possible, we would add other exogenous factors to equation (9.64) that are partially correlated with educ, such as mother’s and father’s education. In a system procedure, because we have assumed that educ is uncorrelated with u1, educ can, and should, be included in the list of instruments for estimating equation (9.28). This example shows that having di¤erent instruments for di¤erent equations changes nothing for single-equation analysis: we simply determine the valid list of instruments for the endogenous variables in the equation of interest and then estimate the equations separately by 2SLS. Instruments may be required to deal with simul- taneity, omitted variables, or measurement error, in any combination. Estimation is more complicated for system methods. First, if 3SLS is to be used, then the GMM 3SLS version must be used to produce consistent estimators of any equation; the more traditional 3SLS estimator discussed in Section 8.3.5 is generally valid only when all instruments are uncorrelated with all errors. When we have dif- ferent instruments for di¤erent equations, the instrument matrix has the form in equation (8.15). There is a more subtle issue that arises in system analysis with di¤erent instruments for di¤erent equations. While it is still popular to use 3SLS methods for such prob- lems, it turns out that the key assumption that makes 3SLS the e‰cient GMM esti- mator, Assumption SIV.5, is often violated. In such cases the GMM estimator with general weighting matrix enhances asymptotic e‰ciency and simpliﬁes inference. As a simple example, consider a two-equation system y1 ¼ d10 þ g12y2 þ d11z1 þ u1 ð9:65Þ y2 ¼ d20 þ g21y1 þ d22z2 þ d23z3 þ u2 ð9:66Þ where ðu1; u2Þ has mean zero and variance matrix S. Suppose that z1, z2, and z3 are uncorrelated with u2 but we can only assume that z1 and z3 are uncorrelated with u1. Chapter 9 238", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 252, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p253::c0", "text": "In other words, z2 is not exogenous in equation (9.65). Each equation is still identiﬁed by the order condition, and we just assume that the rank conditions also hold. The instruments for equation (9.65) are ð1; z1; z3Þ, and the instruments for equation (9.66) are ð1; z1; z2; z3Þ. Write these as z1 1 ð1; z1; z3Þ and z2 1 ð1; z1; z2; z3Þ. Assumption SIV.5 requires the following three conditions: Eðu2 1z0 1z1Þ ¼ s2 1Eðz0 1z1Þ ð9:67Þ Eðu2 2z0 2z2Þ ¼ s2 2Eðz0 2z2Þ ð9:68Þ Eðu1u2z0 1z2Þ ¼ s12Eðz0 1z2Þ ð9:69Þ The ﬁrst two conditions hold if Eðu1j z1Þ ¼ Eðu2 j z2Þ ¼ 0 and Varðu1j z1Þ ¼ s2 1, Varðu2 j z2Þ ¼ s2 2. These are standard zero conditional mean and homoskedasticity assumptions. The potential problem comes with condition (9.69). Since u1 is corre- lated with one of the elements in z2, we can hardly just assume condition (9.69). Generally, there is no conditioning argument that implies condition (9.69). One case where condition (9.69) holds is if Eðu2 j u1; z1; z2; z3Þ ¼ 0, which implies that u2 and u1 are uncorrelated. The left-hand side of condition (9.69) is also easily shown to equal zero. But 3SLS with s12 ¼ 0 imposed is just 2SLS equation by equation. If u1 and u2 are correlated, we should not expect condition (9.69) to hold, and therefore the gen- eral minimum chi-square estimator should be used for estimation and inference. Wooldridge (1996) provides a general discussion and contains other examples of cases in which Assumption SIV.5 can and cannot be expected to hold. Whenever a system contains linear projections for nonlinear functions of endogenous variables, we should expect Assumption SIV.5 to fail. Problems 9.1. Discuss whether each example satisﬁes the autonomy requirement for true simultaneous equations analysis. The speciﬁcation of y1 and y2 means that each is to be written as a function of the other in a two-equation system. a. For an employee, y1 ¼ hourly wage, y2 ¼ hourly fringe beneﬁts. b. At the city level, y1 ¼ per capita crime rate, y2 ¼ per capita law enforcement expenditures. c. For a ﬁrm operating in a developing country, y1 ¼ ﬁrm research and development expenditures, y2 ¼ ﬁrm foreign technology purchases. d. For an individual, y1 ¼ hourly wage, y2 ¼ alcohol consumption. Simultaneous Equations Models 239", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 253, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p254::c0", "text": "e. For a family, y1 ¼ annual housing expenditures, y2 ¼ annual savings. f. For a proﬁt maximizing ﬁrm, y1 ¼ price markup, y2 ¼ advertising expenditures. g. For a single-output ﬁrm, y1 ¼ quantity demanded of its good, y2 ¼ advertising expenditure. h. At the city level, y1 ¼ incidence of HIV, y2 ¼ per capita condom sales. 9.2. Write a two-equation system in the form y1 ¼ g1y2 þ zð1Þdð1Þ þ u1 y2 ¼ g2y1 þ zð2Þdð2Þ þ u2 a. Show that reduced forms exist if and only if g1g2 0 1. b. State in words the rank condition for identifying each equation. 9.3. The following model jointly determines monthly child support payments and monthly visitation rights for divorced couples with children: support ¼ d10 þ g12visits þ d11 finc þ d12 fremarr þ d13dist þ u1 visits ¼ d20 þ g21support þ d21mremarr þ d22dist þ u2: For expository purposes, assume that children live with their mothers, so that fathers pay child support. Thus, the ﬁrst equation is the father’s ‘‘reaction function’’: it describes the amount of child support paid for any given level of visitation rights and the other exogenous variables ﬁnc (father’s income), fremarr (binary indicator if father remarried), and dist (miles currently between the mother and father). Similarly, the second equation is the mother’s reaction function: it describes visitation rights for a given amount of child support; mremarr is a binary indicator for whether the mother is remarried. a. Discuss identiﬁcation of each equation. b. How would you estimate each equation using a single-equation method? c. How would you test for endogeneity of visits in the father’s reaction function? d. How many overidentiﬁcation restrictions are there in the mother’s reaction func- tion? Explain how to test the overidentifying restriction(s). 9.4. Consider the following three-equation structural model: y1 ¼ g12 y2 þ d11z1 þ d12z2 þ d13z3 þ u1 y1 ¼ g22 y2 þ g23y3 þ d21z1 þ u2 Chapter 9 240", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 254, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p255::c0", "text": "y3 ¼ d31z1 þ d32z2 þ d33z3 þ u3 where z1 1 1 (to allow an intercept), EðugÞ ¼ 0, all g, and each zj is uncorrelated with each ug. You might think of the ﬁrst two equations as demand and supply equations, where the supply equation depends on a possibly endogenous variable y3 (such as wage costs) that might be correlated with u2. For example, u2 might contain mana- gerial quality. a. Show that a well-deﬁned reduced form exists as long as g12 0 g22. b. Allowing for the structural errors to be arbitrarily correlated, determine which of these equations is identiﬁed. First consider the order condition, and then the rank condition. 9.5. The following three-equation structural model describes a population: y1 ¼ g12y2 þ g13y3 þ d11z1 þ d13z3 þ d14z4 þ u1 y2 ¼ g21y1 þ d21z1 þ u2 y3 ¼ d31z1 þ d32z2 þ d33z3 þ d34z4 þ u3 where you may set z1 ¼ 1 to allow an intercept. Make the usual assumptions that EðugÞ ¼ 0, g ¼ 1; 2; 3, and that each zj is uncorrelated with each ug. In addition to the exclusion restrictions that have already been imposed, assume that d13 þ d14 ¼ 1. a. Check the order and rank conditions for the ﬁrst equation. Determine necessary and su‰cient conditions for the rank condition to hold. b. Assuming that the ﬁrst equation is identiﬁed, propose a single-equation estimation method with all restrictions imposed. Be very precise. 9.6. The following two-equation model contains an interaction between an endog- enous and exogenous variable (see Example 6.2 for such a model in an omitted variable context): y1 ¼ d10 þ g12y2 þ g13y2z1 þ d11z1 þ d12z2 þ u1 y2 ¼ d20 þ g21 y1 þ d21z1 þ d23z3 þ u2 a. Initially, assume that g13 ¼ 0, so that the model is a linear SEM. Discuss identiﬁ- cation of each equation in this case. b. For any value of g13, ﬁnd the reduced form for y1 (assuming it exists) in terms of the zj, the ug, and the parameters. c. Assuming that Eðu1j zÞ ¼ Eðu2 j zÞ ¼ 0, ﬁnd Eðy1 j zÞ. Simultaneous Equations Models 241", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 255, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p256::c0", "text": "d. Argue that, under the conditions in part a, the model is identiﬁed regardless of the value of g13. e. Suggest a 2SLS procedure for estimating the ﬁrst equation. f. Deﬁne a matrix of instruments suitable for 3SLS estimation. g. Suppose that d23 ¼ 0, but we also known that g13 0 0. Can the parameters in the ﬁrst equation be consistently estimated? If so, how? Can H0: g13 ¼ 0 be tested? 9.7. Assume that wage and alcohol consumption are determined by the system wage ¼ g12alcohol þ g13educ þ zð1Þdð1Þ þ u1 alcohol ¼ g21wage þ g23educ þ zð2Þdð2Þ þ u2 educ ¼ zð3Þdð3Þ þ u3 The third equation is a reduced form for years of education. Elements in zð1Þ include a constant, experience, gender, marital status, and amount of job training. The vector zð2Þ contains a constant, experience, gender, marital status, and local prices (including taxes) on various alcoholic beverages. The vector zð3Þ can contain elements in zð1Þ and zð2Þ and, in addition, exogenous factors a¤ecting educa- tion; for concreteness, suppose one element of zð3Þ is distance to nearest college at age 16. Let z denote the vector containing all nonredundant elements of zð1Þ, zð2Þ, and zð3Þ. In addition to assuming that z is uncorrelated with each of u1, u2, and u3, assume that educ is uncorrelated with u2, but educ might be correlated with u1. a. When does the order condition hold for the ﬁrst equation? b. State carefully how you would estimate the ﬁrst equation using a single-equation method. c. For each observation i deﬁne the matrix of instruments for system estimation of all three equations. d. In a system procedure, how should you choose zð3Þ to make the analysis as robust as possible to factors appearing in the reduced form for educ? 9.8. a. Extend Problem 5.4b using CARD.RAW to allow educ2 to appear in the logðwageÞ equation, without using nearc2 as an instrument. Speciﬁcally, use inter- actions of nearc4 with some or all of the other exogenous variables in the logðwageÞ equation as instruments for educ2. Compute a heteroskedasticity-robust test to be sure that at least one of these additional instruments appears in the linear projection of educ2 onto your entire list of instruments. Test whether educ2 needs to be in the logðwageÞ equation. Chapter 9 242", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 256, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p257::c0", "text": "b. Start again with the model estimated in Problem 5.4b, but suppose we add the interaction black\u0005educ. Explain why black\u0005zj is a potential IV for black\u0005educ, where zj is any exogenous variable in the system (including nearc4). c. In Example 6.2 we used black\u0005nearc4 as the IV for black\u0005educ. Now use 2SLS with black\u0005 ^ educ educ as the IV for black\u0005educ, where ^ educ educ are the ﬁtted values from the ﬁrst- stage regression of educ on all exogenous variables (including nearc4). What do you ﬁnd? d. If Eðeduc j zÞ is linear and Varðu1j zÞ ¼ s2 1, where z is the set of all exogenous variables and u1 is the error in the logðwageÞ equation, explain why the estimator using black\u0005e ^duc as the IV is asymptotically more e‰cient than the estimator using black\u0005nearc4 as the IV. 9.9. Use the data in MROZ.RAW for this question. a. Estimate equations (9.28) and (9.29) jointly by 3SLS, and compare the 3SLS esti- mates with the 2SLS estimates for equations (9.28) and (9.29). b. Now allow educ to be endogenous in equation (9.29), but assume it is exogenous in equation (9.28). Estimate a three-equation system using di¤erent instruments for di¤erent equations, where motheduc, fatheduc, and huseduc are assumed exogenous in equations (9.28) and (9.29). 9.10. Consider a two-equation system of the form y1 ¼ g1y2 þ z1d1 þ u1 y2 ¼ z2d2 þ u2 Assume that z1 contains at least one element not also in z2, and z2 contains at least one element not in z1. The second equation is also the reduced form for y2, but restrictions have been imposed to make it a structural equation. (For example, it could be a wage o¤er equation with exclusion restrictions imposed, whereas the ﬁrst equation is a labor supply function.) a. If we estimate the ﬁrst equation by 2SLS using all exogenous variables as IVs, are we imposing the exclusion restrictions in the second equation? (Hint: Does the ﬁrst- stage regression in 2SLS impose any restrictions on the reduced form?) b. Will the 3SLS estimates of the ﬁrst equation be the same as the 2SLS estimates? Explain. c. Explain why 2SLS is more robust than 3SLS for estimating the parameters of the ﬁrst equation. Simultaneous Equations Models 243", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 257, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p258::c0", "text": "9.11. Consider a two-equation SEM: y1 ¼ g12 y2 þ d11z1 þ u1 y2 ¼ g21y1 þ d22z2 þ d23z3 þ u2 Eðu1j z1; z2; z3Þ ¼ Eðu2 j z1; z2; z3Þ ¼ 0 where, for simplicity, we omit intercepts. The exogenous variable z1 is a policy vari- able, such as a tax rate. Assume that g12g21 0 1. The structural errors, u1 and u2, may be correlated. a. Under what assumptions is each equation identiﬁed? b. The reduced form for y1 can be written in conditional expectation form as Eðy1 j zÞ ¼ p11z1 þ p12z2 þ p13z3, where z ¼ ðz1; z2; z3Þ. Find the p11 in terms of the ggj and dgj. c. How would you estimate the structural parameters? How would you obtain ^p11 in terms of the structural parameter estimates? d. Suppose that z2 should be in the ﬁrst equation, but it is left out in the estimation from part c. What e¤ect does this omission have on estimating qEðy1 j zÞ=qz1? Does it matter whether you use single-equation or system estimators of the structural parameters? e. If you are only interested in qEðy1 j zÞ=qz1, what could you do instead of estimat- ing an SEM? f. Would you say estimating a simultaneous equations model is a robust method for estimating qEðy1 j zÞ=qz1? Explain. 9.12. The following is a two-equation, nonlinear SEM: y1 ¼ d10 þ g12y2 þ g13y2 2 þ z1d1 þ u1 y2 ¼ d20 þ g12y1 þ z2d2 þ u2 where u1 and u2 have zero means conditional on all exogenous variables, z. (For emphasis, we have included separate intercepts.) Assume that both equations are identiﬁed when g13 ¼ 0. a. When g13 ¼ 0, Eðy2 j zÞ ¼ p20 þ zp2. What is Eðy2 2 j zÞ under homoskedasticity assumptions for u1 and u2? b. Use part a to ﬁnd Eðy1 j zÞ when g13 ¼ 0. c. Use part b to argue that, when g13 ¼ 0, the forbidden regression consistently esti- mates the parameters in the ﬁrst equation, including g13 ¼ 0. Chapter 9 244", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 258, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p259::c0", "text": "d. If u1 and u2 have constant variances conditional on z, and g13 happens to be zero, show that the optimal instrumental variables for estimating the ﬁrst equation are f1; z; ½Eðy2 j zÞ\u00032g. (Hint: Use Theorem 8.5; for a similar problem, see Problem 8.11.) e. Reestimate equation (9.61) using IVs ½1; z; ð^y2Þ2\u0003, where z is all exogenous vari- ables appearing in equations (9.61) and (9.62) and ^y2 denotes the ﬁtted values from regressing logðwageÞ on 1, z. Discuss the results. 9.13. For this question use the data in OPENNESS.RAW, taken from Romer (1993). a. A simple simultaneous equations model to test whether ‘‘openness’’ (open) leads to lower inﬂation rates (inf ) is inf ¼ d10 þ g12open þ d11 logðpcincÞ þ u1 open ¼ d20 þ g21inf þ d21 logðpcincÞ þ d22 logðlandÞ þ u2 Assuming that pcinc (per capita income) and land (land area) are exogenous, under what assumption is the ﬁrst equation identiﬁed? b. Estimate the reduced form for open to verify that logðlandÞ is statistically signiﬁcant. c. Estimate the ﬁrst equation from part a by 2SLS. Compare the estimate of g12 with the OLS estimate. d. Add the term g13open2 to the ﬁrst equation, and propose a way to test whether it is statistically signiﬁcant. (Use only one more IV than you used in part c.) e. With g13open2 in the ﬁrst equation, use the following method to estimate d10, g12, g13, and d11: (1) Regress open on 1, logðpcincÞ and logðlandÞ, and obtain the ﬁtted values, o^pen. (2) Regress inf on 1, o^pen, ðo^penÞ2, and logðpcincÞ. Compare the results with those from part d. Which estimates do you prefer? Simultaneous Equations Models 245", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 259, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p260::c0", "text": "10 Basic Linear Unobserved E¤ects Panel Data Models In Chapter 7 we covered a class of linear panel data models where, at a minimum, the error in each time period was assumed to be uncorrelated with the explanatory vari- ables in the same time period. For certain panel data applications this assumption is too strong. In fact, a primary motivation for using panel data is to solve the omitted variables problem. In this chapter we study population models that explicitly contain a time-constant, unobserved e¤ect. The treatment in this chapter is ‘‘modern’’ in the sense that unob- served e¤ects are treated as random variables, drawn from the population along with the observed explained and explanatory variables, as opposed to parameters to be estimated. In this framework, the key issue is whether the unobserved e¤ect is un- correlated with the explanatory variables. 10.1 Motivation: The Omitted Variables Problem It is easy to see how panel data can be used, at least under certain assumptions, to obtain consistent estimators in the presence of omitted variables. Let y and x 1 ðx1; x2; . . . ; xKÞ be observable random variables, and let c be an unobservable ran- dom variable; the vector ðy; x1; x2; . . . ; xK; cÞ represents the population of interest. As is often the case in applied econometrics, we are interested in the partial e¤ects of the observable explanatory variables xj in the population regression function Eðy j x1; x2; . . . ; xK; cÞ ð10:1Þ In words, we would like to hold c constant when obtaining partial e¤ects of the ob- servable explanatory variables. We follow Chamberlain (1984) in using c to denote the unobserved variable. Much of the panel data literature uses a Greek letter, such as a or f, but we want to emphasize that the unobservable is a random variable, not a parameter to be estimated. (We discuss this point further in Section 10.2.1.) Assuming a linear model, with c entering additively along with the xj, we have Eðy j x; cÞ ¼ b0 þ xb þ c ð10:2Þ where interest lies in the K \u0001 1 vector b. On the one hand, if c is uncorrelated with each xj, then c is just another unobserved factor a¤ecting y that is not systematically related to the observable explanatory variables whose e¤ects are of interest. On the other hand, if Covðxj; cÞ 0 0 for some j, putting c into the error term can cause serious problems. Without additional information we cannot consistently estimate b, nor will we be able to determine whether there is a problem (except by introspection, or by concluding that the estimates of b are somehow ‘‘unreasonable’’).", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 260, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p261::c0", "text": "Under additional assumptions there are ways to address the problem Covðx; cÞ 0 0. We have covered at least three possibilities in the context of cross section anal- ysis: (1) we might be able to ﬁnd a suitable proxy variable for c, in which case we can estimate an equation by OLS where the proxy is plugged in for c; (2) we may be able to ﬁnd instruments for the elements of x that are correlated with c and use an in- strumental variables method, such as 2SLS; or (3) we may be able to ﬁnd indicators of c that can then be used in multiple indicator instrumental variables procedure. These solutions are covered in Chapters 4 and 5. If we have access to only a single cross section of observations, then the three remedies listed, or slight variants of them, largely exhaust the possibilities. However, if we can observe the same cross section units at di¤erent points in time—that is, if we can collect a panel data set—then other possibilties arise. For illustration, suppose we can observe y and x at two di¤erent time periods; call these yt, xt for t ¼ 1; 2. The population now represents two time periods on the same unit. Also, suppose that the omitted variable c is time constant. Then we are inter- ested in the population regression function Eðyt j xt; cÞ ¼ b0 þ xtb þ c; t ¼ 1; 2 ð10:3Þ where xtb ¼ b1xt1 þ \u0002 \u0002 \u0002 þ bKxtK and xtj indicates variable j at time t. Model (10.3) assumes that c has the same e¤ect on the mean response in each time period. Without loss of generality, we set the coe‰cient on c equal to one. (Because c is unobserved and virtually never has a natural unit of measurement, it would be meaningless to try to estimate its partial e¤ect.) The assumption that c is constant over time (and has a constant partial e¤ect over time) is crucial to the following analysis. An unobserved, time-constant variable is called an unobserved e¤ect in panel data analysis. When t represents di¤erent time periods for the same individual, the unobserved e¤ect is often interpreted as captur- ing features of an individual, such as cognitive ability, motivation, or early family upbringing, that are given and do not change over time. Similarly, if the unit of ob- servation is the ﬁrm, c contains unobserved ﬁrm characteristics—such as managerial quality or structure—that can be viewed as being (roughly) constant over the period in question. We cover several speciﬁc examples of unobserved e¤ects models in Sec- tion 10.2. To discuss the additional assumptions su‰cient to estimate b, it is useful to write model (10.3) in error form as yt ¼ b0 þ xtb þ c þ ut ð10:4Þ where, by deﬁnition, Chapter 10 248", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 261, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p262::c0", "text": "Eðut j xt; cÞ ¼ 0; t ¼ 1; 2 ð10:5Þ One implication of condition (10.5) is Eðx0 tutÞ ¼ 0; t ¼ 1; 2 ð10:6Þ If we were to assume Eðx0 tcÞ ¼ 0, we could apply pooled OLS, as we covered in Section 7.8. If c is correlated with any element of xt, then pooled OLS is biased and inconsistent. With two years of data we can di¤erence equation (10.4) across the two time periods to eliminate the time-constant unobservable, c. Deﬁne Dy ¼ y2 \u0003 y1, Dx ¼ x2 \u0003 x1, and Du ¼ u2 \u0003 u1. Then, di¤erencing equation (10.4) gives Dy ¼ Dxb þ Du ð10:7Þ which is just a standard linear model in the di¤erences of all variables (although the intercept has dropped out). Importantly, the parameter vector of interest, b, appears directly in equation (10.7), and its presence suggests estimating equation (10.7) by OLS. Given a panel data set with two time periods, equation (10.7) is just a standard cross section equation. Under what assumptions will the OLS estimator from equa- tion (10.7) be consistent? Because we assume a random sample from the population, we can apply the results in Chapter 4 directly to equation (10.7). The key conditions for OLS to consistently estimate b are the orthogonality condition (Assumption OLS.1) EðDx0DuÞ ¼ 0 ð10:8Þ and the rank condition (Assumption OLS.2) rank EðDx0DxÞ ¼ K ð10:9Þ Consider condition (10.8) ﬁrst. It is equivalent to E½ðx2 \u0003 x1Þ0ðu2 \u0003 u1Þ\u0004 ¼ 0 or, after simple algebra, Eðx0 2u2Þ þ Eðx0 1u1Þ \u0003 Eðx0 1u2Þ \u0003 Eðx0 2u1Þ ¼ 0 ð10:10Þ The ﬁrst two terms in equation (10.10) are zero by condition (10.6), which holds for t ¼ 1; 2. But condition (10.5) does not guarantee that x1 and u2 are uncorrelated or that x2 and u1 are uncorrelated. It might be reasonable to assume that condition (10.8) holds, but we must recognize that it does not follow from condition (10.5). Assuming that the error ut is uncorrelated with x1 and x2 for t ¼ 1; 2 is an example of a strict exogeneity assumption in unobserved components panel data models. We dis- cuss strict exogeneity assumptions generally in Section 10.2. For now, we emphasize Basic Linear Unobserved E¤ects Panel Data Models 249", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 262, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p263::c0", "text": "that assuming Covðxt; usÞ ¼ 0 for all t and s puts no restrictions on the correlation between xt and the unobserved e¤ect, c. The second assumption, condition (10.9), also deserves some attention now be- cause the elements of xt appearing in structural equation (10.3) have been di¤erenced across time. If xt contains a variable that is constant across time for every member of the population, then Dx contains an entry that is identically zero, and condition (10.9) fails. This outcome is not surprising: if c is allowed to be arbitrarily correlated with the elements of xt, the e¤ect of any variable that is constant across time cannot be distinguished from the e¤ect of c. Therefore, we can consistently estimate bj only if there is some variation in xtj over time. In the remainder of this chapter, we cover various ways of dealing with the pres- ence of unobserved e¤ects under di¤erent sets of assumptions. We assume we have repeated observations on a cross section of N individuals, families, ﬁrms, school dis- tricts, cities, or some other economic unit. As in Chapter 7, we assume in this chapter that we have the same time periods, denoted t ¼ 1; 2; . . . ; T, for each cross section observation. Such a data set is usually called a balanced panel because the same time periods are available for all cross section units. While the mechanics of the unbal- anced case are similar to the balanced case, a careful treatment of the unbalanced case requires a formal description of why the panel may be unbalanced, and the sample selection issues can be somewhat subtle. Therefore, we hold o¤ covering un- balanced panels until Chapter 17, where we discuss sample selection and attrition issues. We still focus on asymptotic properties of estimators, where the time dimension, T, is ﬁxed and the cross section dimension, N, grows without bound. With large-N asymptotics it is convenient to view the cross section observations as independent, identically distributed draws from the population. For any cross section observation i—denoting a single individual, ﬁrm, city, and so on—we denote the observable variables for all T time periods by fðyit; xitÞ: t ¼ 1; 2; . . . ; Tg. Because of the ﬁxed T assumption, the asymptotic analysis is valid for arbitrary time dependence and dis- tributional heterogeneity across t. When applying asymptotic analysis to panel data methods it is important to re- member that asymptotics are useful insofar as they provide a reasonable approxi- mation to the ﬁnite sample properties of estimators and statistics. For example, a priori it is di‰cult to know whether N ! y asymptotics works well with, say, N ¼ 50 states in the United States and T ¼ 8 years. But we can be pretty conﬁdent that N ! y asymptotics are more appropriate than T ! y asymptotics, even though N is practically ﬁxed while T can grow. With large geographical regions, the random sampling assumption in the cross section dimension is conceptually ﬂawed. Chapter 10 250", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 263, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p264::c0", "text": "Nevertheless, if N is su‰ciently large relative to T, and we can assume rough inde- pendence in the cross section, then our asymptotic analysis should provide suitable approximations. If T is of the same order as N—for example, N ¼ 60 countries and T ¼ 55 post– World War II years—an asymptotic analysis that makes explicit assumptions about the nature of the time series dependence is needed. (In special cases, the conclusions about consistent estimation and approximate normality of t statistics will be the same, but not generally.) This area is just beginning to receive careful attention. If T is much larger than N, say N ¼ 5 companies and T ¼ 40 years, the framework becomes multiple time series analysis: N can be held ﬁxed while T ! y. 10.2 Assumptions about the Unobserved E¤ects and Explanatory Variables Before analyzing panel data estimation methods in more detail, it is useful to gener- ally discuss the nature of the unobserved e¤ects and certain features of the observed explanatory variables. 10.2.1 Random or Fixed E¤ects? The basic unobserved e¤ects model (UEM) can be written, for a randomly drawn cross section observation i, as yit ¼ xitb þ ci þ uit; t ¼ 1; 2; . . . ; T ð10:11Þ where xit is 1 \u0001 K and can contain observable variables that change across t but not i, variables that change across i but not t, and variables that change across i and t. In addition to unobserved e¤ect, there are many other names given to ci in applications: unobserved component, latent variable, and unobserved heterogeneity are common. If i indexes individuals, then ci is sometimes called an individual e¤ect or individual het- erogeneity; analogous terms apply to families, ﬁrms, cities, and other cross-sectional units. The uit are called the idiosyncratic errors or idiosyncratic disturbances because these change across t as well as across i. Especially in methodological papers, but also in applications, one often sees a dis- cussion about whether ci will be treated as a random e¤ect or a ﬁxed e¤ect. Origi- nally, such discussions centered on whether ci is properly viewed as a random variable or as a parameter to be estimated. In the traditional approach to panel data models, ci is called a ‘‘random e¤ect’’ when it is treated as a random variable and a ‘‘ﬁxed e¤ect’’ when it is treated as a parameter to be estimated for each cross section ob- servation i. Our view is that discussions about whether the ci should be treated as Basic Linear Unobserved E¤ects Panel Data Models 251", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 264, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p265::c0", "text": "random variables or as parameters to be estimated are wrongheaded for micro- econometric panel data applications. With a large number of random draws from the cross section, it almost always makes sense to treat the unobserved e¤ects, ci, as random draws from the population, along with yit and xit. This approach is certainly appropriate from an omitted variables or neglected heterogeneity perspective. As our discussion in Section 10.1 suggests, the key issue involving ci is whether or not it is uncorrelated with the observed explanatory variables xit, t ¼ 1; 2; . . . ; T. Mundlak (1978) made this argument many years ago, and it still is persuasive. In modern econometric parlance, ‘‘random e¤ect’’ is synonymous with zero cor- relation between the observed explanatory variables and the unobserved e¤ect: Covðxit; ciÞ ¼ 0, t ¼ 1; 2; . . . ; T. [Actually, a stronger conditional mean independence assumption, Eðci j xi1; . . . ; xiTÞ ¼ EðciÞ, will be needed to fully justify statistical in- ference; more on this subject in Section 10.4.] In applied papers, when ci is referred to as, say, an ‘‘individual random e¤ect,’’ then ci is probably being assumed to be uncorrelated with the xit. In microeconometric applications, the term ‘‘ﬁxed e¤ect’’ does not usually mean that ci is being treated as nonrandom; rather, it means that one is allowing for arbi- trary correlation between the unobserved e¤ect ci and the observed explanatory vari- ables xit. So, if ci is called an ‘‘individual ﬁxed e¤ect’’ or a ‘‘ﬁrm ﬁxed e¤ect,’’ then, for practical purposes, this terminology means that ci is allowed to be correlated with xit. In this book, we avoid referring to ci as a random e¤ect or a ﬁxed e¤ect. Instead, we will refer to ci as unobserved e¤ect, unobserved heterogeneity, and so on. Never- theless, later we will label two di¤erent estimation methods random e¤ects estimation and ﬁxed e¤ects estimation. This terminology is so ingrained that it is pointless to try to change it now. 10.2.2 Strict Exogeneity Assumptions on the Explanatory Variables Traditional unobserved components panel data models take the xit as ﬁxed. We will never assume the xit are nonrandom because potential feedback from yit to xis for s > t needs to be addressed explicitly. In Chapter 7 we discussed strict exogeneity assumptions in panel data models that did not explicitly contain unobserved e¤ects. We now provide strict exogeneity assumptions for models with unobserved e¤ects. In Section 10.1 we stated the strict exogeneity assumption in terms of zero corre- lation. For inference and e‰ciency discussions, we need to state the strict exogeneity assumption in terms of conditional expectations, and this statement also gives the assumption a clear meaning. With an unobserved e¤ect, the most revealing form of the strict exogeneity assumption is Chapter 10 252", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 265, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p266::c0", "text": "Eðyit j xi1; xi2; . . . ; xiT; ciÞ ¼ Eðyit j xit; ciÞ ¼ xitb þ ci ð10:12Þ for t ¼ 1; 2; . . . ; T. The second equality is the functional form assumption on Eðyit j xit; ciÞ. It is the ﬁrst equality that gives the strict exogeneity its interpretation. It means that, once xit and ci are controlled for, xis has no partial e¤ect on yit for s 0 t. When assumption (10.12) holds, we say that the fxit: t ¼ 1; 2; . . . ; Tg are strictly exogenous conditional on the unobserved e¤ect ci. Assumption (10.12) and the corre- sponding terminology were introduced and used by Chamberlain (1982). We will explicitly cover Chamberlain’s approach to estimating unobserved e¤ects models in the next chapter, but his manner of stating assumptions is instructive even for tradi- tional panel data analysis. Assumption (10.12) restricts how the expected value of yit can depend on explan- atory variables in other time periods, but it is more reasonable than strict exogeneity without conditioning on the unobserved e¤ect. Without conditioning on an unob- served e¤ect, the strict exogeneity assumption is Eðyit j xi1; xi2; . . . ; xiTÞ ¼ Eðyit j xitÞ ¼ xitb ð10:13Þ t ¼ 1; . . . ; T. To see that assumption (10.13) is less likely to hold than assumption (10.12), ﬁrst consider an example. Suppose that yit is output of soybeans for farm i during year t, and xit contains capital, labor, materials (such as fertilizer), rainfall, and other observable inputs. The unobserved e¤ect, ci, can capture average quality of land, managerial ability of the family running the farm, and other unobserved, time- constant factors. A natural assumption is that, once current inputs have been con- trolled for along with ci, inputs used in other years have no e¤ect on output during the current year. However, since the optimal choice of inputs in every year generally depends on ci, it is likely that some partial correlation between output in year t and inputs in other years will exist if ci is not controlled for: assumption (10.12) is rea- sonable while assumption (10.13) is not. More generally, it is easy to see that assumption (10.13) fails whenever assumption (10.12) holds and the expected value of ci depends on ðxi1; . . . ; xiTÞ. From the law of iterated expectations, if assumption (10.12) holds, then Eðyit j xi1; . . . ; xiTÞ ¼ xitb þ Eðci j xi1; . . . ; xiTÞ and so assumption (10.13) fails if Eðci j xi1; . . . ; xiTÞ 0 EðciÞ. In particular, assump- tion (10.13) fails if ci is correlated with any of the xit. Given equation (10.11), the strict exogeneity assumption can be stated in terms of the idiosyncratic errors as Eðuit j xi1; . . . ; xiT; ciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð10:14Þ Basic Linear Unobserved E¤ects Panel Data Models 253", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 266, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p267::c0", "text": "This assumption, in turn, implies that explanatory variables in each time period are uncorrelated with the idiosyncratic error in each time period: Eðx0 isuitÞ ¼ 0; s; t ¼ 1; . . . ; T ð10:15Þ This assumption is much stronger than assuming zero contemporaneous correlation: Eðx0 ituitÞ ¼ 0, t ¼ 1; . . . ; T. Nevertheless, assumption (10.15) does allow arbitary cor- relation between ci and xit for all t, something we ruled out in Section 7.8. Later, we will use the fact that assumption (10.14) implies that uit and ci are uncorrelated. For examining consistency of panel data estimators, the zero correlation assump- tion (10.15) generally su‰ces. Further, assumption (10.15) is often the easiest way to think about whether strict exogeneity is likely to hold in a particular application. But standard forms of statistical inference, as well as the e‰ciency properties of standard estimators, rely on the stronger conditional mean formulation in assumption (10.14). Therefore, we focus on assumption (10.14). 10.2.3 Some Examples of Unobserved E¤ects Panel Data Models Our discussions in Sections 10.2.1 and 10.2.2 emphasize that in any panel data ap- plication we should initially focus on two questions: (1) Is the unobserved e¤ect, ci, uncorrelated with xit for all t? (2) Is the strict exogeneity assumption (conditional on ci) reasonable? The following examples illustrate how we might organize our thinking on these two questions. Example 10.1 (Program Evaluation): A standard model for estimating the e¤ects of job training or other programs on subsequent wages is logðwageitÞ ¼ yt þ zitg þ d1progit þ ci þ uit ð10:16Þ where i indexes individual and t indexes time period. The parameter yt denotes a time-varying intercept, and zit is a set of observable characteristics that a¤ect wage and may also be correlated with program participation. Evaluation data sets are often collected at two points in time. At t ¼ 1, no one has participated in the program, so that progi1 ¼ 0 for all i. Then, a subgroup is chosen to participate in the program (or the individuals choose to participate), and subsequent wages are observed for the control and treatment groups in t ¼ 2. Model (10.16) allows for any number of time periods and general patterns of program participation. The reason for including the individual e¤ect, ci, is the usual omitted ability story: if individuals choose whether or not to participate in the program, that choice could be correlated with ability. This possibility is often called the self-selection problem. Alternatively, administrators might assign people based on characteristics that the econometrician cannot observe. Chapter 10 254", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 267, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p268::c0", "text": "The other issue is the strict exogeneity assumption of the explanatory variables, particularly progit. Typically, we feel comfortable with assuming that uit is uncorre- lated with progit. But what about correlation between uit and, say, progi;tþ1? Future program participation could depend on uit if people choose to participate in the future based on shocks to their wage in the past, or if administrators choose people as participants at time t þ 1 who had a low uit. Such feedback might not be very im- portant, since ci is being allowed for, but it could be. See, for example, Bassi (1984) and Ham and Lalonde (1996). Another issue, which is more easily dealt with, is that the training program could have lasting e¤ects. If so, then we should include lags of progit in model (10.16). Or, the program itself might last more than one period, in which case progit can be replaced by a series of dummy variables for how long unit i at time t has been subject to the program. Example 10.2 (Distributed Lag Model): Hausman, Hall, and Griliches (1984) esti- mate nonlinear distributed lag models to study the relationship between patents awarded to a ﬁrm and current and past levels of R&D spending. A linear, ﬁve-lag version of their model is patentsit ¼ yt þ zitg þ d0RDit þ d1RDi;t\u00031 þ \u0002 \u0002 \u0002 þ d5RDi;t\u00035 þ ci þ uit ð10:17Þ where RDit is spending on R&D for ﬁrm i at time t and zit contains variables such as ﬁrm size (as measured by sales or employees). The variable ci is a ﬁrm heterogeneity term that may inﬂuence patentsit and that may be correlated with current, past, and future R&D expenditures. Interest lies in the pattern of the dj coe‰cients. As with the other examples, we must decide whether R&D spending is likely to be correlated with ci. In addition, if shocks to patents today (changes in uit) inﬂuence R&D spending at future dates, then strict exogeneity can fail, and the methods in this chapter will not apply. The next example presents a case where the strict exogeneity assumption is neces- sarily false, and the unobserved e¤ect and the explanatory variable must be correlated. Example 10.3 (Lagged Dependent Variable): A simple dynamic model of wage de- termination with unobserved heterogeneity is logðwageitÞ ¼ b1 logðwagei;t\u00031Þ þ ci þ uit; t ¼ 1; 2; . . . ; T ð10:18Þ Often, interest lies in how persistent wages are (as measured by the size of b1) after controlling for unobserved heterogeneity (individual productivity), ci. Letting yit ¼ logðwageitÞ, a standard assumption would be Eðuit j yi;t\u00031; . . . ; yi0; ciÞ ¼ 0 ð10:19Þ Basic Linear Unobserved E¤ects Panel Data Models 255", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 268, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p269::c0", "text": "which means that all of the dynamics are captured by the ﬁrst lag. Let xit ¼ yi;t\u00031. Then, under assumption (10.19), uit is uncorrelated with ðxit; xi;t\u00031; . . . ; xi1Þ, but uit cannot be uncorrelated with ðxi;tþ1; . . . ; xiTÞ, as xi;tþ1 ¼ yit. In fact, EðyituitÞ ¼ b1Eðyi;t\u00031uitÞ þ EðciuitÞ þ Eðu2 itÞ ¼ Eðu2 itÞ > 0 ð10:20Þ because Eðyi;t\u00031uitÞ ¼ 0 and EðciuitÞ ¼ 0 under assumption (10.19). Therefore, the strict exogeneity assumption never holds in unobserved e¤ects models with lagged dependent variables. In addition, yi;t\u00031 and ci are necessarily correlated (since at time t \u0003 1, yi;t\u00031 is the left-hand-side variable). Not only must strict exogeneity fail in this model, but the exogeneity assumption required for pooled OLS estimation of model (10.18) is also violated. We will study estimation of such models in Chapter 11. 10.3 Estimating Unobserved E¤ects Models by Pooled OLS Under certain assumptions, the pooled OLS estimator can be used to obtain a con- sistent estimator of b in model (10.11). Write the model as yit ¼ xitb þ vit; t ¼ 1; 2; . . . ; T ð10:21Þ where vit 1 ci þ uit, t ¼ 1; . . . ; T are the composite errors. For each t, vit is the sum of the unobserved e¤ect and an idiosyncratic error. From Section 7.8, we know that pooled OLS estimation of this equation is consistent if Eðx0 itvitÞ ¼ 0, t ¼ 1; 2; . . . ; T. Practically speaking, no correlation between xit and vit means that we are assuming Eðx0 ituitÞ ¼ 0 and Eðx0 itciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð10:22Þ Equation (10.22) is the restrictive assumption, since Eðx0 ituitÞ ¼ 0 holds if we have successfully modeled Eðyit j xit; ciÞ. In static and ﬁnite distributed lag models we are sometimes willing to make the assumption (10.22); in fact, we will do so in the next section on random e¤ects esti- mation. As seen in Example 10.3, models with lagged dependent variables in xit must violate assumption (10.22) because yi;t\u00031 and ci must be correlated. Even if assumption (10.22) holds, the composite errors will be serially correlated due to the presence of ci in each time period. Therefore, inference using pooled OLS requires the robust variance matrix estimator and robust test statistics from Chapter 7. Because vit depends on ci for all t, the correlation between vit and vis does not generally decrease as the distance jt \u0003 sj increases; in time-series parlance, the vit are Chapter 10 256", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 269, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p270::c0", "text": "not weakly dependent across time. (We show this fact explicitly in the next section when fuit: t ¼ 1; . . . ; Tg is homoskedastic and serially uncorrelated.) Therefore, it is important that we be able to do large-N and ﬁxed-T asymptotics when applying pooled OLS. As we discussed in Chapter 7, each ðyi; XiÞ has T rows and should be ordered chronologically, and the ðyi; XiÞ should be stacked from i ¼ 1; . . . ; N. The order of the cross section observations is, as usual, irrelevant. 10.4 Random E¤ects Methods 10.4.1 Estimation and Inference under the Basic Random E¤ects Assumptions As with pooled OLS, a random e¤ects analysis puts ci into the error term. In fact, random e¤ects analysis imposes more assumptions than those needed for pooled OLS: strict exogeneity in addition to orthogonality between ci and xit. Stating the assumption in terms of conditional means, we have assumption RE.1: (a) Eðuit j xi; ciÞ ¼ 0, t ¼ 1; . . . ; T. (b) Eðci j xiÞ ¼ EðciÞ ¼ 0 where xi 1 ðxi1; xi2; . . . ; xiTÞ: In Section 10.2 we discussed the meaning of the strict exogeneity Assumption RE.1a. Assumption RE.1b is how we will state the orthogonality between ci and each xit. For obtaining consistent results, we could relax RE.1b to assumption (10.22), but in practice this approach a¤ords little more generality, and we will use Assumption RE.1b later to derive the traditional asymptotic variance for the random e¤ects esti- mator. Assumption RE.1b is always implied by the assumption that the xit are ﬁxed and EðciÞ ¼ 0, or by the assumption that ci is independent of xi. The important part is Eðci j xiÞ ¼ EðciÞ; the assumption EðciÞ ¼ 0 is without loss of generality, provided an intercept is included in xit, as should almost always be the case. Why do we maintain Assumption RE.1 when it is more restrictive than needed for a pooled OLS analysis? The random e¤ects approach exploits the serial correlation in the composite error, vit ¼ ci þ uit, in a generalized least squares (GLS) framework. In order to ensure that feasible GLS is consistent, we need some form of strict exoge- neity between the explanatory variables and the composite error. Under Assumption RE.1 we can write yit ¼ xitb þ vit ð10:23Þ Basic Linear Unobserved E¤ects Panel Data Models 257", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 270, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p271::c0", "text": "Eðvit j xiÞ ¼ 0; t ¼ 1; 2; . . . ; T ð10:24Þ where vit ¼ ci þ uit ð10:25Þ Equation (10.24) shows that fxit: t ¼ 1; . . . ; Tg satisﬁes the strict exogeneity as- sumption SGLS.1 (see Chapter 7) in the model (10.23). Therefore, we can apply GLS methods that account for the particular error structure in equation (10.25). Write the model (10.23) for all T time periods as yi ¼ Xib þ vi ð10:26Þ and vi can be written as vi ¼ cijT þ ui, where jT is the T \u0001 1 vector of ones. Deﬁne the (unconditional) variance matrix of vi as W 1 Eðviv0 iÞ ð10:27Þ a T \u0001 T matrix that we assume to be positive deﬁnite. Remember, this matrix is necessarily the same for all i because of the random sampling assumption in the cross section. For consistency of GLS, we need the usual rank condition for GLS: assumption RE.2: rank EðX0 iW\u00031XiÞ ¼ K. Applying the results from Chapter 7, we know that GLS and feasible GLS are consistent under Assumptions RE.1 and RE.2. A general FGLS analysis, using an unrestricted variance estimator W, is consistent and ﬃﬃﬃﬃ N p -asymptotically normal as N ! y. But we would not be exploiting the unobserved e¤ects structure of vit. A standard random e¤ects analysis adds assumptions on the idiosyncratic errors that give W a special form. The ﬁrst assumption is that the idiosyncratic errors uit have a constant unconditional variance across t: Eðu2 itÞ ¼ s2 u; t ¼ 1; 2; . . . ; T ð10:28Þ The second assumption is that the idiosyncratic errors are serially uncorrelated: EðuituisÞ ¼ 0; all t 0 s ð10:29Þ Under these two assumptions, we can derive the variances and covariances of the elements of vi. Under Assumption RE.1a, EðciuitÞ ¼ 0, t ¼ 1; 2; . . . ; T, and so Eðv2 itÞ ¼ Eðc2 i Þ þ 2EðciuitÞ þ Eðu2 itÞ ¼ s2 c þ s2 u where s2 c ¼ Eðc2 i Þ. Also, for all t 0 s, Chapter 10 258", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 271, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p272::c0", "text": "EðvitvisÞ ¼ E½ðci þ uitÞðci þ uisÞ\u0004 ¼ Eðc2 i Þ ¼ s2 c Therefore, under assumptions RE.1, (10.28), and (10.29), W takes the special form W ¼ Eðviv0 iÞ ¼ s2 c þ s2 u s2 c \u0002 \u0002 \u0002 s2 c s2 c s2 c þ s2 u \u0002 \u0002 \u0002 .. . .. . .. . s2 c s2 c s2 c þ s2 u 0 B B B B B @ 1 C C C C C A ð10:30Þ Because jTj0 T is the T \u0001 T matrix with unity in every element, we can write the matrix (10.30) as W ¼ s2 uIT þ s2 c jTj0 T ð10:31Þ When W has the form (10.31), we say it has the random e¤ects structure. Rather than depending on TðT þ 1Þ=2 unrestricted variances and covariances, as would be the case in a general GLS analysis, W depends only on two parameters, s2 c and s2 u, regardless of the size of T. The correlation between the composite errors vit and vis does not depend on the di¤erence between t and s: Corrðvis; vitÞ ¼ s2 c =ðs2 c þ s2 uÞ b 0; s 0 t. This correlation is also the ratio of the variance of ci to the variance of the composite error, and it is useful as a measure of the relative importance of the unobserved e¤ect ci. Assumptions (10.28) and (10.29) are special to random e¤ects. For e‰ciency of feasible GLS, we assume that the variance matrix of vi conditional on xi is constant: Eðviv0 i j xiÞ ¼ Eðviv0 iÞ ð10:32Þ Assumptions (10.28), (10.29), and (10.32) are implied by our third random e¤ects assumption: assumption RE.3: (a) Eðuiu0 i j xi; ciÞ ¼ s2 uIT. (b) Eðc2 i j xiÞ ¼ s2 c . Under Assumption RE.3a, Eðu2 it j xi; ciÞ ¼ s2 u, t ¼ 1; . . . ; T, which implies assump- tion (10.28), and Eðuituis j xi; ciÞ ¼ 0, t 0 s, t; s ¼ 1; . . . ; T, which implies assumption (10.29) (both by the usual iterated expectations argument). But Assumption RE.3a is stronger because it assumes that the conditional variances are constant and the con- ditional covariances are zero. Along with Assumption RE.1b, Assumption RE.3b is the same as Varðci j xiÞ ¼ VarðciÞ, which is a homoskedasticity assumption on the unobserved e¤ect ci. Under Assumption RE.3, assumption (10.32) holds and W has the form (10.30). Basic Linear Unobserved E¤ects Panel Data Models 259", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 272, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p273::c0", "text": "To implement an FGLS procedure, deﬁne s2 v ¼ s2 c þ s2 u. For now, assume that we have consistent estimators of s2 u and s2 c . Then we can form ^W 1 ^s2 uIT þ ^s2 c jTj0 T ð10:33Þ a T \u0001 T matrix that we assume to be positive deﬁnite. In a panel data context, the FGLS estimator that uses the variance matrix (10.33) is what is known as the random e¤ects estimator: ^bRE ¼ X N i¼1 X0 i ^W\u00031Xi !\u00031 X N i¼1 X0 i ^W\u00031yi ! ð10:34Þ The random e¤ects estimator is clearly motivated by Assumption RE.3. Never- theless, ^bRE is consistent whether or not Assumption RE.3 holds. As long as As- sumption RE.1 and the appropriate rank condition hold, ^bRE ! p b as N ! y. The argument is almost the same as showing that consistency of the FGLS estimator does not rely on Eðviv0 i j XiÞ ¼ W. The only di¤erence is that, even if W does not have the special form in equation (10.31), ^W still has a well-deﬁned probability limit. The fact that it does not necessarily converge to Eðviv0 iÞ does not a¤ect the consistency of the random e¤ects procedure. (Technically, we need to replace W with plimð ^WÞ in stating Assumption RE.2.) Under Assumption RE.3 the random e¤ects estimator is e‰cient in the class of estimators consistent under Eðvi j xiÞ ¼ 0, including pooled OLS and a variety of weighted least squares estimators, because RE is asymptotically equivalent to GLS under Assumptions RE.1–RE.3. The usual feasible GLS variance matrix—see equation (7.51)—is valid under Assumptions RE.1–RE.3. The only di¤erence from the general analysis is that ^W is chosen as in expression (10.33). In order to implement the RE procedure, we need to obtain ^s2 c and ^s2 u. Actually, it is easiest to ﬁrst ﬁnd ^s2 v ¼ ^s2 c þ ^s2 u. Under Assumption RE.3a, s2 v ¼ T\u00031 PT t¼1 Eðv2 itÞ for all i; therefore, averaging v2 it across all i and t would give a consistent estimator of s2 v . But we need to estimate b to make this method operational. A convenient initial estimator of b is the pooled OLS estimator, denoted here by ^^b^b. Let ^^v^vit denote the pooled OLS residuals. A consistent estimator of s2 v is ^s2 v ¼ 1 ðNT \u0003 KÞ X N i¼1 X T t¼1 ^^v^v2 it ð10:35Þ which is the usual variance estimator from the OLS regression on the pooled data. The degrees-of-freedom correction in equation (10.35)—that is, the use of NT \u0003 K Chapter 10 260", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 273, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p274::c0", "text": "rather than NT—has no e¤ect asymptotically. Under Assumptions RE.1–RE.3, equation (10.35) is a consistent estimator of s2 v . To ﬁnd a consistent estimator of s2 c , recall that s2 c ¼ EðvitvisÞ, all t 0 s. Therefore, for each i, there are TðT \u0003 1Þ=2 nonredundant error products that can be used to estimate s2 c . If we sum all these combinations and take the expectation, we get, for each i, E X T\u00031 t¼1 X T s¼tþ1 vitvis ! ¼ X T\u00031 t¼1 X T s¼tþ1 EðvitvisÞ ¼ X T\u00031 t¼1 X T s¼tþ1 s2 c ¼ s2 c X T\u00031 t¼1 ðT \u0003 tÞ ¼ s2 c ððT \u0003 1Þ þ ðT \u0003 2Þ þ \u0002 \u0002 \u0002 þ 2 þ 1Þ ¼ s2 c TðT \u0003 1Þ=2 ð10:36Þ where we have used the fact that the sum of the ﬁrst T \u0003 1 positive integers is TðT \u0003 1Þ=2. As usual, a consistent estimator is obtained by replacing the expectation with an average (across i) and replacing vit with its pooled OLS residual. We also make a degrees-of-freedom adjustment as a small-sample correction: ^s2 c ¼ 1 ½NTðT \u0003 1Þ=2 \u0003 K\u0004 X N i¼1 X T\u00031 t¼1 X T s¼tþ1 ^^v^vit^^v^vis ð10:37Þ is a consistent estimator of s2 c under Assumptions RE.1–RE.3. Given ^s2 v and ^s2 c , we can form ^s2 u ¼ ^s2 v \u0003 ^s2 c . [The idiosyncratic error variance, s2 u, can also be estimated using the ﬁxed e¤ects method, which we discuss in Section 10.5. Also, there are other methods of estimating s2 c . A common estimator of s2 c is based on the between esti- mator of b, which we touch on in Section 10.5; see Hsiao (1986, Section 3.3) and Baltagi (1995, Section 2.3). Because the RE estimator is a feasible GLS estimator, all that we need are consistent estimators of s2 c and s2 u in order to obtain a ﬃﬃﬃﬃ N p -e‰cient estimator of b.] As a practical matter, equation (10.37) is not guaranteed to be positive, although it is in the vast majority of applications. A negative value for ^s2 c is indicative of nega- tive serial correlation in uit, probably a substantial amount, which means that As- sumption RE.3a is violated. Alternatively, some other assumption in the model can be false. We should make sure that time dummies are included in the model if they are signiﬁcant; omitting them can induce serial correlation in the implied uit. If ^s2 c is negative, unrestricted FGLS may be called for; see Section 10.4.3. Example 10.4 (RE Estimation of the E¤ects of Job Training Grants): We now use the data in JTRAIN1.RAW to estimate the e¤ect of job training grants on ﬁrm scrap rates, using a random e¤ects analysis. There are 54 ﬁrms that reported scrap rates for each of the years 1987, 1988, and 1989. Grants were not awarded in 1987. Some ﬁrms Basic Linear Unobserved E¤ects Panel Data Models 261", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 274, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p275::c0", "text": "received grants in 1988, others received grants in 1989, and a ﬁrm could not receive a grant twice. Since there are ﬁrms in 1989 that received a grant only in 1988, it is im- portant to allow the grant e¤ect to persist one period. The estimated equation is logð^scrapÞ ¼ :415 ð:243Þ \u0003 :093 ð:109Þ d88 \u0003 :270 ð:132Þ d89 þ :548 ð:411Þ union \u0003 :215 ð:148Þ grant \u0003 :377 ð:205Þ grant\u00031 The lagged value of grant has the larger impact and is statistically signiﬁcant at the 5 percent level against a one-sided alternative. You are invited to estimate the equation without grant\u00031 to verify that the estimated grant e¤ect is much smaller (on the order of 6.7 percent) and statistically insigniﬁcant. Multiple hypotheses tests are carried out as in any FGLS analysis; see Section 7.6, where G ¼ T. In computing an F-type statistic based on weighted sums of squared residuals, ^W in expression (10.33) should be based on the pooled OLS residuals from the unrestricted model. Then, obtain the residuals from the unrestricted random e¤ects estimation as ^vi 1 yi \u0003 Xi ^bRE. Let ~bRE denote the random e¤ects estimator with the Q linear restrictions imposed, and deﬁne the restricted random e¤ects resid- uals as ~vi 1 yi \u0003 Xi ~bRE. Insert these into equation (7.52) in place of ^ui and ~ui for a chi-square statistic or into equation (7.53) for an F-type statistic. In Example 10.4, the Wald test for joint signiﬁcance of grant and grant\u00031 (against a two-sided alternative) yields a w2 2 statistic equal to 3.66, with p-value ¼ :16. (This test comes from Stata9.) 10.4.2 Robust Variance Matrix Estimator Because failure of Assumption RE.3 does not cause inconsistency in the RE esti- mator, it is very useful to be able to conduct statistical inference without this as- sumption. Assumption RE.3 can fail for two reasons. First, Eðviv0 i j xiÞ may not be constant, so that Eðviv0 i j xiÞ 0 Eðviv0 iÞ. This outcome is always a possibility with GLS analysis. Second, Eðviv0 iÞ may not have the random e¤ects structure: the idiosyncratic errors uit may have variances that change over time, or they could be serially corre- lated. In either case a robust variance matrix is available from the analysis in Chapter 7. We simply use equation (7.49) with ^ui replaced by ^vi ¼ yi \u0003 Xi ^bRE, i ¼ 1; 2; . . . ; N, the T \u0001 1 vectors of RE residuals. Robust standard errors are obtained in the usual way from the robust variance matrix estimator, and robust Wald statistics are obtained by the usual formula W ¼ Chapter 10 262", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 275, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p276::c0", "text": "ðR ^b \u0003 rÞ0ðR^VR0Þ\u00031ðR ^b \u0003 rÞ, where ^V is the robust variance matrix estimator. Re- member, if Assumption RE.3 is violated, the sum of squared residuals form of the F statistic is not valid. The idea behind using a robust variance matrix is the following. Assumptions RE.1–RE.3 lead to a well-known estimation technique whose properties are under- stood under these assumptions. But it is always a good idea to make the analysis robust whenever feasible. With ﬁxed T and large N asymptotics, we lose nothing in using the robust standard errors and test statistics even if Assumption RE.3 holds. In Section 10.7.2, we show how the RE estimator can be obtained from a particular pooled OLS regression, which makes obtaining robust standard errors and t and F statistics especially easy. 10.4.3 A General FGLS Analysis If the idiosyncratic errors fuit: t ¼ 1; 2; . . . ; Tg are generally heteroskedastic and serially correlated across t, a more general estimator of W can be used in FGLS: ^W ¼ N\u00031 X N i¼1 ^^v^vi^^v^v0 i ð10:38Þ where the ^^v^vi would be the pooled OLS residuals. The FGLS estimator is consistent under Assumptions RE.1 and RE.2, and, if we assume that Eðviv0 i j xiÞ ¼ W, then the FGLS estimator is asymptotically e‰cient and its asymptotic variance estimator takes the usual form. Using equation (10.38) is more general than the RE analysis. In fact, with large N asymptotics, the general FGLS estimator is just as e‰cient as the random e¤ects es- timator under Assumptions RE.1–RE.3. Using equation (10.38) is asymptotically more e‰cient if Eðviv0 i j xiÞ ¼ W, but W does not have the random e¤ects form. So why not always use FGLS with ^W given in equation (10.38)? There are historical reasons for using random e¤ects methods rather than a general FGLS analysis. The structure of W in the matrix (10.30) was once synonomous with unobserved e¤ects models: any correlation in the composite errors fvit: t ¼ 1; 2; . . . ; Tg was assumed to be caused by the presence of ci. The idiosyncratic errors, uit, were, by deﬁnition, taken to be serially uncorrelated and homoskedastic. If N is not several times larger than T, an unrestricted FGLS analysis can have poor ﬁnite sample properties because ^W has TðT þ 1Þ=2 estimated elements. Even though estimation of W does not a¤ect the asymptotic distribution of the FGLS estimator, it certainly a¤ects its ﬁnite sample properties. Random e¤ects estimation requires estimation of only two variance parameters for any T. Basic Linear Unobserved E¤ects Panel Data Models 263", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 276, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p277::c0", "text": "With very large N, using the general estimate of W is an attractive alternative, es- pecially if the estimate in equation (10.38) appears to have a pattern di¤erent from the random e¤ects pattern. As a middle ground between a traditional random e¤ects analysis and a full-blown FGLS analysis, we might specify a particular structure for the idiosyncratic error variance matrix Eðuiu0 iÞ. For example, if fuitg follows a stable ﬁrst-order autoregressive process with autocorrelation coe‰cient r and variance s2 u, then W ¼ Eðuiu0 iÞ þ s2 c jTj0 T depends in a known way on only three parameters, s2 u, s2 c , and r. These parameters can be estimated after initial pooled OLS estimation, and then an FGLS procedure using the particular structure of W is easy to implement. We do not cover such possibilities explicitly; see, for example, MaCurdy (1982). 10.4.4 Testing for the Presence of an Unobserved E¤ect If the standard random e¤ects assumptions RE.1–RE.3 hold but the model does not actually contain an unobserved e¤ect, pooled OLS is e‰cient and all associated pooled OLS statistics are asymptotically valid. The absence of an unobserved e¤ect is statistically equivalent to H0: s2 c ¼ 0. To test H0: s2 c ¼ 0, we can use the simple test for AR(1) serial correlation covered in Chapter 7 [see equation (7.77)]. The AR(1) test is valid because the errors vit are serially uncorrelated under the null H0: s2 c ¼ 0 (and we are assuming that fxitg is strictly exogenous). However, a better test is based directly on the estimator of s2 c in equation (10.37). Breusch and Pagan (1980) derive a statistic using the Lagrange multiplier principle in a likelihood setting (something we cover in Chapter 13). We will not derive the Breusch and Pagan statistic because we are not assuming any particular distribution for the vit. Instead, we derive a similar test that has the advantage of being valid for any distribution of vi and only states that the vit are uncorrelated under the null. (In particular, the statistic is valid for heteroskedasticity in the vit.) From equation (10.37), we base a test of H0: s2 c ¼ 0 on the null asymptotic distri- bution of N\u00031=2 X N i¼1 X T\u00031 t¼1 X T s¼tþ1 ^vit^vis ð10:39Þ which is essentially the estimator ^s2 c scaled up by ﬃﬃﬃﬃ N p . Because of strict exogeneity, this statistic has the same limiting distribution (as N ! y with ﬁxed T ) when we replace the pooled OLS residuals ^vit with the errors vit (see Problem 7.4). For any distribution of the vit, N\u00031=2 PN i¼1 PT\u00031 t¼1 PT s¼tþ1 vitvis has a limiting normal distribution (under the null that the vit are serially uncorrelated) with variance Chapter 10 264", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 277, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p278::c0", "text": "EðPT\u00031 t¼1 PT s¼tþ1 vitvisÞ2. We can estimate this variance in the usual way (take away the expectation, average across i, and replace vit with ^vit). When we put expression (10.39) over its asymptotic standard error we get the statistic PN i¼1 PT\u00031 t¼1 PT s¼tþ1 ^vit^vis h PN i¼1 PT\u00031 t¼1 PT s¼tþ1 ^vit^vis \u0002 \u00032i1=2 ð10:40Þ Under the null hypothesis that the vit are serially uncorrelated, this statistic is dis- tributed asymptotically as standard normal. Unlike the Breusch-Pagan statistic, with expression (10.40) we can reject H0 for negative estimates of s2 c , although negative estimates are rare in practice (unless we have already di¤erenced the data, something we discuss in Section 10.6). The statistic in expression (10.40) can detect many kinds of serial correlation in the composite error vit, and so a rejection of the null should not be interpreted as imply- ing that the random e¤ects error structure must be true. Finding that the vit are seri- ally uncorrelated is not very surprising in applications, especially since xit cannot contain lagged dependent variables for the methods in this chapter. It is probably more interesting to test for serial correlation in the fuitg, as this is a test of the random e¤ects form of W. Baltagi and Li (1995) obtain a test under nor- mality of ci and fuitg, based on the Lagrange multiplier principle. In Section 10.7.2, we discuss a simpler test for serial correlation in fuitg using a pooled OLS regression on transformed data, which does not rely on normality. 10.5 Fixed E¤ects Methods 10.5.1 Consistency of the Fixed E¤ects Estimator Again consider the linear unobserved e¤ects model for T time periods: yit ¼ xitb þ ci þ uit; t ¼ 1; . . . ; T ð10:41Þ The random e¤ects approach to estimating b e¤ectively puts ci into the error term, under the assumption that ci is orthogonal to xit, and then accounts for the implied serial correlation in the composite error vit ¼ ci þ uit using a GLS analysis. In many applications the whole point of using panel data is to allow for ci to be arbitrarily correlated with the xit. A ﬁxed e¤ects analysis achieves this purpose explicitly. The T equations in the model (10.41) can be written as yi ¼ Xib þ cijT þ ui ð10:42Þ Basic Linear Unobserved E¤ects Panel Data Models 265", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 278, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p279::c0", "text": "where jT is still the T \u0001 1 vector of ones. As usual, equation (10.42) represents a sin- gle random draw from the cross section. The ﬁrst ﬁxed e¤ects (FE) assumption is strict exogeneity of the explanatory vari- ables conditional on ci: assumption FE.1: Eðuit j xi; ciÞ ¼ 0, t ¼ 1; 2; . . . ; T. This assumption is identical to the ﬁrst part of Assumption RE.1. Thus, we maintain strict exogeneity of fxit: t ¼ 1; . . . ; Tg conditional on the unobserved e¤ect. The key di¤erence is that we do not assume RE.1b. In other words, for ﬁxed e¤ects analysis, Eðci j xiÞ is allowed to be any function of xi. By relaxing RE.1b we can consistently estimate partial e¤ects in the presence of time-constant omitted variables that can be arbitrarily related to the observables xit. Therefore, ﬁxed e¤ects analysis is more robust than random e¤ects analysis. As we suggested in Section 10.1, this robustness comes at a price: without further assump- tions, we cannot include time-constant factors in xit. The reason is simple: if ci can be arbitrarily correlated with each element of xit, there is no way to distinguish the e¤ects of time-constant observables from the time-constant unobservable ci. When analyzing individuals, factors such as gender or race cannot be included in xit. For analyzing ﬁrms, industry cannot be included in xit unless industry designation changes over time for at least some ﬁrms. For cities, variables describing ﬁxed city attributes, such as whether or not the city is near a river, cannot be included in xit. The fact that xit cannot include time-constant explanatory variables is a drawback in certain applications, but when the interest is only on time-varying explanatory variables, it is convenient not to have to worry about modeling time-constant factors that are not of direct interest. In panel data analysis the term ‘‘time-varying explanatory variables’’ means that each element of xit varies over time for some cross section units. Often there are ele- ments of xit that are constant across time for a subset of the cross section. For ex- ample, if we have a panel of adults and one element of xit is education, we can allow education to be constant for some part of the sample. But we must have education changing for some people in the sample. As a general speciﬁcation, let d2t; . . . ; dTt denote time period dummies so that dst ¼ 1 if s ¼ t, and zero otherwise (often these are deﬁned in terms of speciﬁc years, such as d88t, but at this level we call them time period dummies). Let zi be a vector of time-constant observables, and let wit be a vector of time-varying variables. Suppose yit is determined by Chapter 10 266", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 279, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p280::c0", "text": "yit ¼ y1 þ y2d2t þ \u0002 \u0002 \u0002 þ yTdTt þ zig1 þ d2tzig2 þ \u0002 \u0002 \u0002 þ dTtzigT þ witd þ ci þ uit ð10:43Þ Eðuit j zi; wi1; wi2; . . . ; wiT; ciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð10:44Þ We hope that this model represents a causal relationship, where the conditioning on ci allows us to control for unobserved factors that are time constant. Without further assumptions, the intercept y1 cannot be identiﬁed and the vector g1 on zi cannot be identiﬁed, because y1 þ zig1 cannot be distinguished from ci. Note that y1 is the intercept for the base time period, t ¼ 1, and g1 measures the e¤ects of zi on yit in period t ¼ 1. Even though we cannot identify the e¤ects of the zi in any particular time period, g2; g3; . . . ; gT are identiﬁed, and therefore we can estimate the di¤erences in the partial e¤ects on time-constant variables relative to a base period. In particu- lar, we can test whether the e¤ects of time-constant variables have changed over time. As a speciﬁc example, if yit ¼ logðwageitÞ and one element of zi is a female binary variable, then we can estimate how the gender gap has changed over time, even though we cannot estimate the gap in any particular time period. The idea for estimating b under Assumption FE.1 is to transform the equations to eliminate the unobserved e¤ect ci. When at least two time periods are available, there are several transformations that accomplish this purpose. In this section we study the ﬁxed e¤ects transformation, also called the within transformation. The FE transfor- mation is obtained by ﬁrst averaging equation (10.41) over t ¼ 1; . . . ; T to get the cross section equation yi ¼ xib þ ci þ ui ð10:45Þ where yi ¼ T\u00031 PT t¼1 yit, xi ¼ T\u00031 PT t¼1 xit, and ui ¼ T\u00031 PT t¼1 uit. Subtracting equation (10.45) from equation (10.41) for each t gives the FE transformed equation, yit \u0003 yi ¼ ðxit \u0003 xiÞb þ uit \u0003 ui or €yit ¼ €xitb þ €uit; t ¼ 1; 2; . . . ; T ð10:46Þ where €yit 1 yit \u0003 yi, €xit 1 xit \u0003 xi, and €uit 1 uit \u0003 ui. The time demeaning of the original equation has removed the individual speciﬁc e¤ect ci. With ci out of the picture, it is natural to think of estimating equation (10.46) by pooled OLS. Before investigating this possibility, we must remember that equation (10.46) is an estimating equation: the interpretation of b comes from the (structural) conditional expectation Eðyit j xi; ciÞ ¼ Eðyit j xit; ciÞ ¼ xitb þ ci. Basic Linear Unobserved E¤ects Panel Data Models 267", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 280, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p281::c0", "text": "To see whether pooled OLS estimation of equation (10.46) will be consistent, we need to show that the key pooled OLS assumption (Assumption POLS.1 from Chapter 7) holds in equation (10.46). That is, Eð€x0 it€uitÞ ¼ 0; t ¼ 1; 2; . . . ; T ð10:47Þ For each t, the left-hand side of equation (10.47) can be written as E½ðxit \u0003 xiÞ0ðuit \u0003 uiÞ\u0004. Now, under Assumption FE.1, uit is uncorrelated with xis, for all s; t ¼ 1; 2; . . . ; T. It follows that uit and ui are uncorrelated with xit and xi for t ¼ 1; 2; . . . ; T. Therefore, assumption (10.47) holds under Assumption FE.1, and so pooled OLS applied to equation (10.46) can be expected to produce con- sistent estimators. We can actually say a lot more than condition (10.47): under Assumption FE.1, Eð€uit j xiÞ ¼ Eðuit j xiÞ \u0003 Eðui j xiÞ ¼ 0, which in turn implies that Eð€uit j €xi1; . . . ; €xiTÞ ¼ 0, since each €xit is just a function of xi ¼ ðxi1; . . . ; xiTÞ. This result shows that the €xit satisfy the conditional expectation form of the strict exoge- neity assumption in the model (10.46). Among other things, this conclusion implies that the ﬁxed e¤ects estimator of b that we will derive is actually unbiased under Assumption FE.1. It is important to see that assumption (10.47) fails if we try to relax the strict exo- geneity assumption to something weaker, such as Eðx0 ituitÞ ¼ 0, all t, because this as- sumption does not ensure that xis is uncorrelated with uit, s 0 t. The ﬁxed e¤ects (FE) estimator, denoted by ^bFE, is the pooled OLS estimator from the regression €yit on €xit; t ¼ 1; 2; . . . ; T; i ¼ 1; 2; . . . ; N ð10:48Þ The FE estimator is simple to compute once the time demeaning has been carried out. Some econometrics packages have special commands to carry out ﬁxed e¤ects estimation (and commands to carry out the time demeaning for all i). It is also fairly easy to program this estimator in matrix-oriented languages. To study the FE estimator a little more closely, write equation (10.46) for all time periods as €yi ¼ €Xib þ €ui ð10:49Þ where €yi is T \u0001 1, €Xi is T \u0001 K, and €ui is T \u0001 1. This set of equations can be obtained by premultiplying equation (10.42) by a time-demeaning matrix. Deﬁne QT 1 IT \u0003 jTðj0 TjTÞ\u00031j0 T, which is easily seen to be a T \u0001 T symmetric, idempotent matrix with rank T \u0003 1. Further, QTjT ¼ 0, QTyi ¼ €yi, QTXi ¼ €Xi, and QTui ¼ €ui, and so pre- multiplying equation (10.42) by QT gives the demeaned equations (10.49). Chapter 10 268", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 281, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p282::c0", "text": "In order to ensure that the FE estimator is well behaved asymptotically, we need a standard rank condition on the matrix of time-demeaned explanatory variables: assumption FE.2: rank PT t¼1 Eð€x0 it€xitÞ \u0002 \u0003 ¼ rank½Eð€X0 i €XiÞ\u0004 ¼ K. If xit contains an element that does not vary over time for any i, then the corre- sponding element in €xit is identically zero for all t and any draw from the cross sec- tion. Since €Xi would contain a column of zeros for all i, Assumption FE.2 could not be true. Assumption FE.2 shows explicitly why time-constant variables are not allowed in ﬁxed e¤ects analysis (unless they are interacted with time-varying vari- ables, such as time dummies). The ﬁxed e¤ects estimator can be expressed as ^bFE ¼ X N i¼1 €X0 i €Xi !\u00031 X N i¼1 €X0 i€yi ! ¼ X N i¼1 X T t¼1 €x0 it€xit !\u00031 X N i¼1 X T t¼1 €x0 it €yit ! ð10:50Þ It is also called the within estimator because it uses the time variation within each cross section. The between estimator, which uses only variation between the cross section observations, is the OLS estimator applied to the time-averaged equation (10.45). This estimator is not consistent under Assumption FE.1 because Eðx0 iciÞ is not necessarily zero. The between estimator is consistent under Assumption RE.1 and a standard rank condition, but it e¤ectively discards the time series information in the data set. It is more e‰cient to use the random e¤ects estimator. Under Assumption FE.1 and the ﬁnite sample version of Assumption FE.2, namely, rankð€X0 €XÞ ¼ K, ^bFE can be shown to be unbiased conditional on X. 10.5.2 Asymptotic Inference with Fixed E¤ects Without further assumptions the FE estimator is not necessarily the most e‰cient estimator based on Assumption FE.1. The next assumption ensures that FE is e‰cient. assumption FE.3: Eðuiu0 i j xi; ciÞ ¼ s2 uIT. Assumption FE.3 is identical to Assumption RE.3a. Since Eðui j xi; ciÞ ¼ 0 by As- sumption FE.1, Assumption FE.3 is the same as saying Varðui j xi; ciÞ ¼ s2 uIT if Assumption FE.1 also holds. As with Assumption RE.3a, it is useful to think of Assumption FE.3 as having two parts. The ﬁrst is that Eðuiu0 i j xi; ciÞ ¼ Eðuiu0 iÞ, which is standard in system estimation contexts [see equation (7.50)]. The second is that the unconditional variance matrix Eðuiu0 iÞ has the special form s2 uIT. This implies that the idiosyncratic errors uit have a constant variance across t and are serially uncorrelated, just as in assumptions (10.28) and (10.29). Basic Linear Unobserved E¤ects Panel Data Models 269", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 282, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p283::c0", "text": "Assumption FE.3, along with Assumption FE.1, implies that the unconditional variance matrix of the composite error vi ¼ cijT þ ui has the random e¤ects form. However, without Assumption RE.3b, Eðviv0 i j xiÞ 0 Eðviv0 iÞ. While this result matters for inference with the RE estimator, it has no bearing on a ﬁxed e¤ects analysis. It is not obvious that Assumption FE.3 has the desired consequences of ensuring e‰ciency of ﬁxed e¤ects and leading to simple computation of standard errors and test statistics. Consider the demeaned equation (10.46). Normally, for pooled OLS to be relatively e‰cient, we require that the f€uit: t ¼ 1; 2; . . . ; Tg be homoskedastic across t and serially uncorrelated. The variance of €uit can be computed as Eð€u2 itÞ ¼ E½ðuit \u0003 uiÞ2\u0004 ¼ Eðu2 itÞ þ Eðu2 i Þ \u0003 2EðuituiÞ ¼ s2 u þ s2 u=T \u0003 2s2 u=T ¼ s2 uð1 \u0003 1=TÞ ð10:51Þ which veriﬁes (unconditional) homoskedasticity across t. However, for t 0 s, the covariance between €uit and €uis is Eð€uit€uisÞ ¼ E½ðuit \u0003 uiÞðuis \u0003 uiÞ\u0004 ¼ EðuituisÞ \u0003 EðuituiÞ \u0003 EðuisuiÞ þ Eðu2 i Þ ¼ 0 \u0003 s2 u=T \u0003 s2 u=T þ s2 u=T ¼ \u0003s2 u=T < 0 Combining this expression with the variance in equation (10.51) gives, for all t 0 s, Corrð€uit; €uisÞ ¼ \u00031=ðT \u0003 1Þ ð10:52Þ which shows that the time-demeaned errors €uit are negatively serially correlated. (As T gets large, the correlation tends to zero.) It turns out that, because of the nature of time demeaning, the serial correlation in the €uit under Assumption FE.3 causes only minor complications. To ﬁnd the asymp- totic variance of ^bFE, write ﬃﬃﬃﬃ N p ð ^bFE \u0003 bÞ ¼ N\u00031 X N i¼1 €X0 i €Xi !\u00031 N\u00031=2 X N i¼1 €X0 iui ! where we have used the important fact that €X0 i€ui ¼ X0 iQTui ¼ €X0 iui. Under Assump- tion FE.3, Eðuiu0 i j €XiÞ ¼ s2 uIT. From the system OLS analysis in Chapter 7 it follows that ﬃﬃﬃﬃ N p ð ^bFE \u0003 bÞ @ Normalð0; s2 u½Eð€X0 i €XiÞ\u0004\u00031Þ and so Avarð ^bFEÞ ¼ s2 u½Eð€X0 i €XiÞ\u0004\u00031=N ð10:53Þ Chapter 10 270", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 283, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p284::c0", "text": "Given a consistent estimator ^s2 u of s2 u, equation (10.53) is easily estimated by also replacing Eð€X0 i €XiÞ with its sample analogue N\u00031 PN i¼1 €X0 i €Xi: A ^ var varð ^bFEÞ ¼ ^s2 u X N i¼1 €X0 i €Xi !\u00031 ¼ ^s2 u X N i¼1 X T t¼1 €x0 it€xit !\u00031 ð10:54Þ The asymptotic standard errors of the ﬁxed e¤ects estimates are obtained as the square roots of the diagonal elements of the matrix (10.54). Expression (10.54) is very convenient because it looks just like the usual OLS variance matrix estimator that would be reported from the pooled OLS regression (10.48). However, there is one catch, and this comes in obtaining the estimator ^s2 u of s2 u. The errors in the transformed model are €uit, and these errors are what the OLS residuals from regression (10.48) estimate. Since s2 u is the variance of uit, we must use a little care. To see how to estimate s2 u, we use equation (10.51) summed across t: PT t¼1 Eð€u2 itÞ ¼ ðT \u0003 1Þs2 u, and so ½NðT \u0003 1Þ\u0004\u00031 PN i¼1 PT t¼1 Eð€u2 itÞ ¼ s2 u. Now, deﬁne the ﬁxed e¤ects residuals as ^uit ¼ €yit \u0003 €xit ^bFE; t ¼ 1; 2; . . . ; T; i ¼ 1; 2; . . . ; N ð10:55Þ which are simply the OLS residuals from the pooled regression (10.48). Then a con- sistent estimator of s2 u under Assumptions FE.1–FE.3 is ^s2 u ¼ SSR=½NðT \u0003 1Þ \u0003 K\u0004 ð10:56Þ where SSR ¼ PN i¼1 PT t¼1 ^u2 it. The subtraction of K in the denominator of equation (10.56) does not matter asymptotically, but it is standard to make such a correction. In fact, under Assumptions FE.1–FE.3, it can be shown that ^s2 u is actually an un- biased estimator of s2 u conditional on X (and therefore unconditionally as well). Pay careful attention to the denominator in equation (10.56). This is not the degrees of freedom that would be obtained from regression (10.48). In fact, the usual variance estimate from regression (10.48) would be SSR/ðNT \u0003 KÞ, which has a probability limit less than s2 u as N gets large. The di¤erence between SSR/ðNT \u0003 KÞ and equation (10.56) can be substantial when T is small. The upshot of all this is that the usual standard errors reported from the regression (10.48) will be too small on average because they use the incorrect estimate of s2 u. Of course, computing equation (10.56) directly is pretty trivial. But, if a standard re- gression package is used after time demeaning, it is perhaps easiest to adjust the usual standard errors directly. Since ^su appears in the standard errors, each standard error Basic Linear Unobserved E¤ects Panel Data Models 271", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 284, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p285::c0", "text": "is simply multiplied by the factor fðNT \u0003 KÞ=½NðT \u0003 1Þ \u0003 K\u0004g1=2. As an example, if N ¼ 500, T ¼ 3, and K ¼ 10, the correction factor is about 1.227. If an econometrics package has an option for explicitly obtaining ﬁxed e¤ects estimates using panel data, s2 u will be properly estimated, and you do not have to worry about adjusting the standard errors. Many software packages also compute an estimate of s2 c , which is useful to determine how large the variance of the unob- served component is to the variance of the idiosyncratic component. Given ^bFE, ^s2 v ¼ ðNT \u0003 K\u00031Þ PN i¼1 PT t¼1ðyit \u0003 xit ^bFEÞ2 is a consistent estimator of s2 v ¼ s2 c þ s2 u, and so a consistent estimator of s2 c is ^s2 v \u0003 ^s2 u. (See Problem 10.14 for a discussion of why the estimated variance of the unobserved e¤ect in a ﬁxed e¤ects analysis is generally larger than that for a random e¤ects analysis.) Example 10.5 (FE Estimation of the E¤ects of Job Training Grants): Using the data in JTRAIN1.RAW, we estimate the e¤ect of job training grants using the ﬁxed e¤ects estimator. The variable union has been dropped because it does not vary over time for any of the ﬁrms in the sample. The estimated equation with standard errors is logð^scrapÞ ¼ \u0003:080 ð:109Þ d88 \u0003 :247 ð:133Þ d89 \u0003 :252 ð:151Þ grant \u0003 :422 ð:210Þ grant\u00031 Compared with the random e¤ects, the grant is estimated to have a larger e¤ect, both contemporaneously and lagged one year. The t statistics are also somewhat more signiﬁcant with ﬁxed e¤ects. Under Assumptions FE.1–FE.3, multiple restrictions are most easily tested using an F statistic, provided the degrees of freedom are appropriately computed. Let SSRur be the unrestricted SSR from regression (10.48), and let SSRr denote the restricted sum of squared residuals from a similar regression, but with Q restrictions imposed on b. Then F ¼ ðSSRr \u0003 SSRurÞ SSRur \u0002 ½NðT \u0003 1Þ \u0003 K\u0004 Q is approximately F distributed with Q and NðT \u0003 1Þ \u0003 K degrees of freedom. (The precise statement is that Q \u0002 F @ w2 Q as N ! y under H0.) When this equation is applied to Example 10.5, the F statistic for joint signiﬁcance of grant and grant\u00031 is F ¼ 2:23, with p-value ¼ :113. 10.5.3 The Dummy Variable Regression So far we have viewed the ci as being unobservable random variables, and for most applications this approach gives the appropriate interpretation of b. Traditional Chapter 10 272", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 285, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p286::c0", "text": "approaches to ﬁxed e¤ects estimation view the ci as parameters to be estimated along with b. In fact, if Assumption FE.2 is changed to its ﬁnite sample version, rankð€X0 €XÞ ¼ K, then the model under Assumptions FE.1–FE.3 satisﬁes the Gauss- Markov assumptions conditional on X. If the ci are parameters to estimate, how would we estimate each ci along with b? One possibility is to deﬁne N dummy variables, one for each cross section observa- tion: dni ¼ 1 if n ¼ i, dni ¼ 0 if n 0 i. Then, run the pooled OLS regression yit on d1i; d2i; . . . ; dNi; xit; t ¼ 1; 2; . . . ; T; i ¼ 1; 2; . . . ; N ð10:57Þ Then, ^c1 is the coe‰cient on d1i, ^c2 is the coe‰cient on d2i, and so on. It is a nice exercise in least squares mechanics—in particular, partitioned regres- sion (see Davidson and MacKinnon, 1993, Section 1.4)—to show that the estimator of b obtained from regression (10.57) is, in fact, the ﬁxed e¤ects estimator. This is why ^bFE is sometimes referred to as the dummy variable estimator. Also, the residuals from regression (10.57) are identical to the residuals from regression (10.48). One beneﬁt of regression (10.57) is that it produces the appropriate estimate of s2 u because it uses NT \u0003 N \u0003 K ¼ NðT \u0003 1Þ \u0003 K as the degrees of freedom. Therefore, if it can be done, regression (10.57) is a convenient way to carry out ﬁxed e¤ects analysis under Assumptions FE.1–FE.3. There is an important di¤erence between the ^ci and ^bFE. We already know that ^bFE is consistent with ﬁxed T as N ! y. This is not the case with the ^ci. Each time a new cross section observation is added, another ci is added, and information does not accumulate on the ci as N ! y. Each ^ci is an unbiased estimator of ci when the ci are treated as parameters, at least if we maintain Assumption FE.1 and the ﬁnite sample analogue of Assumption FE.2. When we add Assumption FE.3, the Gauss- Markov assumptions hold (conditional on X ), and ^c1; ^c2; . . . ; ^cN are best linear unbiased conditional on X. (The ^ci give practical examples of estimators that are unbiased but not consistent.) Econometric software that employs ﬁxed e¤ects usually suppresses the ‘‘estimates’’ of the ci, although an overall intercept is often reported. The overall intercept is either for an arbitrary cross section unit or, more commonly, for the average of the ^ci across i. Sometimes it is useful to obtain the ^ci even when regression (10.57) is infeasible. Using the OLS ﬁrst-order conditions, each ^ci can be shown to be ^ci ¼ yi \u0003 xi ^bFE; i ¼ 1; 2; . . . ; N ð10:58Þ After obtaining the ^ci, the sample average, sample standard deviation, and quantiles can be obtained to get some idea of how much heterogeneity is in the population. Basic Linear Unobserved E¤ects Panel Data Models 273", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 286, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p287::c0", "text": "(For example: Is the population distribution of ci spread out or tightly centered about its mean? Is the distribution symmetric?) With large T, the ^ci can be precise enough to learn something about the distribution of ci. With small T, the ^ci can contain sub- stantial noise. Under the classical linear model assumptions (which require, in addi- tion to Assumptions FE.1–FE.3, normality of the uit), we can test the equality of the ci using a standard F test for T of any size. [The degrees of freedom are N \u0003 1 and NðT \u0003 1Þ \u0003 K.] Unfortunately, the properties of this test as N ! y with T ﬁxed are unknown without the normality assumption. Generally, we should view the fact that the dummy variable regression (10.57) produces ^bFE as the coe‰cient vector on xit as a coincidence. While there are other unobserved e¤ects models where ‘‘estimating’’ the unobserved e¤ects along with the vector b results in a consistent estimator of b, there are many cases where this approach leads to trouble. As we will see in Part IV, many nonlinear panel data models with unobserved e¤ects su¤er from an incidental parameters problem, where estimating the incidental parameters, ci, along with b produces an inconsistent esti- mator of b. 10.5.4 Serial Correlation and the Robust Variance Matrix Estimator Recall that the FE estimator is consistent and asymptotically normal under Assumptions FE.1 and FE.2. But without Assumption FE.3, expression (10.54) gives an improper variance matrix estimator. While heteroskedasticity in uit is always a potential problem, serial correlation is likely to be more important in certain appli- cations. When applying the FE estimator, it is important to remember that nothing rules out serial correlation in fuit: t ¼ 1; . . . ; Tg. While it is true that the observed serial correlation in the composite errors, vit ¼ ci þ uit, is dominated by the presence of ci, there can also be serial correlation that dies out over time. Sometimes, fuitg can have very strong serial dependence, in which case the usual FE standard errors obtained from expression (10.54) can be very misleading. This possibility tends to be a bigger problem with large T. (As we will see, there is no reason to worry about serial correlation in uit when T ¼ 2.) Testing the idiosyncratic errors, fuitg, for serial correlation is somewhat tricky. A key point is that we cannot estimate the uit; because of the time demeaning used in FE, we can only estimate the time-demeaned errors, €uit. As shown in equation (10.52), the time-demeaned errors are negatively correlated if the uit are uncorrelated. When T ¼ 2, €ui1 ¼ \u0003€ui2 for all i, and so there is perfect negative correlation. This result shows that for T ¼ 2 it is pointless to use the €uit to test for any kind of serial correlation pattern. Chapter 10 274", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 287, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p288::c0", "text": "When T b 3, we can use equation (10.52) to determine if there is serial correlation in fuitg. Naturally, we use the ﬁxed e¤ects residuals, ^uit. One simpliﬁcation is obtained by applying Problem 7.4: we can ignore the estimation error in b in obtaining the asymptotic distribution of any test statistic based on sample covariances and vari- ances. In other words, it is as if we are using the €uit, rather than the ^uit. The test is complicated by the fact that the f€uitg are serially correlated under the null hypothesis. There are two simple possibilities for dealing with this. First, we can just use any two time periods (say, the last two), to test equation (10.52) using a simple regression. In other words, run the regression ^uiT on ^ui;T\u00031; i ¼ 1; . . . ; N and use ^d, the coe‰cient on ^ui;T\u00031, along with its standard error, to test H0: d ¼ \u00031=ðT \u0003 1Þ, where d ¼ Corrð€ui;T\u00031; €uiTÞ. Under Assumptions FE.1–FE.3, the usual t statistic has an asymptotic normal distribution. (It is trivial to make this test robust to heteroskedasticity.) Alternatively, we can use more time periods if we make the t statistic robust to arbitrary serial correlation. In other words, run the pooled OLS regression ^uit on ^ui;t\u00031; t ¼ 3; . . . ; T; i ¼ 1; . . . ; N and use the fully robust standard error for pooled OLS; see equation (7.26). It may seem a little odd that we make a test for serial correlation robust to serial correlation, but this need arises because the null hypothesis is that the time-demeaned errors are serially correlated. This approach clearly does not produce an optimal test against, say, AR(1) correlation in the uit, but it is very simple and may be good enough to indicate a problem. If we ﬁnd serial correlation, we should, at a minimum, adjust the asymptotic vari- ance matrix estimator and test statistics. Fortunately, we can apply the results from Chapter 7 directly to obtain a fully robust asymptotic variance matrix estimator. Let ^ui 1 €yi \u0003 €Xi ^bFE, i ¼ 1; 2; . . . ; N denote the T \u0001 1 vectors ﬁxed e¤ects residuals. Applying equation (7.26), the robust variance matrix estimator of ^bFE is Ava^rð ^bFEÞ ¼ ð€X0 €XÞ\u00031 X N i¼1 €X0 i^ui^u0 i €Xi ! ð€X0 €XÞ\u00031 ð10:59Þ which was suggested by Arellano (1987) and follows from the general results of White (1984, Chapter 6). The robust variance matrix estimator is valid in the pres- ence of any heteroskedasticity or serial correlation in fuit: t ¼ 1; . . . ; Tg, provided Basic Linear Unobserved E¤ects Panel Data Models 275", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 288, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p289::c0", "text": "that T is small relative to N. [Remember, equation (7.26) is justiﬁed for ﬁxed T, N ! y asymptotics.] The robust standard errors are obtained as the square roots of the diagonal elements of the matrix (10.59), and matrix (10.59) can be used as the ^V matrix in constructing Wald statistics. Unfortunately, the sum of squared resid- uals form of the F statistic is no longer asymptotically valid when Assumption FE.3 fails. Example 10.5 (continued): We now report the robust standard errors for the logðscrapÞ equation along with the usual FE standard errors: logð^scrapÞ ¼ \u0003:080 ð:109Þ ½:096\u0004 d88 \u0003 :247 ð:133Þ ½:193\u0004 d89 \u0003 :252 ð:151Þ ½:140\u0004 grant \u0003 :422 ð:210Þ ½:276\u0004 grant\u00031 The robust standard error on grant is actually smaller than the usual standard error, while the robust standard error on grant\u00031 is larger than the usual one. As a result, the absolute value of the t statistic on grant\u00031 drops from about 2 to just over 1.5. Remember, with ﬁxed T as N ! y, the robust standard errors are just as valid asymptotically as the nonrobust ones when Assumptions FE.1–FE.3 hold. But the usual standard errors and test statistics may be better behaved under Assumptions FE.1–FE.3 if N is not very large relative to T, especially if uit is normally distributed. 10.5.5 Fixed E¤ects GLS Recall that Assumption FE.3 can fail for two reasons. The ﬁrst is that the conditional variance matrix does not equal the unconditional variance matrix: Eðuiu0 i j xi; ciÞ 0 Eðuiu0 iÞ. Even if Eðuiu0 i j xi; ciÞ ¼ Eðuiu0 iÞ, the unconditional variance matrix may not be scalar: Eðuiu0 iÞ 0 s2 uIT, which means either that the variance of uit changes with t or, probably more importantly, that there is serial correlation in the idiosyncratic errors. The robust variance matrix (10.59) is valid in any case. Rather than compute a robust variance matrix for the FE estimator, we can in- stead relax Assumption FE.3 to allow for an unrestricted, albeit constant, conditional covariance matrix. This is a natural route to follow if the robust standard errors of the ﬁxed e¤ects estimator are too large to be useful and if there is evidence of serial dependence or a time-varying variance in the uit. assumption FEGLS.3: Eðuiu0 i j xi; ciÞ ¼ L, a T \u0001 T positive deﬁnite matrix. Under Assumption FEGLS.3, Eð€ui€u0 i j €xiÞ ¼ Eð€ui€u0 iÞ. Further, using €ui ¼ QTui, Eð€ui€u0 iÞ ¼ QTEðuiu0 iÞQT ¼ QTLQT ð10:60Þ Chapter 10 276", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 289, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p290::c0", "text": "which has rank T \u0003 1. The deﬁcient rank in expression (10.60) causes problems for the usual approach to GLS, because the variance matrix cannot be inverted. One way to proceed is to use a generalized inverse. A much easier approach—and one that turns out to be algebraically identical—is to drop one of the time periods from the analysis. It can be shown (see Im, Ahn, Schmidt, and Wooldridge, 1999) that it does not matter which of these time periods is dropped: the resulting GLS estimator is the same. For concreteness, suppose we drop time period T, leaving the equations €yi1 ¼ €xi1b þ €ui1 ... €yi;T\u00031 ¼ €xi;T\u00031b þ €ui;T\u00031 ð10:61Þ So that we do not have to introduce new notation, we write the system (10.61) as equation (10.49), with the understanding that now €yi is ðT \u0003 1Þ \u0001 1, €Xi is ðT \u0003 1Þ \u0001 K, and €ui is ðT \u0003 1Þ \u0001 1. Deﬁne the ðT \u0003 1Þ \u0001 ðT \u0003 1Þ positive deﬁnite matrix W 1 Eð€ui€u0 iÞ. We do not need to make the dependence of W on L and QT explicit; the key point is that, if no restrictions are made on L, then W is also unrestricted. To estimate W, we estimate b by ﬁxed e¤ects in the ﬁrst stage. After dropping the last time period for each i, deﬁne the ðT \u0003 1Þ \u0001 1 residuals ^^u^ui ¼ €yi \u0003 €Xi ^bFE, i ¼ 1; 2; . . . ; N. A consistent estimator of W is ^W ¼ N\u00031 X N i¼1 ^^u^ui^^u^u0 i ð10:62Þ The ﬁxed e¤ects GLS (FEGLS) estimator is deﬁned by ^bFEGLS ¼ X N i¼1 €X0 i ^W\u00031 €Xi !\u00031 X N i¼1 €X0 i ^W\u00031€yi ! where €Xi and €yi are deﬁned with the last time period dropped. For consistency of FEGLS, we replace Assumption FE.2 with a new rank condition: assumption FEGLS.2: rank Eð€X0 iW\u00031 €XiÞ ¼ K: Under Assumptions FE.1 and FEGLS.2, the FEGLS estimator is consistent. When we add Assumption FEGLS.3, the asymptotic variance is easy to estimate: A ^ var varð ^bFEGLSÞ ¼ X N i¼1 €X0 i ^W\u00031 €Xi !\u00031 Basic Linear Unobserved E¤ects Panel Data Models 277", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 290, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p291::c0", "text": "The sum of squared residual statistics from FGLS can be used to test multiple restrictions. Note that G ¼ T \u0003 1 in the F statistic in equation (7.53). The FEGLS estimator was proposed by Kiefer (1980) when the ci are treated as parameters. As we just showed, the procedure consistently estimates b when we view ci as random and allow it to be arbitrarily correlated with xit. The FEGLS estimator is asymptotically no less e‰cient than the FE estimator under Assumption FEGLS.3, even when L ¼ s2 uIT. Generally, if L 0 s2 uIT, FEGLS is more e‰cient than FE, but this conclusion relies on the large-N, ﬁxed-T asymp- totics. Unfortunately, because FEGLS still uses the ﬁxed e¤ects transformation to remove ci, it can have large asymptotic standard errors if the matrices €Xi have col- umns close to zero. Rather than allowing W to be an unrestricted matrix, we can impose restrictions on L that imply W has a restricted form. For example, Bhargava, Franzini, and Naren- dranatahn (1982) (BFN) assume that fuitg follows a stable, homoskedastic AR(1) model. This assumption implies that W depends on only three parameters, s2 c , s2 u, and the AR coe‰cient, r, no matter how large T is. BFN obtain a transformation that eliminates the unobserved e¤ect, ci, and removes the serial correlation in uit. They also propose estimators of r, so that feasible GLS is possible. Modeling fuitg as a speciﬁc time series process is attractive when N is not very large relative to T, as estimating an unrestricted covariance matrix for €ui [the ðT \u0003 1Þ \u0001 1 vector of time- demeaned errors] without large N can lead to poor ﬁnite-sample performance of the FGLS estimator. However, the only general statements we can make concern ﬁxed- T, N ! y asymptotics. In this scenario, the FGLS estimator that uses unrestricted W is no less asymptotically e‰cient than an FGLS estimator that puts restrictions on W. And, if the restrictions on W are incorrect, the estimator that imposes the restric- tions is less asymptotically e‰cient. Therefore, on theoretical grounds, we prefer an estimator of the type in equation (10.62). 10.5.6 Using Fixed E¤ects Estimation for Policy Analysis There are other ways to interpret the ﬁxed e¤ects transformation to illustrate why ﬁxed e¤ects is useful for policy analysis and program evaluation. Consider the model yit ¼ xitb þ vit ¼ zitg þ dwit þ vit where vit may or may not contain an unobserved e¤ect. Let wit be the policy variable of interest; it could be continuous or discrete. The vector zit contains other controls that might be correlated with wit, including time-period dummy variables. As an exercise, you can show that su‰cient for consistency of ﬁxed e¤ects, along with the rank condition FE.2, is Chapter 10 278", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 291, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p292::c0", "text": "E½x0 itðvit \u0003 viÞ\u0004 ¼ 0; t ¼ 1; 2; . . . ; T This assumption shows that each element of xit, and in particular the policy variable wit, can be correlated with vi. What ﬁxed e¤ects requires for consistency is that wit be uncorrelated with deviations of vit from the average over the time period. So a policy variable, such as program participation, can be systematically related to the persistent component in the error vit as measured by vi. It is for this reason that FE is often superior to be pooled OLS or random e¤ects for applications where participation in a program is determined by preprogram attributes that also a¤ect yit. 10.6 First Di¤erencing Methods 10.6.1 Inference In Section 10.1 we used di¤erencing to eliminate the unobserved e¤ect ci with T ¼ 2. We now study the di¤erencing transformation in the general case of model (10.41). For completeness, we state the ﬁrst assumption as follows: assumption FD.1: Same as Assumption FE.1. We emphasize that the model and the interpretation of b are exactly as in Section 10.5. What di¤ers is our method for estimating b. Lagging the model (10.41) one period and subtracting gives Dyit ¼ Dxitb þ Duit; t ¼ 2; 3; . . . ; T ð10:63Þ where Dyit ¼ yit \u0003 yi;t\u00031, Dxit ¼ xit \u0003 xi;t\u00031, and Duit ¼ uit \u0003 ui;t\u00031. As with the FE transformation, this ﬁrst-di¤erencing transformation eliminates the unobserved e¤ect ci. In di¤erencing we lose the ﬁrst time period for each cross section: we now have T \u0003 1 time periods for each i, rather than T. If we start with T ¼ 2, then, after dif- ferencing, we arrive at one time period for each cross section: Dyi2 ¼ Dxi2b þ Dui2. Equation (10.63) makes it clear that the elements of xit must be time varying (for at least some cross section units); otherwise Dxit has elements that are identically zero for all i and t. Also, while the intercept in the original equation gets di¤erenced away, equation (10.63) contains changes in time dummies if xit contains time dummies. In the T ¼ 2 case, the coe‰cient on the second-period time dummy becomes the inter- cept in the di¤erenced equation. If we di¤erence the general equation (10.43) we get Dyit ¼ y2ðDd2tÞ þ \u0002 \u0002 \u0002 þ yTðDdTtÞ þ ðDd2tÞzig2 þ \u0002 \u0002 \u0002 þ ðDdTtÞzigT þ Dwitd þ Duit ð10:64Þ Basic Linear Unobserved E¤ects Panel Data Models 279", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 292, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p293::c0", "text": "The parameters y1 and g1 are not identiﬁed because they disappear from the trans- formed equation, just as with ﬁxed e¤ects. The ﬁrst-di¤erence (FD) estimator, ^bFD, is the pooled OLS estimator from the regression Dyit on Dxit; t ¼ 2; . . . ; T; i ¼ 1; 2; . . . ; N ð10:65Þ Under Assumption FD.1, pooled OLS estimation of the ﬁrst-di¤erenced equations will be consistent because EðDx0 itDuitÞ ¼ 0; t ¼ 2; 3; . . . ; T ð10:66Þ Therefore, Assumption POLS.1 from Section 7.8 holds. In fact, strict exogeneity holds in the ﬁrst-di¤erenced equation: EðDuit j Dxi2; Dxi3; . . . ; DxiTÞ ¼ 0; t ¼ 2; 3; . . . ; T which means the FD estimator is actually unbiased conditional on X. To arrive at assumption (10.66) we clearly can get by with an assumption weaker than Assumption FD.1. The key point is that assumption (10.66) fails if uit is corre- lated with xi;t\u00031, xit, or xi;tþ1, and so we just assume that xis is uncorrelated with uit for all t and s. For completeness, we state the rank condition for the FD estimator: assumption FD.2: rank PT t¼2 EðDx0 itDxitÞ \u0002 \u0003 ¼ K. In practice, Assumption FD.2 rules out time-constant explanatory variables and perfect collinearity among the time-varying variables. Assuming the data have been ordered as we discussed earlier, ﬁrst di¤erencing is easy to implement provided we keep track of which transformed observations are valid and which are not. Di¤erences for observation numbers 1, T þ 1, 2T þ 1; 3T þ 1; . . . ; and ðN \u0003 1ÞT þ 1 should be set to missing. These observations corre- spond to the ﬁrst time period for every cross section unit in the original data set; by deﬁnition, there is no ﬁrst di¤erence for the t ¼ 1 observations. A little care is needed so that di¤erences between the ﬁrst time period for unit i þ 1 and the last time period for unit i are not treated as valid observations. Making sure these are set to missing is easy when a year variable or time period dummies have been included in the data set. One reason to prefer the FD estimator to the FE estimator is that FD is easier to implement without special software. Are there statistical reasons to prefer FD to FE? Recall that, under Assumptions FE.1–FE.3, the ﬁxed e¤ects estimator is asymp- Chapter 10 280", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 293, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p294::c0", "text": "totically e‰cient in the class of estimators using the strict exogeneity assumption FE.1. Therefore, the ﬁrst di¤erence estimator is less e‰cient than ﬁxed e¤ects under Assumptions FE.1–FE.3. Assumption FE.3 is key to the e‰ciency of FE. It assumes homoskedasticity and no serial correlation in uit. Assuming that the fuit: t ¼ 1; 2; . . . Tg are serially uncorrelated may be too strong. An alternative assumption is that the ﬁrst di¤erence of the idiosyncratic errors, feit 1 Duit; t ¼ 2; . . . ; Tg, are seri- ally uncorrelated (and have constant variance): assumption FD.3: Eðeie0 i j xi1; . . . ; xiT; ciÞ ¼ s2 e IT\u00031, where ei is the ðT \u0003 1Þ \u0001 1 vector containing eit, t ¼ 2; . . . ; T. Under Assumption FD.3 we can write uit ¼ ui;t\u00031 þ eit, so that no serial correlation in the eit implies that uit is a random walk. A random walk has substantial serial de- pendence, and so Assumption FD.3 represents an opposite extreme from Assumption FE.3. Under Assumptions FD.1–FD.3 it can be shown that the FD estimator is most e‰cient in the class of estimators using the strict exogeneity assumption FE.1. Fur- ther, from the pooled OLS analysis in Section 7.8, A ^ var varð ^bFDÞ ¼ ^s2 e ðDX0DXÞ\u00031 ð10:67Þ where ^s2 e is a consistent estimator of s2 e . The simplest estimator is obtained by com- puting the OLS residuals ^eit ¼ Dyit \u0003 Dxit ^bFD ð10:68Þ from the pooled regression (10.65). A consistent estimator of s2 e is ^s2 e ¼ ½NðT \u0003 1Þ \u0003 K\u0004\u00031 X N i¼1 X T t¼2 ^e2 it ð10:69Þ which is the usual error variance estimator from regression (10.65). These equations show that, under Assumptions FD.1–FD.3, the usual OLS standard errors from the ﬁrst di¤erence regression (10.65) are asymptotically valid. Unlike in the FE regression (10.48), the denominator in equation (10.69) is cor- rectly obtained from regression (10.65). Dropping the ﬁrst time period appropriately captures the lost degrees of freedom (N of them). Under Assumption FD.3, all statistics reported from the pooled regression on the ﬁrst-di¤erenced data are asymptotically valid, including F statistics based on sums of squared residuals. Basic Linear Unobserved E¤ects Panel Data Models 281", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 294, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p295::c0", "text": "10.6.2 Robust Variance Matrix If Assumption FD.3 is violated, then, as usual, we can compute a robust variance matrix. The estimator in equation (7.26) applied in this context is A ^ var varð ^bFDÞ ¼ ðDX0DXÞ\u00031 X N i¼1 DX0 i^ei^e0 iDXi ! ðDX0DXÞ\u00031 ð10:70Þ where DX denotes the NðT \u0003 1Þ \u0001 K matrix of stacked ﬁrst di¤erences of xit. Example 10.6 (FD Estimation of the E¤ects of Job Training Grants): We now esti- mate the e¤ect of job training grants on logðscrapÞ using ﬁrst di¤erencing. Speciﬁ- cally, we use pooled OLS on DlogðscrapitÞ ¼ d1 þ d2d89t þ b1Dgrantit þ b2Dgranti;t\u00031 þ Duit Rather than di¤erence the year dummies and omit the intercept, we simply include an intercept and a dummy variable for 1989 to capture the aggregate time e¤ects. If we were speciﬁcally interested in the year e¤ects from the structural model (in levels), then we should di¤erence those as well. The estimated equation is Dlogð^scrapÞ ¼ \u0003:091 ð:091Þ ½:088\u0004 \u0003 :096 ð:125Þ ½:111\u0004 d89 \u0003 :223 ð:131Þ ½:128\u0004 Dgrant \u0003 :351 ð:235Þ ½:265\u0004 Dgrant\u00031 R2 ¼ :037 where the usual standard errors are in parentheses and the robust standard errors are in brackets. We report R2 here because it has a useful interpretation: it measures the amount of variation in the growth in the scrap rate that is explained by Dgrant and Dgrant\u00031 (and d89). The estimates on grant and grant\u00031 are fairly similar to the ﬁxed e¤ects estimates, although grant is now statistically more signiﬁcant than grant\u00031. The usual F test for joint signiﬁcance of Dgrant and Dgrant\u00031 is 1.53 with p- value ¼ :222. 10.6.3 Testing for Serial Correlation Under Assumption FD.3, the errors eit 1 Duit should be serially uncorrelated. We can easily test this assumption given the pooled OLS residuals from regression (10.65). Since the strict exogeneity assumption holds, we can apply the simple form of the test in Section 7.8. The regression is based on T \u0003 2 time periods: Chapter 10 282", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 295, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p296::c0", "text": "^eit ¼ ^r1^ei;t\u00031 þ errorit; t ¼ 3; 4; . . . ; T; i ¼ 1; 2; . . . ; N ð10:71Þ The test statistic is the usual t statistic on ^r1. With T ¼ 2 this test is not available, nor is it necessary. With T ¼ 3, regression (10.71) is just a cross section regression be- cause we lose the t ¼ 1 and t ¼ 2 time periods. If the idiosyncratic errors fuit: t ¼ 1; 2; . . . ; Tg are uncorrelated to begin with, feit: t ¼ 2; 3; . . . ; Tg will be autocorrelated. In fact, under Assumption FE.3 it is easily shown that Corrðeit; ei;t\u00031Þ ¼ \u0003:5. In any case, a ﬁnding of signiﬁcant serial correla- tion in the eit warrants computing the robust variance matrix for the FD estimator. Example 10.6 (continued): We test for AR(1) serial correlation in the ﬁrst-di¤erenced equation by regressing ^eit on ^ei;t\u00031 using the year 1989. We get ^r1 ¼ :237 with t statistic ¼ 1.76. There is marginal evidence of positive serial correlation in the ﬁrst di¤erences Duit. Further, ^r1 ¼ :237 is very di¤erent from r1 ¼ \u0003:5, which is implied by the stan- dard random and ﬁxed e¤ects assumption that the uit are serially uncorrelated. An alternative to computing robust standard errors and test statistics is to use an FDGLS analysis under the assumption that Eðeie0 i j xiÞ is a constant ðT \u0003 1Þ \u0001 ðT \u0003 1Þ matrix. We omit the details, as they are similar to the FEGLS case in Section 10.5.5. As with FEGLS, we could impose structure on Eðuiu0 iÞ, such as a stable, homo- skedastic AR(1) model, and then derive Eðeie0 iÞ in terms of a small set of parameters. 10.6.4 Policy Analysis Using First Di¤erencing First di¤erencing a structural equation with an unobserved e¤ect is a simple yet powerful method of program evaluation. Many questions can be addressed by having a two-year panel data set with control and treatment groups available at two points in time. In applying ﬁrst di¤erencing, we should di¤erence all variables appearing in the structural equation to obtain the estimating equation, including any binary indicators indicating participation in the program. The estimates should be interpreted in the orginal equation because it allows us to think of comparing di¤erent units in the cross section at any point in time, where one unit receives the treatment and the other does not. In one special case it does not matter whether the policy variable is di¤erenced. Assume that T ¼ 2, and let progit denote a binary indicator set to one if person i was in the program at time t. For many programs, progi1 ¼ 0 for all i: no one participated in the program in the initial time period. In the second time period, progi2 is unity for those who participate in the program and zero for those who do not. In this one case, Dprogi ¼ progi2, and the ﬁrst-di¤erenced equation can be written as Basic Linear Unobserved E¤ects Panel Data Models 283", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 296, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p297::c0", "text": "Dyi2 ¼ y2 þ Dzi2g þ d1progi2 þ Dui2 ð10:72Þ The e¤ect of the policy can be obtained by regressing the change in y on the change in z and the policy indicator. When Dzi2 is omitted, the estimate of d1 from equation (10.72) is the di¤erence-in-di¤erences (DID) estimator (see Problem 10.4): ^d1 ¼ Dytreat \u0003 Dycontrol. This is similar to the DID estimator from Section 6.3—see equation (6.32)—but there is an important di¤erence: with panel data, the di¤erences over time are for the same cross section units. If some people participated in the program in the ﬁrst time period, or if more than two periods are involved, equation (10.72) can give misleading answers. In general, the equation that should be estimated is Dyit ¼ xt þ Dzitg þ d1Dprogit þ Duit ð10:73Þ where the program participation indicator is di¤erenced along with everything else, and the xt are new period intercepts. Example 10.6 is one such case. Extensions of the model, where progit appears in other forms, are discussed in Chapter 11. 10.7 Comparison of Estimators 10.7.1 Fixed E¤ects versus First Di¤erencing When we have only two time periods, ﬁxed e¤ects estimation and ﬁrst di¤erencing produce identical estimates and inference, as you are asked to show in Problem 10.3. First di¤erencing is easier to implement, and all procedures that can be applied to a single cross section—such as heteroskedasticity-robust inference—can be applied directly. When T > 2, the choice between FD and FE hinges on the assumptions about the idiosyncratic errors, uit. In particular, the FE estimator is more e‰cient under As- sumption FE.3—the uit are serially uncorrelated—while the FD estimator is more e‰cient when uit follows a random walk. In many cases, the truth is likely to lie somewhere in between. If FE and FD estimates di¤er in ways that cannot be attributed to sampling error, we should worry about the strict exogeneity assumption. If uit is correlated with xis for any t and s, FE and FD generally have di¤erent probability limits. Any of the standard endogeneity problems, including measurement error, time-varying omitted variables, and simultaneity, generally cause correlation between xit and uit—that is, contemporaneous correlation—which then causes both FD and FE to be inconsistent and to have di¤erent probability limits. (We explicitly consider these problems in Chapter 10 284", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 297, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p298::c0", "text": "Chapter 11.) In addition, correlation between uit and xis for s 0 t causes FD and FE to be inconsistent. When lagged xit is correlated with uit, we can solve lack of strict exogeneity by including lags and interpreting the equation as a distributed lag model. More problematical is when uit is correlated with future xit: only rarely does putting future values of explanatory variables in an equation lead to an interesting economic model. In Chapter 11 we show how to estimate the parameters consistently when there is feedback from uit to xis, s > t. We can formally test the assumptions underlying the consistency of the FE and FD estimators by using a Hausman test. It might be important to use a robust form of the Hausman test that maintains neither Assumption FE.3 nor Assumption FD.3 under the null hypothesis. This approach is not di‰cult—see Problem 10.6—but we focus here on regression-based tests, which are easier to compute. If T ¼ 2, it is easy to test for strict exogeneity. In the equation Dyi ¼ Dxib þ Dui, neither xi1 nor xi2 should be signiﬁcant as additional explanatory variables in the ﬁrst-di¤erenced equation. We simply add, say, xi2 to the FD equation and carry out an F test for signiﬁcance of xi2. With more than two time periods, a test of strict exogeneity is a test of H0: g ¼ 0 in the expanded equation Dyt ¼ Dxtb þ wtg þ Dut; t ¼ 2; . . . ; T where wt is a subset of xt (that would exclude time dummies). Using the Wald approach, this test can be made robust to arbitrary serial correlation or hetero- skedasticity; under Assumptions FD.1–FD.3 the usual F statistic is asymptotically valid. A test of strict exogeneity using ﬁxed e¤ects, when T > 2, is obtained by specifying the equation yit ¼ xitb þ wi;tþ1d þ ci þ uit; t ¼ 1; 2; . . . ; T \u0003 1 where wi;tþ1 is again a subset of xi;tþ1. Under strict exogeneity, d ¼ 0, and we can carry out the test using ﬁxed e¤ects estimation. (We lose the last time period by leading wit.) An example is given in Problem 10.12. Under strict exogeneity, we can use a GLS procedure on either the time-demeaned equation or the ﬁrst-di¤erenced equation. If the variance matrix of ui is unrestricted, it does not matter which transformation we use. Intuitively, this point is pretty clear, since allowing Eðuiu0 iÞ to be unrestricted places no restrictions on Eð€ui€u0 iÞ or EðDuiDu0 iÞ. Im, Ahn, Schmidt, and Wooldridge (1999) show formally that the FEGLS and FDGLS estimators are asymptotically equivalent under Assumptions FE.1 and FEGLS.3 and the appropriate rank conditions. Basic Linear Unobserved E¤ects Panel Data Models 285", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 298, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p299::c0", "text": "10.7.2 The Relationship between the Random E¤ects and Fixed E¤ects Estimators In cases where the key variables in xt do not vary much over time, ﬁxed e¤ects and ﬁrst-di¤erencing methods can lead to imprecise estimates. We may be forced to use random e¤ects estimation in order to learn anything about the population param- eters. If a random e¤ects analysis is appropriate—that is, if ci is orthogonal to xit— then the random e¤ects estimators can have much smaller variances than the FE or FD estimators. We now obtain an expression for the RE estimator that allows us to compare it with the FE estimator. Using the fact that j0 TjT ¼ T, we can write W under the random e¤ects structure as W ¼ s2 uIT þ s2 c jTj0 T ¼ s2 uIT þ Ts2 c jTðj0 TjTÞ\u00031j0 T ¼ s2 uIT þ Ts2 c PT ¼ ðs2 u þ Ts2 c ÞðPT þ hQTÞ where PT 1 IT \u0003 QT ¼ jTðj0 TjTÞ\u00031j0 T and h 1 s2 u=ðs2 u þ Ts2 c Þ. Next, deﬁne ST 1 PT þ hQT. Then S\u00031 T ¼ PT þ ð1=hÞQT, as can be seen by direct matrix multiplica- tion. Further, S\u00031=2 T ¼ PT þ ð1= ﬃﬃh p ÞQT, because multiplying this matrix by itself gives S\u00031 T (the matrix is clearly symmetric, since PT and QT are symmetric). After simple algebra, it can be shown that S\u00031=2 T ¼ ð1 \u0003 lÞ\u00031½IT \u0003 lPT\u0004, where l ¼ 1 \u0003 ﬃﬃh p . Therefore, W\u00031=2 ¼ ðs2 u þ Ts2 c Þ\u00031=2ð1 \u0003 lÞ\u00031½IT \u0003 lPT\u0004 ¼ ð1=suÞ½IT \u0003 lPT\u0004 where l ¼ 1 \u0003 ½s2 u=ðs2 u þ Ts2 c Þ\u00041=2. Assume for the moment that we know l. Then the RE estimator is obtained by estimating the transformed equation CTyi ¼ CTXib þ CTvi by system OLS, where CT 1 ½IT \u0003 lPT\u0004. Write the transformed equation as \u0001yi ¼ \u0001Xib þ \u0001vi ð10:74Þ The variance matrix of \u0001vi is Eð\u0001vi\u0001v0 iÞ ¼ CTWCT ¼ s2 uIT, which veriﬁes that \u0001vi has variance matrix ideal for system OLS estimation. The tth element of \u0001yi is easily seen to be yit \u0003 lyi, and similarly for \u0001Xi. Therefore, system OLS estimation of equation (10.74) is just pooled OLS estimation of yit \u0003 lyi ¼ ðxit \u0003 lxiÞb þ ðvit \u0003 lviÞ over all t and i. The errors in this equation are serially uncorrelated and homo- skedastic under Assumption RE.3; therefore, they satisfy the key conditions for pooled OLS analysis. The feasible RE estimator replaces the unknown l with its es- timator, ^l, so that ^bRE can be computed from the pooled OLS regression \u0001yit on \u0001xit; t ¼ 1; . . . ; T; i ¼ 1; . . . ; N ð10:75Þ Chapter 10 286", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 299, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p300::c0", "text": "where now \u0001xit ¼ xit \u0003 ^lxi and \u0001yit ¼ yit \u0003 ^lyi, all t and i. Therefore, we can write ^bRE ¼ X N i¼1 X T t¼1 \u0001x0 it\u0001xit !\u00031 X N i¼1 X T t¼1 \u0001x0 it \u0001yit ! ð10:76Þ The usual variance estimate from the pooled OLS regression (10.75), SSR/ðNT \u0003 KÞ, is a consistent estimator of s2 u. The usual t statistics and F statistics from the pooled regression are asymptotically valid under Assumptions RE.1–RE.3. For F tests, we obtain ^l from the unrestricted model. Equation (10.76) shows that the random e¤ects estimator is obtained by a quasi- time demeaning: rather than removing the time average from the explanatory and dependent variables at each t, random e¤ects removes a fraction of the time average. If ^l is close to unity, the random e¤ects and ﬁxed e¤ects estimates tend to be close. To see when this result occurs, write ^l as ^l ¼ 1 \u0003 f1=½1 þ Tð^s2 c =^s2 uÞ\u0004g1=2 ð10:77Þ where ^s2 u and ^s2 c are consistent estimators of s2 u and s2 c (see Section 10.4). When Tð^s2 c =^s2 uÞ is large, the second term in ^l is small, in which case ^l is close to unity. In fact, ^l ! 1 as T ! y or as ^s2 c =^s2 u ! y. For large T, it is not surprising to ﬁnd similar estimates from ﬁxed e¤ects and random e¤ects. Even with small T, random e¤ects can be close to ﬁxed e¤ects if the estimated variance of ci is large relative to the estimated variance of uit, a case often relevant for applications. (As l approaches unity, the precision of the random e¤ects estimator approaches that of the ﬁxed e¤ects estimator, and the e¤ects of time-constant explanatory variables become harder to estimate.) Example 10.7 (Job Training Grants): In Example 10.4, T ¼ 3, ^s2 u A:248, and ^s2 c A1:932, which gives ^lA:797. This helps explain why the RE and FE estimates are reasonably close. Equations (10.76) and (10.77) also show how random e¤ects and pooled OLS are related. Pooled OLS is obtained by setting ^l ¼ 0, which is never exactly true but could be close. In practice, ^l is not usually close to zero because this outcome would require ^s2 u to be large relative to ^s2 c . In Section 10.4 we emphasized that consistency of random e¤ects hinges on the orthogonality between ci and xit. In fact, Assumption POLS.1 is weaker than As- sumption RE.1. We now see, because of the particular transformation used by the RE estimator, that its inconsistency when Assumption RE.1b is violated can be small relative to pooled OLS if s2 c is large relative to s2 u or if T is large. Basic Linear Unobserved E¤ects Panel Data Models 287", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 300, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p301::c0", "text": "If we are primarily interested in the e¤ect of a time-constant variable in a panel data study, the robustness of the FE estimator to correlation between the unobserved e¤ect and the xit is practically useless. Without using an instrumental variables approach—something we take up in Chapter 11—random e¤ects is probably our only choice. Sometimes, applications of the RE estimator attempt to control for the part of ci correlated with xit by including dummy variables for various groups, assuming that we have many observations within each group. For example, if we have panel data on a group of working people, we might include city dummy vari- ables in a wage equation. Or, if we have panel data at the student level, we might in- clude school dummy variables. Including dummy variables for groups controls for a certain amount of heterogeneity that might be correlated with the (time-constant) elements of xit. By using RE, we can e‰ciently account for any remaining serial correlation due to unobserved time-constant factors. (Unfortunately, the language used in empirical work can be confusing. It is not uncommon to see school dummy variables referred to as ‘‘school ﬁxed e¤ects’’ even though they appear in a random e¤ects analysis at the individual level.) Regression (10.75) using the quasi-time-demeaned data has several other practical uses. Since it is just a pooled OLS regression that is asymptotically the same as using l in place of ^l, we can easily obtain standard errors that are robust to arbitrary het- eroskedasticity in ci and uit as well as arbitrary serial correlation in the fuitg. All that is required is an econometrics package that computes robust standard errors, t, and F statistics for pooled OLS regression, such as Stata9. Further, we can use the residuals from regression (10.75), say ^rit, to test for serial correlation in rit 1 vit \u0003 lvi, which are serially uncorrelated under Assumption RE.3a. If we detect serial correlation in fritg, we conclude that Assumption RE.3a is false, and this result means that the uit are serially correlated. Although the arguments are tedious, it can be shown that es- timation of l and b has no e¤ect on the null limiting distribution of the usual (or heteroskedasticity-robust) t statistic from the pooled OLS regression ^rit on ^ri;t\u00031, t ¼ 2; . . . ; T; i ¼ 1; . . . ; N. 10.7.3 The Hausman Test Comparing the RE and FE Estimators Since the key consideration in choosing between a random e¤ects and ﬁxed e¤ects approach is whether ci and xit are correlated, it is important to have a method for testing this assumption. Hausman (1978) proposed a test based on the di¤erence be- tween the random e¤ects and ﬁxed e¤ects estimates. Since FE is consistent when ci and xit are correlated, but RE is inconsistent, a statistically signiﬁcant di¤erence is interpreted as evidence against the random e¤ects assumption RE.1b. Chapter 10 288", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 301, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p302::c0", "text": "Before we obtain the Hausman test, there are two caveats. First, strict exogeneity, Assumption RE.1a, is maintained under the null and the alternative. Correlation between xis and uit for any s and t causes both FE and RE to be inconsistent, and generally their plims will di¤er. A second caveat is that the test is usually implemented assuming that Assumption RE.3 holds under the null. As we will see, this setup implies that the random e¤ects estimator is more e‰cient than the FE estimator, and it simpliﬁes computation of the test statistic. But we must emphasize that Assumption RE.3 is an auxiliary assump- tion, and it is not being tested by the Hausman statistic: the Hausman test has no systematic power against the alternative that Assumption RE.1 is true but Assump- tion RE.3 is false. Failure of Assumption RE.3 causes the usual Hausman test to have a nonstandard limiting distribution, which means the resulting test could have asymptotic size larger or smaller than the nominal size. Assuming that Assumptions RE.1–RE.3 hold, consider the case where xit contains only time-varying elements, since these are the only coe‰cients that we can estimate using ﬁxed e¤ects. Then Avarð ^bFEÞ ¼ s2 u½Eð€X0 i €XiÞ\u0004\u00031=N and Avarð ^bREÞ ¼ s2 u½Eð\u0001X0 i \u0001XiÞ\u0004\u00031=N where the tth row of €Xi is xit \u0003 xi and the tth row of \u0001Xi is xit \u0003 lxi. Now Eð\u0001X0 i \u0001XiÞ \u0003 Eð€X0 i €XiÞ ¼ E½X0 iðIT \u0003 lPTÞXi\u0004 \u0003 E½X0 iðIT \u0003 PTÞXi\u0004 ¼ ð1 \u0003 lÞEðX0 iPTXiÞ ¼ ð1 \u0003 lÞTEðx0 ixiÞ from which it follows that ½Avarð ^bREÞ\u0004\u00031 \u0003 ½Avarð ^bFEÞ\u0004\u00031 is positive deﬁnite, imply- ing that Avarð ^bFEÞ \u0003 Avarð ^bREÞ is positive deﬁnite. Since l ! 1 as T ! y, these expressions show that the asymptotic variance of the RE estimator tends to that of FE as T gets large. The original form of the Hausman statistic can be computed as follows. Let ^dRE denote the vector of random e¤ects estimates without the coe‰cients on time-constant variables or aggregate time variables, and let ^dFE denote the corresponding ﬁxed e¤ects estimates; let these each be M \u0001 1 vectors. Then H ¼ ð^dFE \u0003 ^dREÞ0½A ^ var varð^dFEÞ \u0003 A ^ var varð^dREÞ\u0004\u00031ð^dFE \u0003 ^dREÞ ð10:78Þ is distributed asymptotically as w2 M under Assumptions RE.1–RE.3. A key to estab- lishing the limiting chi-square distribution of H is to show that Avar½ ﬃﬃﬃﬃ N p ð^dFE \u0003 ^dREÞ\u0004 ¼ Avar½ ﬃﬃﬃﬃ N p ð^dFE \u0003 dÞ\u0004 \u0003 Avar½ ﬃﬃﬃﬃ N p ð^dRE \u0003 dÞ\u0004. Newey and McFadden (1994, Section 5.3) provide general su‰cient conditions, which are met by the FE and RE estimators under Assumptions RE.1–RE.3. (We cover these conditions in Chapter 14 in our Basic Linear Unobserved E¤ects Panel Data Models 289", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 302, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p303::c0", "text": "discussion of general e‰ciency issues; see Lemma 14.1 and the surrounding discus- sion.) The usual estimators of Avarð^dFEÞ and Avarð^dREÞ can be used in equation (10.78), but if di¤erent estimates of s2 u are used, the matrix A ^ var varð^dFEÞ \u0003 A ^ var varð^dREÞ need not be positive deﬁnite. Thus it is best to use either the ﬁxed e¤ects estimate or the random e¤ects estimate of s2 u in both places. Often, we are primarly interested in a single parameter, in which case we can use a t statistic that ignores the other parameters. (For example, if one element of xit is a policy variable, and the other elements of xit are just controls or aggregrate time dummies, we may only care about the coe‰cient on the policy variable.) Let d be the element of b that we wish to use in the test. The Hausman test can be computed as a t statistic version of (10.78), ð^dFE \u0003 ^dREÞ=f½seð^dFEÞ\u00042 \u0003 ½seð^dREÞ\u00042g1=2, where the standard errors are computed under the usual assumptions. Under Assumptions RE.1–RE.3, the t statistic has an asymptotic standard normal distribution. For testing more than one parameter, it is often easier to use an F statistic version of the Hausman test. Let \u0001xit and \u0001yit be the quasi-demeaned data deﬁned previously. Let wit denote a 1 \u0001 M subset of time-varying elements of xit (excluding time dum- mies); one can include all elements of xit that vary across i and t or a subset. Let €wit denote the time-demeaned version of wit, and consider the extended model \u0001yit ¼ \u0001xitb þ €witx þ errorit; t ¼ 1; . . . ; T; i ¼ 1; . . . ; N ð10:79Þ where x is an M \u0001 1 vector. The error terms are complicated because ^l replaces l in obtaining the quasi-demeaned data, but they can be treated as being homoskedastic and serially uncorrelated because replacing l with ^l does not matter asymptotically. (This comment is just the usual observation that, in feasible GLS analysis, replacing W with ^W has no e¤ect on the asymptotic distribution of the feasible GLS estimator as N ! y under strict exogeneity.) Now, the Hausman test can be implemented by testing H0: x ¼ 0 using standard pooled OLS analysis. The simplest approach is to compute the F statistic. The restricted SSR is obtained from the pooled regression that can be used to obtain ^bRE, namely regression (10.75). Call this sum of squared residuals SSRr. The unrestricted SSR comes from the pooled estimation of (10.79). Then the F statistic is F ¼ ðSSRr \u0003 SSRurÞ SSRur \u0002 ðNT \u0003 K \u0003 MÞ M ð10:80Þ Under H0 (which is Assumptions RE.1–RE.3 in this case), F can be treated as an FM;NT\u0003K\u0003M random variable (because M \u0002 F @ a w2 M). This statistic turns out to be identical to a statistic derived by Mundlak (1978), who suggested putting wi in place of €wit. Mundlak’s motivation is to test an alternative to Chapter 10 290", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 303, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p304::c0", "text": "Assumption RE.1b of the form Eðci j xiÞ ¼ Eðci j wiÞ ¼ g0 þ wig. The equivalence of the two approaches follows because the regressors ð\u0001xit; €witÞ are just a nonsingular linear transformation of the regressors ð\u0001xit; wiÞ, and so the SSRs in the unrestricted regression are the same; the restricted SSRs are clearly the same. If Assumption RE.3 fails, then a robust form of the Hausman statistic is needed. Probably the easiest approach is to test H0: x ¼ 0 via a robust Wald statistic in the context of pooled OLS estimation of (10.79), or with wi in place of €wi. The robust test should account for serial correlation across time as well as general heteroskedasticity. As in any other context that uses statistical inference, it is possible to get a statis- tical rejection of RE.1b (say, at the 5 percent level) with the di¤erences between the RE and FE estimates being practically small. The opposite case is also possible: there can be seemingly large di¤erences between the random e¤ects and ﬁxed e¤ects esti- mates but, due to large standard errors, the Hausman statistic fails to reject. What should be done in this case? A typical response is to conclude that the random e¤ects assumptions hold and to focus on the RE estimates. Unfortunately, we may be committing a Type II error: failing to reject Assumption RE.1b when it is false. Problems 10.1. Consider a model for new capital investment in a particular industry (say, manufacturing), where the cross section observations are at the county level and there are T years of data for each county: logðinvestitÞ ¼ yt þ zitg þ d1taxit þ d2disasterit þ ci þ uit The variable taxit is a measure of the marginal tax rate on capital in the county, and disasterit is a dummy indicator equal to one if there was a signiﬁcant natural disaster in county i at time period t (for example, a major ﬂood, a hurricane, or an earth- quake). The variables in zit are other factors a¤ecting capital investment, and the yt represent di¤erent time intercepts. a. Why is allowing for aggregate time e¤ects in the equation important? b. What kinds of variables are captured in ci? c. Interpreting the equation in a causal fashion, what sign does economic reasoning suggest for d1? d. Explain in detail how you would estimate this model; be speciﬁc about the assumptions you are making. Basic Linear Unobserved E¤ects Panel Data Models 291", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 304, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p305::c0", "text": "e. Discuss whether strict exogeneity is reasonable for the two variables taxit and disasterit; assume that neither of these variables has a lagged e¤ect on capital investment. 10.2. Suppose you have T ¼ 2 years of data on the same group of N working indi- viduals. Consider the following model of wage determination: logðwageitÞ ¼ y1 þ y2d2t þ zitg þ d1 femalei þ d2d2t \u0002 femalei þ ci þ uit The unobserved e¤ect ci is allowed to be correlated with zit and femalei. The variable d2t is a time period indicator, where d2t ¼ 1 if t ¼ 2 and d2t ¼ 0 if t ¼ 1. In what follows, assume that Eðuit j femalei; zi1; zi2; ciÞ ¼ 0; t ¼ 1; 2 a. Without further assumptions, what parameters in the log wage equation can be consistently estimated? b. Interpret the coe‰cients y2 and d2. c. Write the log wage equation explicitly for the two time periods. Show that the di¤erenced equation can be written as DlogðwageiÞ ¼ y2 þ Dzig þ d2 femalei þ Dui where DlogðwageiÞ ¼ logðwagei2Þ \u0003 logðwagei1Þ, and so on. 10.3. For T ¼ 2 consider the standard unoberved e¤ects model yit ¼ xitb þ ci þ uit; t ¼ 1; 2 Let ^bFE and ^bFD denote the ﬁxed e¤ects and ﬁrst di¤erence estimators, respectively. a. Show that the FE and FD estimates are numerically identical. b. Show that the error variance estimates from the FE and FD methods are numer- ically identical. 10.4. A common setup for program evaluation with two periods of panel data is the following. Let yit denote the outcome of interest for unit i in period t. At t ¼ 1, no one is in the program; at t ¼ 2, some units are in the control group, and others are in the experimental group. Let progit be a binary indicator equal to one if unit i is in the program in period t; by the program design, progi1 ¼ 0 for all i. An unobserved e¤ects model without additional covariates is yit ¼ y1 þ y2d2t þ d1progit þ ci þ uit; Eðuit j progi2; ciÞ ¼ 0 Chapter 10 292", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 305, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p306::c0", "text": "where d2t is a dummy variable equal to unity if t ¼ 2, and zero if t ¼ 1, and ci is the unobserved e¤ect. a. Explain why including d2t is important in these contexts. In particular, what problems might be caused by leaving it out? b. Why is it important to include ci in the equation? c. Using the ﬁrst di¤erencing method, show that ^y2 ¼ Dycontrol and ^d1 ¼ Dytreat \u0003 Dycontrol, where Dycontrol is the average change in y over the two periods for the group with progi2 ¼ 0, and Dytreat is the average change in y for the group where progi2 ¼ 1. This formula shows that ^d1, the di¤erence-in-di¤erences estimator, arises out of an unobserved e¤ects panel data model. d. Write down the extension of the model for T time periods. e. A common way to obtain the DID estimator for two years of panel data is from the model yit ¼ a1 þ a2startt þ a3progi þ d1starttprogi þ uit ð10:81Þ where Eðuit j startt; progiÞ ¼ 0, progi denotes whether unit i is in the program in the second period, and startt is a binary variable indicating when the program starts. In the two-period setup, startt ¼ d2t and progit ¼ starttprogi. The pooled OLS estimator of d1 is the DID estimator from part c. With T > 2, the unobserved e¤ects model from part d and pooled estimation of equation (10.81) no longer generally give the same estimate of the program e¤ect. Which approach do you prefer, and why? 10.5. Assume that Assumptions RE.1 and RE.3a hold, but Varðci j xiÞ 0 VarðciÞ. a. Describe the general nature of Eðviv0 i j xiÞ. b. What are the asymptotic properties of the random e¤ects estimator and the asso- ciated test statistics? How should the random e¤ects statistics be modiﬁed? 10.6. Deﬁne the K \u0001 K symmetric matrices A1 1 EðDX0 iDXiÞ and A2 1 Eð€X0 i €XiÞ, and assume both are positive deﬁnite. Deﬁne ^y 1 ð ^b 0 FD; ^b 0 FEÞ0 and y 1 ðb 0; b 0Þ0, both 2K \u0001 1 vectors. a. Under Assumption FE.1 (and the rank conditions we have given), ﬁnd ﬃﬃﬃﬃ N p ð^y \u0003 yÞ in terms of A1; A2, N\u00031=2 PN i¼1 DX0 iDui, and N\u00031=2 PN i¼1 €X0 i€ui [with a opð1Þ remainder]. b. Explain how to consistently estimate Avar ﬃﬃﬃﬃ N p ð^y \u0003 yÞ without further assumptions. c. Use parts a and b to obtain a robust Hausman statistic comparing the FD and FE estimators. What is the limiting distribution of your statistic under H0? Basic Linear Unobserved E¤ects Panel Data Models 293", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 306, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p307::c0", "text": "10.7. Use the two terms of data in GPA.RAW to estimate an unobserved e¤ects version of the model in Example 7.8. You should drop the variable cumgpa (since this variable violates strict exogeneity). a. Estimate the model by random e¤ects, and interpret the coe‰cient on the in-season variable. b. Estimate the model by ﬁxed e¤ects; informally compare the estimates to the RE estimates, in particular that on the in-season e¤ect. c. Construct the nonrobust Hausman test comparing RE and FE. Include all vari- ables in wit that have some variation across i and t, except for the term dummy. 10.8. Use the data in NORWAY.RAW for the years 1972 and 1978 for a two-year panel data analysis. The model is a simple distributed lag model: logðcrimeitÞ ¼ y0 þ y1d78t þ b1clrprci;t\u00031 þ b2clrprci;t\u00032 þ ci þ uit The variable clrprc is the clear-up percentage (the percentage of crimes solved). The data are stored for two years, with the needed lags given as variables for each year. a. First estimate this equation using a pooled OLS analysis. Comment on the deter- rent e¤ect of the clear-up percentage, including interpreting the size of the coe‰- cients. Test for serial correlation in the composite error vit assuming strict exogeneity (see Section 7.8). b. Estimate the equation by ﬁxed e¤ects, and compare the estimates with the pooled OLS estimates. Is there any reason to test for serial correlation? Obtain heteroskedasticity-robust standard errors for the FE estimates. c. Using FE analysis, test the hypothesis H0: b1 ¼ b2. What do you conclude? If the hypothesis is not rejected, what would be a more parsimonious model? Estimate this model. 10.9. Use the data in CORNWELL.RAW for this problem. a. Estimate both a random e¤ects and a ﬁxed e¤ects version of the model in Problem 7.11a. Compute the regression-based version of the Hausman test comparing RE and FE. b. Add the wage variables (in logarithmic form), and test for joint signiﬁcance after estimation by ﬁxed e¤ects. c. Estimate the equation by ﬁrst di¤erencing, and comment on any notable changes. Do the standard errors change much between ﬁxed e¤ects and ﬁrst di¤erencing? d. Test the ﬁrst-di¤erenced equation for AR(1) serial correlation. Chapter 10 294", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 307, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p308::c0", "text": "10.10. An unobserved e¤ects model explaining current murder rates in terms of the number of executions in the last three years is mrdrteit ¼ yt þ b1execit þ b2unemit þ ci þ uit where mrdrteit is the number of murders in state i during year t, per 10,000 people; execit is the total number of executions for the current and prior two years; and unemit is the current unemployment rate, included as a control. a. Using the data in MURDER.RAW, estimate this model by ﬁrst di¤erencing. Notice that you should allow di¤erent year intercepts. Test the errors in the ﬁrst- di¤erenced equation for serial correlation. b. Estimate the model by ﬁxed e¤ects. Are there any important di¤erences from the FD estimates? c. Under what circumstances would execit not be strictly exogenous (conditional on ci)? 10.11. Use the data in LOWBIRTH.RAW for this question. a. For 1987 and 1990, consider the state-level equation lowbrthit ¼ y1 þ y2d90t þ b1afdcprcit þ b2 logðphypcitÞ þ b3 logðbedspcitÞ þ b4 logðpcincitÞ þ b5 logðpopulitÞ þ ci þ uit where the dependent variable is percentage of births that are classiﬁed as low birth weight and the key explanatory variable is afdcprc, the percentage of the population in the welfare program, Aid to Families with Dependent Children (AFDC). The other variables, which act as controls for quality of health care and income levels, are physicians per capita, hospital beds per capita, per capita income, and population. Interpretating the equation causally, what sign should each bj have? (Note: Partici- pation in AFDC makes poor women eligible for nutritional programs and prenatal care.) b. Estimate the preceding equation by pooled OLS, and discuss the results. You should report the usual standard errors and serial correlation–robust standard errors. c. Di¤erence the equation to eliminate the state ﬁxed e¤ects, ci, and reestimate the equation. Interpret the estimate of b1 and compare it to the estimate from part b. What do you make of ^b2? d. Add afdcprc2 to the model, and estimate it by FD. Are the estimates on afdcprc and afdcprc2 sensible? What is the estimated turning point in the quadratic? Basic Linear Unobserved E¤ects Panel Data Models 295", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 308, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p309::c0", "text": "10.12. The data in WAGEPAN.RAW are from Vella and Verbeek (1998) for 545 men who worked every year from 1980 to 1987. Consider the wage equation logðwageitÞ ¼ yt þ b1educi þ b2blacki þ b3hispani þ b4experit þ b5exper2 it þ b6marriedit þ b7unionit þ ci þ uit The variables are described in the data set. Notice that education does not change over time. a. Estimate this equation by pooled OLS, and report the results in standard form. Are the usual OLS standard errors reliable, even if ci is uncorrelated with all ex- planatory variables? Explain. Compute appropriate standard errors. b. Estimate the wage equation by random e¤ects. Compare your estimates with the pooled OLS estimates. c. Now estimate the equation by ﬁxed e¤ects. Why is experit redundant in the model even though it changes over time? What happens to the marriage and union pre- miums as compared with the random e¤ects estimates? d. Now add interactions of the form d81\u0002educ, d82\u0002educ; . . . ; d87\u0002educ and estimate the equation by ﬁxed e¤ects. Has the return to education increased over time? e. Return to the original model estimated by ﬁxed e¤ects in part c. Add a lead of the union variable, unioni;tþ1 to the equation, and estimate the model by ﬁxed e¤ects (note that you lose the data for 1987). Is unioni;tþ1 signiﬁcant? What does this result say about strict exogeneity of union membership? 10.13. Consider the standard linear unobserved e¤ects model (10.11), under the assumptions Eðuit j xi; hi; ciÞ ¼ 0; Varðuit j xi; hi; ciÞ ¼ s2 uhit; t ¼ 1; . . . ; T where hi ¼ ðhi1; . . . ; hiTÞ. In other words, the errors display heteroskedasticity that depends on hit. (In the leading case, hit is a function of xit.) Suppose you estimate b by minimizing the weighted sum of squared residuals X N i¼1 X T t¼1 ðyit \u0003 a1d1i \u0003 \u0002 \u0002 \u0002 \u0003 aNdNi \u0003 xitbÞ2=hit with respect to the ai, i ¼ 1; . . . ; N and b, where dni ¼ 1 if i ¼ n. (This would seem to be the natural analogue of the dummy variable regression, modiﬁed for known het- eroskedasticity.) Can you justify this procedure with ﬁxed T as N ! y? Chapter 10 296", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 309, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p310::c0", "text": "10.14. Suppose that we have the unobserved e¤ects model yit ¼ a þ xitb þ zig þ hi þ uit where the xitð1 \u0001 KÞ are time-varying, the zið1 \u0001 MÞ are time-constant, Eðuit j xi; zi; hiÞ ¼ 0, t ¼ 1; . . . ; T, and Eðhi j xi; ziÞ ¼ 0. Let s2 h ¼ VarðhiÞ and s2 u ¼ VarðuitÞ. If we estimate b by ﬁxed e¤ects, we are estimating the equation yit ¼ xitb þ ci þ uit, where ci ¼ a þ zig þ hi. a. Find s2 c 1 VarðciÞ. Show that s2 c is at least as large as s2 h, and usually strictly larger. b. Explain why estimation of the model by ﬁxed e¤ects will lead to a larger estimated variance of the unobserved e¤ect than if we estimate the model by random e¤ects. Does this result make intuitive sense? Basic Linear Unobserved E¤ects Panel Data Models 297", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 310, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p311::c0", "text": "11 More Topics in Linear Unobserved E¤ects Models This chapter continues our treatment of linear, unobserved e¤ects panel data models. We ﬁrst cover estimation of models where the strict exogeneity Assumption FE.1 fails but sequential moment conditions hold. A simple approach to consistent esti- mation involves di¤erencing combined with instrumental variables methods. We also cover models with individual slopes, where unobservables can interact with explana- tory variables, and models where some of the explanatory variables are assumed to be orthogonal to the unobserved e¤ect while others are not. The ﬁnal section in this chapter brieﬂy covers some non-panel-data settings where unobserved e¤ects models and panel data estimation methods can be used. 11.1 Unobserved E¤ects Models without the Strict Exogeneity Assumption 11.1.1 Models under Sequential Moment Restrictions In Chapter 10 all the estimation methods we studied assumed that the explanatory variables were strictly exogenous (conditional on an unobserved e¤ect in the case of ﬁxed e¤ects and ﬁrst di¤erencing). As we saw in the examples in Section 10.2.3, strict exogeneity rules out certain kinds of feedback from yit to future values of xit. Gen- erally, random e¤ects, ﬁxed e¤ects, and ﬁrst di¤erencing are inconsistent if an ex- planatory variable in some time period is correlated with uit. While the size of the inconsistency might be small—something we will investigate further—in other cases it can be substantial. Therefore, we should have general ways of obtaining consistent estimators as N ! y with T ﬁxed when the explanatory variables are not strictly exogenous. The model of interest can still be written as yit ¼ xitb þ ci þ uit; t ¼ 1; 2; . . . ; T ð11:1Þ but, in addition to allowing ci and xit to be arbitrarily correlated, we now allow uit to be correlated with future values of the explanatory variables, ðxi;tþ1; xi;tþ2; . . . ; xiTÞ. We saw in Example 10.3 that uit and xi;tþ1 must be correlated because xi;tþ1 ¼ yit. Nevertheless, there are many models, including the AR(1) model, for which it is reasonable to assume that uit is uncorrelated with current and past values of xit. Following Chamberlain (1992b), we introduce sequential moment restrictions: Eðuit j xit; xi;t\u00011; . . . ; xi1; ciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð11:2Þ When assumption (11.2) holds, we will say that the xit are sequentially exogenous conditional on the unobserved e¤ect.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 311, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p312::c0", "text": "Given model (11.1), assumption (11.2) is equivalent to Eðyit j xit; xi;t\u00011; . . . ; xi1; ciÞ ¼ Eðyit j xit; ciÞ ¼ xitb þ ci ð11:3Þ which makes it clear what sequential exogeneity implies about the explanatory vari- ables: after xit and ci have been controlled for, no past values of xit a¤ect the expected value of yit. This condition is more natural than the strict exogeneity assumption, which requires conditioning on future values of xit as well. Example 11.1 (Dynamic Unobserved E¤ects Model): An AR(1) model with addi- tional explanatory variables is yit ¼ zitg þ r1 yi;t\u00011 þ ci þ uit ð11:4Þ and so xit 1 ðzit; yi;t\u00011Þ. Therefore, ðxit; xi;t\u00011; . . . ; xi1Þ ¼ ðzit; yi;t\u00011; zi;t\u00011; . . . ; zi1; yi0Þ, and the sequential exogeneity assumption (11.3) requires Eðyit j zit; yi;t\u00011; zi;t\u00011; . . . ; zi1; yi0; ciÞ ¼ Eðyit j zit; yi;t\u00011; ciÞ ¼ zitg þ r1 yi;t\u00011 þ ci ð11:5Þ An interesting hypothesis in this model is H0: r1 ¼ 0, which means that, after unob- served heterogeneity, ci, has been controlled for (along with current and past zit), yi;t\u00011 does not help to predict yit. When r1 0 0, we say that fyitg exhibits state de- pendence: the current state depends on last period’s state, even after controlling for ci and ðzit; . . . ; zi1Þ. In this example, assumption (11.5) is an example of dynamic completeness condi- tional on ci; we covered the unconditional version of dynamic completeness in Section 7.8.2. It means that one lag of yit is su‰cient to capture the dynamics in the con- ditional expectation; neither further lags of yit nor lags of zit are important once ðzit; yi;t\u00011; ciÞ have been controlled for. In general, if xit contains yi;t\u00011, then as- sumption (11.3) implies dynamic completeness conditional on ci. Assumption (11.3) does not require that zi;tþ1 . . . ; ziT be uncorrelated with uit, so that feedback is allowed from yit to ðzi;tþ1; . . . ; ziTÞ. If we think that zis is uncorre- lated with uit for all s, then additional orthogonality conditions can be used. Finally, we do not need to restrict the value of r1 in any way because we are doing ﬁxed-T asymptotics; the arguments from Section 7.8.3 are also valid here. Example 11.2 (Static Model with Feedback): Consider a static panel data model yit ¼ zitg þ dwit þ ci þ uit ð11:6Þ where zit is strictly exogenous and wit is sequentially exogenous: Chapter 11 300", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 312, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p313::c0", "text": "Eðuit j zi; wit; wi;t\u00011; . . . ; wi1; ciÞ ¼ 0 ð11:7Þ However, wit is inﬂuenced by past yit, as in this case: wit ¼ zitx þ r1 yi;t\u00011 þ cci þ rit ð11:8Þ For example, let yit be per capita condom sales in city i during year t, and let wit be the HIV infection rate for year t. Model (11.6) can be used to test whether condom usage is inﬂuenced by the spread of HIV. The unobserved e¤ect ci contains city- speciﬁc unobserved factors that can a¤ect sexual conduct, as well as the incidence of HIV. Equation (11.8) is one way of capturing the fact that the spread of HIV is in- ﬂuenced by past condom usage. Generally, if Eðri;tþ1uitÞ ¼ 0, it is easy to show that Eðwi;tþ1uitÞ ¼ r1EðyituitÞ ¼ r1Eðu2 itÞ > 0 under equations (11.7) and (11.8), and so strict exogeneity fails unless r1 ¼ 0. Lagging variables that are thought to violate strict exogeneity can mitigate but does not usually solve the problem. Suppose we use wi;t\u00011 in place of wit in equation (11.6) because we think wit might be correlated with uit. For example, let yit be the percentage of ﬂights canceled by airline i during year t, and let wi;t\u00011 be airline proﬁts during the previous year. In this case xi;tþ1 ¼ ðzi;tþ1; witÞ, and so xi;tþ1 is correlated with uit; this fact results in failure of strict exogeneity. In the airline example this issue may be important: poor airline performance this year (as measured by canceled ﬂights) can a¤ect proﬁts in subsequent years. Nevertheless, the sequential exogeneity condition (11.2) is reasonable. Keane and Runkle (1992) argue that panel data models for testing rational expectations using individual-level data generally do not satisfy the strict exogeneity requirement. But they do satisfy sequential exogeneity: in fact, in the conditioning set in assumption (11.2), we can include all variables observed at time t \u0001 1. What happens if we apply the standard ﬁxed e¤ects estimator when the strict exo- geneity assumption fails? Generally, plimð ^bFEÞ ¼ b þ T \u00011 X T t¼1 Eð€x0 it€xitÞ \" #\u00011 T \u00011 X T t¼1 Eð€x0 ituitÞ \" # where €xit ¼ xit \u0001 xi, as in Chapter 10 (i is a random draw from the cross section). Now, under sequential exogeneity, Eð€x0 ituitÞ ¼ E½ðxit \u0001 xiÞ0uit\u0002 ¼ \u0001EðxiuitÞ because Eðx0 ituitÞ ¼ 0, and so T \u00011 PT t¼1 Eð€x0 ituitÞ ¼ \u0001T \u00011 PT t¼1 EðxiuitÞ ¼ \u0001EðxiuiÞ. We can bound the size of the inconsistency as a function of T if we assume that the time series process is appropriately stable and weakly dependent. Under such assumptions, T \u00011 PT t¼1 Eð€x0 it€xitÞ is bounded. Further, VarðxiÞ and VarðuiÞ are of order T \u00011. By the More Topics in Linear Unobserved E¤ects Models 301", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 313, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p314::c0", "text": "Cauchy-Schwartz inequality (for example, Davidson, 1994, Chapter 9), jEðxijuiÞj a ½VarðxijÞVarðuiÞ\u00021=2 ¼ OðT \u00011Þ. Therefore, under bounded moments and weak de- pendence assumptions, the inconsistency from using ﬁxed e¤ects when the strict exogeneity assumption fails is of order T \u00011. With large T the bias may be minimal. See Hamilton (1994) and Wooldridge (1994) for general discussions of weak depen- dence for time series processes. Hsiao (1986, Section 4.2) works out the inconsistency in the FE estimator for the AR(1) model. The key stability condition su‰cient for the bias to be of order T \u00011 is jr1j < 1. However, for r1 close to unity, the bias in the FE estimator can be sizable, even with fairly large T. Generally, if the process fxitg has very persistent elements— which is often the case in panel data sets—the FE estimator can have substantial bias. If our choice were between ﬁxed e¤ects and ﬁrst di¤erencing, we would tend to prefer ﬁxed e¤ects because, when T > 2, FE can have less bias as N ! y. To see this point, write plimð ^bFDÞ ¼ b þ T \u00011 X T t¼1 EðDx0 itDxitÞ \" #\u00011 T \u00011 X T t¼1 EðDx0 itDuitÞ \" # ð11:9Þ If fxitg is weakly dependent, so is fDxitg, and so the ﬁrst average in equation (11.9) is bounded as a function of T. (In fact, under stationarity, this average does not depend on T.) Under assumption (11.2), we have EðDx0 itDuitÞ ¼ Eðx0 ituitÞ þ Eðx0 i;t\u00011ui;t\u00011Þ \u0001 Eðx0 i;t\u00011uitÞ \u0001 Eðx0 itui;t\u00011Þ ¼ \u0001Eðx0 itui;t\u00011Þ which is generally di¤erent from zero. Under stationarity, Eðx0 itui;t\u00011Þ does not de- pend on t, and so the second average in equation (11.9) is constant. This result shows not only that the FD estimator is inconsistent, but also that its inconsistency does not depend on T. As we showed previously, the time demeaning underlying FE results in its bias being on the order of T \u00011. But we should caution that this analysis assumes that the original series, fðxit; yitÞ: t ¼ 1; . . . ; Tg, is weakly dependent. Without this assumption, the inconsistency in the FE estimator cannot be shown to be of order T \u00011. If we make certain assumptions, we do not have to settle for estimators that are inconsistent with ﬁxed T. A general approach to estimating equation (11.1) under assumption (11.2) is to use a transformation to remove ci, but then search for in- strumental variables. The FE transformation can be used provided that strictly ex- ogenous instruments are available (see Problem 11.9). For models under sequential exogeneity assumptions, ﬁrst di¤erencing is more attractive. Chapter 11 302", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 314, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p315::c0", "text": "First di¤erencing equation (11.1) gives Dyit ¼ Dxitb þ Duit; t ¼ 2; 3; . . . ; T ð11:10Þ Now, under assumption (11.2), Eðx0 isuitÞ ¼ 0; s ¼ 1; 2; . . . ; t ð11:11Þ Assumption (11.11) implies the orthogonality conditions Eðx0 isDuitÞ ¼ 0; s ¼ 1; 2; . . . ; t \u0001 1 ð11:12Þ so at time t we can use xo i;t\u00011 as potential instruments for Dxit, where xo it 1 ðxi1; xi2; . . . ; xitÞ ð11:13Þ The fact that xo i;t\u00011 is uncorrelated with Duit opens up a variety of estimation procedures. For example, a simple estimator uses Dxi;t\u00011 as the instruments for Dxit: EðDx0 i;t\u00011DuitÞ ¼ 0 under assumption (11.12), and the rank condition rank EðDx0 i;t\u00011DxitÞ ¼ K is usually reasonable. Then, the equation Dyit ¼ Dxitb þ Duit; t ¼ 3; . . . ; T ð11:14Þ can be estimated by pooled 2SLS using instruments Dxi;t\u00011. This choice of instru- ments loses an additional time period. If T ¼ 3, estimation of equation (11.14) becomes 2SLS on a cross section: ðxi2 \u0001 xi1Þ is used as instruments for ðxi3 \u0001 xi2Þ. When T > 3, equation (11.14) is a pooled 2SLS procedure. There is a set of assumptions—the sequential exogeneity analogues of Assumptions FD.1–FD.3— under which the usual 2SLS statistics obtained from the pooled 2SLS estimation are valid; see Problem 11.8 for details. With Dxi;t\u00011 as the instruments, equation (11.14) is just identiﬁed. Rather than use changes in lagged xit as instruments, we can use lagged levels of xit. For example, choosing ðxi;t\u00011; xi;t\u00012Þ as instruments at time t is no less e‰cient than the procedure that uses Dxi;t\u00011, as the latter is a linear combination of the for- mer. It also gives K overidentifying restrictions that can be used to test assumption (11.2). (There will be fewer than K if xit contains time dummies.) When T ¼ 2, b may be poorly identiﬁed. The equation is Dyi2 ¼ Dxi2b þ Dui2, and, under assumption (11.2), xi1 is uncorrelated with Dui2. This is a cross section equation that can be estimated by 2SLS using xi1 as instruments for Dxi2. The esti- mator in this case may have a large asymptotic variance because the correlations between xi1, the levels of the explanatory variables, and the di¤erences Dxi2 ¼ xi2 \u0001 xi1 are often small. Of course, whether the correlation is su‰cient to yield small enough standard errors depends on the application. More Topics in Linear Unobserved E¤ects Models 303", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 315, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p316::c0", "text": "Even with large T, the available IVs may be poor in the sense that they are not highly correlated with Dxit. As an example, consider the AR(1) model (11.4) without zit: yit ¼ r1 yi;t\u00011 þ ci þ uit; Eðuit j yi;t\u00011; . . . ; yi0; ciÞ ¼ 0, t ¼ 1; 2; . . . ; T. Dif- ferencing to eliminate ci gives Dyit ¼ r1Dyi;t\u00011 þ Duit, t b 2. At time t, all elements of ðyi;t\u00012; . . . ; yi0Þ are IV candidates because Duit is uncorrelated with yi;t\u0001h, h b 2. Anderson and Hsiao (1982) suggested pooled IV with instruments yi;t\u00012 or Dyi;t\u00012, whereas Arellano and Bond (1991) proposed using the entire set of instruments in a GMM procedure. Now, suppose that r1 ¼ 1 and, in fact, there is no unobserved e¤ect. Then Dyi;t\u00011 is uncorrelated with any variable dated at time t \u0001 2 or earlier, and so the elements of ðyi;t\u00012; . . . ; yi0Þ cannot be used as IVs for Dyi;t\u00011. What this conclusion shows is that we cannot use IV methods to test H0: r1 ¼ 1 in the absence of an unobserved e¤ect. Even if r1 < 1, IVs from ðyi;t\u00012; . . . ; yi0Þ tend to be weak if r1 is close to one. Recently, Arellano and Bover (1995) and Ahn and Schmidt (1995) suggested addi- tional orthogonality conditions that improve the e‰ciency of the GMM estimator, but these are nonlinear in the parameters. (In Chapter 14 we will see how to use these kinds of moment restrictions.) Blundell and Bond (1998) obtained additional linear moment restrictions in the levels equation yit ¼ r1 yi;t\u00011 þ vit, vit ¼ ci þ uit. The ad- ditional restrictions are based on yi0 being drawn from a steady-state distribution, and they are especially helpful in improving the e‰ciency of GMM for r1 close to one. (Actually, the Blundell-Bond orthogonality conditions are valid under weaker assumptions.) See also Hahn (1999). Of course, when r1 ¼ 1, it makes no sense to assume that there is a steady-state distribution. In Chapter 13 we cover conditional maximum likelihood methods that can be applied to the AR(1) model. A general feature of pooled 2SLS procedures where the dimension of the IVs is constant across t is that they do not use all the instruments available in each time period; therefore, they cannot be expected to be e‰cient. The optimal procedure is to use expression (11.13) as the instruments at time t in a GMM procedure. Write the system of equations as Dyi ¼ DXib þ Dui ð11:15Þ using the same deﬁnitions as in Section 10.6. Deﬁne the matrix of instruments as Zi ¼ xo i1 0 0 \u0003 \u0003 \u0003 0 0 xo i2 0 \u0003 \u0003 \u0003 0 ... ... 0 0 0 \u0003 \u0003 \u0003 xo i;T\u00011 0 B B B B @ 1 C C C C A ð11:16Þ where xo it is deﬁned in expression (11.13). Note that Zi has T \u0001 1 rows to correspond Chapter 11 304", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 316, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p317::c0", "text": "with the T \u0001 1 time periods in the system (11.15). Since each row contains di¤erent instruments, di¤erent instruments are used for di¤erent time periods. E‰cient estimation of b now proceeds in the GMM framework from Chapter 8 with instruments (11.16). Without further assumptions, the unrestricted weighting matrix should be used. In most applications there is a reasonable set of assumptions under which EðZ0 ieie0 iZiÞ ¼ EðZ0 iWZiÞ ð11:17Þ where ei 1 Dui and W 1 Eðeie0 iÞ. Recall from Chapter 8 that assumption (11.17) is the assumption under which the GMM 3SLS estimator is the asymptotically e‰cient GMM estimator (see Assumption SIV.5). The full GMM analysis is not much more di‰cult. The traditional form of 3SLS estimator that ﬁrst transforms the instruments should not be used because it is not consistent under assumption (11.2). As a practical matter, the column dimension of Zi can be very large, making GMM estimation di‰cult. In addition, GMM estimators—including 2SLS and 3SLS—using many overidentifying restrictions are known to have poor ﬁnite sample properties (see, for example, Tauchen, 1986; Altonji and Segal, 1996; and Ziliak, 1997). In practice, it may be better to use a couple of lags rather than lags back to t ¼ 1. Example 11.3 (Testing for Persistence in County Crime Rates): We use the data in CORNWELL.RAW to test for state dependence in county crime rates, after allow- ing for unobserved county e¤ects. Thus, the model is equation (11.4) with yit 1 logðcrmrteitÞ but without any other explanatory variables. As instruments for Dyi;t\u00011, we use ðyi;t\u00012; yi;t\u00013Þ. Further, so that we do not have to worry about correcting the standard error for possible serial correlation in Duit, we use just the 1986–1987 dif- ferenced equation. The F statistic for joint signiﬁcance of yi;t\u00012; yi;t\u00013 in the reduced form for Dyi;t\u00011 yields p-value ¼ :023, although the R-squared is only .083. The 2SLS estimates of the ﬁrst-di¤erenced equation are Dlogðc^rmrteÞ ¼ :065 ð:040Þ þ :212 ð:497Þ DlogðcrmrteÞ\u00011; N ¼ 90 so that we cannot reject H0: r1 ¼ 0 ðt ¼ :427Þ: 11.1.2 Models with Strictly and Sequentially Exogenous Explanatory Variables Estimating models with both strictly exogenous and sequentially exogenous variables is not di‰cult. For t ¼ 1; 2; . . . ; T, suppose that yit ¼ zitg þ witd þ ci þ uit ð11:18Þ Assume that zis is uncorrelated with uit for all s and t, but that uit is uncorrelated with More Topics in Linear Unobserved E¤ects Models 305", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 317, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p318::c0", "text": "wis only for s a t; su‰cient is Eðuit j zi; wit; wi;t\u00011; . . . ; wi1Þ ¼ 0. This model covers many cases of interest, including when wit contains a lagged dependent variable. After ﬁrst di¤erencing we have Dyit ¼ Dzitg þ Dwitd þ Duit ð11:19Þ and the instruments available at time t are ðzi; wi;t\u00011; . . . ; wi1Þ. In practice, so that there are not so many overidentifying restrictions, we might replace zi with Dzit and choose something like ðDzit; wi;t\u00011; wi;t\u00012Þ as the instruments at time t. Or, zit and a couple of lags of zit can be used. In the AR(1) model (11.4), this approach would mean something like ðzit; zi;t\u00011; zi;t\u00012; yi;t\u00012; yi;t\u00013Þ. We can even use leads of zit, such as zi;tþ1, when zit is strictly exogenous. Such choices are amenable to a pooled 2SLS procedure to estimate g and d. Of course, whether or not the usual 2SLS standard errors are valid depends on serial correlation and variance properties of Duit. Never- theless, assuming that the changes in the errors are (conditionally) homoskedastic and serially uncorrelated is a reasonable start. Example 11.4 (E¤ects of Enterprise Zones): Papke (1994) uses several di¤erent panel data models to determine the e¤ect of enterprise zone designation on economic outcomes for 22 communities in Indiana. One model she uses is yit ¼ yt þ r1 yi;t\u00011 þ d1ezit þ ci þ uit ð11:20Þ where yit is the log of unemployment claims. The coe‰cient of interest is on the binary indicator ezit, which is unity if community i in year t was designated as an enterprise zone. The model holds for the years 1981 to 1988, with yi0 corresponding to 1980, the ﬁrst year of data. Di¤erencing gives Dyit ¼ xt þ r1Dyi;t\u00011 þ d1Dezit þ Duit ð11:21Þ The di¤erenced equation has new time intercepts, but as we are not particularly interested in these, we just include year dummies in equation (11.21). Papke estimates equation (11.21) by 2SLS, using Dyi;t\u00012 as an instrument for Dyi;t\u00011; because of the lags used, equation (11.21) can be estimated for six years of data. The enterprise zone indicator is assumed to be strictly exogenous in equation (11.20), and so Dezit acts as its own instrument. Strict exogeneity of ezit is valid be- cause, over the years in question, each community was a zone in every year following initial designation: future zone designation did not depend on past performance. The estimated equation in ﬁrst di¤erences is Dlog ^ ðuclmsÞ ¼ ^xt þ :165 ð:288Þ DlogðuclmsÞ\u00011 \u0001 :219 ð:106Þ Dez Chapter 11 306", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 318, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p319::c0", "text": "where the intercept and year dummies are supressed for brevity. Based on the usual pooled 2SLS standard errors, ^r1 is not signiﬁcant (or practially very large), while ^d1 is economically large and statistically signiﬁcant at the 5 percent level. If the uit in equation (11.20) are serially uncorrelated, then, as we saw in Chapter 10, Duit must be serially correlated. Papke found no important di¤erences when the standard error for ^d1 was adjusted for serial correlation and heteroskedasticity. In the pure AR(1) model, using lags of yit as an instrument for Dyi;t\u00011 means that we are assuming the AR(1) model captures all of the dynamics. If further lags of yit are added to the structural model, then we must go back even further to obtain instruments. If strictly exogenous variables appear in the model along with yi;t\u00011— such as in equation (11.4)—then lags of zit are good candidates as instruments for Dyi;t\u00011. Much of the time inclusion of yi;t\u00011 (or additional lags) in a model with other explanatory variables is intended to simply control for another source of omitted variables bias; Example 11.4 falls into this class. Things are even trickier in ﬁnite distributed lag models. Consider the patents-R&D model of Example 10.2: after ﬁrst di¤erencing, we have Dpatentsit ¼ Dyt þ Dzitg þ d0DRDit þ \u0003 \u0003 \u0003 þ d5DRDi;t\u00015 þ Duit ð11:22Þ If we are concerned that strict exogeneity fails because of feedback from uit to future R&D expenditures, then DRDit and Duit are potentially correlated (because ui;t\u00011 and RDit are correlated). Assuming that the distributed lag dynamics are correct—and assuming strict exogeneity of zit—all other explanatory variables in equation (11.22) are uncorrelated with Duit. What can we use as an instrument for DRDit in equation (11.22)? We can include RDi;t\u00011; RDi;t\u00012; . . . in the instrument list at time t (along with all of zi). This approach identiﬁes the parameters under the assumptions made, but it is problematic. What if we have the distributed lag dynamics wrong, so that six lags, rather than ﬁve, belong in the structural model? Then choosing additional lags of RDit as instruments fails. If DRDit is su‰ciently correlated with the elements of zis for some s, then using all of zi as instruments can help. Generally, some exogenous factors either in zit or from outside the structural equation are needed for a convincing analysis. 11.1.3 Models with Contemporaneous Correlation between Some Explanatory Variables and the Idiosyncratic Error Consider again model (11.18), where zit is strictly exogenous in the sense that Eðz0 isuitÞ ¼ 0; all s; t ð11:23Þ More Topics in Linear Unobserved E¤ects Models 307", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 319, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p320::c0", "text": "but where we allow wit to be contemporaneously correlated with uit. This correlation can be due to any of the three problems that we studied earlier: omission of an im- portant time-varying explanatory variable, measurement error in some elements of wit, or simultaneity between yit and one or more elements of wit. We assume that equation (11.18) is the equation of interest. In a simultaneous equations model with panel data, equation (11.18) represents a single equation. A system approach is also possible. See, for example, Baltagi (1981); Cornwell, Schmidt, and Wyhowski (1992); and Kinal and Lahiri (1993). Example 11.5 (E¤ects of Smoking on Earnings): A panel data model to examine the e¤ects of cigarette smoking on earnings is logðwageitÞ ¼ zitg þ d1cigsit þ ci þ uit ð11:24Þ (For an empirical analysis, see Levine, Gustafson, and Velenchik, 1997.) As always, we would like to know the causal e¤ect of smoking on hourly wage. For concrete- ness, assume cigsit is measured as average packs per day. This equation has a causal interpretation: holding ﬁxed the factors in zit and ci, what is the e¤ect of an exoge- nous change in cigarette smoking on wages? Thus equation (11.24) is a structural equation. The presence of the individual heterogeneity, ci, in equation (11.24) recognizes that cigarette smoking might be correlated with individual characteristics that also a¤ect wage. An additional problem is that cigsit might also be correlated with uit, some- thing we have not allowed so far. In this example the correlation could be from a variety of sources, but simultaneity is one possibility: if cigarettes are a normal good, then, as income increases—holding everything else ﬁxed—cigarette consumption increases. Therefore, we might add another equation to equation (11.24) that reﬂects that cigsit may depend on income, which clearly depends on wage. If equation (11.24) is of interest, we do not need to add equations explicitly, but we must ﬁnd some in- strumental variables. To get an estimable model, we must ﬁrst deal with the presence of ci, since it might be correlated with zit as well as cigsit. In the general model (11.18), either the FE or FD transformations can be used to eliminate ci before addressing the correlation be- tween wit and uit. If we ﬁrst di¤erence, as in equation (11.19), we can use the entire vector zi as valid instruments in equation (11.19) because zit is strictly exogenous. Neither wit nor wi;t\u00011 is valid as instruments at time t, but it could be that wi;t\u00012 is valid, provided we assume that uit is uncorrelated with wis for s < t. This assumption means that wit has only a contemporaneous e¤ect on yit, something that is likely to be false in example 11.5. [If smoking a¤ects wages, the e¤ects are likely to be deter- Chapter 11 308", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 320, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p321::c0", "text": "mined by prior smoking behavior as well as current smoking behavior. If we include a measure of past smoking behavior in equation (11.24), then this must act as its own instrument in a di¤erenced equation, and so using cigsis for s < t as IVs becomes untenable.] Another thought is to use lagged values of yit as instruments, but this approach e¤ectively rules out serial correlation in uit. In the wage equation (11.24), it would mean that lagged wage does not predict current wage, once ci and the other variables are controlled for. If this assumption is false, using lags of yit is not a valid way of identifying the parameters. If zi is the only valid set of instruments for equation (11.18), the analysis probably will not be convincing: it relies on Dwit being correlated with some linear combina- tion of zi other than Dzit. Such partial correlation is likely to be small, resulting in poor IV estimators; see Problem 11.2. Perhaps the most convincing possibility for obtaining additional instruments is to follow the standard SEM approach from Chapter 9: use exclusion restrictions in the structural equations. For example, we can hope to ﬁnd exogenous variables that do not appear in equation (11.24) but that do a¤ect cigarette smoking. The local price of cigarettes (or level of cigarette taxes) is one possibility. Such variables can usually be considered strictly exogenous, unless we think people change their residence based on the price of cigarettes. If we di¤erence equation (11.24) we get DlogðwageitÞ ¼ Dzitg þ d1Dcigsit þ Duit ð11:25Þ Now, for each t, we can study identiﬁcation of this equation just as in the cross sec- tional case: we must ﬁrst make sure the order condition holds, and then argue (or test) that the rank condition holds. Equation (11.25) can be estimated using a pooled 2SLS analysis, where corrections to standard errors and test statistics for hetero- skedasticity or serial correlation might be warranted. With a large cross section, a GMM system procedure that exploits general heteroskedasticity and serial correla- tion in Duit can be used instead. Example 11.6 (E¤ects of Prison Population on Crime Rates): In order to estimate the causal e¤ect of prison population increases on crime rates at the state level, Levitt (1996) uses instances of prison overcrowding litigation as instruments for the growth in prison population. The equation Levitt estimates is in ﬁrst di¤erences. We can write an underlying unobserved e¤ects model as logðcrimeitÞ ¼ yt þ b1 logðprisonitÞ þ xitg þ ci þ uit ð11:26Þ More Topics in Linear Unobserved E¤ects Models 309", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 321, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p322::c0", "text": "where yt denotes di¤erent time intercepts and crime and prison are measured per 100,000 people. (The prison population variable is measured on the last day of the previous year.) The vector xit contains other controls listed in Levitt, including mea- sures of police per capita, income per capita, unemployment rate, and race, metro- politan, and age distribution proportions. Di¤erencing equation (11.26) gives the equation estimated by Levitt: DlogðcrimeitÞ ¼ xt þ b1DlogðprisonitÞ þ Dxitg þ Duit ð11:27Þ Simultaneity between crime rates and prison population, or, more precisely, in the growth rates, makes OLS estimation of equation (11.27) generally inconsistent. Using the violent crime rate and a subset of the data from Levitt (in PRISON.RAW, for the years 1980 to 1993, for 51 \u0003 14 ¼ 714 total observations), the OLS estimate of b1 is \u0001:181 (se ¼ :048). We also estimate the equation by 2SLS, where the instruments for DlogðprisonÞ are two binary variables, one for whether a ﬁnal decision was reached on overcrowding litigation in the current year and one for whether a ﬁnal decision was reached in the previous two years. The 2SLS estimate of b1 is \u00011:032 (se ¼ :370). Therefore, the 2SLS estimated e¤ect is much larger; not surprisingly, it is much less precise, too. Levitt (1996) found similar results when using a longer time period and more instruments. A di¤erent approach to estimating SEMs with panel data is to use the ﬁxed e¤ects transformation and then to apply an IV technique such as pooled 2SLS. A simple procedure is to estimate the time-demeaned equation (10.46) by pooled 2SLS, where the instruments are also time demeaned. This is equivalent to using 2SLS in the dummy variable formulation, where the unit-speciﬁc dummy variables act as their own instruments. See Problem 11.9 for a careful analysis of this approach. Foster and Rosenzweig (1995) use the within transformation along with IV to estimate household- level proﬁt functions for adoption of high-yielding seed varieties in rural India. Ayres and Levitt (1998) apply 2SLS to a time-demeaned equation to estimate the e¤ect of Lojack electronic theft prevention devices on city car-theft rates. The FE transformation precludes the use of lagged values of wit among the instruments, for essentially the same reasons discussed for models with sequentially exogenous explanatory variables: uit will be correlated with the time-demeaned in- struments. Therefore, if we make assumptions on the dynamics in the model that ensure that uit is uncorrelated with wis, s < t, di¤erencing is preferred in order to use the extra instruments. Di¤erencing or time demeaning followed by some sort of IV procedure is useful when uit contains an important, time-varying omitted variable that is correlated with Chapter 11 310", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 322, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p323::c0", "text": "uit. The same considerations for choosing instruments in the simultaneity context are relevant in the omitted variables case as well. In some cases, wis, s < t \u0001 1, can be used as instruments at time t in a ﬁrst-di¤erenced equation (11.18); in other cases, we might not want identiﬁcation to hinge on using lagged exploratory variables as IVs. For example, suppose that we wish to study the e¤ects of per student spending on test scores, using three years of data, say 1980, 1985, and 1990. A structural model at the school level is avgscoreit ¼ yt þ zitg þ d1spendingit þ ci þ uit ð11:28Þ where zit contains other school and student characteristics. In addition to worrying about the school ﬁxed e¤ect ci, uit contains average family income for school i at time t (unless we are able to collect data on income); average family income is likely to be correlated with spendingit. After di¤erencing away ci, we need an instrument for Dspendingit. One possibility is to use exogenous changes in property taxes that arose because of an unexpected change in the tax laws. [Such changes occurred in California in 1978 (Proposition 13) and in Michigan in 1994 (Proposal A).] Using lagged spend- ing changes as IVs is probably not a good idea, as spending might a¤ect test scores with a lag. The third form of endogeneity, measurement error, can also be solved by elimi- nating ci and ﬁnding appropriate IVs. Measurement error in panel data was studied by Solon (1985) and Griliches and Hausman (1986). It is widely believed in econo- metrics that the di¤erencing and FE transformations exacerbate measurement error bias (even though they eliminate heterogeneity bias). However, it is important to know that this conclusion rests on the classical errors-in-variables model under strict exogeneity, as well as on other assumptions. To illustrate, consider a model with a single explanatory variable, yit ¼ bx\u0004 it þ ci þ uit ð11:29Þ under the strict exogeneity assumption Eðuit j x\u0004 i ; xi; ciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð11:30Þ where xit denotes the observed measure of the unobservable x\u0004 it. Condition (11.30) embodies the standard redundancy condition—that xit does not matter once x\u0004 it is controlled for—in addition to strict exogeneity of the unmeasured and measured regressors. Denote the measurement error as rit ¼ xit \u0001 x\u0004 it. Assuming that rit is un- correlated with x\u0004 it—the key CEV assumption—and that variances and covariances are all constant across t, it is easily shown that, as N ! y, the plim of the pooled OLS estimator is More Topics in Linear Unobserved E¤ects Models 311", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 323, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p324::c0", "text": "plim N!y ^bPOLS ¼ b þ Covðxit; ci þ uit \u0001 britÞ VarðxitÞ ¼ b þ Covðxit; ciÞ \u0001 bs2 r VarðxitÞ ð11:31Þ where s2 r ¼ VarðritÞ ¼ Covðxit; ritÞ; this is essentially the formula derived by Solon (1985). From equation (11.31), we see that there are two sources of asymptotic bias in the POLS estimator: correlation between xit and the unobserved e¤ect, ci, and a mea- surement error bias term, \u0001bs2 r . If xit and ci are positively correlated and b > 0, the two sources of bias tend to cancel each other out. Now assume that ris is uncorrelated with x\u0004 it for all t and s, and for simplicity sup- pose that T ¼ 2. If we ﬁrst di¤erence to remove ci before performing OLS we obtain plim N!y ^bFD ¼ b þ CovðDxit; Duit \u0001 bDritÞ VarðDxitÞ ¼ b \u0001 b CovðDxit; DritÞ VarðDxitÞ ¼ b \u0001 2b ½s2 r \u0001 Covðrit; ri;t\u00011Þ\u0002 VarðDxitÞ ¼ b 1 \u0001 s2 r ð1 \u0001 rrÞ s2 x \u0004ð1 \u0001 rx \u0004Þ þ s2 r ð1 \u0001 rrÞ \u0001 \u0002 ð11:32Þ where rx \u0004 ¼ Corrðx\u0004 it; x\u0004 i;t\u00011Þ and rr ¼ Corrðrit; ri;t\u00011Þ, where we have used the fact that Covðrit; ri;t\u00011Þ ¼ s2 r rr and VarðDxitÞ ¼ 2½s2 x \u0004ð1 \u0001 rx \u0004Þ þ s2 r ð1 \u0001 rrÞ\u0002; see also Solon (1985) and Hsiao (1986, p. 64). Equation (11.32) shows that, in addition to the ratio s2 r =s2 x\u0004 being important in determining the size of the measurement error bias, the ratio ð1 \u0001 rrÞ=ð1 \u0001 rx\u0004Þ is also important. As the autocorrelation in x\u0004 it increases rel- ative to that in rit, the measurement error bias in ^bFD increases. In fact, as rx \u0004 ! 1, the measurement error bias approaches \u0001b. Of course, we can never know whether the bias in equation (11.31) is larger than that in equation (11.32), or vice versa. Also, both expressions are based on the CEV assumptions, and then some. If there is little correlation between Dxit and Drit, the measurement error bias from ﬁrst di¤erencing may be small, but the small correlation is o¤set by the fact that di¤erencing can considerably reduce the variation in the explanatory variables. Consistent estimation in the presence of measurement error is possible under cer- tain assumptions. Consider the more general model yit ¼ zitg þ dw\u0004 it þ ci þ uit; t ¼ 1; 2; . . . ; T ð11:33Þ Chapter 11 312", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 324, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p325::c0", "text": "where w\u0004 it is measured with error. Write rit ¼ wit \u0001 w\u0004 it, and assume strict exogeneity along with redundancy of wit: Eðuit j zi; w\u0004 i ; wi; ciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð11:34Þ Replacing w\u0004 it with wit and ﬁrst di¤erencing gives Dyit ¼ Dzitg þ dDwit þ Duit \u0001 dDrit ð11:35Þ The standard CEV assumption in the current context can be stated as Eðrit j zi; w\u0004 i ; ciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð11:36Þ which implies that rit is uncorrelated with zis, w\u0004 is for all t and s. (As always in the context of linear models, assuming zero correlation is su‰cient for consistency, but not for usual standard errors and test statistics to be valid.) Under assumption (11.36) (and other measurement error assumptions), Drit is correlated with Dwit. To apply an IV method to equation (11.35), we need at least one instrument for Dwit. As in the omitted variables and simultaneity contexts, we may have additional variables out- side the model that can be used as instruments. Analogous to the cross section case (as in Chapter 5), one possibility is to use another measure on w\u0004 it, say hit. If the measurement error in hit is orthogonal to the measurement error in wis, all t and s, then Dhit is a natural instrument for Dwit in equation (11.35). Of course, we can use many more instruments in equation (11.35), as any linear combination of zi and hi is uncorrelated with the composite error under the given assumptions. Alternatively, a vector of variables hit may exist that are known to be redundant in equation (11.33), strictly exogenous, and uncorrelated with ris for all s. If Dhit is correlated with Dwit, then an IV procedure, such as pooled 2SLS, is easy to apply. It may be that in applying something like pooled 2SLS to equation (11.35) results in asymptotically valid statistics; this imposes serial independence and homoskedasticity assumptions on Duit. Generally, however, it is a good idea to use standard errors and test statistics robust to arbitrary serial correlation and heteroskedasticity, or to use a full GMM approach that e‰ciently accounts for these. An alternative is to use the FE transformation, as explained in Problem 11.9. Ziliak, Wilson, and Stone (1999) ﬁnd that, for a model explaining cyclicality of real wages, the FD and FE estimates are di¤erent in important ways. The di¤erences largely disappear when IV methods are used to account for measurement error in the local unemployment rate. So far, the solutions to measurement error in the context of panel data have assumed nothing about the serial correlation in rit. Suppose that, in addition to as- sumption (11.34), we assume that the measurement error is serially uncorrelated: EðritrisÞ ¼ 0; s 0 t ð11:37Þ More Topics in Linear Unobserved E¤ects Models 313", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 325, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p326::c0", "text": "Assumption (11.37) opens up a solution to the measurement error problem with panel data that is not available with a single cross section or independently pooled cross sections. Under assumption (11.36), rit is uncorrelated with w\u0004 is for all t and s. Thus, if we assume that the measurement error rit is serially uncorrelated, then rit is uncorrelated with wis for all t 0 s. Since, by the strict exogeneity assumption, Duit is uncorrelated with all leads and lags of zit and wit, we have instruments readily avail- able. For example, wi;t\u00012 and wi;t\u00013 are valid as instruments for Dwit in equation (11.35); so is wi;tþ1. Again, pooled 2SLS or some other IV procedure can be used once the list of instruments is speciﬁed for each time period. However, it is important to remember that this approach requires the rit to be serially uncorrelated, in addition to the other CEV assumptions. The methods just covered for solving measurement error problems all assume strict exogeneity of all explanatory variables. Naturally, things get harder when measure- ment error is combined with models with only sequentially exogenous explanatory variables. Nevertheless, di¤erencing away the unobserved e¤ect and then selecting instruments—based on the maintained assumptions—generally works in models with a variety of problems. 11.1.4 Summary of Models without Strictly Exogenous Explanatory Variables Before leaving this section, it is useful to summarize the general approach we have taken to estimate models that do not satisfy strict exogeneity: ﬁrst, a transformation is used to eliminate the unobserved e¤ect; next, instruments are chosen for the endog- enous variables in the transformed equation. In the previous subsections we have stated various assumptions, but we have not catalogued them as in Chapter 10, largely because there are so many variants. For example, in Section 11.1.3 we saw that di¤erent assumptions lead to di¤erent sets of instruments. The importance of carefully stating assumptions—such as (11.2), (11.34), (11.36), and (11.37)—cannot be overstated. First di¤erencing, which allows for more general violations of strict exogeneity than the within transformation, has an additional beneﬁt: it is easy to test the ﬁrst- di¤erenced equation for serial correlation after pooled 2SLS estimation. The test suggested in Problem 8.10 is immediately applicable with the change in notation that all variables are in ﬁrst di¤erences. Arellano and Bond (1991) propose tests for serial correlation in the original errors, fuit: t ¼ 1; . . . ; Tg; the tests are based on GMM estimation. When the original model has a lagged dependent variable, it makes more sense to test for serial correlation in fuitg: models with lagged dependent variables are usually taken to have errors that are serially uncorrelated, in which case the ﬁrst- di¤erenced errors must be serially correlated. As Arellano and Bond point out, serial Chapter 11 314", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 326, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p327::c0", "text": "correlation in fuitg generally invalidates using lags of yit as IVs in the ﬁrst-di¤erenced equation. Of course, one might ask why we would be interested in r1 in model (11.4) if fuitg is generally serially correlated. 11.2 Models with Individual-Speciﬁc Slopes The unobserved e¤ects models we have studied up to this point all have an additive unobserved e¤ect that has the same partial e¤ect on yit in all time periods. This assumption may be too strong for some applications. We now turn to models that allow for individual-speciﬁc slopes. 11.2.1 A Random Trend Model Consider the following extension of the standard unobserved e¤ects model: yit ¼ ci þ git þ xitb þ uit; t ¼ 1; 2; . . . ; T ð11:38Þ This is sometimes called a random trend model, as each individual, ﬁrm, city, and so on is allowed to have its own time trend. The individual-speciﬁc trend is an additional source of heterogeneity. If yit is the natural log of a variable, as is often the case in economic studies, then gi is (roughly) the average growth rate over a period (holding the explanatory variables ﬁxed). Then equation (11.38) is referred to a random growth model; see, for example, Heckman and Hotz (1989). In many applications of equation (11.38) we want to allow ðci; giÞ to be arbitrarily correlated with xit. (Unfortunately, allowing this correlation makes the name ‘‘ran- dom trend model’’ conﬂict with our previous usage of random versus ﬁxed e¤ects.) For example, if one element of xit is an indicator of program participation, equation (11.38) allows program participation to depend on individual-speciﬁc trends (or growth rates) in addition to the level e¤ect, ci. We proceed without imposing restric- tions on correlations among ðci; gi; xitÞ, so that our analysis is of the ﬁxed e¤ects variety. A random e¤ects approach is also possible, but it is more cumbersome; see Problem 11.5. For the random trend model, the strict exogeneity assumption on the explanatory variables is Eðuit j xi1; . . . ; xiT; ci; giÞ ¼ 0 ð11:39Þ which follows deﬁnitionally from the conditional mean speciﬁcation Eðyit j xi1; . . . ; xiT; ci; giÞ ¼ Eðyit j xit; ci; giÞ ¼ ci þ git þ xitb ð11:40Þ We are still primarily interested in consistently estimating b. More Topics in Linear Unobserved E¤ects Models 315", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 327, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p328::c0", "text": "One approach to estimating b is to di¤erence away ci: Dyit ¼ gi þ Dxitb þ Duit; t ¼ 2; 3; . . . ; T ð11:41Þ where we have used the fact that git \u0001 giðt \u0001 1Þ ¼ gi. Now equation (11.41) is just the standard unobserved e¤ects model we studied in Chapter 10. The key strict exo- geneity assumption, EðDuit j gi; Dxi2; . . . ; DxiTÞ ¼ 0, t ¼ 2; 3; . . . ; T, holds under as- sumption (11.39). Therefore, we can apply ﬁxed e¤ects or ﬁrst-di¤erencing methods to equation (11.41) in order to estimate b. In di¤erencing the equation to eliminate ci we lose one time period, so that equa- tion (11.41) applies to T \u0001 1 time periods. To apply FE or FD methods to equation (11.41) we must have T \u0001 1 b 2, or T b 3. In other words, b can be estimated con- sistently in the random trend model only if T b 3. Whether we prefer FE or FD estimation of equation (11.41) depends on the properties of fDuit: t ¼ 2; 3; . . . ; Tg. As we argued in Section 10.6, in some cases it is reasonable to assume that the ﬁrst di¤erence of fuitg is serially uncorrelated, in which case the FE method applied to equation (11.41) is attractive. If we make the as- sumption that the uit are serially uncorrelated and homoskedastic (conditional on xi, ci, gi), then FE applied to equation (11.41) is still consistent and asymptotically nor- mal, but not e‰cient. The next subsection covers that case explicitly. Example 11.7 (Random Growth Model for Analyzing Enterprise Zones): Papke (1994) estimates a random growth model to examine the e¤ects of enterprise zones on unemployment claims: logðuclmsitÞ ¼ yt þ ci þ git þ d1ezit þ uit so that aggregate time e¤ects are allowed in addition to a jurisdiction-speciﬁc growth rate, gi. She ﬁrst di¤erences the equation to eliminate ci and then applies ﬁxed e¤ects to the di¤erences. The estimate of d1 is ^d1 ¼ \u0001:192 with seð^d1Þ ¼ :085. Thus enter- prise zone designation is predicted to lower unemployment claims by about 19.2 percent, and the e¤ect is statistically signiﬁcant at the 5 percent level. Friedberg (1998) provides an example, using state-level panel data on divorce rates and divorce laws, that shows how important it can be to allow for state-speciﬁc trends. Without state-speciﬁc trends, she ﬁnds no e¤ect of unilateral divorce laws on divorce rates; with state-speciﬁc trends, the estimated e¤ect is large and statistically signiﬁcant. The estimation method Friedberg uses is the one we discuss in the next subsection. In using the random trend or random growth model for program evaluation, it may make sense to allow the trend or growth rate to depend on program participa- Chapter 11 316", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 328, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p329::c0", "text": "tion: in addition to shifting the level of y, program participation may also a¤ect the rate of change. In addition to progit, we would include progit \u0003 t in the model: yit ¼ yt þ ci þ git þ zitg þ d1progit þ d2progit \u0003 t þ uit Di¤erencing once, as before, removes ci, Dyit ¼ xt þ gi þ Dzitg þ d1D progit þ d2Dðprogit \u0003 tÞ þ Duit We can estimate this di¤erenced equation by ﬁxed e¤ects. An even more ﬂexible speciﬁcation is to replace progit and progit \u0003 t with a series of program indicators, prog1it; . . . ; progMit, where progjit is one if unit i in time t has been in the program exactly j years, and M is the maximum number of years the program has been around. If fuitg contains substantial serial correlation—more than a random walk—then di¤erencing equation (11.41) might be more attractive. Denote the second di¤erence of yit by D2yit 1 Dyit \u0001 Dyi;t\u00011 ¼ yit \u0001 2yi;t\u00011 þ yi;t\u00012 with similar expressions for D2xit and D2uit. Then D2yit ¼ D2xitb þ D2uit; t ¼ 3; . . . ; T ð11:42Þ As with the FE transformation applied to equation (11.41), second di¤erencing also eliminates gi. Because D2uit is uncorrelated with D2xis, for all t and s, we can estimate equation (11.42) by pooled OLS or a GLS procedure. When T ¼ 3, second di¤erencing is the same as ﬁrst di¤erencing and then apply- ing ﬁxed e¤ects. Second di¤erencing results in a single cross section on the second- di¤erenced data, so that if the second-di¤erence error is homoskedastic conditional on xi, the standard OLS analysis on the cross section of second di¤erences is appro- priate. Hoxby (1996) uses this method to estimate the e¤ect of teachers’ unions on education production using three years of census data. If xit contains a time trend, then Dxit contains the same constant for t ¼ 2; 3; . . . ; T, which then gets swept away in the FE or FD transformation applied to equation (11.41). Therefore, xit cannot have time-constant variables or variables that have exact linear time trends for all cross section units. 11.2.2 General Models with Individual-Speciﬁc Slopes We now consider a more general model with interactions between time-varying ex- planatory variables and some unobservable, time-constant variables: More Topics in Linear Unobserved E¤ects Models 317", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 329, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p330::c0", "text": "yit ¼ zitai þ xitb þ uit; t ¼ 1; 2; . . . ; T ð11:43Þ where zit is 1 \u0005 J, ai is J \u0005 1, xit is 1 \u0005 K, and b is K \u0005 1. The standard unobserved e¤ects model is a special case with zit 1 1; the random trend model is a special case with zit ¼ zt ¼ ð1; tÞ. Equation (11.43) allows some time-constant unobserved heterogeneity, contained in the vector ai, to interact with some of the observable explanatory variables. For example, suppose that progit is a program participation indicator and yit is an out- come variable. The model yit ¼ xitb þ ai1 þ ai2 \u0003 progit þ uit allows the e¤ect of the program to depend on the unobserved e¤ect ai2 (which may or may not be tied to ai1). While we are interested in estimating b, we are also interested in the average e¤ect of the program, m2 ¼ Eðai2Þ. We cannot hope to get good esti- mators of the ai2 in the usual case of small T. Polachek and Kim (1994) study such models, where the return to experience is allowed to be person-speciﬁc. Lemieux (1998) estimates a model where unobserved heterogeneity is rewarded di¤erently in the union and nonunion sectors. In the general model, we initially focus on estimating b and then turn to estimation of a ¼ EðaiÞ, which is the vector of average partial e¤ects for the covariates zit. The strict exogeneity assumption is the natural extension of assumption (11.39): assumption FE.10: Eðuit j zi; xi; aiÞ ¼ 0, t ¼ 1; 2; . . . ; T. Along with equation (11.43), Assumption FE.10 is equivalent to Eðyit j zi1; . . . ; ziT; xi1; . . . ; xiT; aiÞ ¼ Eðyit j zit; xit; aiÞ ¼ zitai þ xitb which says that, once zit, xit, and ai have been controlled for, ðzis; xisÞ for s 0 t do not help to explain yit. Deﬁne Zi as the T \u0005 J matrix with tth row zit, and similarly for the T \u0005 K matrix Xi. Then equation (11.43) can be written as yi ¼ Ziai þ Xib þ ui ð11:44Þ Assuming that Z0 iZi is nonsingular (technically, with probability one), deﬁne Mi 1 IT \u0001 ZiðZ0 iZiÞ\u00011Z0 i ð11:45Þ the projection matrix onto the null space of Zi [the matrix ZiðZ0 iZiÞ\u00011Z0 i is the pro- jection matrix onto the column space of Zi]. In other words, for each cross section observation i, Miyi is the T \u0005 1 vector of residuals from the time series regression Chapter 11 318", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 330, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p331::c0", "text": "yit on zit; t ¼ 1; 2; . . . ; T ð11:46Þ In the basic ﬁxed e¤ects case, regression (11.46) is the regression yit on 1, t ¼ 1; 2; . . . ; T, and the residuals are simply the time-demeaned variables. In the random trend case, the regression is yit on 1, t, t ¼ 1; 2; . . . ; T, which linearly detrends yit for each i. The T \u0005 K matrix MiXi contains as its rows the 1 \u0005 K vectors of residuals from the regression xit on zit, t ¼ 1; 2; . . . ; T. The usefulness of premultiplying by Mi is that it allows us to eliminate the unobserved e¤ect ai by premultiplying equation (11.44) through by Mi and noting that MiZi ¼ 0: €yi ¼ €Xib þ €ui ð11:47Þ where €yi ¼ Miyi, €Xi ¼ MiXi, and €ui ¼ Miui. This is an extension of the within transformation used in basic ﬁxed e¤ects estimation. To consistently estimate b by system OLS on equation (11.47), we make the fol- lowing assumption: assumption FE.20: rank Eð€X0 i €XiÞ ¼ K, where €Xi ¼ MiXi. The rank of Mi is T \u0001 J, so a necessary condition for Assumption FE.20 is J < T. In other words, we must have at least one more time period than the number of ele- ments in ai. In the basic unobserved e¤ects model, J ¼ 1, and we know that T b 2 is needed. In the random trend model, J ¼ 2, and we need T b 3 to estimate b. The system OLS estimator of equation (11.47) is ^bFE ¼ X N i¼1 €X0 i €Xi !\u00011 X N i¼1 €X0 i€yi ! ¼ b þ N\u00011 X N i¼1 €X0 i €Xi !\u00011 N\u00011 X N i¼1 €X0 iui ! Under Assumption FE.10, Eð€X0 iuiÞ ¼ 0, and under Assumption FE.20, rank Eð€X0 i €XiÞ ¼ K, and so the usual consistency argument goes through. Generally, it is possible that for some observations, €X0 i €Xi has rank less than K. For example, this result occurs in the standard ﬁxed e¤ects case when xit does not vary over time for unit i. However, under Assumption FE.20, ^bFE should be well deﬁned unless our cross section sample size is small or we are unlucky in obtaining the sample. Naturally, the FE estimator is ﬃﬃﬃﬃ N p -asymptotically normally distributed. To obtain the simplest expression for its asymptotic variance, we add the assumptions of con- stant conditional variance and no (conditional) serial correlation on the idiosyncratic errors fuit: t ¼ 1; 2; . . . ; Tg. assumption FE.30: Eðuiu0 i j zi; xi; aiÞ ¼ s2 uIT. More Topics in Linear Unobserved E¤ects Models 319", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 331, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p332::c0", "text": "Under Assumption FE.30, iterated expectations implies Eð€X0 iuiu0 i €XiÞ ¼ E½ €X0 iEðuiu0 i j Zi; XiÞ€Xi\u0002 ¼ s2 uEð€X0 i €XiÞ Using essentially the same argument as in Section 10.5.2, under Assumptions FE.10, FE.20, and FE.30, Avar ﬃﬃﬃﬃ N p ð ^bFE \u0001 bÞ ¼ s2 u½Eð€X0 i €XiÞ\u0002\u00011, and so Avarð ^bFEÞ is con- sistently estimated by Ava^rð ^bFEÞ ¼ ^s2 u X N i¼1 €X0 i €Xi !\u00011 ð11:48Þ where ^s2 u is a consistent estimator for s2 u. As with the standard FE analysis, we must use some care in obtaining ^s2 u. We have X T t¼1 Eð€u2 itÞ ¼ Eð€u0 i€uiÞ ¼ E½Eðu0 iMiui j Zi; XiÞ\u0002 ¼ Eftr½Eðu0 iuiMi j Zi; XiÞ\u0002g ¼ Eftr½Eðu0 iui j Zi; XiÞMi\u0002g ¼ E½trðs2 uMiÞ\u0002 ¼ ðT \u0001 JÞs2 u ð11:49Þ since trðMiÞ ¼ T \u0001 J. Let ^uit ¼ €yit \u0001 €xit ^bFE. Then equation (11.49) and standard arguments imply that an unbiased and consistent estimator of s2 u is ^s2 u ¼ ½NðT \u0001 JÞ \u0001 K\u0002\u00011 X N i¼1 X T t¼1 ^u2 it ¼ SSR=½NðT \u0001 JÞ \u0001 K\u0002 ð11:50Þ The SSR in equation (11.50) is from the pooled regression €yit on €xit; t ¼ 1; 2; . . . ; T; i ¼ 1; 2; . . . ; N ð11:51Þ which can be used to obtain ^bFE. Division of the SSR from regression (11.51) by NðT \u0001 JÞ \u0001 K produces ^s2 u. The standard errors reported from regression (11.51) will be o¤ because the SSR is only divided by NT \u0001 K; the adjustment factor is fðNT \u0001 KÞ=½NðT \u0001 JÞ \u0001 K\u0002g1=2. A standard F statistic for testing hypotheses about b is also asymptotically valid. Let Q be the number of restrictions on b under H0, and let SSRr be the restricted sum of squared residuals from a regression like regression (11.51) but with the restrictions on b imposed. Let SSRur be the unrestricted sum of squared residuals. Then F ¼ ðSSRr \u0001 SSRurÞ SSRur \u0003 ½NðT \u0001 JÞ \u0001 K\u0002 Q ð11:52Þ can be treated as having an F distribution with Q and NðT \u0001 JÞ \u0001 K degrees of freedom. Unless we add a (conditional) normality assumption on ui, equation (11.52) Chapter 11 320", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 332, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p333::c0", "text": "does not have an exact F distribution, but it is asymptotically valid because Q \u0003 F @ a w2 Q. Without Assumption FE.30, equation (11.48) is no longer valid as the variance esti- mator and equation (11.52) is not a valid test statistic. But the robust variance matrix estimator (10.59) can be used with the new deﬁnitions for €Xi and ^ui. This step leads directly to robust Wald statistics for multiple restrictions. To obtain a consistent estimator of a ¼ EðaiÞ, premultiply equation (11.44) by ðZ0 iZiÞ\u00011Z0 i and rearrange to get ai ¼ ðZ0 iZiÞ\u00011Z0 iðyi \u0001 XibÞ \u0001 ðZ0 iZiÞ\u00011Z0 iui ð11:53Þ Under Assumption FE.10, Eðui j ZiÞ ¼ 0, and so the second term in equation (11.53) has a zero expected value. Therefore, assuming that the expected value exists, a ¼ E½ðZ0 iZiÞ\u00011Z0 iðyi \u0001 XibÞ\u0002 So a consistent, ﬃﬃﬃﬃ N p -asymptotically normal estimator of a is ^a ¼ N\u00011 X N i¼1 ðZ0 iZiÞ\u00011Z0 iðyi \u0001 Xi ^bFEÞ ð11:54Þ With ﬁxed T we cannot consistently estimate the ai when they are viewed as parameters. However, for each i, the term in the summand in equation (11.54), call it ^ai, is an unbiased estimator of ai under Assumptions FE.10 and FE.20. This con- clusion is easy to show: Eð^ai j Z; XÞ ¼ ðZ0 iZiÞ\u00011Z0 i½Eðyi j Z; XÞ \u0001 XiEð ^bFE j Z; XÞ\u0002 ¼ ðZ0 iZiÞ\u00011Z0 i½Ziai þ Xib \u0001 Xib\u0002 ¼ ai, where we have used the fact that Eð ^bFE j Z; XÞ ¼ b. The estimator ^a simply averages the ^ai over all cross section observations. The asymptotic variance of ﬃﬃﬃﬃ N p ð^a \u0001 aÞ can be obtained by expanding equation (11.54) and plugging in ﬃﬃﬃﬃ N p ð ^bFE \u0001 bÞ ¼ ½Eð€X0 i €XiÞ\u0002\u00011ðN\u00011=2 PN i¼1 €X0 iuiÞ þ opð1Þ. A consistent estimator of Avar ﬃﬃﬃﬃ N p ð^a \u0001 aÞ can be shown to be N\u00011 X N i¼1 ½ð^si \u0001 ^aÞ \u0001 ^C^A\u00011 €X0 i^ui\u0002½ð^si \u0001 ^aÞ \u0001 ^C^A\u00011 €X0 i^ui\u00020 ð11:55Þ where ^si1ðZ0 iZiÞ\u00011Z0 iðyi \u0001 Xi ^bFEÞ, ^C1N\u00011 PN i¼1ðZ0 iZiÞ\u00011Z0 iXi, ^A 1 N\u00011 PN i¼1 €X0 i €Xi, and ^ui 1 €yi \u0001 €Xi ^bFE. This estimator is fully robust in the sense that it does not rely on Assumption FE.30. As usual, asymptotic standard errors of the elements of ^a are obtained by multiplying expression (11.55) by N and taking the square roots of the diagonal elements. As special cases, expression (11.55) can be applied to the tradi- tional unobserved e¤ects and random trend models. More Topics in Linear Unobserved E¤ects Models 321", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 333, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p334::c0", "text": "The estimator ^a in equation (11.54) is not necessarily the most e‰cient. A better approach is to use the moment conditions for ^bFE and ^a simultaneously. This leads to nonlinear instrumental variables methods, something we take up in Chapter 14. Chamberlain (1992a) covers the e‰cient method of moments approach to estimating a and b; see also Lemieux (1998). 11.3 GMM Approaches to Linear Unobserved E¤ects Models 11.3.1 Equivalence between 3SLS and Standard Panel Data Estimators Random e¤ects, ﬁxed e¤ects, and ﬁrst di¤erencing are still the most popular ap- proaches to estimating unobserved e¤ects panel data models under strict exogeneity of the explanatory variables. As we saw in Chapter 10, each of these is e‰cient under a particular set of assumptions. If these assumptions fail, we can do worse than using an optimal GMM approach. We have already seen how to generalize Assumption RE.3, FE.3, or FD.3 by allowing the idiosyncratic error variance matrix, VarðuiÞ, to be unrestricted. But we still assumed that either VarðcijT þ ui j xiÞ (random e¤ects) or Varðui j xi; ciÞ was constant. Suppose ﬁrst that Assumption RE.1 holds, so that Eðci j xiÞ ¼ 0. Write the model in composite error form as yi ¼ Xib þ vi ð11:56Þ Under Assumption RE.1, xis is uncorrelated with vit for all s and t. [In fact, any function of xi 1 ðxi1; . . . ; xiTÞ is uncorrelated with vit for all t, but we will only use the xis themselves.] Let xo i denote the row vector of nonredundant elements of xi, so that any time constant element appears only once in xo i . Then Eðxo0 i vitÞ ¼ 0, t ¼ 1; 2; . . . ; T. This orthogonality condition suggests a system instrumental variables procedure, with matrix of instruments Zi 1 IT n xo i ð11:57Þ In other words, use instruments Zi to estimate equation (11.56) by 3SLS or, more generally, by minimum chi-square. The matrix (11.57) can contain many instruments. If xit contains only time-varying variables, then Zi is T \u0005 TK. With only K parameters to estimate, this choice of instruments implies many overidentifying restrictions even for moderately sized T. Even if computation is not an issue, using many overidentifying restrictions can result in poor ﬁnite sample properties. In some cases, we can reduce the number of moment conditions without sacriﬁcing e‰ciency. Im, Ahn, Schmidt, and Wooldridge (1999) (IASW) show the following Chapter 11 322", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 334, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p335::c0", "text": "result. If ^W has the random e¤ects structure—which means we impose the RE struc- ture in estimating W—then 3SLS applied to equation (11.56), using instruments Zi 1 ðPTXi; QTWiÞ ð11:58Þ where PT ¼ jTðj0 TjTÞ\u00011j0 T, QT 1 IT \u0001 PT, jT 1 ð1; 1; . . . ; 1Þ0, and Wi is the T \u0005 M submatrix of Xi obtained by removing the time-constant variables, is identical to the random e¤ects estimator. The column dimension of matrix (11.58) is only K þ M, so there are only M overidentifying restrictions in using the 3SLS estimator. The algebraic equivalence between 3SLS and random e¤ects has some useful ap- plications. First, it provides a di¤erent way of testing the orthogonality between ci and xit for all t: after 3SLS estimation, we simply apply the GMM overidentiﬁcation statistic from Chapter 8. (We discussed regression-based tests in Section 10.7.3.) Second, it provides a way to obtain a more e‰cient estimator when Assumption RE.3 does not hold. If W does not have the random e¤ects structure [see equation (10.30)], then the 3SLS estimator that imposes this structure is ine‰cient; an unre- stricted estimator of W should be used instead. Because an unrestricted estimator of W is consistent with or without the random e¤ects structure, 3SLS with unrestricted ^W and IVs in matrix (11.58) is no less e‰cient than the RE estimator. Further, if Eðvivi j xiÞ 0 Eðviv0 iÞ, any 3SLS estimator is ine‰cient relative to GMM with the op- timal weighting matrix. Therefore, if Assumption RE.3 fails, minimum chi-square estimation with IVs in matrix (11.58) generally improves on the random e¤ects esti- mator. In other words, we can gain asymptotic e‰ciency by using only M a K ad- ditional moment conditions. A di¤erent 3SLS estimator can be shown to be equivalent to the ﬁxed e¤ects esti- mator. In particular, IASW (1999, Theorem 4.1) verify an assertion of Arellano and Bover (1995): when ^W has the random e¤ects form, the 3SLS estimator applied to equation (11.56) using instruments LT n xo i —where LT is the T \u0005 ðT \u0001 1Þ di¤er- encing matrix deﬁned in IASW [1999, equation (4.1)]—is identical to the ﬁxed e¤ects estimator. Therefore, we might as well use ﬁxed e¤ects. 11.3.2 Chamberlain’s Approach to Unobserved E¤ects Models We now study an approach to estimating the linear unobserved e¤ects model (11.1) due to Chamberlain (1982, 1984) and related to Mundlak (1978). We maintain the strict exogeneity assumption on xit conditional on ci (see Assumption FE.1), but we allow arbitrary correlation between ci and xit. Thus we are in the ﬁxed e¤ects envi- ronment, and xit contains only time-varying explanatory variables. In Chapter 10 we saw that the FE and FD transformations eliminate ci and pro- duce consistent estimators under strict exogeneity. Chamberlain’s approach is to re- More Topics in Linear Unobserved E¤ects Models 323", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 335, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p336::c0", "text": "place the unobserved e¤ect ci with its linear projection onto the explanatory variables in all time periods (plus the projection error). Assuming ci and all elements of xi have ﬁnite second moments, we can always write ci ¼ c þ xi1l1 þ xi2l2 þ \u0003 \u0003 \u0003 þ xiTlT þ ai ð11:59Þ where c is a scalar and l1; . . . ; lT are 1 \u0005 K vectors. The projection error ai, by def- inition, has zero mean and is uncorrelated with xi1; . . . ; xiT. This equation assumes nothing about the conditional distribution of ci given xi. In particular, Eðci j xiÞ is unrestricted, as in the usual ﬁxed e¤ects analysis. Plugging equation (11.59) into equation (11.1) gives, for each t, yit ¼ c þ xi1l1 þ \u0003 \u0003 \u0003 þ xitðb þ ltÞ þ \u0003 \u0003 \u0003 þ xiTlT þ rit ð11:60Þ where, under Assumption FE.1, the errors rit 1 ai þ uit satisfy EðritÞ ¼ 0; Eðx0 iritÞ ¼ 0; t ¼ 1; 2; . . . ; T ð11:61Þ However, unless we assume that Eðci j xiÞ is linear, it is not the case that Eðrit j xiÞ ¼ 0. Nevertheless, assumption (11.61) suggests a variety of methods for estimating b (along with c; l1; . . . ; lT). Write the system (11.60) for all time periods t as yi1 yi2 .. . yiT 0 B B B B @ 1 C C C C A ¼ 1 xi1 xi2 \u0003 \u0003 \u0003 xiT xi1 1 xi1 xi2 \u0003 \u0003 \u0003 xiT xi2 .. . 1 xi1 xi2 \u0003 \u0003 \u0003 xiT xiT 0 B B B B @ 1 C C C C A c l1 l2 ... lT b 0 B B B B B B B B @ 1 C C C C C C C C A þ ri1 ri2 .. . riT 0 B B B B @ 1 C C C C A ð11:62Þ or yi ¼ Wiy þ ri ð11:63Þ where Wi is T \u0005 ð1 þ TK þ KÞ and y is ð1 þ TK þ KÞ \u0005 1. From equation (11.61), EðW0 iriÞ ¼ 0, and so system OLS is one way to consistently estimate y. The rank condition requires that rank EðW0 iWiÞ ¼ 1 þ TK þ K; essentially, it su‰ces that the elements of xit are not collinear and that they vary su‰ciently over time. While sys- tem OLS is consistent, it is very unlikely to be the most e‰cient estimator. Not only is the scalar variance assumption Eðrir0 iÞ ¼ s2 r IT highly unlikely, but also the homo- skedasticity assumption Eðrir0 i j xiÞ ¼ Eðrir0 iÞ ð11:64Þ Chapter 11 324", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 336, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p337::c0", "text": "fails unless we impose further assumptions. Generally, assumption (11.64) is violated if Eðuiu0 i j ci; xiÞ 0 Eðuiu0 iÞ, if Eðci j xiÞ is not linear in xi, or if Varðci j xiÞ is not constant. If assumption (11.64) does happen to hold, feasible GLS is a natural approach. The matrix W ¼ Eðrir0 iÞ can be consistently estimated by ﬁrst estimating y by system OLS, and then proceeding with FGLS as in Section 7.5. If assumption (11.64) fails, a more e‰cient estimator is obtained by applying GMM to equation (11.63) with the optimal weighting matrix. Because rit is orthogonal to xo i ¼ ð1; xi1; . . . ; xiTÞ, xo i can be used as instruments for each time period, and so we choose the matrix of instruments (11.57). Interestingly, the 3SLS estimator, which uses ½Z0ðIN n ^WÞZ=N\u0002\u00011 as the weighting matrix—see Section 8.3.4—is numerically identical to FGLS with the same ^W. Arellano and Bover (1995) showed this result in the special case that ^W has the random e¤ects structure, and IASW (1999, Theorem 3.1) obtained the general case. In expression (11.63) there are 1 þ TK þ K parameters, and the matrix of instru- ments is T \u0005 Tð1 þ TKÞ; there are Tð1 þ TKÞ \u0001 ð1 þ TK þ KÞ ¼ ðT \u0001 1Þð1 þ TKÞ \u0001 K overidentifying restrictions. Testing these restrictions is precisely a test of the strict exogeneity Assumption FE.1, and it is a fully robust test when full GMM is used because no additional assumptions are used. Chamberlain (1982) works from the system (11.62) under assumption (11.61), but he uses a di¤erent estimation approach, known as minimum distance estimation. We cover this approach to estimation in Chapter 14. 11.4 Hausman and Taylor-Type Models In the panel data methods we covered in Chapter 10, and so far in this chapter, coe‰cients on time-constant explanatory variables are not identiﬁed unless we make Assumption RE.1. In some cases the explanatory variable of primary interest is time constant, yet we are worried that ci is correlated with some explanatory variables. Random e¤ects will produce inconsistent estimators of all parameters if such cor- relation exists, while ﬁxed e¤ects or ﬁrst di¤erencing eliminates the time-constant variables. When all time-constant variables are assumed to be uncorrelated with the unob- served e¤ect, but the time-varying variables are possibly correlated with ci, consistent estimation is fairly simple. Write the model as yit ¼ zig þ xitb þ ci þ uit; t ¼ 1; 2; . . . ; T ð11:65Þ where all elements of xit display some time variation, and it is convenient to include unity in zi and assume that EðciÞ ¼ 0. We assume strict exogeneity conditional on ci: More Topics in Linear Unobserved E¤ects Models 325", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 337, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p338::c0", "text": "Eðuit j zi; xi1; . . . ; xiT; ciÞ ¼ 0; t ¼ 1; . . . ; T ð11:66Þ Estimation of b can proceed by ﬁxed e¤ects: the FE transformation eliminates zig and ci. As usual, this approach places no restrictions on the correlation between ci and ðzi; xitÞ. What about estimation of g? If, in addition to assumption (11.66) we assume Eðz0 iciÞ ¼ 0 ð11:67Þ then a ﬃﬃﬃﬃ N p -consistent estimator is easy to obtain: average equation (11.65) across t, premultiply by z0 i, take expectations, use the fact that E½z0 iðci þ uiÞ\u0002 ¼ 0, and re- arrange to get Eðz0 iziÞg ¼ E½z0 iðyi \u0001 xibÞ\u0002 Now, making the standard assumption that Eðz0 iziÞ is nonsingular, it follows by the usual analogy principle argument that ^g ¼ N\u00011 X N i¼1 z0 izi !\u00011 N\u00011 X N i¼1 z0 iðyi \u0001 xi ^bFEÞ \" # is consistent for g. The asymptotic variance of ﬃﬃﬃﬃ N p ð^g \u0001 gÞ can be obtained by stan- dard arguments for two-step estimators. Rather than derive this asymptotic variance, we turn to a more general model. Hausman and Taylor (1981) (HT) partition zi and xit as zi ¼ ðzi1; zi2Þ, xit ¼ ðxit1; xit2Þ—where zi1 is 1 \u0005 J1, zi2 is 1 \u0005 J2, xit1 is 1 \u0005 K1, xit2 is 1 \u0005 K2—and assume that Eðz0 i1ciÞ ¼ 0 and Eðx0 it1ciÞ ¼ 0; all t ð11:68Þ We still maintain assumption (11.66), so that zi and xis are uncorrelated with uit for all t and s. Assumptions (11.66) and (11.68) provide orthogonality conditions that can be used in a method of moments procedure. HT actually imposed enough assumptions so that the variance matrix W of the composite error vi ¼ cijT þ ui has the random e¤ects structure and Assumption SIV.5 from Section 8.3.4 holds. Neither of these is necessary, but together they a¤ord some simpliﬁcations. Write equation (11.65) for all T time periods as yi ¼ Zig þ Xib þ vi ð11:69Þ Since xit is strictly exogenous and QTvi ¼ QTui [where QT 1 IT \u0001 jTðj0 TjTÞ\u00011j0 T is again the T \u0005 T time-demeaning matrix], it follows that E½ðQTXiÞ0vi\u0002 ¼ 0. Thus, the Chapter 11 326", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 338, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p339::c0", "text": "T \u0005 K matrix QTXi can be used as instruments in estimating equation (11.69). If these were the only instruments available, then we would be back to ﬁxed e¤ects estimation of b without being able to estimate g. Additional instruments come from assumption (11.68). In particular, zi1 is orthog- onal to vit for all t, and so is xo i1, the 1 \u0005 TK1 vector containing xit1 for all t ¼ 1; . . . ; T. Thus, deﬁne a set of instruments for equation (11.69) by ½QTXi; jT n ðzi1; xo i1Þ\u0002 ð11:70Þ which is a T \u0005 ðK þ J1 þ TK1Þ matrix. Simply put, the vector of IVs for time period t is ð€xit; zi1; xo i1Þ. With this set of instruments, the order condition for identiﬁcation of ðg; bÞ is that K þ J1 þ TK1 b J þ K, or TK1 b J2. In e¤ect, we must have a su‰cient number of elements in xo i1 to act as instruments for zi2. (€xit are the IVs for xit, and zi1 act as their own IVs.) Whether we do depends on the number of time periods, as well as on K1. Actually, matrix (11.70) does not include all possible instruments under assump- tions (11.66) and (11.68), even when we only focus on zero covariances. However, under the full set of Hausman-Taylor assumptions mentioned earlier—including the assumption that W has the random e¤ects structure—it can be shown that all in- struments other than those in matrix (11.70) are redundant in the sense of Section 8.6; see IASW (1999, Theorem 4.4) for details. In fact, a very simple estimation strategy is available. First, estimate equation (11.65) by pooled 2SLS, using IVs ð€xit; zi1; xo i1Þ. Use the pooled 2SLS residuals, say ^^v^vit, in the formulas from Section 10.4.1, namely, equations (10.35) and (10.37), to obtain ^s2 c and ^s2 u, which can then be used to obtain ^l in equation (10.77). Then, perform quasi–time demeaning on all the dependent variables, explanatory variables, and IVs, and use these in a pooled 2SLS estimation. Under the Hausman-Taylor assumptions, this estimator—sometimes called a gener- alized IV (GIV ) estimator—is the e‰cient GMM estimator, and all statistics from pooled 2SLS on the quasi-demeaned data are asymptotically valid. If W is not of the random e¤ects form, or if Assumption SIV.5 fails, many more instruments than are in matrix (11.70) can help improve e‰ciency. Unfortunately, the value of these additional IVs is unclear. For practical purposes, 3SLS with ^W of the RE form, 3SLS with ^W unrestricted, or GMM with optimal weighting matrix—using the instruments in matrix (11.70)—should be su‰cient, with the latter being the most e‰cient in the presence of conditional heteroskedasticity. The ﬁrst-stage estimator can be the system 2SLS estimator using matrix (11.70) as instruments. The GMM over- identiﬁcation test statistic can be used to test the TK1 \u0001 J2 overidentifying restrictions. In cases where K1 b J2, we can reduce the instrument list even further and still achieve identiﬁcation: we use xi1 as the instruments for zi2. Then, the IVs at time t are More Topics in Linear Unobserved E¤ects Models 327", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 339, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p340::c0", "text": "ð€xit; zi1; xi1Þ. We can then use the pooled 2SLS estimators described previously with this new set of IVs. Quasi-demeaning leads to an especially simple analysis. Although it generally reduces asymptotic e‰ciency, replacing xo i1 with xi1 is a reasonable way to reduce the instrument list because much of the partial correlation between zi2 and xo i1 is likely to be through the time average, xi1. HT provide an application of their model to estimating the return to education, where education levels do not vary over the two years in their sample. Initially, HT include as the elements of xit1 all time-varying explanatory variables: experience, an indicator for bad health, and a previous-year unemployment indicator. Race and union status are assumed to be uncorrelated with ci, and, because these do not change over time, they comprise zi1. The only element of zi2 is years of schooling. HT apply the GIV estimator and obtain a return to schooling that is almost twice as large as the pooled OLS estimate. When they allow some of the time-varying explanatory variables to be correlated with ci, the estimated return to schooling gets even larger. It is di‰cult to know what to conclude, as the identifying assumptions are not espe- cially convincing. For example, assuming that experience and union status are un- correlated with the unobserved e¤ect and then using this information to identify the return to schooling seems tenuous. Breusch, Mizon, and Schmidt (1989) studied the Hausman-Taylor model under the additional assumption that Eðx0 it2ciÞ is constant across t. This adds more orthogonality conditions that can be exploited in estimation. See IASW (1999) for a recent analysis. It is easy to bring in outside, exogenous variables in the Hausman-Taylor frame- work. For example, if the model (11.65) is an equation in a simultaneous equations model, and if elements of xit2 are simultaneously determined with yit, then we can use exogenous variables appearing elsewhere in the system as IVs. If such variables do not vary over time, we need to assume that they are uncorrelated with ci as well as with uit for all t. If they do vary over time and are correlated with ci, we can use their deviations from means as IVs, provided these instruments are strictly exogenous with respect to uit. The time averages can be added to the instrument list if the external variables are uncorrelated with ci. For example, in a wage equation containing alco- hol consumption, which is determined simultaneously with the wage, we can, under reasonable assumptions, use the time-demeaned local price of alcohol as an IV for alcohol consumption. 11.5 Applying Panel Data Methods to Matched Pairs and Cluster Samples Unobserved e¤ects structures arise in contexts other than repeated cross sections over time. One simple data structure is a matched pairs sample. To illustrate, we consider Chapter 11 328", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 340, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p341::c0", "text": "the case of sibling data, which are often used in the social sciences in order to control for the e¤ect of unobserved family background variables. For each family i in the population, there are two siblings, described by yi1 ¼ xi1b þ fi þ ui1 ð11:71Þ yi2 ¼ xi2b þ fi þ ui2 ð11:72Þ where the equations are for siblings 1 and 2, respectively, and fi is an unobserved family e¤ect. The strict exogeneity assumption now means that the idiosyncratic error uis in each sibling’s equation is uncorrelated with the explanatory variables in both equations. For example, if y denotes logðwageÞ and x contains years of schooling as an explanatory variable, then we must assume that sibling’s schooling has no e¤ect on wage after controlling for the family e¤ect, own schooling, and other observed covariates. Such assumptions are often reasonable, although the condition should be studied in each application. If fi is assumed to be uncorrelated with xi1 and xi2, then a random e¤ects analysis can be used. The mechanics of random e¤ects for matched pairs are identical to the case of two time periods. More commonly, fi is allowed to be arbitrarily correlated with the observed factors in xi1 and xi2, in which case di¤erencing across siblings to remove fi is the appropri- ate strategy. Under this strategy, x cannot contain common observable family back- ground variables, as these are indistinguishable from fi. The IV methods developed in Section 11.1 to account for omitted variables, measurement error, and simulta- neity, can be applied directly to the di¤erenced equation. Examples of where sibling (in some cases twin) di¤erences have been used in economics include Geronimus and Korenman (1992), Ashenfelter and Krueger (1994), Bronars and Grogger (1994), and Ashenfelter and Rouse (1998). A matched pairs sample is a special case of a cluster sample, which we touched on in Section 6.3.4. A cluster sample is typically a cross section on individuals (or families, ﬁrms, and so on), where each individual is part of a cluster. For example, students may be clustered by the high school they attend, or workers may be clus- tered by employer. Observations within a cluster are thought to be correlated as a result of an unobserved cluster e¤ect. The unobserved e¤ects model yis ¼ xisb þ ci þ uis ð11:73Þ is often reasonable, where i indexes the group or cluster and s indexes units within a cluster. In some ﬁelds, an unobserved e¤ects model for a cluster sample is called a hierarchical model. More Topics in Linear Unobserved E¤ects Models 329", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 341, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p342::c0", "text": "One complication that arises in cluster samples, which we have not yet addressed, is that the number of observations within a cluster usually di¤ers across clusters. Nevertheless, for cluster i, we can write yi ¼ Xib þ cijGi þ ui ð11:74Þ where the row dimension of yi, Xi, jGi, and ui is Gi, the number of units in cluster i. The dimension of b is K \u0005 1. To apply the panel data methods we have discussed so far, we assume that the number of clusters, N, is large, because we ﬁx the number of units within each cluster in analyzing the asymptotic properties of the estimators. Because the dimension of the vectors and matrix in equation (11.74) changes with i, we cannot assume an identical distribution across i. However, in most cases it is reasonable to assume that the observations are independent across cluster. The fact that they are not also identically distributed makes the theory more complicated but has no practical consequences. The strict exogeneity assumption in the model (11.73) requires that the error uis be uncorrelated with the explanatory variables for all units within cluster i. This as- sumption is often reasonable when a cluster e¤ect ci is explicitly included. (In other words, we assume strict exogeneity conditional on ci.) If we also assume that ci is uncorrelated with xis for all s ¼ 1; . . . ; Gi, then pooled OLS across all clusters and units is consistent as N ! y. However, the composite error will be correlated within cluster, just as in a random e¤ects analysis. Even with di¤erent cluster sizes a valid variance matrix for pooled OLS is easy to obtain: just use formula (7.26) but where ^vi, the Gi \u0005 1 vector of pooled OLS residuals for cluster i, replaces ^ui. The resulting variance matrix estimator is robust to any kind of intracluster correlation and arbi- trary heteroskedasticity, provided N is large relative to the Gi. In the hierarchical models literature, ci is often allowed to depend on cluster-level covariates, for example, ci ¼ d0 þ wid þ ai, where ai is assumed to be independent of (or at least uncorrelated with) wi and xis, s ¼ 1; . . . ; Gi. But this is equivalent to simply adding cluster-level observables to the original model and relabeling the unobserved cluster e¤ect. The ﬁxed e¤ects transformation can be used to eliminate ci in equation (11.74) when ci is thought to be correlated with xis. The di¤erent cluster sizes cause no problems here: demeaning is done within each cluster. Any explanatory variable that is con- stant within each cluster for all clusters—for example, the gender of the teacher if the clusters are elementary school classrooms—is eliminated, just as in the panel data case. Pooled OLS can be applied to the demeaned data, just as with panel data. Under the immediate generalizations of Assumptions FE.1–FE.3 to allow for di¤er- ent cluster sizes, the variance matrix of the FE estimator for cluster samples can be Chapter 11 330", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 342, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p343::c0", "text": "estimated as in expression (10.54), but s2 u must be estimated with care. A consistent estimator is ^s2 u ¼ SSR=½PN i¼1ðGi \u0001 1Þ \u0001 K \u0002, which is exactly the estimator that would be obtained from the pooled regression that includes a dummy variable for each cluster. The robust variance matrix (10.59) is valid very generally, where ^ui ¼ €yi \u0001 €Xi ^bFE, as usual. The 2SLS estimator described in Section 11.1.3 can also be applied to cluster sam- ples, once we adjust for di¤erent cluster sizes in doing the within-cluster demeaning. Rather than include a cluster e¤ect, ci, sometimes the goal is to see whether person s within cluster i is a¤ected by the characteristics of other people within the cluster. One way to estimate the importance of peer e¤ects is to specify yis ¼ xisb þ wiðsÞd þ vis ð11:75Þ where wiðsÞ indicates averages of a subset of elements of xis across all other people in the cluster. If equation (11.75) represents Eðyis j xiÞ ¼ Eðyis j xis; wiðsÞÞ for each s, then the strict exogeneity assumption Eðvis j xiÞ ¼ 0, s ¼ 1; . . . ; Gi, necessarily holds. Pooled OLS will consistently estimate b and d, although a robust variance matrix may be needed to account for correlation in vis across s, and possibly for heteroskedasticity. If Covðvis; vir j xiÞ ¼ 0, s 0 r, and Varðvis j xiÞ ¼ s2 v are assumed, then pooled OLS is e‰cient, and the usual test standard errors and test statistics are valid. It is also easy to allow the unconditional variance to change across cluster using a simple weighting; for a similar example, see Problem 7.7. We can also apply the more general models from Section 11.2.2, where unobserved cluster e¤ects interact with some of the explanatory variables. If we allow arbitrary dependence between the cluster e¤ects and the explanatory variables, the transfor- mations in Section 11.2.2 should be used. In the hierarchical models literature, the unobserved cluster e¤ects are assumed to be either independent of the covariates xis or independent of the covariates after netting out observed cluster covariates. This assumption results in a particular form of heteroskedasticity that can be exploited for e‰ciency. However, it makes as much sense to include cluster-level covariates, individual-level covariates, and possibly interactions of these in an initial model, and then to make inference in pooled OLS robust to arbitrary heteroskedasticity and cluster correlation. (See Problem 11.5 for a related analysis in the context of panel data.) We should remember that the methods described in this section are known to have good properties only when the number of clusters is large relative to the number of units within a cluster. Case and Katz (1991) and Evans, Oates, and Schwab (1992) apply cluster-sampling methods to the problem of estimating peer e¤ects. More Topics in Linear Unobserved E¤ects Models 331", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 343, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p344::c0", "text": "Problems 11.1. Let yit denote the unemployment rate for city i at time t. You are interested in studying the e¤ects of a federally funded job training program on city unemployment rates. Let zi denote a vector of time-constant city-speciﬁc variables that may inﬂuence the unemployment rate (these could include things like geographic location). Let xit be a vector of time-varying factors that can a¤ect the unemployment rate. The vari- able progit is the dummy indicator for program participation: progit ¼ 1 if city i par- ticipated at time t. Any sequence of program participation is possible, so that a city may participate in one year but not the next. a. Discuss the merits of including yi;t\u00011 in the model yit ¼ yt þ zig þ xitb þ r1 yi;t\u00011 þ d1progit þ uit; t ¼ 1; 2; . . . ; T State an assumption that allows you to consistently estimate the parameters by pooled OLS. b. Evaluate the following statement: ‘‘The model in part a is of limited value because the pooled OLS estimators are inconsistent if the fuitg are serially correlated.’’ c. Suppose that it is more realistic to assume that program participation depends on time-constant, unobservable city heterogeneity, but not directly on past unemploy- ment. Write down a model that allows you to estimate the e¤ectiveness of the pro- gram in this case. Explain how to estimate the parameters, describing any minimal assumptions you need. d. Write down a model that allows the features in parts a and c. In other words, progit can depend on unobserved city heterogeneity as well as the past unemployment history. Explain how to consistently estimate the e¤ect of the program, again stating minimal assumptions. 11.2. Consider the following unobserved components model: yit ¼ zitg þ dwit þ ci þ uit; t ¼ 1; 2; . . . ; T where zit is a 1 \u0005 K vector of time-varying variables (which could include time-period dummies), wit is a time-varying scalar, ci is a time-constant unobserved e¤ect, and uit is the idiosyncratic error. The zit are strictly exogenous in the sense that Eðz0 isuitÞ ¼ 0; all s; t ¼ 1; 2; . . . ; T ð11:76Þ but ci is allowed to be arbitrarily correlated with each zit. The variable wit is endog- enous in the sense that it can be correlated with uit (as well as with ci). Chapter 11 332", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 344, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p345::c0", "text": "a. Suppose that T ¼ 2, and that assumption (11.76) contains the only available orthogonality conditions. What are the properties of the OLS estimators of g and d on the di¤erenced data? Support your claim (but do not include asymptotic derivations). b. Under assumption (11.76), still with T ¼ 2, write the linear reduced form for the di¤erence Dwi as Dwi ¼ zi1p1 þ zi2p2 þ ri, where, by construction, ri is uncorrelated with both zi1 and zi2. What condition on ðp1; p2Þ is needed to identify g and d? (Hint: It is useful to rewrite the reduced form of Dwi in terms of Dzi and, say, zi1.) How can you test this condition? c. Now consider the general T case, where we add to assumption (11.76) the as- sumption EðwisuitÞ ¼ 0, s < t, so that previous values of wit are uncorrelated with uit. Explain carefully, including equations where appropriate, how you would estimate g and d. d. Again consider the general T case, but now use the ﬁxed e¤ects transformation to eliminate ci: €yit ¼ €zitg þ d€wit þ €uit What are the properties of the IV estimators if you use €zit and wi;t\u0001p, p b 1, as instruments in estimating this equation by pooled IV? (You can only use time periods p þ 1; . . . ; T after the initial demeaning.) 11.3. Show that, in the simple model (11.29) with T > 2, under the assumptions (11.30), Eðrit j x\u0004 i ; ciÞ ¼ 0 for all t, and Varðrit \u0001 riÞ and Varðx\u0004 it \u0001 x\u0004 i Þ constant across t, the plim of the FE estimator is plim N!y ^bFE ¼ b 1 \u0001 Varðrit \u0001 riÞ ½Varðx\u0004 it \u0001 x\u0004 i Þ þ Varðrit \u0001 riÞ\u0002 \u0004 \u0005 Thus, there is attenuation bias in the FE estimator under these assumptions. 11.4. a. Show that, in the ﬁxed e¤ects model, a consistent estimator of mc 1 EðciÞ is ^mc ¼ N\u00011 PN i¼1ðyi \u0001 xi ^bFEÞ. b. In the random trend model, how would you estimate mg ¼ EðgiÞ? 11.5. A random e¤ects analysis of model (11.43) would add Eðai j zi; xiÞ ¼ EðaiÞ ¼ a to Assumption FE.10 and, to Assumption FE.30, Varðai j zi; xiÞ ¼ L, where L is a J \u0005 J positive semideﬁnite matrix. (This approach allows the elements of ai to be arbitrarily correlated.) a. Deﬁne the T \u0005 1 composite error vector vi 1 Ziðai \u0001 aÞ þ ui. Find Eðvi j zi; xiÞ and Varðvi j zi; xiÞ. Comment on the conditional variance. More Topics in Linear Unobserved E¤ects Models 333", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 345, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p346::c0", "text": "b. If you apply the usual RE procedure to the equation yit ¼ zita þ xitb þ vit; t ¼ 1; 2; . . . ; T what are the asymptotic properties of the RE estimator and the usual RE standard errors and test statistics? c. How could you modify your inference from part b to be asymptotically valid? 11.6. Does the measurement error model in equations (11.33) to (11.37) apply when w\u0004 it is a lagged dependent variable? Explain. 11.7. In the Chamberlain model in Section 11.3.2, suppose that lt ¼ l=T for all t. Show that the pooled OLS coe‰cient on xit in the regression yit on 1, xit, xi, t ¼ 1; . . . ; T; i ¼ 1; . . . ; N, is the FE estimator. (Hint: Use partitioned regression.) 11.8. In model (11.1), ﬁrst di¤erence to remove ci: Dyit ¼ Dxitb þ Duit; t ¼ 2; . . . ; T ð11:77Þ Assume that a vector of instruments, zit, satisﬁes EðDuit j zitÞ ¼ 0, t ¼ 2; . . . ; T. Typi- cally, several elements in Dxit would be included in zit, provided they are appropri- ately exogenous. Of course the elements of zit can be arbitrarily correlated with ci. a. State the rank condition that is necessary and su‰cient for pooled 2SLS estima- tion of equation (11.77) using instruments zit to be consistent (for ﬁxed T ). b. Under what additional assumptions are the usual pooled 2SLS standard errors and test statistics asymptotically valid? (Hint: See Problem 8.8.) c. How would you test for ﬁrst-order serial correlation in Duit? (Hint: See Problem 8.10.) 11.9. Consider model (11.1) under the assumption Eðuit j zi; ciÞ ¼ 0; t ¼ 1; 2; . . . ; T ð11:78Þ where zi ¼ ðzi1; . . . ; ziTÞ and each zit is 1 \u0005 L. Typically, zit would contain some ele- ments of xit. However, fzit: t ¼ 1; 2; . . . ; Tg is assumed to be strictly exogenous (conditional on ci). All elements of zit are allowed to be correlated with ci. a. Use the ﬁxed e¤ects transformation to eliminate ci: €yit ¼ €xitb þ €uit; t ¼ 1; . . . ; T; i ¼ 1; . . . ; N ð11:79Þ Let €zit denote the time-demeaned IVs. State the rank condition that is necessary and su‰cient for pooled 2SLS estimation of equation (11.79) using instruments €zit to be consistent (for ﬁxed T ). Chapter 11 334", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 346, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p347::c0", "text": "b. Show that, under the additional assumption Eðuiu0 i j zi; ciÞ ¼ s2 uIT ð11:80Þ the asymptotic variance of ﬃﬃﬃﬃ N p ð ^b \u0001 bÞ is s2 ufEð€X0 i €ZiÞ½Eð€Z0 i €ZiÞ\u0002\u00011Eð€Z0 i €XiÞg\u00011 where the notation should be clear from Chapter 10. c. Propose a consistent estimator of s2 u. d. Show that the 2SLS estimator of b from part a can be obtained by means of a dummy variable approach: estimate yit ¼ c1 d1i þ \u0003 \u0003 \u0003 þ cN dNi þ xitb þ uit ð11:81Þ by pooled 2SLS, using instruments ðd1i; d2i; . . . ; dNi; zitÞ. (Hint: Use the obvious ex- tension of Problem 5.1 to pooled 2SLS, and repeatedly apply the algebra of partial regression.) This is another case where, even though we cannot estimate the ci con- sistently with ﬁxed T, we still get a consistent estimator of b. e. In using the 2SLS approach from part d, explain why the usually reported stan- dard errors are valid under assumption (11.80). f. How would you obtain valid standard errors for 2SLS without assumption (11.80)? g. If some elements of zit are not strictly exogenous, but we perform the procedure in part c, what are the asymptotic (N ! y, T ﬁxed) properties of ^b? 11.10. Consider the general model (11.43) where unobserved heterogeneity interacts with possibly several variables. Show that the ﬁxed e¤ects estimator of b is also obtained by running the regression yit on d1izit; d2izit; . . . ; dNizit; xit; t ¼ 1; 2; . . . ; T; i ¼ 1; 2; . . . ; N ð11:82Þ where dni ¼ 1 if and only if n ¼ i. In other words, we interact zit in each time period with a full set of cross section dummies, and then include all of these terms in a pooled OLS regression with xit. You should also verify that the residuals from re- gression (11.82) are identical to those from regression (11.51), and that regression (11.82) yields equation (11.50) directly. This proof extends the material on the basic dummy variable regression from Section 10.5.3. 11.11. Apply the random growth model to the data in JTRAIN1.RAW (see Ex- ample 10.6): logðscrapitÞ ¼ yt þ ci þ git þ b1grantit þ b2granti;t\u00011 þ uit More Topics in Linear Unobserved E¤ects Models 335", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 347, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p348::c0", "text": "Speciﬁcally, di¤erence once and then either di¤erence again or apply ﬁxed e¤ects to the ﬁrst-di¤erenced equation. Discuss the results. 11.12. An unobserved e¤ects model explaining current murder rates in terms of the number of executions in the last three years is mrdrteit ¼ yt þ b1execit þ b2unemit þ ci þ uit where mrdrteit is the number of murders in state i during year t, per 10,000 people; execit is the total number of executions for the current and prior two years; and unemit is the current unemployment rate, included as a control. a. Using the data for 1990 and 1993 in MURDER.RAW, estimate this model by ﬁrst di¤erencing. Notice that you should allow di¤erent year intercepts. b. Under what circumstances would execit not be strictly exogenous (conditional on ci)? Assuming that no further lags of exec appear in the model and that unem is strictly exogenous, propose a method for consistently estimating b when exec is not strictly exogenous. c. Apply the method from part b to the data in MURDER.RAW. Be sure to also test the rank condition. Do your results di¤er much from those in part a? d. What happens to the estimates from parts a and c if Texas is dropped from the analysis? 11.13. Use the data in PRISON.RAW for this question to estimate model (11.26). a. Estimate the reduced form equation for DlogðprisonÞ to ensure that ﬁnal1 and ﬁnal2 are partially correlated with DlogðprisonÞ. Test whether the parameters on ﬁnal1 and ﬁnal2 are equal. What does this ﬁnding say about choosing an IV for D logðprisonÞ? The elements of Dx should be the changes in the following variables: logðpolpcÞ, logðincpcÞ, unem, black, metro, ag0_14, ag15_17, ag18_24, and ag25_34. Is there serial correlation in this reduced form? b. Use Problem 11.8c to test for serial correlation in Duit. What do you conclude? c. Add a ﬁxed e¤ect to equation (11.27). [This procedure is appropriate if we add a random growth term to equation (11.26).] Estimate the equation in ﬁrst di¤erences using the method of Problem 11.9. (Since N is only 51, you might be able to include 51 state dummies and use them as their own IVs.) d. Estimate equation (11.26) using the property crime rate, and test for serial corre- lation in Duit. Are there important di¤erences compared with the violent crime rate? 11.14. An extension of the model in Example 11.7 that allows enterprise zone des- ignation to a¤ect the growth of unemployment claims is Chapter 11 336", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 348, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p349::c0", "text": "logðuclmsitÞ ¼ yt þ ci þ git þ d1ezit þ d2ezit \u0003 t þ uit Notice that each jurisdiction also has a separate growth rate gi. a. Use the data in EZUNEM.RAW to estimate this model by ﬁrst di¤erencing fol- lowed by ﬁxed e¤ects on the di¤erenced equation. Interpret your estimate of ^d2. Is it statistically signiﬁcant? b. Reestimate the model setting d1 ¼ 0. Does this model ﬁt better than the basic model in Example 11.7? c. Let wi be an observed, time-constant variable, and suppose we add b1wi þ b2wi \u0003 t to the random growth model. Can either b1 or b2 be estimated? Explain. 11.15. Use the data in JTRAIN1.RAW for this question. a. Consider the simple equation logðscrapitÞ ¼ yt þ b1hrsempit þ ci þ uit where scrapit is the scrap rate for ﬁrm i in year t, and hrsempit is hours of training per employee. Suppose that you di¤erence to remove ci, but you still think that Dhrsempit and DlogðscrapitÞ are simultaneously determined. Under what assumption is Dgrantit a valid IV for Dhrsempit? b. Using the di¤erences from 1987 to 1988 only, test the rank condition for identiﬁ- cation for the method described in part a. c. Estimate the ﬁrst-di¤erenced equation by IV, and discuss the results. d. Compare the IV estimates on the ﬁrst di¤erences with the OLS estimates on the ﬁrst di¤erences. e. Use the IV method described in part a, but use all three years of data. How does the estimate of b1 compare with only using two years of data? 11.16. Consider a Hausman and Taylor–type model with a single time-constant explanatory variable: yit ¼ gzi þ xitb þ ci þ uit Eðuit j zi; xi; ciÞ ¼ 0; t ¼ 1; . . . ; T where xit is 1 \u0005 K vector of time-varying explanatory variables. a. If we are interested only in estimating b, how should we proceed, without making additional assumptions (other than a standard rank assumption)? b. Let wi be a time-constant proxy variable for ci in the sense that More Topics in Linear Unobserved E¤ects Models 337", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 349, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p350::c0", "text": "Eðci j wi; zi; xiÞ ¼ Eðci j wi; xiÞ ¼ d0 þ d1wi þ xid2 The key assumption is that, once we condition on wi and xi, zi is not partially related to ci. Assuming the standard proxy variable redundancy assumption Eðuit j zi; xi; ci; wiÞ ¼ 0, ﬁnd Eðyit j zi; xi; wiÞ: c. Using part b, argue that g is identiﬁed. Suggest a pooled OLS estimator. d. Assume now that (1) Varðuit j zi; xi; ci; wiÞ ¼ s2 u, t ¼ 1; . . . ; T; (2) Covðuit; uis j zi; xi; ci; wiÞ ¼ 0, all t 0 s; (3) Varðci j zi; xi; wiÞ ¼ s2 a. How would you e‰ciently estimate g (along with b, d0, d1, and d2)? [Hint: It might be helpful to write ci ¼ d0 þ d1wi þ xid2 þ ai, where Eðai j zi; xi; wiÞ ¼ 0 and Varðai j zi; xi; wiÞ ¼ s2 a.] 11.17. Derive equation (11.55). Chapter 11 338", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 350, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p351::c0", "text": "III GENERAL APPROACHES TO NONLINEAR ESTIMATION In this part we begin our study of nonlinear econometric methods. What we mean by nonlinear needs some explanation because it does not necessarily mean that the underlying model is what we would think of as nonlinear. For example, suppose the population model of interest can be written as y ¼ xb þ u, but, rather than assuming Eðu j xÞ ¼ 0, we assume that the median of u given x is zero for all x. This assumption implies Medðy j xÞ ¼ xb, which is a linear model for the conditional median of y given x. [The conditional mean, Eðy j xÞ, may or may not be linear in x.] The stan- dard estimator for a conditional median turns out to be least absolute deviations (LAD), not ordinary least squares. Like OLS, the LAD estimator solves a minimi- zation problem: it minimizes the sum of absolute residuals. However, there is a key di¤erence between LAD and OLS: the LAD estimator cannot be obtained in closed form. The lack of a closed-form expression for LAD has implications not only for obtaining the LAD estimates from a sample of data, but also for the asymptotic theory of LAD. All the estimators we studied in Part II were obtained in closed form, a fact which greatly facilitates asymptotic analysis: we needed nothing more than the weak law of large numbers, the central limit theorem, and the basic algebra of probability limits. When an estimation method does not deliver closed-form solutions, we need to use more advanced asymptotic theory. In what follows, ‘‘nonlinear’’ describes any prob- lem in which the estimators cannot be obtained in closed form. The three chapters in this part provide the foundation for asymptotic analysis of most nonlinear models encountered in applications with cross section or panel data. We will make certain assumptions concerning continuity and di¤erentiability, and so problems violating these conditions will not be covered. In the general development of M-estimators in Chapter 12, we will mention some of the applications that are ruled out and provide references. This part of the book is by far the most technical. We will not dwell on the some- times intricate arguments used to establish consistency and asymptotic normality in nonlinear contexts. For completeness, we do provide some general results on consis- tency and asymptotic normality for general classes of estimators. However, for speciﬁc estimation methods, such as nonlinear least squares, we will only state assumptions that have real impact for performing inference. Unless the underlying regularity conditions—which involve assuming that certain moments of the population random variables are ﬁnite, as well as assuming continuity and di¤erentiability of the regres- sion function or log-likelihood function—are obviously false, they are usually just assumed. Where possible, the assumptions will correspond closely with those given previously for linear models.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 351, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p352::c0", "text": "The analysis of maximum likelihood methods in Chapter 13 is greatly simpliﬁed once we have given a general treatment of M-estimators. Chapter 14 contains results for generalized method of moments estimators for models nonlinear in parameters. We also brieﬂy discuss the related topic of minimum distance estimation in Chapter 14. Readers who are not interested in general approaches to nonlinear estimation might use these chapters only when needed for reference in Part IV. Part III 340", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 352, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p353::c0", "text": "12 M-Estimation 12.1 Introduction We begin our study of nonlinear estimation with a general class of estimators known as M-estimators, a term introduced by Huber (1967). (You might think of the ‘‘M’’ as standing for minimization or maximization.) M-estimation methods include max- imum likelihood, nonlinear least squares, least absolute deviations, quasi-maximum likelihood, and many other procedures used by econometricians. This chapter is somewhat abstract and technical, but it is useful to develop a uni- ﬁed theory early on so that it can be applied in a variety of situations. We will carry along the example of nonlinear least squares for cross section data to motivate the general approach. In a nonlinear regression model, we have a random variable, y, and we would like to model Eðy j xÞ as a function of the explanatory variables x, a K-vector. We already know how to estimate models of Eðy j xÞ when the model is linear in its parameters: OLS produces consistent, asymptotically normal estimators. What happens if the re- gression function is nonlinear in its parameters? Generally, let mðx; yÞ be a parametric model for Eðy j xÞ, where m is a known function of x and y, and y is a P \u0001 1 parameter vector. [This is a parametric model because mð\u0002 ; yÞ is assumed to be known up to a ﬁnite number of parameters.] The dimension of the parameters, P, can be less than or greater than K. The parameter space, Y, is a subset of RP. This is the set of values of y that we are willing to con- sider in the regression function. Unlike in linear models, for nonlinear models the asymptotic analysis requires explicit assumptions on the parameter space. An example of a nonlinear regression function is the exponential regression func- tion, mðx; yÞ ¼ expðxyÞ, where x is a row vector and contains unity as its ﬁrst ele- ment. This is a useful functional form whenever y b 0. A regression model suitable when the response y is restricted to the unit interval is the logistic function, mðx; yÞ ¼ expðxyÞ=½1 þ expðxyÞ\u0003. Both the exponential and logistic functions are nonlinear in y. In any application, there is no guarantee that our chosen model is adequate for Eðy j xÞ. We say that we have a correctly speciﬁed model for the conditional mean, Eðy j xÞ, if, for some yo A Y, Eðy j xÞ ¼ mðx; yoÞ ð12:1Þ We introduce the subscript ‘‘o’’ on theta to distinguish the parameter vector appear- ing in Eðy j xÞ from other candidates for that vector. (Often, the value yo is called ‘‘the true value of theta,’’ a phrase that is somewhat loose but still useful as short- hand.) As an example, for y b 0 and a single explanatory variable x, consider the model mðx; yÞ ¼ y1xy2. If the population regression function is Eðy j xÞ ¼ 4x1:5, then", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 353, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p354::c0", "text": "yo1 ¼ 4 and yo2 ¼ 1:5. We will never know the actual yo1 and yo2 (unless we some- how control the way the data have been generated), but, if the model is correctly speciﬁed, then these values exist, and we would like to estimate them. Generic can- didates for yo1 and yo2 are labeled y1 and y2, and, without further information, y1 is any positive number and y2 is any real number: the parameter space is Y 1 fðy1; y2Þ: y1 > 0; y2 A Rg. For an exponential regression model, mðx; yÞ ¼ expðxyÞ is a correctly speciﬁed model for Eðy j xÞ if and only if there is some K-vector yo such that Eðy j xÞ ¼ expðxyoÞ. In our analysis of linear models, there was no need to make the distinction between the parameter vector in the population regression function and other candidates for this vector, because the estimators in linear contexts are obtained in closed form, and so their asymptotic properties can be studied directly. As we will see, in our theoret- ical development we need to distinguish the vector appearing in Eðy j xÞ from a generic element of Y. We will often drop the subscripting by ‘‘o’’ when studying particular applications because the notation can be cumbersome. Equation (12.1) is the most general way of thinking about what nonlinear least squares is intended to do: estimate models of conditional expectations. But, as a sta- tistical matter, equation (12.1) is equivalent to a model with an additive, unobserv- able error with a zero conditional mean: y ¼ mðx; yoÞ þ u; Eðu j xÞ ¼ 0 ð12:2Þ Given equation (12.2), equation (12.1) clearly holds. Conversely, given equation (12.1), we obtain equation (12.2) by deﬁning the error to be u 1 y \u0004 mðx; yoÞ. In interpreting the model and deciding on appropriate estimation methods, we should not focus on the error form in equation (12.2) because, evidently, the additivity of u has some unintended connotations. In particular, we must remember that, in writing the model in error form, the only thing implied by equation (12.1) is Eðu j xÞ ¼ 0. Depending on the nature of y, the error u may have some unusual properties. For example, if y b 0 then u b \u0004mðx; yoÞ, in which case u and x cannot be independent. Heteroskedasticity in the error—that is, Varðu j xÞ 0 VarðuÞ—is present whenever Varðy j xÞ depends on x, as is very common when y takes on a restricted range of values. Plus, when we introduce randomly sampled observations fðxi; yiÞ: i ¼ 1; 2; . . . ; Ng, it is too tempting to write the model and its assumptions as ‘‘yi ¼ mðxi; yoÞ þ ui where the ui are i.i.d. errors.’’ As we discussed in Section 1.4 for the linear model, under random sampling the fuig are always i.i.d. What is usually meant is that ui and xi are independent, but, for the reasons we just gave, this as- sumption is often much too strong. The error form of the model does turn out to be useful for deﬁning estimators of asymptotic variances and for obtaining test statistics. Chapter 12 342", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 354, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p355::c0", "text": "For later reference, we formalize the ﬁrst nonlinear least squares (NLS) assumption as follows: assumption NLS.1: For some yo A Y, Eðy j xÞ ¼ mðx; yoÞ. This form of presentation represents the level at which we will state assumptions for particular econometric methods. In our general development of M-estimators that follows, we will need to add conditions involving moments of mðx; yÞ and y, as well as continuity assumptions on mðx; \u0002Þ. If we let w 1 ðx; yÞ, then yo indexes a feature of the population distribution of w, namely, the conditional mean of y given x. More generally, let w be an M-vector of random variables with some distribution in the population. We let W denote the subset of RM representing the possible values of w. Let yo denote a parameter vector describing some feature of the distribution of w. This could be a conditional mean, a conditional mean and conditional variance, a conditional median, or a conditional distribution. As shorthand, we call yo ‘‘the true parameter’’ or ‘‘the true value of theta.’’ These phrases simply mean that yo is the parameter vector describing the underlying population, something we will make precise later. We assume that yo belongs to a known parameter space Y H RP. We assume that our data come as a random sample of size N from the population; we label this random sample fwi: i ¼ 1; 2; . . .g, where each wi is an M-vector. This assumption is much more general than it may initially seem. It covers cross section models with many equations, and it also covers panel data settings with small time series dimension. The extension to independently pooled cross sections is almost im- mediate. In the NLS example, wi consists of xi and yi, the ith draw from the popu- lation on x and y. What allows us to estimate yo when it indexes Eðy j xÞ? It is the fact that yo is the value of y that minimizes the expected squared error between y and mðx; yÞ. That is, yo solves the population problem min y A Y Ef½y \u0004 mðx; yÞ\u00032g ð12:3Þ where the expectation is over the joint distribution of ðx; yÞ. This conclusion follows immediately from basic properties of conditional expectations (in particular, condi- tion CE.8 in Chapter 2). We will give a slightly di¤erent argument here. Write ½y \u0004 mðx; yÞ\u00032 ¼ ½y \u0004 mðx; yoÞ\u00032 þ 2½mðx; yoÞ \u0004 mðx; yÞ\u0003u þ ½mðx; yoÞ \u0004 mðx; yÞ\u00032 ð12:4Þ M-Estimation 343", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 355, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p356::c0", "text": "where u is deﬁned in equation (12.2). Now, since Eðu j xÞ ¼ 0, u is uncorrelated with any function of x, including mðx; yoÞ \u0004 mðx; yÞ. Thus, taking the expected value of equation (12.4) gives Ef½y \u0004 mðx; yÞ\u00032g ¼ Ef½y \u0004 mðx; yoÞ\u00032g þ Ef½mðx; yoÞ \u0004 mðx; yÞ\u00032g ð12:5Þ Since the last term in equation (12.5) is nonnegative, it follows that Ef½y \u0004 mðx; yÞ\u00032g b Ef½y \u0004 mðx; yoÞ\u00032g; all y A Y ð12:6Þ The inequality is strict when y 0 yo unless Ef½mðx; yoÞ \u0004 mðx; yÞ\u00032g ¼ 0; for yo to be identiﬁed, we will have to rule this possibility out. Because yo solves the population problem in expression (12.3), the analogy principle—which we introduced in Chapter 4—suggests estimating yo by solving the sample analogue. In other words, we replace the population moment Ef½ðy\u0004mðx; yÞ\u00032g with the sample average. The nonlinear least squares (NLS) estimator of yo, ^y, solves min y A Y N\u00041 X N i¼1 ½yi \u0004 mðxi; yÞ\u00032 ð12:7Þ For now, we assume that a solution to this problem exists. The NLS objective function in expression (12.7) is a special case of a more general class of estimators. Let qðw; yÞ be a function of the random vector w and the parameter vector y. An M-estimator of yo solves the problem min y A Y N\u00041 X N i¼1 qðwi; yÞ ð12:8Þ assuming that a solution, call it ^y, exists. The estimator clearly depends on the sample fwi: i ¼ 1; 2; . . . ; Ng, but we suppress that fact in the notation. The objective function for an M-estimator is a sample average of a function of wi and y. The division by N, while needed for the theoretical development, does not a¤ect the minimization problem. Also, the focus on minimization, rather than maxi- mization, is without loss of generality because maximiziation can be trivially turned into minimization. The parameter vector yo is assumed to uniquely solve the population problem min y A Y E½qðw; yÞ\u0003 ð12:9Þ Comparing equations (12.8) and (12.9), we see that M-estimators are based on the analogy principle. Once yo has been deﬁned, ﬁnding an appropriate function q that Chapter 12 344", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 356, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p357::c0", "text": "delivers yo as the solution to problem (12.9) requires basic results from probability theory. Usually there is more than one choice of q such that yo solves problem (12.9), in which case the choice depends on e‰ciency or computational issues. In this chap- ter we carry along the NLS example; we treat maximum likelihood estimation in Chapter 13. How do we translate the fact that yo solves the population problem (12.9) into consistency of the M-estimator ^y that solves problem (12.8)? Heuristically, the argu- ment is as follows. Since for each y A Y fqðwi; yÞ: i ¼ 1; 2; . . .g is just an i.i.d. sequence, the law of large numbers implies that N\u00041 X N i¼1 qðwi; yÞ ! p E½qðw; yÞ\u0003 ð12:10Þ under very weak ﬁnite moment assumptions. Since ^y minimizes the function on the left side of equation (12.10) and yo minimizes the function on the right, it seems plausible that ^y ! p yo. This informal argument turns out to be correct, except in pathological cases. There are essentially two issues to address. The ﬁrst is identiﬁ- ability of yo, which is purely a population issue. The second is the sense in which the convergence in equation (12.10) happens across di¤erent values of y in Y. 12.2 Identiﬁcation, Uniform Convergence, and Consistency We now present a formal consistency result for M-estimators under fairly weak assumptions. As mentioned previously, the conditions can be broken down into two parts. The ﬁrst part is the identiﬁcation or identiﬁability of yo. For nonlinear regres- sion, we showed how yo solves the population problem (12.3). However, we did not argue that yo is always the unique solution to problem (12.3). Whether or not this is the case depends on the distribution of x and the nature of the regression function: assumption NLS.2: Ef½mðx; yoÞ \u0004 mðx; yÞ\u00032g > 0, all y A Y, y 0 yo. Assumption NLS.2 plays the same role as Assumption OLS.2 in Chapter 4. It can fail if the explanatory variables x do not have su‰cient variation in the population. In fact, in the linear case mðx; yÞ ¼ xy, Assumption NLS.2 holds if and only if rank Eðx0xÞ ¼ K, which is just Assumption OLS.2 from Chapter 4. In nonlinear models, Assumption NLS.2 can fail if mðx; yoÞ depends on fewer parameters than are actually in y. For example, suppose that we choose as our model mðx; yÞ ¼ y1 þ y2x2 þ y3xy4 3 , but the true model is linear: yo3 ¼ 0. Then E½ðy \u0004 mðx; yÞÞ\u00032 is minimized for any y with y1 ¼ yo1, y2 ¼ yo2, y3 ¼ 0, and y4 any value. If yo3 0 0, Assumption NLS.2 M-Estimation 345", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 357, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p358::c0", "text": "would typically hold provided there is su‰cient variation in x2 and x3. Because identiﬁcation fails for certain values of yo, this is an example of a poorly identiﬁed model. (See Section 9.5 for other examples of poorly identiﬁed models.) Identiﬁcation in commonly used nonlinear regression models, such as exponential and logistic regression functions, holds under weak conditions, provided perfect col- linearity in x can be ruled out. For the most part, we will just assume that, when the model is correctly speciﬁed, yo is the unique solution to problem (12.3). For the general M-estimation case, we assume that qðw; yÞ has been chosen so that yo is a solution to problem (12.9). Identiﬁcation requires that yo be the unique solution: E½qðw; yoÞ\u0003 < E½qðw; yÞ\u0003; all y A Y; y 0 yo ð12:11Þ The second component for consistency of the M-estimator is convergence of the sample average N\u00041 PN i¼1 qðwi; yÞ to its expected value. It turns out that point- wise convergence in probability, as stated in equation (12.10), is not su‰cient for consistency. That is, it is not enough to simply invoke the usual weak law of large numbers at each y A Y. Instead, uniform convergence in probability is su‰cient. Mathematically, max y A Y N\u00041 X N i¼1 qðwi; yÞ \u0004 E½qðw; yÞ\u0003 \u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001 ! p 0 ð12:12Þ Uniform convergence clearly implies pointwise convergence, but the converse is not true: it is possible for equation (12.10) to hold but equation (12.12) to fail. Never- theless, under certain regularity conditions, the pointwise convergence in equation (12.10) translates into the uniform convergence in equation (12.12). To state a formal result concerning uniform convergence, we need to be more careful in stating assumptions about the function qð\u0002 ; \u0002Þ and the parameter space Y. Since we are taking expected values of qðw; yÞ with respect to the distribution of w, qðw; yÞ must be a random variable for each y A Y. Technically, we should assume that qð\u0002 ; yÞ is a Borel measurable function on W for each y A Y. Since it is very di‰- cult to write down a function that is not Borel measurable, we spend no further time on it. Rest assured that any objective function that arises in econometrics is Borel measurable. You are referred to Billingsley (1979) and Davidson (1994, Chapter 3). The next assumption concerning q is practically more important. We assume that, for each w A W, qðw; \u0002Þ is a continuous function over the parameter space Y. All of the problems we treat in detail have objective functions that are continuous in the parameters, but these do not cover all cases of interest. For example, Manski’s (1975) maximum score estimator for binary response models has an objective function that is not continuous in y. (We cover binary response models in Chapter 15.) It is possi- Chapter 12 346", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 358, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p359::c0", "text": "ble to somewhat relax the continuity assumption in order to handle such cases, but we will not need that generality. See Manski (1988, Section 7.3) and Newey and McFadden (1994). Obtaining uniform convergence is generally di‰cult for unbounded parameter sets, such as Y ¼ RP. It is easiest to assume that Y is a compact subset of RP, which means that Y is closed and bounded (see Rudin, 1976, Theorem 2.41). Because the natural parameter spaces in most applications are not bounded (and sometimes not closed), the compactness assumption is unattractive for developing a general theory of estimation. However, for most applications it is not an assumption to worry about: Y can be deﬁned to be such a large closed and bounded set as to always contain yo. Some consistency results for nonlinear estimation without compact parameter spaces are available; see the discussion and references in Newey and McFadden (1994). We can now state a theorem concerning uniform convergence appropriate for the random sampling environment. This result, known as the uniform weak law of large numbers (UWLLN), dates back to LeCam (1953). See also Newey and McFadden (1994, Lemma 2.4). theorem 12.1 (Uniform Weak Law of Large Numbers): Let w be a random vector taking values in W H RM, let Y be a subset of RP, and let q:W \u0001 Y ! R be a real- valued function. Assume that (a) Y is compact; (b) for each y A Y, qð\u0002 ; yÞ is Borel measurable on W; (c) for each w A W, qðw; \u0002Þ is continuous on Y; and (d) jqðw; yÞj a bðwÞ for all y A Y, where b is a nonnegative function on W such that E½bðwÞ\u0003 < y. Then equation (12.12) holds. The only assumption we have not discussed is assumption d, which requires the expected absolute value of qðw; yÞ to be bounded across y. This kind of moment condition is rarely veriﬁed in practice, although, with some work, it can be; see Newey and McFadden (1994) for examples. The continuity and compactness assumptions are important for establishing uni- form convergence, and they also ensure that both the sample minimization problem (12.8) and the population minimization problem (12.9) actually have solutions. Con- sider problem (12.8) ﬁrst. Under the assumptions of Theorem 12.1, the sample average is a continuous function of y, since qðwi; yÞ is continuous for each wi. Since a continu- ous function on a compact space always achieves its minimum, the M-estimation problem is well deﬁned (there could be more than one solution). As a technical mat- ter, it can be shown that ^y is actually a random variable under the measurability as- sumption on qð\u0002 ; yÞ. See, for example, Gallant and White (1988). It can also be shown that, under the assumptions of Theorem 12.1, the function E½qðw; yÞ\u0003 is continuous as a function of y. Therefore, problem (12.9) also has at least M-Estimation 347", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 359, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p360::c0", "text": "one solution; identiﬁability ensures that it has only one solution, and this fact implies consistency of the M-estimator. theorem 12.2 (Consistency of M-Estimators): Under the assumptions of Theorem 12.1, assume that the identiﬁcation assumption (12.11) holds. Then a random vector, ^y, solves problem (12.8), and ^y ! p yo. A proof of Theorem 12.2 is given in Newey and McFadden (1994). For nonlinear least squares, once Assumptions NLS.1 and NLS.2 are maintained, the practical re- quirement is that mðx; \u0002Þ be a continuous function over Y. Since this assumption is almost always true in applications of NLS, we do not list it as a separate assumption. Noncompactness of Y is not much of a concern for most applications. Theorem 12.2 also applies to median regression. Suppose that the conditional median of y given x is Medðy j xÞ ¼ mðx; yoÞ, where mðx; yÞ is a known function of x and y. The leading case is a linear model, mðx; yÞ ¼ xy, where x contains unity. The least absolute deviations (LAD) estimator of yo solves min y A Y N\u00041 X N i¼1 jyi \u0004 mðxi; yÞj If Y is compact and mðx; \u0002Þ is continuous over Y for each x, a solution always exists. The LAD estimator is motivated by the fact that yo minimizes E½jy \u0004 mðx; yÞj\u0003 over the parameter space Y; this follows by the fact that for each x, the conditional median is the minimum absolute loss predictor conditional on x. (See, for example, Bassett and Koenker, 1978, and Manski, 1988, Section 4.2.2.) If we assume that yo is the unique solution—a standard identiﬁcation assumption—then the LAD estimator is consistent very generally. In addition to the continuity, compactness, and identiﬁca- tion assumptions, it su‰ces that E½jyj\u0003 < y and jmðx; yÞj a aðxÞ for some function að\u0002Þ such that E½aðxÞ\u0003 < y. [To see this point, take bðwÞ 1 jyj þ aðxÞ in Theorem 12.2.] Median regression is a special case of quantile regression, where we model quantiles in the distribution of y given x. For example, in addition to the median, we can es- timate how the ﬁrst and third quartiles in the distribution of y given x change with x. Except for the median (which leads to LAD), the objective function that identiﬁes a conditional quantile is asymmetric about zero. See, for example, Koenker and Bassett (1978) and Manski (1988, Section 4.2.4). Buchinsky (1994) applies quantile regression methods to examine factors a¤ecting the distribution of wages in the United States over time. We end this section with a lemma that we use repeatedly in the rest of this chapter. It follows from Lemma 4.3 in Newey and McFadden (1994). Chapter 12 348", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 360, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p361::c0", "text": "lemma 12.1: Suppose that ^y ! p yo, and assume that rðw; yÞ satisﬁes the same assumptions on qðw; yÞ in Theorem 12.2. Then N\u00041 X N i¼1 rðwi; ^yÞ ! p E½rðw; yoÞ\u0003 ð12:13Þ That is, N\u00041 PN i¼1 rðwi; ^yÞ is a consistent estimator of E½rðw; yoÞ\u0003. Intuitively, Lemma 12.1 is quite reasonable. We know that N\u00041 PN i¼1 rðwi; yoÞ gen- erally converges in probability to E½rðw; yoÞ\u0003 by the law of large numbers. Lemma 12.1 shows that, if we replace yo with a consistent estimator, the convergence still holds, at least under standard regularity conditions. 12.3 Asymptotic Normality Under additional assumptions on the objective function, we can also show that M- estimators are asymptotically normally distributed (and converge at the rate ﬃﬃﬃﬃ N p ). It turns out that continuity over the parameter space does not ensure asymptotic nor- mality. We will assume more than is needed because all of the problems we cover in this book have objective functions with many continuous derivatives. The simplest asymptotic normality proof proceeds as follows. Assume that yo is in the interior of Y, which means that Y must have nonempty interior; this assumption is true in most applications. Then, since ^y ! p yo, ^y is in the interior of Y with prob- ability approaching one. If qðw; \u0002Þ is continuously di¤erentiable on the interior of Y, then (with probability approaching one) ^y solves the ﬁrst-order condition X N i¼1 sðwi; ^yÞ ¼ 0 ð12:14Þ where sðw; yÞ is the P \u0001 1 vector of partial derivatives of qðw; yÞ: sðw; yÞ0 ¼ ½qqðw; yÞ=qy1; qqðw; yÞ=qy2; . . . ; qqðw; yÞ=qyP\u0003. [Or, sðw; yÞ is the transpose of the gradient of qðw; yÞ.] We call sðw; yÞ the score of the objective function, qðw; yÞ. While condition (12.14) can only be guaranteed to hold with probability approaching one, usually it holds exactly; at any rate, we will drop the qualiﬁer, as it does not a¤ect the derivation of the limiting distribution. If qðw; \u0002Þ is twice continuously di¤erentiable, then each row of the left-hand side of equation (12.14) can be expanded about yo in a mean-value expansion: X N i¼1 sðwi; ^yÞ ¼ X N i¼1 sðwi; yoÞ þ X N i¼1 €Hi ! ð^y \u0004 yoÞ ð12:15Þ M-Estimation 349", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 361, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p362::c0", "text": "The notation €Hi denotes the P \u0001 P Hessian of the objective function, qðwi; yÞ, with respect to y, but with each row of Hðwi; yÞ 1 q2qðwi; yÞ=qyqy 0 1 ‘2 yqðwi; yÞ evaluated at a di¤erent mean value. Each of the P mean values is on the line segment between yo and ^y. We cannot know what these mean values are, but we do know that each must converge in probability to yo (since each is ‘‘trapped’’ between ^y and yo). Combining equations (12.14) and (12.15) and multiplying through by 1= ﬃﬃﬃﬃ N p gives 0 ¼ N\u00041=2 X N i¼1 sðwi; yoÞ þ N\u00041 X N i¼1 €Hi ! ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ Now, we can apply Lemma 12.1 to get N\u00041 PN i¼1 €Hi ! p E½Hðw; yoÞ\u0003 (under some moment conditions). If Ao 1 E½Hðw; yoÞ\u0003 is nonsingular, then N\u00041 PN i¼1 €Hi is non- singular w.p.a.1 and ðN\u00041 PN i¼1 €HiÞ\u00041 ! p A\u00041 o . Therefore, we can write ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ ¼ N\u00041 X N i¼1 €Hi !\u00041 \u0004N\u00041=2 X N i¼1 siðyoÞ \" # where siðyoÞ 1 sðwi; yoÞ. As we will show, E½siðyoÞ\u0003 ¼ 0. Therefore, N\u00041=2 PN i¼1 siðyoÞ generally satisﬁes the central limit theorem because it is the average of i.i.d. random vectors with zero mean, multiplied by the usual ﬃﬃﬃﬃ N p . Since opð1Þ \u0002 Opð1Þ ¼ opð1Þ, we have ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ ¼ A\u00041 o \u0004N\u00041=2 X N i¼1 siðyoÞ \" # þ opð1Þ ð12:16Þ This is an important equation. It shows that ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ inherits its limiting distri- bution from the average of the scores, evaluated at yo. The matrix A\u00041 o simply acts as a linear transformation. If we absorb this linear transformation into siðyoÞ, we can write ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ ¼ N\u00041=2 X N i¼1 riðyoÞ þ opð1Þ ð12:17Þ where riðyoÞ 1 \u0004A\u00041 o siðyoÞ; this is sometimes called the inﬂuence function representa- tion of ^y, where rðw; yÞ is the inﬂuence function. Equation (12.16) [or (12.17)] allows us to derive the ﬁrst-order asymptotic distribu- tion of ^y. Higher order representations attempt to reduce the error in the opð1Þ term in equation (12.16); such derivations are much more complicated than equation (12.16) and are beyond the scope of this book. Chapter 12 350", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 362, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p363::c0", "text": "We have essentially proven the following result: theorem 12.3 (Asymptotic Normality of M-estimators): In addition to the assump- tions in Theorem 12.2, assume (a) yo is in the interior of Y; (b) sðw; \u0002Þ is continu- ously di¤erentiable on the interior of Y for all w A W; (c) Each element of Hðw; yÞ is bounded in absolute value by a function bðwÞ, where E½bðwÞ\u0003 < y; (d) Ao 1 E½Hðw; yoÞ\u0003 is positive deﬁnite; (e) E½sðw; yoÞ\u0003 ¼ 0; and (f ) each element of sðw; yoÞ has ﬁnite second moment. Then ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ ! d Normalð0; A\u00041 o BoA\u00041 o Þ ð12:18Þ where Ao 1 E½Hðw; yoÞ\u0003 ð12:19Þ and Bo 1 E½sðw; yoÞsðw; yoÞ0\u0003 ¼ Var½sðw; yoÞ\u0003 ð12:20Þ Thus, Avar ^y ¼ A\u00041 o BoA\u00041 o =N ð12:21Þ Theorem 12.3 implies asymptotic normality of most of the estimators we study in the remainder of the book. A leading example that is not covered by Theorem 12.3 is the LAD estimator. Even if mðx; yÞ is twice continuously di¤erentiable in y, the ob- jective function for each i, qðwi; yÞ 1 jyi \u0004 mðxi; yÞj, is not twice continuously di¤er- entiable because the absolute value function is nondi¤erentiable at zero. By itself, this limitation is a minor nuisance. More importantly, by any reasonable deﬁnition, the Hessian of the LAD objective function is the zero matrix in the leading case of a linear conditional median function, and this fact violates assumption d of Theorem 12.3. It turns out that the LAD estimator is generally ﬃﬃﬃﬃ N p -asymptotically normal, but Theorem 12.3 cannot be applied. Newey and McFadden (1994) contains results that can be used. A key component of Theorem 12.3 is that the score evaluated at yo has expected value zero. In many applications, including NLS, we can show this result directly. But it is also useful to know that it holds in the abstract M-estimation framework, at least if we can interchange the expectation and the derivative. To see this point, note that, if yo is in the interior of Y, and E½qðw; yÞ\u0003 is di¤erentiable for y A int Y, then ‘yE½qðw; yÞ\u0003jy¼yo ¼ 0 ð12:22Þ M-Estimation 351", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 363, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p364::c0", "text": "where ‘y denotes the gradient with respect to y. Now, if the derivative and expec- tations operator can be interchanged (which is the case quite generally), then equation (12.22) implies E½‘yqðw; yoÞ\u0003 ¼ E½sðw; yoÞ\u0003 ¼ 0 ð12:23Þ A similar argument shows that, in general, E½Hðw; yoÞ\u0003 is positive semideﬁnite. If yo is identiﬁed, E½Hðw; yoÞ\u0003 is positive deﬁnite. For the remainder of this chapter, it is convenient to divide the original NLS ob- jective function by two: qðw; yÞ ¼ ½y \u0004 mðx; yÞ\u00032=2 ð12:24Þ The score of equation (12.24) can be written as sðw; yÞ ¼ \u0004‘ymðx; yÞ0½y \u0004 mðx; yÞ\u0003 ð12:25Þ where ‘ymðx; yÞ is the 1 \u0001 P gradient of mðx; yÞ, and therefore ‘ymðx; yÞ0 is P \u0001 1. We can show directly that this expression has an expected value of zero at y ¼ yo by showing that expected value of sðw; yoÞ conditional on x is zero: E½sðw; yoÞ j x\u0003 ¼ \u0004‘ymðx; yÞ0½Eðy j xÞ \u0004 mðx; yoÞ\u0003 ¼ 0 ð12:26Þ The variance of sðw; yoÞ is Bo 1 E½sðw; yoÞsðw; yoÞ0\u0003 ¼ E½u2‘ymðx; yoÞ0‘ymðx; yoÞ\u0003 ð12:27Þ where the error u 1 y \u0004 mðx; yoÞ is the di¤erence between y and Eðy j xÞ. The Hessian of qðw; yÞ is Hðw; yÞ ¼ ‘ymðx; yÞ0‘ymðx; yÞ \u0004 ‘2 ymðx; yÞ½y \u0004 mðx; yÞ\u0003 ð12:28Þ where ‘2 ymðx; yÞ is the P \u0001 P Hessian of mðx; yÞ with respect to y. To ﬁnd the expected value of Hðw; yÞ at y ¼ yo, we ﬁrst ﬁnd the expectation conditional on x. When evaluated at yo, the second term in equation (12.28) is ‘2 ymðx; yoÞu, and it therefore has a zero mean conditional on x [since Eðu j xÞ ¼ 0]. Therefore, E½Hðw; yoÞ j x\u0003 ¼ ‘ymðx; yoÞ0‘ymðx; yoÞ ð12:29Þ Taking the expected value of equation (12.29) over the distribution of x gives Ao ¼ E½‘ymðx; yoÞ0‘ymðx; yoÞ\u0003 ð12:30Þ This matrix plays a fundamental role in nonlinear regression. When yo is identiﬁed, Ao is generally positive deﬁnite. In the linear case mðx; yÞ ¼ xy, Ao ¼ Eðx0xÞ. In the Chapter 12 352", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 364, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p365::c0", "text": "exponential case mðx; yÞ ¼ expðxyÞ, Ao ¼ E½expð2xyoÞx0x\u0003, which is generally posi- tive deﬁnite whenever Eðx0xÞ is. In the example mðx; yÞ ¼ y1 þ y2x2 þ y3xy4 3 with yo3 ¼ 0, it is easy to show that matrix (12.30) has rank less than four. For nonlinear regression, Ao and Bo are similar in that they both depend on ‘ymðx; yoÞ0‘ymðx; yoÞ. Generally, though, there is no simple relationship between Ao and Bo because the latter depends on the distribution of u2, the squared population error. In Section 12.5 we will show that a homoskedasticity assumption implies that Bo is proportional to Ao. 12.4 Two-Step M-Estimators Sometimes applications of M-estimators involve a ﬁrst-stage estimation (an example is OLS with generated regressors, as in Chapter 6). Let ^g be a preliminary estimator, usually based on the random sample fwi: i ¼ 1; 2; . . . ; Ng. Where this estimator comes from must be vague at this point. A two-step M-estimator ^y of yo solves the problem min y A Y X N i¼1 qðwi; y;^gÞ ð12:31Þ where q is now deﬁned on W \u0001 Y \u0001 G, and G is a subset of RJ. We will see several examples of two-step M-estimators in the applications in Part IV. An example of a two-step M-estimator is the weighted nonlinear least squares (WNLS) estimator, where the weights are estimated in a ﬁrst stage. The WNLS estimator solves min y A Y 1 2 X N i¼1 ½yi \u0004 mðxi; yÞ\u00032=hðxi;^gÞ ð12:32Þ where the weighting function, hðx; gÞ, depends on the explanatory variables and a parameter vector. As with NLS, mðx; yÞ is a model of Eðy j xÞ. The function hðx; gÞ is chosen to be a model of Varðy j xÞ. The estimator ^g comes from a problem used to estimate the conditional variance. We list the key assumptions needed for WNLS to have desirable properties here, but several of the derivations are left for the problems. assumption WNLS.1: Same as Assumption NLS.1. 12.4.1 Consistency For the general two-step M-estimator, when will ^y be consistent for yo? In practice, the important condition is the identiﬁcation assumption. To state the identiﬁcation M-Estimation 353", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 365, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p366::c0", "text": "condition, we need to know about the asymptotic behavior of ^g. A general assump- tion is that ^g ! p g\u0005, where g\u0005 is some element in G. We label this value g\u0005 to allow for the possibility that ^g does not converge to a parameter indexing some interesting feature of the distribution of w. In some cases, the plim of ^g will be of direct interest. In the weighted regression case, if we assume that hðx; gÞ is a correctly speciﬁed model for Varðy j xÞ, then it is possible to choose an estimator such that ^g ! p go, where Varðy j xÞ ¼ hðx; goÞ. (For an example, see Problem 12.2.) If the variance model is misspeciﬁed, plim ^g is generally well deﬁned, but Varðy j xÞ 0 hðx; g\u0005Þ; it is for this reason that we use the notation g\u0005. The identiﬁcation condition for the two-step M-estimator is E½qðw; yo; g\u0005Þ\u0003 < E½qðw; y; g\u0005Þ\u0003; all y A Y; y 0 yo The consistency argument is essentially the same as that underlying Theorem 12.2. If qðwi; y; gÞ satisﬁes the UWLLN over Y \u0001 G then expression (12.31) can be shown to converge to E½qðw; y; g\u0005Þ\u0003 uniformly over Y. Along with identiﬁcation, this result can be shown to imply consistency of ^y for yo. In some applications of two-step M-estimation, identiﬁcation of yo holds for any g A G. This result can be shown for the WNLS estimator (see Problem 12.4). It is for this reason that WNLS is still consistent even if the function hðx; gÞ is not correctly speciﬁed for Varðy j xÞ. The weakest version of the identiﬁcation assumption for WNLS is the following: assumption WNLS.2: Ef½mðx; yoÞ \u0004 mðx; yÞ\u00032=hðx; g\u0005Þg > 0, all y A Y, y 0 yo, where g\u0005 ¼ plim ^g. As with the case of NLS, we know that weak inequality holds in Assumption WNLS.2 under Assumption WNLS.1. The strict inequality in Assumption WNLS.2 puts restrictions on the distribution of x and the functional forms of m and h. In other cases, including several two-step maximum likelihood estimators we en- counter in Part IV, the identiﬁcation condition for yo holds only for g ¼ g\u0005 ¼ go, where go also indexes some feature of the distribution of w. 12.4.2 Asymptotic Normality With the two-step M-estimator, there are two cases worth distinguishing. The ﬁrst occurs when the asymptotic variance of ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ does not depend on the asymp- totic variance of ﬃﬃﬃﬃ N p ð^g \u0004 g\u0005Þ, and the second occurs when the asymptotic variance of ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ must be adjusted to account for the ﬁrst-stage estimation of g\u0005. We ﬁrst derive conditions under which we can ignore the ﬁrst-stage estimation error. Chapter 12 354", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 366, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p367::c0", "text": "Using arguments similar to those in Section 12.3, it can be shown that, under standard regularity conditions, ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ ¼ A\u00041 o \u0004N\u00041=2 X N i¼1 siðyo;^gÞ ! þ opð1Þ ð12:33Þ where now Ao ¼ E½Hðw; yo; g\u0005Þ\u0003. In obtaining the score and the Hessian, we take derivatives only with respect to y; g\u0005 simply appears as an extra argument. Now, if N\u00041=2 X N i¼1 siðyo;^gÞ ¼ N\u00041=2 X N i¼1 siðyo; g\u0005Þ þ opð1Þ ð12:34Þ then ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ behaves the same asymptotically whether we used ^g or its plim in deﬁning the M-estimator. When does equation (12.34) hold? Assuming that ﬃﬃﬃﬃ N p ð^g \u0004 g\u0005Þ ¼ Opð1Þ, which is standard, a mean value expansion similar to the one in Section 12.3 gives N\u00041=2 X N i¼1 siðyo;^gÞ ¼ N\u00041=2 X N i¼1 siðyo; g\u0005Þ þ Fo ﬃﬃﬃﬃ N p ð^g \u0004 g\u0005Þ þ opð1Þ ð12:35Þ where Fo is the P \u0001 J matrix Fo 1 E½‘gsðw; yo; g\u0005Þ\u0003 ð12:36Þ (Remember, J is the dimension of g.) Therefore, if E½‘gsðw; yo; g\u0005Þ\u0003 ¼ 0 ð12:37Þ then equation (12.34) holds, and the asymptotic variance of the two-step M-estimator is the same as if g\u0005 were plugged in. In other words, under assumption (12.37), we conclude that equation (12.18) holds, where Ao and Bo are given in expressions (12.19) and (12.20), respectively, except that g\u0005 appears as an argument in the score and Hessian. For deriving the asymptotic distribution of ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ, we can ignore the fact that ^g was obtained in a ﬁrst-stage estimation. One case where assumption (12.37) holds is weighted nonlinear least squares, something you are asked to show in Problem 12.4. Naturally, we must assume that the conditional mean is correctly speciﬁed, but, interestingly, assumption (12.37) holds whether or not the conditional variance is correctly speciﬁed. There are many problems for which assumption (12.37) does not hold, including some of the methods for correcting for endogeneity in probit and Tobit models in Part IV. In Chapter 17 we will see that two-step methods for correcting sample selection M-Estimation 355", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 367, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p368::c0", "text": "bias are two-step M-estimators, but assumption (12.37) fails. In such cases we need to make an adjustment to the asymptotic variance of ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ. The adjustment is easily obtained from equation (12.35), once we have a ﬁrst-order representation for ﬃﬃﬃﬃ N p ð^g \u0004 g\u0005Þ. We assume that ﬃﬃﬃﬃ N p ð^g \u0004 g\u0005Þ ¼ N\u00041=2 X N i¼1 riðg\u0005Þ þ opð1Þ ð12:38Þ where riðg\u0005Þ is a J \u0001 1 vector with E½riðg\u0005Þ\u0003 ¼ 0 (in practice, ri depends on parameters other than g\u0005, but we suppress those here for simplicity). Therefore, ^g could itself be an M-estimator or, as we will see in Chapter 14, a generalized method of moments estimator. In fact, every estimator considered in this book has a representation as in equation (12.38). Now we can write ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ ¼ A\u00041 o N\u00041=2 X N i¼1 ½\u0004giðyo; g\u0005Þ\u0003 þ opð1Þ ð12:39Þ where giðyo; g\u0005Þ 1 siðyo; g\u0005Þ þ Foriðg\u0005Þ. Since giðyo; g\u0005Þ has zero mean, the standard- ized partial sum in equation (12.39) can be assumed to satisfy the central limit theorem. Deﬁne the P \u0001 P matrix Do 1 E½giðyo; g\u0005Þgiðyo; g\u0005Þ0\u0003 ¼ Var½giðyo; g\u0005Þ\u0003 ð12:40Þ Then Avar ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ ¼ A\u00041 o DoA\u00041 o ð12:41Þ We will discuss estimation of this matrix in the next section. 12.5 Estimating the Asymptotic Variance 12.5.1 Estimation without Nuisance Parameters We ﬁrst consider estimating the asymptotic variance of ^y in the case where there are no nuisance parameters. This task requires consistently estimating the matrices Ao and Bo. One thought is to solve for the expected values of Hðw; yoÞ and sðw; yoÞ \u0002 sðw; yoÞ0 over the distribution of w, and then to plug in ^y for yo. When we have completely speciﬁed the distribution of w, obtaining closed-form expressions for Ao and Bo is, in principle, possible. However, except in simple cases, it would be di‰cult. More importantly, we rarely specify the entire distribution of w. Even in a maximum Chapter 12 356", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 368, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p369::c0", "text": "likelihood setting, w is almost always partitioned into two parts: a set of endogenous variables, y, and conditioning variables, x. Rarely do we wish to specify the distri- bution of x, and so the expected values needed to obtain Ao and Bo are not available. We can always estimate Ao consistently by taking away the expectation and replacing yo with ^y. Under regularity conditions that ensure uniform converge of the Hessian, the estimator N\u00041 X N i¼1 Hðwi; ^yÞ 1 N\u00041 X N i¼1 ^Hi ð12:42Þ is consistent for Ao, by Lemma 12.1. The advantage of the estimator (12.42) is that it is always available in problems with a twice continuously di¤erentiable objective function. The drawbacks are that it requires calculation of the second derivatives—a nontrivial task for some problems—and it is not guaranteed to be positive deﬁnite, or even positive semideﬁnite, for the particular sample we are working with. As we will see shortly, in some cases the asymptotic variance of ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ is proportional to A\u00041 o , in which case using the estimator (12.42) to estimate Ao can result in a non- positive deﬁnite variance matrix estimator. Without a positive deﬁnite variance matrix estimator, some asymptotic standard errors need not even be deﬁned, and test statis- tics that have limiting chi-square distributions could actually be negative. In most econometric applications, more structure is available that allows a di¤er- ent estimator. Suppose we can partition w into x and y, and that yo indexes some feature of the distribution of y given x (such as the conditional mean or, in the case of maximum likelihood, the conditional distribution). Deﬁne Aðx; yoÞ 1 E½Hðw; yoÞ j x\u0003 ð12:43Þ While Hðw; yoÞ is generally a function of x and y, Aðx; yoÞ is a function only of x. By the law of iterated expectations, E½Aðx; yoÞ\u0003 ¼ E½Hðw; yoÞ\u0003 ¼ Ao. From Lemma 12.1 and standard regularity conditions it follows that N\u00041 X N i¼1 Aðxi; ^yÞ 1 N\u00041 X N i¼1 ^Ai ! p Ao ð12:44Þ The estimator (12.44) of Ao is useful in cases where E½Hðw; yoÞ j x\u0003 can be obtained in closed form or is easily approximated. In some leading cases, including NLS and certain maximum likelihood problems, Aðx; yoÞ depends only on the ﬁrst derivatives of the conditional mean function. When the estimator (12.44) is available, it is usually the case that yo actually min- imizes E½qðw; yÞ j x\u0003 for any value of x; this is easily seen to be the case for NLS from M-Estimation 357", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 369, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p370::c0", "text": "equation (12.4). Under assumptions that allow the interchange of derivative and ex- pectation, this result implies that Aðx; yoÞ is positive semideﬁnite. The expected value of Aðx; yoÞ over the distribution of x is positive deﬁnite provided yo is identiﬁed. Therefore, the estimator (12.44) is usually positive deﬁnite in the sample; as a result, it is more attractive than the estimator (12.42). Obtaining a positive semideﬁnite estimator of Bo is straightforward. By Lemma 12.1, under standard regularity conditions we have N\u00041 X N i¼1 sðwi; ^yÞsðwi; ^yÞ0 1 N\u00041 X N i¼1 ^si^s0 i ! p Bo ð12:45Þ Combining the estimator (12.45) with the consistent estimators for Ao, we can con- sistently estimate Avar ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ by Av^ar ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ ¼ ^A\u00041^B^A\u00041 ð12:46Þ where ^A is one of the estimators (12.42) or (12.44). The asymptotic standard errors are obtained from the matrix ^V 1 Av^arð^yÞ ¼ ^A\u00041^B^A\u00041=N ð12:47Þ which can be expressed as X N i¼1 ^Hi !\u00041 X N i¼1 ^si^s0 i ! X N i¼1 ^Hi !\u00041 ð12:48Þ or X N i¼1 ^Ai !\u00041 X N i¼1 ^si^s0 i ! X N i¼1 ^Ai !\u00041 ð12:49Þ depending on the estimator used for Ao. Expressions (12.48) and (12.49) are both at least positive semideﬁnite when they are well deﬁned. In the case of nonlinear least squares, the estimator of Ao in equation (12.44) is always available and always used: X N i¼1 ^Ai ¼ X N i¼1 ‘y ^m0 i‘y ^mi where ‘y ^mi 1 ‘ymðxi; ^yÞ for every observation i. Also, the estimated score for NLS can be written as Chapter 12 358", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 370, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p371::c0", "text": "^si ¼ \u0004‘y ^m0 i½yi \u0004 mðxi; ^yÞ\u0003 ¼ \u0004‘y ^m0 i^ui ð12:50Þ where the nonlinear least squares residuals, ^ui, are deﬁned as ^ui 1 yi \u0004 mðxi; ^yÞ ð12:51Þ The estimated asymptotic variance of the NLS estimator is Av^arð^yÞ ¼ X N i¼1 ‘y ^m0 i‘y ^mi !\u00041 X N i¼1 ^u2 i ‘y ^m0 i‘y ^mi ! X N i¼1 ‘y ^m0 i‘y ^mi !\u00041 ð12:52Þ This is called the heteroskedasticity-robust variance matrix estimator for NLS because it places no restrictions on Varðy j xÞ. It was ﬁrst proposed by White (1980a). [Sometimes the expression is multiplied by N=ðN \u0004 PÞ as a degrees-of-freedom ad- justment, where P is the dimension of y.] As always, the asymptotic standard error of each element of ^y is the square root of the appropriate diagonal element of matrix (12.52). As a speciﬁc example, suppose that mðx; yÞ ¼ expðxyÞ. Then ‘y ^m0 i‘y ^mi ¼ expð2xi ^yÞx0 ixi, which has dimension K \u0001 K. We can plug this equation into expres- sion (12.52) along with ^ui ¼ yi \u0004 expðxi ^yÞ. In many contexts, including nonlinear least squares and certain quasi-likelihood methods, the asymptotic variance estimator can be simpliﬁed under additional as- sumptions. For our purposes, we state the assumption as follows: For some s2 o > 0, E½sðw; yoÞsðw; yoÞ0\u0003 ¼ s2 oE½Hðw; yoÞ\u0003 ð12:53Þ This assumption simply says that the expected outer product of the score, evaluated at yo, is proportional to the expected value of the Hessian (evaluated at yo): Bo ¼ s2 oAo. Shortly we will provide an assumption under which assumption (12.53) holds for NLS. In the next chapter we will show that assumption (12.53) holds for s2 o ¼ 1 in the context of maximum likelihood with a correctly speciﬁed conditional density. For reasons we will see in Chapter 13, we refer to assumption (12.53) as the generalized information matrix equality (GIME). lemma 12.2: Under regularity conditions of the type contained in Theorem 12.3 and assumption (12.53), Avarð^yÞ ¼ s2 oA\u00041 o =N. Therefore, under assumption (12.53), the asymptotic variance of ^y can be estimated as ^V ¼ ^s2 X N i¼1 ^Hi !\u00041 ð12:54Þ M-Estimation 359", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 371, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p372::c0", "text": "or ^V ¼ ^s2 X N i¼1 ^Ai !\u00041 ð12:55Þ where ^Hi and ^Ai are deﬁned as before, and ^s2 ! p s2 o. In the case of nonlinear regression, the parameter s2 o is the variance of y given x, or equivalently Varðu j xÞ, under homoskedasticity: assumption NLS.3: Varðy j xÞ ¼ Varðu j xÞ ¼ s2 o. Under Assumption NLS.3, we can show that assumption (12.53) holds with s2 o ¼ Varðy j xÞ. First, since sðw; yoÞsðw; yoÞ0 ¼ u2‘ymðx; yoÞ0‘ymðx; yoÞ, it follows that E½sðw; yoÞsðw; yoÞ0 j x\u0003 ¼ Eðu2 j xÞ‘ymðx; yoÞ0‘ymðx; yoÞ ¼ s2 o‘ymðx; yoÞ0‘ymðx; yoÞ ð12:56Þ under Assumptions NLS.1 and NLS.3. Taking the expected value with respect to x gives equation (12.53). Under Assumption NLS.3, a simpliﬁed estimator of the asymptotic variance of the NLS estimator exists from equation (12.55). Let ^s2 ¼ 1 ðN \u0004 PÞ X N i¼1 ^u2 i ¼ SSR=ðN \u0004 PÞ ð12:57Þ where the ^ui are the NLS residuals (12.51) and SSR is the sum of squared NLS residuals. Using Lemma 12.1, ^s2 can be shown to be consistent very generally. The subtraction of P in the denominator of equation (12.57) is an adjustment that is thought to improve the small sample properties of ^s2. Under Assumptions NLS.1–NLS.3, the asymptotic variance of the NLS estimator is estimated as ^s2 X N i¼1 ‘y ^m0 i‘y ^mi !\u00041 ð12:58Þ This is the default asymptotic variance estimator for NLS, but it is valid only under homoskedasticity; the estimator (12.52) is valid with or without Assump- tion NLS.3. For an exponential regression function, expression (12.58) becomes ^s2ðPN i¼1 expð2xi ^yÞx0 ixiÞ\u00041. Chapter 12 360", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 372, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p373::c0", "text": "12.5.2 Adjustments for Two-Step Estimation In the case of the two-step M-estimator, we may or may not need to adjust the asymptotic variance. If assumption (12.37) holds, estimation is very simple. The most general estimators are expressions (12.48) and (12.49), where ^si, ^Hi, and ^Ai depend on ^g, but we only compute derivatives with respect to y. In some cases under assumption (12.37), the analogue of assumption (12.53) holds (with go ¼ plim ^g appearing in H and s). If so, the simpler estimators (12.54) and (12.55) are available. In Problem 12.4 you are asked to show this result for weighted NLS when Varðy j xÞ ¼ s2 ohðx; goÞ and go ¼ plim ^g. The natural third assumption for WNLS is that the variance function is correctly speciﬁed: assumption WNLS.3: For some go A G and s2 o, Varðy j xÞ ¼ s2 ohðx; goÞ. Further, ﬃﬃﬃﬃ N p ð^g \u0004 goÞ ¼ Opð1Þ. Under Assumption WNLS.3, the asymptotic variance of the WNLS estimator is estimated as ^s2 X N i¼1 ð‘y ^m0 i‘y ^miÞ=^hi !\u00041 ð12:59Þ where ^hi ¼ hðxi; ^gÞ and ^s2 is as in equation (12.57) except that the residual ^ui is replaced with the standardized residual, ^ui= ^hi ﬃﬃﬃﬃ p . The sum in expression (12.59) is simply the outer product of the weighted gradients, ‘y ^mi= ^hi ﬃﬃﬃﬃ p . Thus the NLS for- mulas can be used but with all quantities weighted by 1= ^hi ﬃﬃﬃﬃ p . It is important to re- member that expression (12.59) is not valid without Assumption WNLS.3. When assumption (12.37) is violated, the asymptotic variance estimator of ^y must account for the asymptotic variance of ^g; we must estimate equation (12.41). We already know how to consistently estimate Ao: use expression (12.42) or (12.44) where ^g is also plugged in. Estimation of Do is also straightforward. First, we need to estimate Fo. An estimator that is always available is ^F ¼ N\u00041 X N i¼1 ‘gsið^y; ^gÞ ð12:60Þ In cases with conditioning variables, such as nonlinear least squares, a simpler esti- mator can be obtained by computing E½‘gsðwi; yo; g\u0005Þ j xi\u0003, replacing ðyo; g\u0005Þ with ð^y; ^gÞ, and using this in place of ‘gsið^y; ^gÞ. Next, replace riðg\u0005Þ with ^ri 1 rið^gÞ. Then M-Estimation 361", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 373, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p374::c0", "text": "^D 1 N\u00041 X N i¼1 ^gi^g0 i ð12:61Þ is consistent for Do, where ^gi ¼ ^si þ ^F^ri. The asymptotic variance of the two-step M- estimator can be obtained as in expression (12.48) or (12.49), but where ^si is replaced with ^gi. 12.6 Hypothesis Testing 12.6.1 Wald Tests Wald tests are easily obtained once we choose a form of the asymptotic variance. To test the Q restrictions H0: cðyoÞ ¼ 0 ð12:62Þ we can form the Wald statistic W 1 cð^yÞ0ð^C^V^C0Þ\u00041cð^yÞ ð12:63Þ where ^V is an asymptotic variance matrix estimator of ^y, ^C 1 Cð^yÞ, and CðyÞ is the Q \u0001 P Jacobian of cðyÞ. The estimator ^V can be chosen to be fully robust, as in ex- pression (12.48) or (12.49); under assumption (12.53), the simpler forms in Lemma 12.2 are available. Also, ^V can be chosen to account for two-step estimation, when necessary. Provided ^V has been chosen appropriately, W @ a w2 Q under H0. A couple of practical restrictions are needed for W to have a limiting w2 Q distribu- tion. First, yo must be in the interior of Y; that is, yo cannot be on the boundary. If, for example, the ﬁrst element of y must be nonnegative—and we impose this restric- tion in the estimation—then expression (12.63) does not have a limiting chi-square distribution under H0: yo1 ¼ 0. The second condition is that CðyoÞ ¼ ‘ycðyoÞ must have rank Q. This rules out cases where yo is unidentiﬁed under the null hypothesis, such as the NLS example where mðx; yÞ ¼ y1 þ y2x2 þ y3xy4 3 and yo3 ¼ 0 under H0. One drawback to the Wald statistic is that it is not invariant to how the nonlinear restrictions are imposed. We can change the outcome of a hypothesis test by rede- ﬁning the constraint function, cð\u0002Þ. We can illustrate the lack of invariance by study- ing an asymptotic t statistic (since a t statistic is a special case of a Wald statistic). Suppose that for a parameter y1 > 0, the null hypothesis is H0: yo1 ¼ 1. The asymp- totic t statistic is ð^y1 \u0004 1Þ=seð^y1Þ, where seð^y1Þ is the asymptotic standard error of ^y1. Now deﬁne f1 ¼ logðy1Þ, so that fo1 ¼ logðyo1Þ and ^f1 ¼ logð^y1Þ. The null hypothe- sis can be stated as H0 : fo1 ¼ 0. Using the delta method (see Chapter 3), seð ^f1Þ ¼ Chapter 12 362", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 374, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p375::c0", "text": "^y\u00041 1 seð^y1Þ, and so the t statistic based on ^f1 is ^f1=seð ^f1Þ ¼ logð^y1Þ^y1=seð^y1Þ 0 ð^y1 \u0004 1Þ=seð^y1Þ. The lack of invariance of the Wald statistic is discussed in more detail by Gregory and Veall (1985), Phillips and Park (1988), and Davidson and MacKinnon (1993, Section 13.6). The lack of invariance is a cause for concern because it suggests that the Wald statistic can have poor ﬁnite sample properties for testing nonlinear hypoth- eses. What is much less clear is that the lack of invariance has led empirical researchers to search over di¤erent statements of the null hypothesis in order to obtain a desired result. 12.6.2 Score (or Lagrange Multiplier) Tests In cases where the unrestricted model is di‰cult to estimate but the restricted model is relatively simple to estimate, it is convenient to have a statistic that only requires estimation under the null. Such a statistic is Rao’s (1948) score statistic, also called the Lagrange multiplier statistic in econometrics, based on the work of Aitchison and Silvey (1958). We will focus on Rao’s original motivation for the statistic because it leads more directly to test statistics that are used in econometrics. An important point is that, even though Rao, Aitchison and Silvey, Engle (1984), and many others focused on the maximum likelihood setup, the score principle is applicable to any problem where the estimators solve a ﬁrst-order condition, including the general class of M- estimators. The score approach is ideally suited for speciﬁcation testing. Typically, the ﬁrst step in speciﬁcation testing is to begin with a popular model—one that is relatively easy to estimate and interpret—and nest it within a more complicated model. Then the popular model is tested against the more general alternative to determine if the orig- inal model is misspeciﬁed. We do not want to estimate the more complicated model unless there is signiﬁcant evidence against the restricted form of the model. In stating the null and alternative hypotheses, there is no di¤erence between speciﬁcation test- ing and classical tests of parameter restrictions. However, in practice, speciﬁcation testing gives primary importance to the restricted model, and we may have no in- tention of actually estimating the general model even if the null model is rejected. We will derive the score test only in the case where no correction is needed for preliminary estimation of nuisance parameters: either there are no such parameters present, or assumption (12.37) holds under H0. If nuisance parameters are present, we do not explicitly show the score and Hessian depending on ^g. We again assume that there are Q continuously di¤erentiable restrictions imposed on yo under H0, as in expression (12.62). However, we must also assume that the M-Estimation 363", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 375, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p376::c0", "text": "restrictions deﬁne a mapping from RP\u0004Q to RP, say, d: RP\u0004Q ! RP. In particular, under the null hypothesis, we can write yo ¼ dðloÞ, where lo is a ðP \u0004 QÞ \u0001 1 vector. We must assume that lo is in the interior of its parameter space, L, under H0. We also assume that d is twice continuously di¤erentiable on the interior of L. Let ~l be the solution to the constrained minimization problem min l A L X N i¼1 q½wi; dðlÞ\u0003 ð12:64Þ The constrained estimator of yo is simply ~y 1 dð~lÞ. In practice, we do not have to explicitly ﬁnd the function d; solving problem (12.64) is easily done just by directly imposing the restrictions, especially when the restrictions set certain parameters to hypothesized values (such as zero). Then, we just minimize the resulting objective function over the free parameters. As an example, consider the nonlinear regression model mðx; yÞ ¼ exp½xb þ d1ðxbÞ2 þ d2ðxbÞ3\u0003 where x is 1 \u0001 K and contains unity as its ﬁrst element. The null hypthosis is H0: d1 ¼ d2 ¼ 0, so that the model with the restrictions imposed is just an exponential regression function, mðx; bÞ ¼ expðxbÞ. The simplest method for deriving the LM test is to use Rao’s score principle extended to the M-estimator case. The LM statistic is based on the limiting distribu- tion of N\u00041=2 X N i¼1 sið~yÞ ð12:65Þ under H0. This is the score with respect to the entire vector y, but we are evaluating it at the restricted estimates. If ~y were replaced by ^y, then expression (12.65) would be identically zero, which would make it useless as a test statistic. If the restrictions imposed by the null hypothesis are true, then expression (12.65) will not be statisti- cally di¤erent from zero. Assume initially that yo is in the interior of Y under H0; we will discuss how to relax this assumption later. Now ﬃﬃﬃﬃ N p ð~y \u0004 yoÞ ¼ Opð1Þ by the delta method because ﬃﬃﬃﬃ N p ð~l \u0004 loÞ ¼ Opð1Þ under the given assumptions. A standard mean value expansion yields N\u00041=2 X N i¼1 sið~yÞ ¼ N\u00041=2 X N i¼1 siðyoÞ þ Ao ﬃﬃﬃﬃ N p ð~y \u0004 yoÞ þ opð1Þ ð12:66Þ Chapter 12 364", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 376, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p377::c0", "text": "under H0, where Ao is given in expression (12.19). But 0 ¼ ﬃﬃﬃﬃ N p cð~yÞ ¼ ﬃﬃﬃﬃ N p cðyoÞ þ €C ﬃﬃﬃﬃ N p ð~y \u0004 yoÞ, where €C is the Q \u0001 P Jacobian matrix CðyÞ with rows evaluated at mean values between ~y and yo. Under H0, cðyoÞ ¼ 0, and plim €C ¼ CðyoÞ 1 Co. Therefore, under H0, Co ﬃﬃﬃﬃ N p ð~y \u0004 yoÞ ¼ opð1Þ, and so multiplying equation (12.66) through by CoA\u00041 o gives CoA\u00041 o N\u00041=2 X N i¼1 sið~yÞ ¼ CoA\u00041 o N\u00041=2 X N i¼1 siðyoÞ þ opð1Þ ð12:67Þ By the CLT, CoA\u00041 o N\u00041=2 PN i¼1 siðyoÞ ! d Normalð0; CoA\u00041 o BoA\u00041 o C0 oÞ, where Bo is deﬁned in expression (12.20). Under our assumptions, CoA\u00041 o BoA\u00041 o C0 o has full rank Q, and so N\u00041=2 X N i¼1 sið~yÞ \" #0 A\u00041 o C0 o½CoA\u00041 o BoA\u00041 o C0 o\u0003\u00041CoA\u00041 o N\u00041=2 X N i¼1 sið~yÞ \" # ! d w2 Q The score or LM statistic is given by LM 1 X N i¼1 ~si !0 ~A\u00041 ~C0ð~C~A\u00041~B~A\u00041 ~C0Þ\u00041 ~C~A\u00041 X N i¼1 ~si ! =N ð12:68Þ where all quantities are evaluated at ~y. For example, ~C 1 Cð~yÞ, ~B is given in expres- sion (12.45) but with ~y in place of ^y, and ~A is one of the estimators in expression (12.42) or (12.44), again evaluated at ~y. Under H0, LM ! d w2 Q. For the Wald statistic we assumed that yo A intðYÞ under H0; this assumption is crucial for the statistic to have a limiting chi-square distribution. We will not consider the Wald statistic when yo is on the boundary of Y under H0; see Wolak (1991) for some results. The general derivation of the LM statistic also assumed that yo A intðYÞ under H0. Nevertheless, for certain applications of the LM test we can drop the requirement that yo is in the interior of Y under H0. A leading case occurs when y can be partitioned as y 1 ðy 0 1; y0 2Þ0, where y1 is ðP \u0004 QÞ \u0001 1 and y2 is Q \u0001 1. The null hypothesis is H0: yo2 ¼ 0, so that cðyÞ 1 y2. It is easy to see that the mean value expansion used to derive the LM statistic is valid provided lo 1 yo1 is in the interior of its parameter space under H0; yo 1 ðy0 o1; 0Þ0 can be on the boundary of Y. This observation is useful especially when testing hypotheses about parameters that must be either nonnegative or nonpositive. If we assume the generalized information matrix equality (12.53) with s2 o ¼ 1, the LM statistic simpliﬁes. The simpliﬁcation results from the following reasoning: (1) ~C~D ¼ 0 by the chain rule, where ~D 1 ‘ldð~lÞ, since c½dðlÞ\u0003 1 0 for l in L. (2) If E is M-Estimation 365", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 377, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p378::c0", "text": "a P \u0001 Q matrix E with rank Q, F is a P \u0001 ðP \u0004 QÞ matrix with rank P \u0004 Q, and E0F ¼ 0, then EðE0EÞ\u00041E0 ¼ IP \u0004 FðF0FÞ\u00041F0. (This is simply a statement about projections onto orthogonal subspaces.) Choosing E 1 ~A\u00041=2 ~C0 and F 1 ~A1=2 ~D gives ~A\u00041=2 ~C0ð~C~A\u00041 ~C0Þ\u00041 ~C~A\u00041=2 ¼ IP \u0004 ~A1=2 ~Dð~D0 ~A~DÞ\u00041 ~D0 ~A1=2. Now, pre- and post- multiply this equality by ~A\u00041=2 to get ~A\u00041 ~C0ð~C~A\u00041 ~C 0Þ\u00041 ~C~A\u00041 ¼ ~A\u00041 \u0004 ~Dð~D0 ~A~DÞ\u00041 ~D0. (3) Plug ~B ¼ ~A into expression (12.68) and use step 2, along with the ﬁrst-order con- dition ~D0ðPN i¼1 ~siÞ ¼ 0, to get LM ¼ X N i¼1 ~si !0 ~M\u00041 X N i¼1 ~si ! ð12:69Þ where ~M can be chosen as PN i¼1 ~Ai, PN i¼1 ~Hi, or PN i¼1 ~si~s0 i. (Each of these expressions consistently estimates Ao ¼ Bo when divided by N.) The last choice of ~M results in a statistic that is N times the uncentered R-squared, say R2 0, from the regression 1 on ~s0 i; i ¼ 1; 2; . . . ; N ð12:70Þ (Recall that ~s0 i is a 1 \u0001 P vector.) Because the dependent variable in regression (12.70) is unity, NR2 0 is equivalent to N \u0004 SSR0, where SSR0 is the sum of squared residuals from regression (12.70). This is often called the outer product of the score LM statistic because of the estimator it uses for Ao. While this statistic is simple to compute, there is ample evidence that it can have severe size distortions (typically, the null hypothe- sis is rejected much more often than the nominal size of the test). See, for example, Davidson and MacKinnon (1993), Bera and McKenzie (1986), Orme (1990), and Chesher and Spady (1991). The Hessian form of the LM statistic uses ~M ¼ PN i¼1 ~Hi, and it has a few draw- backs: (1) the LM statistic can be negative if the average estimated Hessian is not positive deﬁnite; (2) it requires computation of the second derivatives; and (3) it is not invariant to reparameterizations. We will discuss the last problem later. A statistic that always avoids the ﬁrst problem, and often the second and third problems, is based on E½Hðw; yoÞ j x\u0003, assuming that w partitions into endogenous variables y and exogenous variables x. We call the LM statistic that uses ~M ¼ PN i¼1 ~Ai the expected Hessian form of the LM statistic. This name comes from the fact that the statistic is based on the conditional expectation of Hðw; yoÞ given x. When it can be computed, the expected Hessian form is usually preferred because it tends to have the best small sample properties. The LM statistic in equation (12.69) is valid only when Bo ¼ Ao, and therefore it is not robust to failures of auxiliary assumptions in some important models. If Bo 0 Ao, the limiting distribution of equation (12.69) is not chi-square and is not suitable for testing. Chapter 12 366", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 378, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p379::c0", "text": "In the context of NLS, the expected Hessian form of the LM statistic needs to be modiﬁed for the presence of s2 o, assuming that Assumption NLS.3 holds under H0. Let ~s2 1 N\u00041 PN i¼1 ~u2 i be the estimate of s2 o using the restricted estimator of yo: ~ui 1 yi \u0004 mðxi; ~yÞ, i ¼ 1; 2; . . . ; N. It is customary not to make a degrees-of-freedom ad- justment when estimating the variance using the null estimates, partly because the sum of squared residuals for the restricted model is always larger than for the un- restricted model. The score evaluated at the restricted estimates can be written as ~si ¼ ‘y ~m0 i ~ui. Thus the LM statistic that imposes homoskedasticity is LM ¼ X N i¼1 ‘y ~m0 i ~ui !0 X N i¼1 ‘y ~m0 i‘y ~mi !\u00041 X N i¼1 ‘y ~m0 i ~ui ! =~s2 ð12:71Þ A little algebra shows that this expression is identical to N times the uncentered R- squared, R2 u, from the auxiliary regression ~ui on ‘y ~mi; i ¼ 1; 2; . . . ; N ð12:72Þ In other words, just regress the residuals from the restricted model on the gradient with respect to the unrestricted mean function but evaluated at the restricted esti- mates. Under H0 and Assumption NLS.3, LM ¼ NR2 u @ a w2 Q. In the nonlinear regression example with mðx; yÞ ¼ exp½xb þ d1ðxbÞ2 þ d2ðxbÞ3\u0003, let ~b be the restricted NLS estimator with d1 ¼ 0 and d2 ¼ 0; in other words, ~b is from a nonlinear regression with an exponential regression function. The restricted resid- uals are ~ui ¼ yi \u0004 expðxi ~bÞ, and the gradient of mðx; yÞ with respect to all parameters, evaluated at the null, is ‘ymðxi; bo; 0Þ ¼ fxi expðxiboÞ; ðxiboÞ2 expðxiboÞ; ðxiboÞ3 expðxiboÞg Plugging in ~b gives ‘y ~mi ¼ ½xi ~mi; ðxi ~bÞ2 ~mi; ðxi ~bÞ3 ~mi\u0003, where ~mi 1 expðxi ~bÞ. Regres- sion (12.72) becomes ~ui on xi ~mi; ðxi ~bÞ2 ~mi; ðxi ~bÞ3 ~mi; i ¼ 1; 2; . . . ; N ð12:73Þ Under H0 and homoskedasticity, NR2 u @ w2 2, since there are two restrictions being tested. This is a fairly simple way to test the exponential functional form without ever estimating the more complicated alternative model. Other models that nest the ex- ponential model are discussed in Wooldridge (1992). This example illustrates an important point: even though PN i¼1ðxi ~miÞ0~ui is identi- cally zero by the ﬁrst-order condition for NLS, the term xi ~mi must generally be included in regression (12.73). The R-squared from the regression without xi ~mi will be di¤erent because the remaining regressors in regression (12.73) are usually corre- lated with xi ~mi in the sample. [More importantly, for h ¼ 2 and 3, ðxibÞh expðxibÞ is M-Estimation 367", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 379, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p380::c0", "text": "probably correlated with xib in the population.] As a general rule, the entire gradient ‘y ~mi must appear in the auxiliary regression. In order to be robust against failure of Assumption NLS.3, the more general form of the statistic in expression (12.68) should be used. Fortunately, this statistic also can be easily computed for most hypotheses. Partition y into the ðP \u0004 QÞ \u0001 1 vector b and the Q vector d. Assume that the null hypothesis is H0: do ¼ d, where d is a prespeciﬁed vector (often containing all zeros, but not always). Let ‘b ~mi [1 \u0001 ðP \u0004 QÞ] and ‘d ~mi ð1 \u0001 QÞ denote the gradients with respect to b and d, respec- tively, evaluated at ~b and d. After tedious algebra, and using the special structure CðyÞ ¼ ½0 j IQ\u0003, where 0 is a Q \u0001 ðP \u0004 QÞ matrix of zero, the following procedure can be shown to produce expression (12.68): 1. Run a multivariate regression ‘d ~mi on ‘b ~mi; i ¼ 1; 2; . . . ; N ð12:74Þ and save the 1 \u0001 Q vector residuals, say ~ri. Then, for each i, form ~ui~ri. (That is, mul- tiply ~ui by each element of ~ri.) 2. LM ¼ N \u0004 SSR0 ¼ NR2 0 from the regression 1 on ~ui~ri; i ¼ 1; 2; . . . ; N ð12:75Þ where SSR0 is the usual sum of squared residuals. This step produces a statistic that has a limiting w2 Q distribution whether or not Assumption NLS.3 holds. See Wooldridge (1991a) for more discussion. We can illustrate the heteroskedasticity-robust test using the preceding exponential model. Regression (12.74) is the same as regressing each of ðxi ~bÞ2 ~mi and ðxi ~bÞ3 ~mi onto xi ~mi, and saving the residuals ~ri1 and ~ri2, respectively (N each). Then, regression (12.75) is simply 1 on ~ui~ri1, ~ui~ri2. The number of regressors in the ﬁnal regression of the robust test is always the same as the degrees of freedom of the test. Finally, these procedures are easily modiﬁed for WNLS. Simply multiply both ~ui and ‘y ~mi by 1= ﬃﬃﬃﬃ~hi p , where the variance estimates ~hi are based on the null model (so we use a @ rather than a 5). The nonrobust LM statistic that maintains Assumption WNLS.3 is obtained as in regression (12.72). The robust form, which allows Varðy j xÞ 0 s2 ohðx; goÞ, follows exactly as in regressions (12.74) and (12.75). The invariance issue for the score statistic is somewhat complicated, but several results are known. First, it is easy to see that the outer product form of the statistic is invariant to di¤erentiable reparameterizations. Write f ¼ gðyÞ as a twice continu- ously di¤erentiable, invertible reparameterization; thus the P \u0001 P Jacobian of g, Chapter 12 368", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 380, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p381::c0", "text": "GðyÞ, is nonsingular for all y A Y. The objective function in terms of f is qgðw; fÞ, and we must have qg½w; gðyÞ\u0003 ¼ qðw; yÞ for all y A Y. Di¤erentiating and transposing gives sðw; yÞ ¼ GðyÞ0sg½w; gðyÞ\u0003, where sgðw; fÞ is the score of qg½w; f\u0003. If ~f is the restricted estimator of f, then ~f ¼ gð~yÞ, and so, for each observation i, ~sg i ¼ ð~G0Þ\u00041~si. Plugging this equation into the LM statistic in equation (12.69), with ~M chosen as the outer product form, shows that the statistic based on ~sg i is identical to that based on ~si. Score statistics based on the estimated Hessian are not generally invariant to re- parameterization because they can involve second derivatives of the function gðyÞ; see Davidson and MacKinnon (1993, Section 13.6) for details. However, when w parti- tions as ðx; yÞ, score statistics based on the expected Hessian (conditional on x), Aðx; yÞ, are often invariant. In Chapter 13 we will see that this is always the case for conditional maximum likelihood estimation. Invariance also holds for NLS and WNLS for both the usual and robust LM statistics because any reparameterization comes through the conditional mean. Predicted values and residuals are invariant to reparameterization, and the statistics obtained from regressions (12.72) and (12.75) only involve the residuals and ﬁrst derivatives of the conditional mean function. As in the usual outer product LM statistic, the Jacobian in the ﬁrst derivative cancels out. 12.6.3 Tests Based on the Change in the Objective Function When both the restricted and unrestricted models are easy to estimate, a test based on the change in the objective function can greatly simplify the mechanics of obtaining a test statistic: we only need to obtain the value of the objective function with and without the restrictions imposed. However, the computational simplicity comes at a price in terms of robustness. Unlike the Wald and score tests, a test based on the change in the objective function cannot be made robust to general failure of as- sumption (12.53). Therefore, throughout this subsection we assume that the general- ized information matrix equality holds. Because the minimized objective function is invariant with respect to any reparameterization, the test statistic is invariant. In the context of two-step estimators, we must also assume that ^g has no e¤ect on the asymptotic distribution of the M-estimator. That is, we maintain assumption (12.37) when nuisance parameter estimates appear in the objective function (see Problem 12.8). We ﬁrst consider the case where s2 o ¼ 1, so that Bo ¼ Ao. Using a second-order Taylor expansion, X N i¼1 qðwi; ~yÞ \u0004 X N i¼1 qðwi; ^yÞ ¼ X N i¼1 sið^yÞ þ ð1=2Þð~y \u0004 ^yÞ0 X N i¼1 €Hi ! ð~y \u0004 ^yÞ M-Estimation 369", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 381, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p382::c0", "text": "where €Hi is the P \u0001 P Hessian evaluate at mean values between ~y and ^y. Therefore, under H0 (using the ﬁrst-order condition for ^y), we have 2 X N i¼1 qðwi; ~yÞ \u0004 X N i¼1 qðwi; ^yÞ \" # ¼ ½ ﬃﬃﬃﬃ N p ð~y \u0004 ^yÞ\u00030A0½ ﬃﬃﬃﬃ N p ð~y \u0004 ^yÞ\u0003 þ opð1Þ ð12:76Þ since N\u00041 PN i¼1 €Hi ¼ Ao þ op (1) and ﬃﬃﬃﬃ N p ð~y \u0004 ^yÞ ¼ Opð1Þ. In fact, it follows from equations (12.33) (without ^g) and (12.66) that ﬃﬃﬃﬃ N p ð~y \u0004 ^yÞ ¼ A\u00041 o N\u00041=2 PN i¼1 sið~yÞ þ opð1Þ. Plugging this equation into equation (12.76) shows that QLR 1 2 X N i¼1 qðwi; ~yÞ \u0004 X N i¼1 qðwi; ^yÞ \" # ¼ N\u00041=2 X N i¼1 ~si !0 A\u00041 o N\u00041=2 X N i¼1 ~si ! þ opð1Þ ð12:77Þ so that QLR has the same limiting distribution, w2 Q, as the LM statistic under H0. [See equation (12.69), remembering that plimð ~M=NÞ ¼ Ao.] We call statistic (12.77) the quasi-likelihood ratio (QLR) statistic, which comes from the fact that the leading ex- ample of equation (12.77) is the likelihood ratio statistic in the context of maximum likelihood estimation, as we will see in Chapter 13. We could also call equation (12.77) a criterion function statistic, as it is based on the di¤erence in the criterion or objective function with and without the restrictions imposed. When nuisance parameters are present, the same estimate, say ^g, should be used in obtaining the restricted and unrestricted estimates. This is to ensure that QLR is nonnegative given any sample. Typically, ^g would be based on initial estimation of the unrestricted model. If s2 o 0 1, we simply divide QLR by ^s2, which is a consistent estimator of s2 o obtained from the unrestricted estimation. For example, consider NLS under Assumptions NLS.1–NLS.3. When equation (12.77) is divided by ^s2 in equation (12.57), we obtain ðSSRr \u0004 SSRurÞ=½SSRur=ðN \u0004 PÞ\u0003, where SSRr and SSRur are the restricted and unrestricted sums of squared residuals. Sometimes an F version of this statistic is used instead, which is obtained by dividing the chi-square version by Q: F ¼ ðSSRr \u0004 SSRurÞ SSRur \u0002 ðN \u0004 PÞ Q ð12:78Þ This has exactly the same form as the F statistic from classical linear regression analysis. Under the null hypothesis and homoskedasticity, F can be treated as having Chapter 12 370", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 382, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p383::c0", "text": "an approximate FQ;N\u0004P distribution. (As always, this treatment is justiﬁed because Q \u0002 FQ;N\u0004P @ a w2 Q as N \u0004 P ! y.) Some authors (for example, Gallant, 1987) have found that F has better ﬁnite sample properties than the chi-square version of the statistic. For weighted NLS, the same statistic works under Assumption WNLS.3 provided the residuals (both restricted and unrestricted) are weighted by 1= ^hi ﬃﬃﬃﬃ p , where the ^hi are obtained from estimation of the unrestricted model. 12.6.4 Behavior of the Statistics under Alternatives To keep the notation and assumptions as simple as possible, and to focus on the computation of valid test statistics under various assumptions, we have only derived the limiting distribution of the classical test statistics under the null hypothesis. It is also important to know how the tests behave under alternative hypotheses in order to choose a test with the highest power. All the tests we have discussed are consistent against the alternatives they are spe- ciﬁcally designed against. While this consistency is desirable, it tells us nothing about the likely ﬁnite sample power that a statistic will have against particular alternatives. A framework that allows us to say more uses the notion of a sequence of local alter- natives. Specifying a local alternative is a device that can approximate the ﬁnite sample power of test statistics for alternatives ‘‘close’’ to H0. If the null hypothesis is H0: cðyoÞ ¼ 0 then a sequence of local alternatives is HN 1 : cðyo;NÞ ¼ do= ﬃﬃﬃﬃ N p ð12:79Þ where do is a given Q \u0001 1 vector. As N ! y, HN 1 approaches H0, since do= ﬃﬃﬃﬃ N p ! 0. The division by ﬃﬃﬃﬃ N p means that the alternatives are local: for given N, equation (12.79) is an alternative to H0, but as N ! y, the alternative gets closer to H0. Dividing do by ﬃﬃﬃﬃ N p ensures that each of the statistics has a well-deﬁned limiting dis- tribution under the alternative that di¤ers from the limiting distribution under H0. It can be shown that, under equation (12.79), the general forms of the Wald and LM statistics have a limiting noncentral chi-square distribution with Q degrees of freedom under the regularity conditions used to obtain their null limiting distribu- tions. The noncentrality parameter depends on Ao, Bo, Co, and do, and can be esti- mated by using consistent estimators of Ao, Bo, and Co. When we add assumption (12.53), then the special versions of the Wald and LM statistics and the QLR statis- tics have limiting noncentral chi-square distributions. For various do, we can estimate what is known as the asymptotic local power of the test statistics by computing probabilities from noncentral chi-square distributions. M-Estimation 371", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 383, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p384::c0", "text": "Consider the Wald statistic where Bo ¼ Ao. Denote by yo the limit of yo;N as N ! y. The usual mean value expansion under HN 1 gives ﬃﬃﬃﬃ N p cð^yÞ ¼ do þ CðyoÞ ﬃﬃﬃﬃ N p ð^y \u0004 yo;NÞ þ opð1Þ and, under standard assumptions, ﬃﬃﬃﬃ N p ð^y \u0004 yo;NÞ @ a Normalð0; A\u00041 o Þ. Therefore, ﬃﬃﬃﬃ N p cð^yÞ @ a Normalðdo; CoA\u00041 o C0 oÞ under the sequence (12.79). This result implies that the Wald statistic has a limiting noncentral chi-square distribution with Q degrees of freedom and noncentrality parameter d0 oðCoA\u00041 o C0 oÞ\u00041do. This turns out to be the same noncentrality parameter for the LM and QLR statistics when Bo ¼ Ao. The details are similar to those under H0; see, for example, Gallant (1987, Section 3.6). The statistic with the largest noncentrality parameter has the largest asymptotic local power. For choosing among the Wald, LM, and QLR statistics, this criterion does not help: they all have the same noncentrality parameters under equation (12.79). [For the QLR statistic, assumption (12.53) must also be maintained.] The notion of local alternatives is useful when choosing among statistics based on di¤erent estimators. Not surprisingly, the more e‰cient estimator produces tests with the best asymptotic local power under standard assumptions. But we should keep in mind the e‰ciency–robustness trade-o¤, especially when e‰cient test statistics are computed under tenuous assumptions. General analyses under local alternatives are available in Gallant (1987), Gallant and White (1988), and White (1994). See Andrews (1989) for innovative suggestions for using local power analysis in applied work. 12.7 Optimization Methods In this section we brieﬂy discuss three iterative schemes that can be used to solve the general minimization problem (12.8) or (12.31). In the latter case, the minimization is only over y, so the presence of ^g changes nothing. If ^g is present, the score and Hessian with respect to y are simply evaluated at ^g. These methods are closely related to the asymptotic variance matrix estimators and test statistics we discussed in Sec- tions 12.5 and 12.6. 12.7.1 The Newton-Raphson Method Iterative methods are deﬁned by an algorithm for going from one iteration to the next. Let yfgg be the P \u0001 1 vector on the gth iteration, and let yfgþ1g be the value on the next iteration. To motivate how we get from yfgg to yfgþ1g, use a mean value ex- pansion (row by row) to write Chapter 12 372", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 384, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p385::c0", "text": "X N i¼1 siðyfgþ1gÞ ¼ X N i¼1 siðyfggÞ þ X N i¼1 HiðyfggÞ \" # ðyfgþ1g \u0004 yfggÞ þ rfgg ð12:80Þ where siðyÞ is the P \u0001 1 score with respect to y, evaluated at observation i, HiðyÞ is the P \u0001 P Hessian, and rfgg is a P \u0001 1 vector of remainder terms. We are trying to ﬁnd the solution ^y to equation (12.14). If yfgþ1g ¼ ^y, then the left-hand side of equa- tion (12.80) is zero. Setting the left-hand side to zero, ignoring rfgg, and assuming that the Hessian evaluated at yfgg is nonsingular, we can write yfgþ1g ¼ yfgg \u0004 X N i¼1 HiðyfggÞ \" #\u00041 X N i¼1 siðyfggÞ \" # ð12:81Þ Equation (12.81) provides an iterative method for ﬁnding ^y. To begin the iterations we must choose a vector of starting values; call this vector yf0g. Good starting values are often di‰cult to come by, and sometimes we must experiment with several choices before the problem converges. Ideally, the iterations wind up at the same place regardless of the starting values, but this outcome is not guaranteed. Given the starting values, we plug yf0g into the right-hand side of equation (12.81) to get yf1g. Then, we plug yf1g into equation (12.81) to get yf2g, and so on. If the iterations are proceeding toward the minimum, the increments yfgþ1g \u0004 yfgg will eventually become very small: as we near the solution, PN i¼1 siðyfggÞ gets close to zero. Some use as a stopping rule the requirement that the largest absolute change jyfgþ1g j \u0004 yfgg j j, for j ¼ 1; 2; . . . ; P, is smaller than some small constant; others prefer to look at the largest percentage change in the parameter values. Another popular stopping rule is based on the quadratic form X N i¼1 siðyfggÞ \" #0 X N i¼1 HiðyfggÞ \" #\u00041 X N i¼1 siðyfggÞ \" # ð12:82Þ where the iterations stop when expression (12.82) is less than some suitably small number, say .0001. The iterative scheme just outlined is usually called the Newton-Raphson method. It is known to work in a variety of circumstances. Our motivation here has been heuristic, and we will not investigate situations under which the Newton-Raphson method does not work well. (See, for example, Quandt, 1983, for some theoretical results.) The Newton-Raphson method has some drawbacks. First, it requires com- puting the second derivatives of the objective function at every iteration. These cal- culations are not very taxing if closed forms for the second partials are available, but M-Estimation 373", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 385, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p386::c0", "text": "in many cases they are not. A second problem is that, as we saw for the case of nonlinear least squares, the sum of the Hessians evaluated at a particular value of y may not be positive deﬁnite. If the inverted Hessian in expression (12.81) is not pos- itive deﬁnite, the procedure may head in the wrong direction. We should always check that progress is being made from one iteration to the next by computing the di¤erence in the values of the objective function from one iteration to the next: X N i¼1 qiðyfgþ1gÞ \u0004 X N i¼1 qiðyfggÞ ð12:83Þ Because we are minimizing the objective function, we should not take the step from g to g þ 1 unless expression (12.83) is negative. [If we are maximizing the function, the iterations in equation (12.81) can still be used because the expansion in equation (12.80) is still appropriate, but then we want expression (12.83) to be positive.] A slight modiﬁcation of the Newton-Raphson method is sometimes useful to speed up convergence: multiply the Hessian term in expression (12.81) by a positive num- ber, say r, known as the step size. Sometimes the step size r ¼ 1 produces too large a change in the parameters. If the objective function does not decrease using r ¼ 1, then try, say, r ¼ 1 2. Again, check the value of the objective function. If it has now decreased, go on to the next iteration (where r ¼ 1 is usually used at the beginning of each iteration); if the objective function still has not decreased, replace r with, say, 1 4. Continue halving r until the objective function decreases. If you have not succeeded in decreasing the objective function after several choices of r, new starting values might be needed. Or, a di¤erent optimization method might be needed. 12.7.2 The Berndt, Hall, Hall, and Hausman Algorithm In the context of maximum likelihood estimation, Berndt, Hall, Hall, and Hausman (1974) (hereafter, BHHH) proposed using the outer product of the score in place of the Hessian. This method can be applied in the general M-estimation case [even though the information matrix equality (12.53) that motivates the method need not hold]. The BHHH iteration for a minimization problem is yfgþ1g ¼ yfgg \u0004 r X N i¼1 siðyfggÞsiðyfggÞ0 \" #\u00041 X N i¼1 siðyfggÞ \" # ð12:84Þ where r is the step size. [If we want to maximize PN i¼1 qðwi; yÞ, the minus sign in equation (12.84) should be replaced with a plus sign.] The term multiplying r, some- Chapter 12 374", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 386, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p387::c0", "text": "times called the direction for the next iteration, can be obtained as the P \u0001 1 OLS coe‰cients from the regression 1 on siðyfggÞ0; i ¼ 1; 2; . . . ; N ð12:85Þ The BHHH procedure is easy to implement because it requires computation of the score only; second derivatives are not needed. Further, since the sum of the outer product of the scores is always at least positive semideﬁnite, it does not su¤er from the potential nonpositive deﬁniteness of the Hessian. A convenient stopping rule for the BHHH method is obtained as in expression (12.82), but with the sum of the outer products of the score replacing the sum of the Hessians. This is identical to N times the uncentered R-squared from regression (12.85). Interestingly, this is the same regression used to obtain the outer product of the score form of the LM statistic when Bo ¼ Ao, and this fact suggests a natural method for estimating a complicated model after a simpler version of the model has been estimated. Set the starting value, yf0g, equal to the vector of restricted estimates, ~y. Then NR2 0 from the regression used to obtain the ﬁrst iteration can be used to test the restricted model against the more general model to be estimated; if the restrictions are not rejected, we could just stop the iterations. Of course, as we discussed in Sec- tion 12.6.2, this form of the LM statistic is often ill-behaved even with fairly large sample sizes. 12.7.3 The Generalized Gauss-Newton Method The ﬁnal iteration scheme we cover is closely related to the estimator of the expected value of the Hessian in expression (12.44). Let Aðx; yoÞ be the expected value of Hðw; yoÞ conditional on x, where w is partitioned into y and x. Then the generalized Gauss-Newton method uses the updating equation yfgþ1g ¼ yfgg \u0004 r X N i¼1 AiðyfggÞ \" #\u00041 X N i¼1 siðyfggÞ \" # ð12:86Þ where yfgg replaces yo in Aðxi; yoÞ. (As before, Ai and si might also depend on ^g.) This scheme works well when Aðx; yoÞ can be obtained in closed form. In the special case of nonlinear least squares, we obtain what is traditionally called the Gauss-Newton method (for example, Quandt, 1983). Since siðyÞ ¼ \u0004‘ymiðyÞ0½yi \u0004 miðyÞ\u0003, the iteration step is yfgþ1g ¼ yfgg þ r X N i¼1 ‘ymfgg0 i ‘ymfgg i !\u00041 X N i¼1 ‘ymfgg0 i ufgg i ! M-Estimation 375", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 387, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p388::c0", "text": "The term multiplying the step size r is obtained as the OLS coe‰cients of the re- gression of the resididuals on the gradient, both evaluated at yfgg. The stopping rule can be based on N times the uncentered R-squared from this regression. Note how closely the Gauss-Newton method of optimization is related to the regression used to obtain the nonrobust LM statistic [see regression (12.72)]. 12.7.4 Concentrating Parameters out of the Objective Function In some cases, it is computationally convenient to concentrate one set of parameters out of the objective function. Partition y into the vectors b and g. Then the ﬁrst-order conditions that deﬁne ^y are X N i¼1 ‘bqðwi; b; gÞ ¼ 0; X N i¼1 ‘gqðwi; b; gÞ ¼ 0 ð12:87Þ Rather than solving these for ^b and ^g, suppose that the second set of equations can be solved for g as a function of W 1 ðw1; w2; . . . ; wNÞ and b for any outcomes W and any b in the parameter set g ¼ gðW; bÞ. Then, by construction, X N i¼1 ‘gq½wi; b; gðW; bÞ\u0003 ¼ 0 ð12:88Þ When we plug gðW; bÞ into the original objective function, we obtain the con- centrated objective function, QcðW; bÞ ¼ X N i¼1 q½wi; b; gðW; bÞ\u0003 ð12:89Þ Under standard di¤erentiability assumptions, the minimizer of equation (12.89) is identical to the ^b that solves equations (12.87) (along with ^g), as can be seen by dif- ferentiating equation (12.89) with respect to b using the chain rule, setting the result to zero, and using equation (12.88); then ^g can be obtained as gðW; ^bÞ. As a device for studying asymptotic properties, the concentrated objective function is of limited value because gðW; bÞ generally depends on all of W, in which case the objective function cannot be written as the sum of independent, identically dis- tributed summands. One setting where equation (12.89) is a sum of i.i.d. functions occurs when we concentrate out individual-speciﬁc e¤ects from certain nonlinear panel data models. In addition, the concentrated objective function can be useful for establishing the equivalence of seemingly di¤erent estimation approaches. Chapter 12 376", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 388, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p389::c0", "text": "12.8 Simulation and Resampling Methods So far we have focused on the asymptotic properties of M-estimators, as these pro- vide a uniﬁed framework for inference. But there are a few good reasons to go be- yond asymptotic results, at least in some cases. First, the asymptotic approximations need not be very good, especially with small sample sizes, highly nonlinear models, or unusual features of the population distribution of wi. Simulation methods, while always special, can help determine how well the asymptotic approximations work. Resampling methods can allow us to improve on the asymptotic distribution approximations. Even if we feel comfortable with asymptotic approximations to the distribution of ^y, we may not be as conﬁdent in the approximations for estimating a nonlinear function of the parameters, say go ¼ gðyoÞ. Under the assumptions in Section 3.5.2, we can use the delta method to approximate the variance of ^g ¼ gð^yÞ. Depending on the nature of gð\u0002Þ, applying the delta method might be di‰cult, and it might not re- sult in a very good approximation. Resampling methods can simplify the calculation of standard errors, conﬁdence intervals, and p-values for test statistics, and we can get a good idea of the amount of ﬁnite-sample bias in the estimation method. In ad- dition, under certain assumptions and for certain statistics, resampling methods can provide quantiﬁable improvements to the usual asymptotics. 12.8.1 Monte Carlo Simulation In a Monte Carlo simulation, we attempt to estimate the mean and variance— assuming that these exist—and possibly other features of the distribution of the M- estimator, ^y. The idea is usually to determine how much bias ^y has for estimating yo, or to determine the e‰ciency of ^y compared with other estimators of yo. In addition, we often want to know how well the asymptotic standard errors approximate the standard deviations of the ^yj. To conduct a simulation, we must choose a population distribution for w, which depends on the ﬁnite dimensional vector yo. We must set the values of yo, and decide on a sample size, N. We then draw a random sample of size N from this distribution and use the sample to obtain an estimate of yo. We draw a new random sample and compute another estimate of yo. We repeat the process for several iterations, say M. Let ^yðmÞ be the estimate of yo based on the mth iteration. Given f^yðmÞ: m ¼ 1; 2; . . . ; Mg, we can compute the sample average and sample variance to estimate Eð^yÞ and Varð^yÞ, respectively. We might also form t statistics or other test statistics to see how well the asymptotic distributions approximate the ﬁnite sample distributions. M-Estimation 377", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 389, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p390::c0", "text": "We can also see how well asymptotic conﬁdence intervals cover the population parameter relative to the nominal conﬁdence level. A good Monte Carlo study varies the value of yo, the sample size, and even the general form of the distribution of w. Obtaining a thorough study can be very chal- lenging, especially for a complicated, nonlinear model. First, to get good estimates of the distribution of ^y, we would like M to be large (perhaps several thousand). But for each Monte Carlo iteration, we must obtain ^yðmÞ, and this step can be computation- ally expensive because it often requires the iterative methods we discussed in Section 12.7. Repeating the simulations for many di¤erent sample sizes N, values of yo, and distributional shapes can be very time-consuming. In most economic applications, wi is partitioned as ðxi; yiÞ. While we can draw the full vector wi randomly in the Monte Carlo iterations, more often the xi are ﬁxed at the beginning of the iterations, and then yi is drawn from the conditional distribution given xi. This method simpliﬁes the simulations because we do not need to vary the distribution of xi along with the distribution of interest, the distribution of yi given xi. If we ﬁx the xi at the beginning of the simulations, the distributional features of ^y that we estimate from the Monte Carlo simulations are conditional on fx1; x2; . . . ; xNg. This conditional approach is especially common in linear and nonlinear regression contexts, as well as conditional maximum likelihood. It is important not to rely too much on Monte Carlo simulations. Many estimation methods, including OLS, IV, and panel data estimators, have asymptotic properties that do not depend on underlying distributions. In the nonlinear regression model, the NLS estimator is ﬃﬃﬃﬃ N p -asymptotically normal, and the usual asymptotic variance matrix (12.58) is valid under Assumptions NLS.1–NLS.3. However, in a typical Monte Carlo simulation, the implied error, u, is assumed to be independent of x, and the distribution of u must be speciﬁed. The Monte Carlo results then pertain to this distribution, and it can be misleading to extrapolate to di¤erent settings. In addition, we can never try more than just a small part of the parameter space. Since we never know the population value yo, we can never be sure how well our Monte Carlo study describes the underlying population. Hendry (1984) discusses how response surface analysis can be used to reduce the speciﬁcity of Monte Carlo studies. See also Davidson and MacKinnon (1993, Chapter 21). 12.8.2 Bootstrapping A Monte Carlo simulation, although it is informative about how well the asymptotic approximations can be expected to work in speciﬁc situations, does not generally help us reﬁne our inference given a particular sample. (Since we do not know yo, we cannot know whether our Monte Carlo ﬁndings apply to the population we are Chapter 12 378", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 390, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p391::c0", "text": "studying. Nevertheless, researchers sometimes use the results of a Monte Carlo sim- ulation to obtain rules of thumb for adjusting standard errors or for adjusting critical values for test statistics.) The method of bootstrapping, which is a popular resampling method, can be used as an alternative to asymptotic approximations for obtaining standard errors, conﬁdence intervals, and p-values for test statistics. Though there are several variants of the bootstrap, we begin with one that can be applied to general M-estimation. The goal is to approximate the distribution of ^y without relying on the usual ﬁrst-order asymptotic theory. Let fw1; w2; . . . ; wNg denote the outcome of the random sample used to obtain the estimate. The non- parametric bootstrap is essentially a Monte Carlo simulation where the observed sample is treated as the population. In other words, at each bootstrap iteration, b, a random sample of size N is drawn from fw1; w2; . . . ; wNg. (That is, we sample with replacement.) In practice, we use a random number generator to obtain N integers from the set f1; 2; . . . ; Ng; in the vast majority of iterations some integers will be repeated at least once. These integers index the elements that we draw from fw1; w2; . . . ; wNg; call these fwðbÞ 1 ; wðbÞ 2 ; . . . ; wðbÞ N g. Next, we use this bootstrap sample to obtain the M-estimate ^yðbÞ by solving min y A Y X N i¼1 qðwðbÞ i ; yÞ We iterate the process B times, obtaining ^yðbÞ, b ¼ 1; . . . ; B. These estimates can now be used as in a Monte Carlo simulation. Computing the average of the ^yðbÞ, say ^y, allows us to estimate the bias in ^y. The sample variance, ðB \u0004 1Þ\u00041 PB b¼1½^yðbÞ \u0004 ^y\u0003 \u0002 ½^yðbÞ \u0004 ^y\u00030, can be used to obtain standard errors for the ^yj—the estimates from the original sample. A 95 percent bootstrapped conﬁdence interval for yoj can be obtained by ﬁnding the 2.5 and 97.5 percentiles in the list of values f^yðbÞ j : b ¼ 1; . . . ; Bg. The p-value for a test statistic is approximated as the fraction of times the bootstrapped test statistic exceeds the statistic computed from the original sample. The parametric bootstrap is even more similar to a standard Monte Carlo simula- tion because we assume that the distribution of w is known up to the parameters yo. Let f ð\u0002 ; yÞ denote the parametric density. Then, on each bootstrap iteration, we draw a random sample of size N from f ð\u0002 ; ^yÞ; this gives fwðbÞ 1 ; wðbÞ 2 ; . . . ; wðbÞ N g and the rest of the calculations are the same as in the nonparametric bootstrap. [With the parametric bootstrap when f ð\u0002 ; yÞ is a continuous density, only rarely would we ﬁnd repeated values among the wðbÞ i .] When wi is partitioned into ðxi; yiÞ, where the xi are conditioning variables, other resampling schemes are sometimes preferred. For example, in a regression model M-Estimation 379", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 391, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p392::c0", "text": "where the error ui is independent of xi, we ﬁrst compute the NLS estimate ^y and the NLS residuals, ^ui ¼ yi \u0004 mðxi; ^yÞ, i ¼ 1; 2; . . . ; N. Then, using the procedure de- scribed for the nonparametric bootstrap, a bootstrap sample of residuals, f^uðbÞ i : i ¼ 1; 2; . . . ; Ng, is obtained, and we compute yðbÞ i ¼ mðxi; ^yÞ þ ^uðbÞ i . Using the generated data fðxi; yðbÞ i Þ: i ¼ 1; 2; . . . ; Ng, we compute the NLS estimate, ^yðbÞ. This procedure is called the nonparametric residual bootstrap. (We resample the residuals and use these to generate a sample on the dependent variable, but we do not resample the conditioning variables, xi.) If the model is nonlinear in y, this method can be com- putationally demanding because we want B to be several hundred, if not several thousand. Nonetheless, such procedures are becoming more and more feasible as computational speed increases. When ui has zero conditional mean ½Eðui j xiÞ ¼ 0\u0003 but is heteroskedastic ½Varðui j xiÞ depends on xi], alternative sampling methods, in par- ticular the wild bootstrap, can be used to obtain heteroskedastic-consistent standard errors. See, for example, Horowitz (in press). For certain test statistics, the bootstrap can be shown to improve upon the ap- proximation provided by the ﬁrst-order asymptotic theory that we treat in this book. A detailed treatment of the bootstrap, including discussions of when it works and when it does not, is given in Horowitz (in press). Problems 12.1. Use equation (12.4) to show that yo minimizes Ef½y \u0004 mðx; yÞ\u00032 j xg over Y for any x. Explain why this result is stronger than stating that yo solves problem (12.3). 12.2. Consider the model Eðy j xÞ ¼ mðx; yoÞ Varðy j xÞ ¼ expðao þ xgoÞ where x is 1 \u0001 K. The vector yo is P \u0001 1 and go is K \u0001 1. a. Deﬁne u 1 y \u0004 Eðy j xÞ. Show that Eðu2 j xÞ ¼ expðao þ xgoÞ. b. Let ^ui denote the residuals from estimating the conditional mean by NLS. Argue that ao and go can be consistently estimated by a nonlinear regression where ^u2 i is the dependent variable and the regression function is expðao þ xgoÞ. (Hint: Use the results on two-step estimation.) c. Using part b, propose a (feasible) weighted least squares procedure for estimat- ing yo. Chapter 12 380", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 392, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p393::c0", "text": "d. If the error u is divided by ½Varðu j xÞ\u00031=2, we obtain v 1 exp½\u0004ðao þ xgoÞ=2\u0003u. Argue that if v is independent of x, then go is consistently estimated from the re- gression logð^u2 i Þ on 1, xi, i ¼ 1; 2; . . . ; N. [The intercept from this regression will not consistently estimate ao, but this fact does not matter, since expðao þ xgoÞ ¼ s2 o expðxgoÞ, and s2 o can be estimated from the WNLS regression.] e. What would you do after running WNLS if you suspect the variance function is misspeciﬁed? 12.3. Consider the exponential regression function mðx; yÞ ¼ expðxyÞ, where x is 1 \u0001 K. a. Suppose you have estimated a special case of the model, ^Eðy j zÞ ¼ exp½^y1 þ ^y2 logðz1Þ þ ^y3z2\u0003, where z1 and z2 are the conditioning variables. Show that ^y2 is approximately the elasticity of ^Eðy j zÞ with respect to z1. b. In the same estimated model from part a, how would you approximate the per- centage change in ^Eðy j zÞ given Dz2 ¼ 1? c. Now suppose a square of z2 is added: ^Eðy j zÞ ¼ exp½^y1 þ ^y2 logðz1Þ þ ^y3z2 þ ^y4z2 2\u0003, where ^y3 > 0 and ^y4 < 0. How would you compute the value of z2 where the partial e¤ect of z2 on ^Eðy j zÞ becomes negative? d. Now write the general model as expðxyÞ ¼ expðx1y1 þ x2y2Þ, where x1 is 1 \u0001 K1 (and probably contains unity as an element) and x2 is 1 \u0001 K2. Derive the usual (nonrobust) and heteroskedasticity-robust LM tests of H0: yo2 ¼ 0, where yo indexes Eðy j xÞ. 12.4. a. Show that the score for WNLS is siðy; gÞ ¼ \u0004‘ymðxi; yÞ0uiðyÞ=hðxi; gÞ. b. Show that, under Assumption WNLS.1, E½siðyo; gÞ j xi\u0003 ¼ 0 for any value of g. c. Show that, under Assumption WNLS.1, E½‘gsiðyo; gÞ\u0003 ¼ 0 for any value of g. d. How would you estimate Avarð^yÞ without Assumption WNLS.3? 12.5. For the regression model mðx; yÞ ¼ G½xb þ d1ðxbÞ2 þ d2ðxbÞ3\u0003 where Gð\u0002Þ is a known, twice continuously di¤erentiable function with derivative gð\u0002Þ, derive the standard LM test of H0: do2 ¼ 0, do3 ¼ 0 using NLS. Show that, when Gð\u0002Þ is the identify function, the test reduces to RESET from Section 6.2.3. 12.6. Consider a panel data model for a random draw i from the population: yit ¼ mðxit; yoÞ þ uit; Eðuit j xitÞ ¼ 0; t ¼ 1; . . . ; T M-Estimation 381", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 393, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p394::c0", "text": "a. If you apply pooled nonlinear least squares to estimate yo, how would you estimate its asymptotic variance without further assumptions? b. Suppose that the model is dynamically complete in the conditional mean, so that Eðuit j xit; ui;t\u00041; xi;t\u00041; . . .Þ ¼ 0 for all t. In addition, Eðu2 it j xitÞ ¼ s2 o. Show that the usual statistics from a pooled NLS regression are valid. fHint: The objective function for each i is qiðyÞ ¼ PT t¼1½yit \u0004 mðxit; yÞ\u00032=2 and the score is siðyÞ ¼ \u0004 PT t¼1 ‘ymðxit; yÞ0uitðyÞ. Now show that Bo ¼ s2 oAo and that s2 o is consistently esti- mated by ðNT \u0004 PÞ\u00041 PN i¼1 PT t¼1 ^u2 it.g 12.7. Consider a nonlinear analogue of the SUR system from Chapter 7: Eðyig j xiÞ ¼ Eðyig j xigÞ ¼ mgðxig; yogÞ; g ¼ 1; . . . ; G Thus, each yog can be estimated by NLS using only equation g; call these ^^y^yg. Suppose also that Varðyi j xiÞ ¼ Wo, where Wo is G \u0001 G and positive deﬁnite. a. Explain how to consistently estimate Wo (as usual, with G ﬁxed and N ! y). Call this estimator ^W. b. Let ^y solve the problem min y X N i¼1 ½yi \u0004 mðxi; yÞ\u00030 ^W\u00041½yi \u0004 mðxi; yÞ\u0003=2 where mðxi; yÞ is the G \u0001 1 vector of conditional mean functions and yi is G \u0001 1; this is sometimes called the nonlinear SUR estimator. Show that Avar ﬃﬃﬃﬃ N p ð^y \u0004 yoÞ ¼ fE½‘ymðxi; yoÞ0W\u00041 o ‘ymðxi; yoÞ\u0003g\u00041 fHint: Under standard regularity conditions, N\u00041=2 PN i¼1 ‘ymðxi; yoÞ0 ^W\u00041½yi \u0004 mðxi; yoÞ\u0003 ¼ N\u00041=2 PN i¼1 ‘ymðxi; yoÞ0W\u00041 o ½yi \u0004 mðxi; yoÞ\u0003 þ opð1Þ.g c. How would you estimate Avarð^yÞ? d. If Wo is diagonal and if the assumptions stated previously hold, show that non- linear least squares equation by equation is just as asymptotically e‰cient as the nonlinear SUR estimator. e. Is there a nonlinear analogue of Theorem 7.7 for linear systems in the sense that nonlinear SUR and NLS equation by equation are asymptotically equivalent when the same explanatory variables appear in each equation? [Hint: When would ‘ymðxi; yoÞ have the form needed to apply the hint in Problem 7.5? You might try Eðyg j xÞ ¼ expðxyogÞ for all g as an example.] Chapter 12 382", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 394, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p395::c0", "text": "12.8. Consider the M-estimator with estimated nuisance parameter ^g, where ﬃﬃﬃﬃ N p ð^g \u0004 goÞ ¼ Opð1Þ. If assumption (12.37) holds under the null hypothesis, show that the QLR statistic still has a limiting chi-square distribution, assuming also that Ao ¼ Bo. [Hint: Start from equation (12.76) but where ﬃﬃﬃﬃ N p ð~y \u0004 ^yÞ ¼ A\u00041 o N\u00041=2\u0002 PN i¼1 sið~y; ^gÞ þ opð1Þ. Now use a mean value expansion of the score about ð~y; goÞ to show that ﬃﬃﬃﬃ N p ð~y \u0004 ^yÞ ¼ A\u00041 o N\u00041=2 PN i¼1 sið~y; goÞ þ opð1Þ.] 12.9. For scalar y, suppose that y ¼ mðx; boÞ þ u, where x is a 1 \u0001 K vector. a. If Eðu j xÞ ¼ 0, what can you say about Medðy j xÞ? b. Suppose that u and x are independent. Show that Eðy j xÞ \u0004 Medðy j xÞ does not depend on x. c. What does part b imply about qEðy j xÞ=qxj and q Medðy j xÞ=qxj? 12.10. For each i, let yi be a nonnegative integer with a conditional binomial dis- tribution with upper bound ni (a positive integer) and probability of success pðxi; boÞ, where 0 < pðx; bÞ < 1 for all x and b. (A leading case is the logistic function.) Therefore, Eðyi j xi; niÞ ¼ nipðxi; boÞ and Varðyi j xi; niÞ ¼ nipðxi; boÞ½1 \u0004 pðxi; boÞ\u0003. Explain in detail how to obtain the weighted nonlinear least squares estimator of bo. 12.11. Let yi be a G \u0001 1 vector (where G could be T, the number of time periods in a panel data application), and let xi be a vector of covariates. Let mðx; bÞ be a model of Eðy j xÞ, where mgðx; bÞ is a model for Eðyg j xÞ. Assume that the model is correctly speciﬁed, and let bo denote the true value. Assume that mðx; \u0002Þ has many continuous derivatives. a. Argue that the multivariate nonlinear least squares (MNLS) estimator, which minimizes X N i¼1 ½yi \u0004 mðxi; bÞ\u00030½yi \u0004 mðxi; bÞ\u0003=2 is generally consistent and ﬃﬃﬃﬃ N p -asymptotically normal. Use Theorems 12.2 and 12.3. What is the identiﬁcation assumption? b. Let Wðx; dÞ be a model for Varðy j xÞ, and suppose that this model is correctly speciﬁed. Let ^d be a ﬃﬃﬃﬃ N p -consistent estimator of do. Argue that the multivariate weighted nonlinear least squares (MWNLS) estimator, which solves X N i¼1 ½yi \u0004 mðxi; bÞ\u00030½Wið^dÞ\u0003\u00041½yi \u0004 mðxi; bÞ\u0003=2 M-Estimation 383", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 395, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p396::c0", "text": "is generally consistent and ﬃﬃﬃﬃ N p -asymptotically normal. Find Avar ﬃﬃﬃﬃ N p ð ^b \u0004 boÞ and show how to consistently estimate it. c. Argue that, even if the variance model for y given x is misspeciﬁed, the MWNLS estimator is still consistent and ﬃﬃﬃﬃ N p -asymptotically normal. How would you estimate its asymptotic variance if you suspect the variance model is misspeciﬁed? Chapter 12 384", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 396, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p397::c0", "text": "13 Maximum Likelihood Methods 13.1 Introduction This chapter contains a general treatment of maximum likelihood estimation (MLE) under random sampling. All the models we considered in Part I could be estimated without making full distributional assumptions about the endogenous variables conditional on the exogenous variables: maximum likelihood methods were not needed. Instead, we focused primarily on zero-covariance and zero-conditional-mean assumptions, and secondarily on assumptions about conditional variances and co- variances. These assumptions were su‰cient for obtaining consistent, asymptotically normal estimators, some of which were shown to be e‰cient within certain classes of estimators. Some texts on advanced econometrics take maximum likelihood estimation as the unifying theme, and then most models are estimated by maximum likelihood. In ad- dition to providing a uniﬁed approach to estimation, MLE has some desirable e‰- ciency properties: it is generally the most e‰cient estimation procedure in the class of estimators that use information on the distribution of the endogenous variables given the exogenous variables. (We formalize the e‰ciency of MLE in Section 14.5.) So why not always use MLE? As we saw in Part I, e‰ciency usually comes at the price of nonrobustness, and this is certainly the case for maximum likelihood. Maximum likelihood estimators are generally inconsistent if some part of the speciﬁed distribution is misspeciﬁed. As an example, consider from Section 9.5 a simultaneous equations model that is linear in its parameters but nonlinear in some endogenous variables. There, we discussed esti- mation by instrumental variables methods. We could estimate SEMs nonlinear in endogenous variables by maximum likelihood if we assumed independence between the structural errors and the exogenous variables and if we assumed a particular dis- tribution for the structural errors, say, multivariate normal. The MLE would be asymptotically more e‰cient than the best GMM estimator, but failure of normality generally results in inconsistent estimators of all parameters. As a second example, suppose we wish to estimate Eðy j xÞ, where y is bounded between zero and one. The logistic function, expðxbÞ=½1 þ expðxbÞ\u0001, is a reasonable model for Eðy j xÞ, and, as we discussed in Section 12.2, nonlinear least squares provides consistent, ﬃﬃﬃﬃ N p -asymptotically normal estimators under weak regularity conditions. We can easily make inference robust to arbitrary heteroskedasticity in Varðy j xÞ. An alternative approach is to model the density of y given x—which, of course, implies a particular model for Eðy j xÞ—and use maximum likelihood esti- mation. As we will see, the strength of MLE is that, under correct speciﬁcation of the", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 397, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p398::c0", "text": "density, we would have the asymptotically e‰cient estimators, and we would be able to estimate any feature of the conditional distribution, such as Pðy ¼ 1 j xÞ. The drawback is that, except in special cases, if we have misspeciﬁed the density in any way, we will not be able to consistently estimate the conditional mean. In most applications, specifying the distribution of the endogenous variables con- ditional on exogenous variables must have a component of arbitrariness, as economic theory rarely provides guidance. Our perspective is that, for robustness reasons, it is desirable to make as few assumptions as possible—at least until relaxing them becomes practically di‰cult. There are cases in which MLE turns out to be robust to failure of certain assumptions, but these must be examined on a case-by-case basis, a process that detracts from the unifying theme provided by the MLE approach. (One such example is nonlinear regression under a homoskedastic normal assumption; the MLE of the parameters bo is identical to the NLS estimator, and we know the latter is consistent and asymptotically normal quite generally. We will cover some other leading cases in Chapter 19.) Maximum likelihood plays an important role in modern econometric analysis, for good reason. There are many problems for which it is indispensable. For example, in Chapters 15 and 16 we study various limited dependent variable models, and MLE plays a central role. 13.2 Preliminaries and Examples Traditional maximum likelihood theory for independent, identically distributed observations fyi A RG: i ¼ 1; 2; . . .g starts by specifying a family of densities for yi. This is the framework used in introductory statistics courses, where yi is a scalar with a normal or Poisson distribution. But in almost all economic applications, we are interested in estimating parameters in conditional distributions. Therefore, we assume that each random draw is partitioned as ðxi; yiÞ, where xi A RK and yi A RG, and we are interested in estimating a model for the conditional distribution of yi given xi. We are not interested in the distribution of xi, so we will not specify a model for it. Consequently, the method of this chapter is properly called conditional maximum likelihood estimation (CMLE). By taking xi to be null we cover unconditional MLE as a special case. An alternative to viewing ðxi; yiÞ as a random draw from the population is to treat the conditioning variables xi as nonrandom vectors that are set ahead of time and that appear in the unconditional distribution of yi. (This is analogous to the ﬁxed regres- sor assumption in classical regression analysis.) Then, the yi cannot be identically distributed, and this fact complicates the asymptotic analysis. More importantly, Chapter 13 386", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 398, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p399::c0", "text": "treating the xi as nonrandom is much too restrictive for all uses of maximum likeli- hood. In fact, later on we will cover methods where xi contains what are endogenous variables in a structural model, but where it is convenient to obtain the distribution of one set of endogenous variables conditional on another set. Once we know how to analyze the general CMLE case, applications follow fairly directly. It is important to understand that the subsequent results apply any time we have random sampling in the cross section dimension. Thus, the general theory applies to system estimation, as in Chapters 7 and 9, provided we are willing to assume a dis- tribution for yi given xi. In addition, panel data settings with large cross sections and relatively small time periods are encompassed, since the appropriate asymptotic analysis is with the time dimension ﬁxed and the cross section dimension tending to inﬁnity. In order to perform maximum likelihood analysis we need to specify, or derive from an underlying (structural) model, the density of yi given xi. We assume this density is known up to a ﬁnite number of unknown parameters, with the result that we have a parametric model of a conditional density. The vector yi can be continuous or discrete, or it can have both discrete and continuous characteristics. In many of our applications, yi is a scalar, but this fact does not simplify the general treatment. We will carry along two examples in this chapter to illustrate the general theory of conditional maximum likelihood. The ﬁrst example is a binary response model, spe- ciﬁcally the probit model. We postpone the uses and interepretation of binary response models until Chapter 15. Example 13.1 (Probit): Suppose that the latent variable y\u0002 i follows y\u0002 i ¼ xiy þ ei ð13:1Þ where ei is independent of xi (which is a 1 \u0003 K vector with ﬁrst element equal to unity for all i), y is a K \u0003 1 vector of parameters, and ei @ Normal(0,1). Instead of observing y\u0002 i we observe only a binary variable indicating the sign of y\u0002 i : yi ¼ 1 if y\u0002 i > 0 0 if y\u0002 i a 0 \u0002 (13.2) (13.3) To be succinct, it is useful to write equations (13.2) and (13.3) in terms of the indi- cator function, denoted 1½ \u0004 \u0001. This function is unity whenever the statement in brackets is true, and zero otherwise. Thus, equations (13.2) and (13.3) are equivalently written as yi ¼ 1½y\u0002 i > 0\u0001. Because ei is normally distributed, it is irrelevant whether the strict inequality is in equation (13.2) or (13.3). Maximum Likelihood Methods 387", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 399, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p400::c0", "text": "We can easily obtain the distribution of yi given xi: Pðyi ¼ 1 j xiÞ ¼ Pðy\u0002 i > 0 j xiÞ ¼ Pðxiy þ ei > 0 j xiÞ ¼ Pðei > \u0005xiy j xiÞ ¼ 1 \u0005 Fð\u0005xiyÞ ¼ FðxiyÞ ð13:4Þ where Fð\u0004Þ denotes the standard normal cumulative distribution function (cdf ). We have used Property CD.4 in the chapter appendix along with the symmetry of the normal distribution. Therefore, Pðyi ¼ 0 j xiÞ ¼ 1 \u0005 FðxiyÞ ð13:5Þ We can combine equations (13.4) and (13.5) into the density of yi given xi: f ðy j xiÞ ¼ ½FðxiyÞ\u0001y½1 \u0005 FðxiyÞ\u00011\u0005y; y ¼ 0; 1 ð13:6Þ The fact that f ðy j xiÞ is zero when y B f0; 1g is obvious, so we will not be explicit about this in the future. Our second example is useful when the variable to be explained takes on non- negative integer values. Such a variable is called a count variable. We will discuss the use and interpretation of count data models in Chapter 19. For now, it su‰ces to note that a linear model for Eðy j xÞ when y takes on nonnegative integer values is not ideal because it can lead to negative predicted values. Further, since y can take on the value zero with positive probability, the transformation logðyÞ cannot be used to obtain a model with constant elasticities or constant semielasticities. A functional form well suited for Eðy j xÞ is expðxyÞ. We could estimate y by using nonlinear least squares, but all of the standard distributions for count variables imply hetero- skedasticity (see Chapter 19). Thus, we can hope to do better. A traditional approach to regression models with count data is to assume that yi given xi has a Poisson distribution. Example 13.2 (Poisson Regression): Let yi be a nonnegative count variable; that is, yi can take on integer values 0; 1; 2; . . . : Denote the conditional mean of yi given the vector xi as Eðyi j xiÞ ¼ mðxiÞ. A natural distribution for yi given xi is the Poisson distribution: f ðy j xiÞ ¼ exp½\u0005mðxiÞ\u0001fmðxiÞgy=y!; y ¼ 0; 1; 2; . . . ð13:7Þ (We use y as the dummy argument in the density, not to be confused with the random variable yi.) Once we choose a form for the conditional mean function, we have completely determined the distribution of yi given xi. For example, from equation (13.7), Pðyi ¼ 0 j xiÞ ¼ exp½\u0005mðxiÞ\u0001. An important feature of the Poisson distribu- Chapter 13 388", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 400, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p401::c0", "text": "tion is that the variance equals the mean: Varðyi j xiÞ ¼ Eðyi j xiÞ ¼ mðxiÞ. The usual choice for mð\u0004Þ is mðxÞ ¼ expðxyÞ, where y is K \u0003 1 and x is 1 \u0003 K with ﬁrst element unity. 13.3 General Framework for Conditional MLE Let poðy j xÞ denote the conditional density of yi given xi ¼ x, where y and x are dummy arguments. We index this density by ‘‘o’’ to emphasize that it is the true density of yi given xi, and not just one of many candidates. It will be useful to let X H RK denote the possible values for xi and Y denote the possible values of yi; X and Y are called the supports of the random vectors xi and yi, respectively. For a general treatment, we assume that, for all x A X, poð\u0004 j xÞ is a density with respect to a s-ﬁnite measure, denoted nðdyÞ. Deﬁning a s-ﬁnite measure would take us too far aﬁeld. We will say little more about the measure nðdyÞ because it does not play a crucial role in applications. It su‰ces to know that nðdyÞ can be chosen to allow yi to be discrete, continuous, or some mixture of the two. When yi is discrete, the measure nðdyÞ simply turns all integrals into sums; when yi is purely continuous, we obtain the usual Riemann integrals. Even in more complicated cases—where, say, yi has both discrete and continuous characteristics—we can get by with tools from basic probability without ever explicitly deﬁning nðdyÞ. For more on measures and general integrals, you are referred to Billingsley (1979) and Davidson (1994, Chapters 3 and 4). In Chapter 12 we saw how nonlinear least squares can be motivated by the fact that moðxÞ 1 Eðy j xÞ minimizes Ef½y \u0005 mðxÞ\u00012g for all other functions mðxÞ with Ef½mðxÞ\u00012g < y. Conditional maximum likelihood has a similar motivation. The result from probability that is crucial for applying the analogy principle is the con- ditional Kullback-Leibler information inequality. Although there are more general statements of this inequality, the following su‰ces for our purpose: for any non- negative function f ð\u0004 j xÞ such that ð Y f ðy j xÞnðdyÞ ¼ 1; all x A X ð13:8Þ Property CD.1 in the chapter appendix implies that Kð f ; xÞ 1 ð Y log½ poðy j xÞ=f ðy j xÞ\u0001poðy j xÞnðdyÞ b 0; all x A X ð13:9Þ Because the integral is identically zero for f ¼ po, expression (13.9) says that, for each x, Kðf ; xÞ is minimized at f ¼ po. Maximum Likelihood Methods 389", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 401, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p402::c0", "text": "We can apply inequality (13.9) to a parametric model for poð\u0004 j xÞ, ff ð\u0004 j x; yÞ; y A Y; Y H RPg ð13:10Þ which we assume satisﬁes condition (13.8) for each x A X and each y A Y; if it does not, then f ð\u0004 j x; yÞ does not integrate to unity (with respect to the measure n), and as a result it is a very poor candidate for poðy j xÞ. Model (13.10) is a correctly speciﬁed model of the conditional density, poð\u0004 j \u0004Þ, if, for some yo A Y, f ð\u0004 j x; yoÞ ¼ poð\u0004 j xÞ; all x A X ð13:11Þ As we discussed in Chapter 12, it is useful to use yo to distinguish the true value of the parameter from a generic element of Y. In particular examples, we will not bother making this distinction unless it is needed to make a point. For each x A X, Kðf ; xÞ can be written as Eflog½ poðyi j xiÞ\u0001 j xi ¼ xg \u0005 Eflog½ f ðyi j xiÞ\u0001 j xi ¼ xg. Therefore, if the parametric model is correctly speciﬁed, then Eflog½ f ðyi j xi; yoÞ\u0001 j xig b Eflog½ f ðyi j xi; yÞ\u0001 j xig, or E½liðyoÞ j xi\u0001 b E½liðyÞ j xi\u0001; y A Y ð13:12Þ where liðyÞ 1 lðyi; xi; yÞ 1 log f ðyi j xi; yÞ ð13:13Þ is the conditional log likelihood for observation i. Note that liðyÞ is a random function of y, since it depends on the random vector ðxi; yiÞ. By taking the expected value of expression (13.12) and using iterated expectations, we see that yo solves max y A Y E½liðyÞ\u0001 ð13:14Þ where the expectation is with respect to the joint distribution of ðxi; yiÞ. The sample analogue of expression (13.14) is max y A Y N\u00051 X N i¼1 log f ðyi j xi; yÞ ð13:15Þ A solution to problem (13.15), assuming that one exists, is the conditional maximum likelihood estimator (CMLE) of yo, which we denote as ^y. We will sometimes drop ‘‘conditional’’ when it is not needed for clarity. The CMLE is clearly an M-estimator, since a maximization problem is easily turned into a minimization problem: in the notation of Chapter 12, take wi 1 ðxi; yiÞ and qðwi; yÞ 1 \u0005log f ðyi j xi; yÞ. As long as we keep track of the minus sign in front of the log likelihood, we can apply the results in Chapter 12 directly. Chapter 13 390", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 402, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p403::c0", "text": "The motivation for the conditional MLE as a solution to problem (13.15) may appear backward if you learned about maximum likelihood estimation in an intro- ductory statistics course. In a traditional framework, we would treat the xi as con- stants appearing in the distribution of yi, and we would deﬁne ^y as the solution to max y A Y Y N i¼1 f ðyi j xi; yÞ ð13:16Þ Under independence, the product in expression (13.16) is the model for the joint density of ðy1; . . . ; yNÞ, evaluated at the data. Because maximizing the function in (13.16) is the same as maximizing its natural log, we are led to problem (13.15). However, the arguments explaining why solving (13.16) should lead to a good esti- mator of yo are necessarily heuristic. By contrast, the analogy principle applies directly to problem (13.15), and we need not assume that the xi are ﬁxed. In our two examples, the conditional log likelihoods are fairly simple. Example 13.1 (continued): In the probit example, the log likelihood for observation i is liðyÞ ¼ yi log FðxiyÞ þ ð1 \u0005 yiÞ log½1 \u0005 FðxiyÞ\u0001. Example 13.2 (continued): In the Poisson example, liðyÞ ¼ \u0005expðxiyÞ þ yixiy \u0005 logðyi!Þ. Normally, we would drop the last term in deﬁning liðyÞ because it does not a¤ect the maximization problem. 13.4 Consistency of Conditional MLE In this section we state a formal consistency result for the CMLE, which is a special case of the M-estimator consistency result Theorem 12.2. theorem 13.1 (Consistency of CMLE): Let fðxi; yiÞ: i ¼ 1; 2; . . .g be a random sam- ple with xi A X H RK, yi A Y H RG. Let Y H RP be the parameter set and denote the parametric model of the conditional density as ff ð\u0004 j x; yÞ: x A X; y A Yg. Assume that (a) f ð\u0004 j x; yÞ is a true density with respect to the measure nðdyÞ for all x and y, so that condition (13.8) holds; (b) for some yo A Y, poð\u0004 j xÞ ¼ f ð\u0004 j x; yoÞ, all x A X, and yo is the unique solution to problem (13.14); (c) Y is a compact set; (d) for each y A Y, lð\u0004 ; yÞ is a Borel measurable function on Y \u0003 X; (e) for each ðy; xÞ A Y \u0003 X, lðy; x; \u0004Þ is a continuous function on Y; and (f ) jlðw; yÞj a bðwÞ, all y A Y, and E½bðwÞ\u0001 < y. Then there exists a solution to problem (13.15), the CMLE ^y, and plim ^y ¼ yo. As we discussed in Chapter 12, the measurability assumption in part d is purely technical and does not need to be checked in practice. Compactness of Y can be Maximum Likelihood Methods 391", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 403, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p404::c0", "text": "relaxed, but doing so usually requires considerable work. The continuity assumption holds in most econometric applications, but there are cases where it fails, such as when estimating certain models of auctions—see Donald and Paarsch (1996). The moment assumption in part f typically restricts the distribution of xi in some way, but such restrictions are rarely a serious concern. For the most part, the key assumptions are that the parametric model is correctly speciﬁed, that yo is identiﬁed, and that the log-likelihood function is continuous in y. For the probit and Poisson examples, the log likelihoods are clearly continuous in y. We can verify the moment condition (f ) if we bound certain moments of xi and make the parameter space compact. But our primary concern is that densities are correctly speciﬁed. For example, in the probit case, the density for yi given xi will be incorrect if the latent error ei is not independent of xi and normally distributed, or if the latent variable model is not linear to begin with. For identiﬁcation we must rule out perfect collinearity in xi. The Poisson CMLE turns out to have desirable prop- erties even if the Poisson distributional assumption does not hold, but we postpone a discussion of the robustness of the Poisson CMLE until Chapter 19. 13.5 Asymptotic Normality and Asymptotic Variance Estimation Under the di¤erentiability and moment assumptions that allow us to apply the the- orems in Chapter 12, we can show that the MLE is generally asymptotically normal. Naturally, the computational methods discussed in Section 12.7, including concen- trating parameters out of the log likelihood, apply directly. 13.5.1 Asymptotic Normality We can derive the limiting distribution of the MLE by applying Theorem 12.3. We will have to assume the regularity conditions there; in particular, we assume that yo is in the interior of Y, and liðyÞ is twice continuously di¤erentiable on the interior of Y. The score of the log likelihood for observation i is simply siðyÞ 1 ‘yliðyÞ0 ¼ qli qy1 ðyÞ; qli qy2 ðyÞ; . . . ; qli qyP ðyÞ \u0003 \u00040 ð13:17Þ a P \u0003 1 vector as in Chapter 12. Example 13.1 (continued): For the probit case, y is K \u0003 1 and ‘yliðyÞ ¼ yi fðxiyÞxi FðxiyÞ \u0005 \u0006 \u0005 ð1 \u0005 yiÞ fðxiyÞxi ½1 \u0005 FðxiyÞ\u0001 \u0002 \u0007 Transposing this equation, and using a little algebra, gives Chapter 13 392", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 404, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p405::c0", "text": "siðyÞ ¼ fðxiyÞx0 i½yi \u0005 FðxiyÞ\u0001 FðxiyÞ½1 \u0005 FðxiyÞ\u0001 ð13:18Þ Recall that x0 i is a K \u0003 1 vector. Example 13.2 (continued): The score for the Poisson case, where y is again K \u0003 1, is siðyÞ ¼ \u0005expðxiyÞx0 i þ yix0 i ¼ x0 i½yi \u0005 expðxiyÞ\u0001 ð13:19Þ In the vast majority of cases, the score of the log-likelihood function has an im- portant zero conditional mean property: E½siðyoÞ j xi\u0001 ¼ 0 ð13:20Þ In other words, when we evaluate the P \u0003 1 score at yo, and take its expectation with respect to f ð\u0004 j xi; yoÞ, the expectation is zero. Under condition (13.20), E½siðyoÞ\u0001 ¼ 0, which was a key condition in deriving the asymptotic normality of the M-estimator in Chapter 12. To show condition (13.20) generally, let Ey½\u0004 j xi\u0001 denote conditional expectation with respect to the density f ð\u0004 j xi; yÞ for any y A Y. Then, by deﬁnition, Ey½siðyÞ j xi\u0001 ¼ ð Y sðy; xi; yÞf ðy j xi; yÞnðdyÞ If integration and di¤erentation can be interchanged on intðYÞ—that is, if ‘y ð Y f ðy j xi; yÞnðdyÞ \u0003 \u0004 ¼ ð Y ‘y f ðy j xi; yÞnðdyÞ ð13:21Þ for all xi A X, y A intðYÞ—then 0 ¼ ð Y ‘y f ðy j xi; yÞnðdyÞ ð13:22Þ since Ð Y f ðy j xi; yÞnðdyÞ is unity for all y, and therefore the partial derivatives with respect to y must be identically zero. But the right-hand side of equation (13.22) can be written as Ð Y½‘ylðy; xi; yÞ\u0001 f ðy j xi; yÞnðdyÞ. Putting in yo for y and transposing yields condition (13.20). Example 13.1 (continued): Deﬁne ui 1 yi \u0005 FðxiyoÞ ¼ yi \u0005 Eðyi j xiÞ. Then siðyoÞ ¼ fðxiyoÞx0 iui FðxiyoÞ½1 \u0005 FðxiyoÞ\u0001 and, since Eðui j xiÞ ¼ 0, it follows that E½siðyoÞ j xi\u0001 ¼ 0. Maximum Likelihood Methods 393", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 405, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p406::c0", "text": "Example 13.2 (continued): Deﬁne ui 1 yi \u0005 expðxiyoÞ. Then siðyoÞ ¼ x0 iui and so E½siðyoÞ j xi\u0001 ¼ 0. Assuming that liðyÞ is twice continuously di¤erentiable on the interior of Y, let the Hessian for observation i be the P \u0003 P matrix of second partial derivatives of liðyÞ: HiðyÞ 1 ‘ysiðyÞ ¼ ‘2 y liðyÞ ð13:23Þ The Hessian is a symmetric matrix that generally depends on ðxi; yiÞ. Since MLE is a maximization problem, the expected value of HiðyoÞ is negative deﬁnite. Thus, to apply the theory in Chapter 12, we deﬁne Ao 1 \u0005E½HiðyoÞ\u0001 ð13:24Þ which is generally a positive deﬁnite matrix when yo is identiﬁed. Under standard regularity conditions, the asymptotic normality of the CMLE follows from Theorem 12.3: ﬃﬃﬃﬃ N p ð^y \u0005 yoÞ @ a Normalð0; A\u00051 o BoA\u00051 o Þ, where Bo 1 Var½siðyoÞ\u0001 1 E½siðyoÞsiðyoÞ0\u0001. It turns out that this general form of the asymptotic variance matrix is too compli- cated. We now show that Bo ¼ Ao. We must assume enough smoothness such that the following interchange of inte- gral and derivative is valid (see Newey and McFadden, 1994, Section 5.1, for the case of unconditional MLE): ‘y ð Y siðyÞf ðy j xi; yÞnðdyÞ \u0003 \u0004 ¼ ð Y ‘y½siðyÞf ðy j xi; yÞ\u0001nðdyÞ ð13:25Þ Then, taking the derivative of the identity ð Y siðyÞf ðy j xi; yÞnðdyÞ 1 Ey½siðyÞ j xi\u0001 ¼ 0; y A intðYÞ and using equation (13.25), gives, for all y A intðYÞ, \u0005Ey½HiðyÞ j xi\u0001 ¼ Vary½siðyÞ j xi\u0001 where the indexing by y denotes expectation and variance when f ð\u0004 j xi; yÞ is the density of yi given xi. When evaluated at y ¼ yo we get a very important equality: \u0005E½HiðyoÞ j xi\u0001 ¼ E½siðyoÞsiðyoÞ0 j xi\u0001 ð13:26Þ where the expectation and variance are with respect to the true conditional distri- bution of yi given xi. Equation (13.26) is called the conditional information matrix equality (CIME). Taking the expectation of equation (13.26) (with respect to the Chapter 13 394", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 406, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p407::c0", "text": "distribution of xi) and using the law of iterated expectations gives \u0005E½HiðyoÞ\u0001 ¼ E½siðyoÞsiðyoÞ0\u0001 ð13:27Þ or Ao ¼ Bo. This relationship is best thought of as the unconditional information matrix equality (UIME). theorem 13.2 (Asymptotic Normality of CMLE): Let the conditions of Theorem 13.1 hold. In addition, assume that (a) yo A intðYÞ; (b) for each ðy; xÞ A Y \u0003 X, lðy; x; \u0004Þ is twice continuously di¤erentiable on intðYÞ; (c) the interchanges of de- rivative and integral in equations (13.21) and (13.25) hold for all y A intðYÞ; (d) the elements of ‘2 y lðy; x; yÞ are bounded in absolute value by a function bðy; xÞ with ﬁnite expectation; and (e) Ao deﬁned by expression (13.24) is positive deﬁnite. Then ﬃﬃﬃﬃ N p ð^y \u0005 yoÞ ! d Normalð0; A\u00051 o Þ ð13:28Þ and therefore Avarð^yÞ ¼ A\u00051 o =N ð13:29Þ In standard applications, the log likelihood has many continuous partial deriva- tives, although there are examples where it does not. Some examples also violate the interchange of the integral and derivative in equation (13.21) or (13.25), such as when the conditional support of yi depends on the parameters yo. In such cases we cannot expect the CMLE to have a limiting normal distribution; it may not even converge at the rate ﬃﬃﬃﬃ N p . Some progress has been made for speciﬁc models when the support of the distribution depends on unknown parameters; see, for example, Donald and Paarsch (1996). 13.5.2 Estimating the Asymptotic Variance Estimating Avarð^yÞ requires estimating Ao. From the equalities derived previously, there are at least three possible estimators of Ao in the CMLE context. In fact, under slight extensions of the regularity conditions in Theorem 13.2, each of the matrices N\u00051 X N i¼1 \u0005Hið^yÞ; N\u00051 X N i¼1 sið^yÞsið^yÞ0; and N\u00051 X N i¼1 Aðxi; ^yÞ ð13:30Þ converges to Ao ¼ Bo, where Aðxi; yoÞ 1 \u0005E½Hðyi; xi; yoÞ j xi\u0001 ð13:31Þ Maximum Likelihood Methods 395", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 407, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p408::c0", "text": "Thus, Avaˆrð^yÞ can be taken to be any of the three matrices \u0005 X N i¼1 Hið^yÞ \" #\u00051 ; X N i¼1 sið^yÞsið^yÞ0 \" #\u00051 ; or X N i¼1 Aðxi; ^yÞ \" #\u00051 ð13:32Þ and the asymptotic standard errors are the square roots of the diagonal elements of any of the matrices. We discussed each of these estimators in the general M-estimator case in Chapter 12, but a brief review is in order. The ﬁrst estimator, based on the Hessian of the log likelihood, requires computing second derivatives and is not guar- anteed to be positive deﬁnite. If the estimator is not positive deﬁnite, standard errors of some linear combinations of the parameters will not be well deﬁned. The second estimator in equation (13.32), based on the outer product of the score, is always positive deﬁnite (whenever the inverse exists). This simple estimator was proposed by Berndt, Hall, Hall, and Hausman (1974). Its primary drawback is that it can be poorly behaved in even moderate sample sizes, as we discussed in Section 12.6.2. If the conditional expectation Aðxi; yoÞ is in closed form (as it is in some leading cases) or can be simulated—as discussed in Porter (1999)—then the estimator based on Aðxi; ^yÞ has some attractive features. First, it often depends only on ﬁrst deriva- tives of a conditional mean or conditional variance function. Second, it is positive deﬁnite when it exists because of the conditional information matrix equality (13.26). Third, this estimator has been found to have signiﬁcantly better ﬁnite sample prop- erties than the outer product of the score estimator in some situations where Aðxi; yoÞ can be obtained in closed form. Example 13.1 (continued): The Hessian for the probit log-likelihood is a mess. Fortunately, E½HiðyoÞ j xi\u0001 has a fairly simple form. Taking the derivative of equation (13.18) and using the product rule gives HiðyÞ ¼ \u0005 ffðxiyÞg2 x0 ixi FðxiyÞ½1 \u0005 FðxiyÞ\u0001 þ ½yi \u0005 FðxiyÞ\u0001LðxiyÞ where LðxiyÞ is a K \u0003 K complicated function of xiy that we need not ﬁnd explicitly. Now, when we evaluate this expression at yo and note that Ef½yi \u0005 FðxiyoÞ\u0001LðxiyoÞ j xig ¼ ½Eðyi j xiÞ \u0005 FðxiyoÞ\u0001LðxiyoÞ ¼ 0, we have \u0005E½HiðyoÞ j xi\u0001 ¼ AiðyoÞ ¼ ffðxiyoÞg2 x0 ixi FðxiyoÞ½1 \u0005 FðxiyoÞ\u0001 Thus, Avaˆrð^yÞ in probit analysis is Chapter 13 396", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 408, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p409::c0", "text": "X N i¼1 ffðxi ^yÞg2 x0 ixi Fðxi ^yÞ½1 \u0005 Fðxi ^yÞ\u0001 !\u00051 ð13:33Þ which is always positive deﬁnite when the inverse exists. Note that x0 ixi is a K \u0003 K matrix for each i. Example 13.2 (continued): For the Poisson model with exponential conditional mean, HiðyÞ ¼ \u0005expðxiyÞx0 ixi. In this example, the Hessian does not depend on yi, so there is no distinction between HiðyoÞ and E½HiðyoÞ j xi\u0001. The positive deﬁnite es- timate of Avaˆrð^yÞ is simply X N i¼1 expðxi ^yÞx0 ixi \" #\u00051 ð13:34Þ 13.6 Hypothesis Testing Given the asymptotic standard errors, it is easy to form asymptotic t statistics for testing single hypotheses. These t statistics are asymptotically distributed as standard normal. The three tests covered in Chapter 12 are immediately applicable to the MLE case. Since the information matrix equality holds when the density is correctly speciﬁed, we need only consider the simplest forms of the test statistics. The Wald statistic is given in equation (12.63), and the conditions su‰cient for it to have a limiting chi-square distribution are discussed in Section 12.6.1. Deﬁne the log-likelihood function for the entire sample by LðyÞ 1 PN i¼1 liðyÞ. Let ^y be the unrestricted estimator, and let ~y be the estimator with the Q nonredundant constraints imposed. Then, under the regularity conditions discussed in Section 12.6.3, the likelihood ratio (LR) statistic, LR 1 2½Lð^yÞ \u0005 Lð~yÞ\u0001 ð13:35Þ is distributed asymptotically as w2 Q under H0. As with the Wald statistic, we cannot use LR as approximately w2 Q when yo is on the boundary of the parameter set. The LR statistic is very easy to compute once the restricted and unrestricted models have been estimated, and the LR statistic is invariant to reparameterizing the conditional density. The score or LM test is based on the restricted estimation only. Let sið~yÞ be the P \u0003 1 score of liðyÞ evaluated at the restricted estimates ~y. That is, we compute the partial derivatives of liðyÞ with respect to each of the P parameters, but then we Maximum Likelihood Methods 397", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 409, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p410::c0", "text": "evaluate this vector of partials at the restricted estimates. Then, from Section 12.6.2 and the information matrix equality, the statistics X N i¼1 ~si !0 \u0005 X N i¼1 ~Hi !\u00051 X N i¼1 ~si ! ; X N i¼1 ~si !0 X N i¼1 ~Ai !\u00051 X N i¼1 ~si ! ; and X N i¼1 ~si !0 X N i¼1 ~si~s0 i !\u00051 X N i¼1 ~si ! ð13:36Þ have limiting w2 Q distributions under H0. As we know from Section 12.6.2, the ﬁrst statistic is not invariant to reparameterizations, but the outer product statistic is. In addition, using the conditional information matrix equality, it can be shown that the LM statistic based on ~Ai is invariant to reparameterization. Davidson and MacKinnon (1993, Section 13.6) show invariance in the case of unconditional maxi- mum likelihood. Invariance holds in the more general conditional ML setup, with xi containing any conditioning variables; see Problem 13.5. We have already used the expected Hessian form of the LM statistic for nonlinear regression in Section 12.6.2. We will use it in several applications in Part IV, including binary response models and Poisson regression models. In these examples, the statistic can be computed conveniently using auxiliary regressions based on weighted residuals. Because the unconditional information matrix equality holds, we know from Sec- tion 12.6.4 that the three classical statistics have the same limiting distribution under local alternatives. Therefore, either small-sample considerations, invariance, or com- putational issues must be used to choose among the statistics. 13.7 Speciﬁcation Testing Since MLE generally relies on its distributional assumptions, it is useful to have available a general class of speciﬁcation tests that are simple to compute. One general approach is to nest the model of interest within a more general model (which may be much harder to estimate) and obtain the score test against the more general alternative. RESET in a linear model and its extension to exponential regression models in Section 12.6.2 are examples of this approach, albeit in a non-maximum-likelihood setting. In the context of MLE, it makes sense to test moment conditions implied by the conditional density speciﬁcation. Let wi ¼ ðxi; yiÞ and suppose that, when f ð\u0004 j x; yÞ is correctly speciﬁed, H0: E½gðwi; yoÞ\u0001 ¼ 0 ð13:37Þ Chapter 13 398", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 410, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p411::c0", "text": "where gðw; yÞ is a Q \u0003 1 vector. Any application implies innumerable choices for the function g. Since the MLE ^y sets the sum of the score to zero, gðw; yÞ cannot contain elements of sðw; yÞ. Generally, g should be chosen to test features of a model that are of primary interest, such as ﬁrst and second conditional moments, or various condi- tional probabilities. A test of hypothesis (13.37) is based on how far the sample average of gðwi; ^yÞ is from zero. To derive the asymptotic distribution, note that N\u00051=2 X N i¼1 gið^yÞ ¼ N\u00051=2 X N i¼1 ½gið^yÞ \u0005 sið^yÞPo\u0001 holds trivially because PN i¼1 sið^yÞ ¼ 0, where Po 1 fE½siðyoÞsiðyoÞ0\u0001g\u00051fE½siðyoÞgiðyoÞ0\u0001g is the P \u0003 Q matrix of population regression coe‰cients from regressing giðyoÞ0 on siðyoÞ0. Using a mean-value expansion about yo and algebra similar to that in Chap- ter 12, we can write N\u00051=2 X N i¼1 ½gið^yÞ \u0005 sið^yÞPo\u0001 ¼ N\u00051=2 X N i¼1 ½giðyoÞ \u0005 siðyoÞPo\u0001 þ E½‘ygiðyoÞ \u0005 ‘ysiðyoÞPo\u0001 ﬃﬃﬃﬃ N p ð^y \u0005 yoÞ þ opð1Þ ð13:38Þ The key is that, when the density is correctly speciﬁed, the second term on the right- hand side of equation (13.38) is identically zero. Here is the reason: First, equation (13.27) implies that ½E‘ysiðyoÞ\u0001fE½siðyoÞsiðyoÞ0\u0001g\u00051 ¼ \u0005IP. Second, an extension of the conditional information matrix equality (Newey, 1985; Tauchen, 1985) implies that \u0005E½‘ygiðyoÞ j xi\u0001 ¼ E½giðyoÞsiðyoÞ0 j xi\u0001: ð13:39Þ To show equation (13.39), write Ey½giðyÞ j xi\u0001 ¼ ð Y gðy; xi; yÞf ðy j xi; yÞnðdyÞ ¼ 0 ð13:40Þ for all y. Now, if we take the derivative with respect to y and assume that the inte- grals and derivative can be interchanged, equation (13.40) implies that ð Y ‘ygðy; xi; yÞf ðy j xi; yÞnðdyÞ þ ð Y gðy; xi; yÞ‘y f ðy j xi; yÞnðdyÞ ¼ 0 Maximum Likelihood Methods 399", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 411, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p412::c0", "text": "or Ey½‘ygiðyÞ j xi\u0001 þ Ey½giðyÞsiðyÞ0 j xi\u0001 ¼ 0, where we use the fact that ‘y f ðy j x; yÞ ¼ sðy; x; yÞ0f ðy j x; yÞ. Plugging in y ¼ yo and rearranging gives equation (13.39). What we have shown is that N\u00051=2 X N i¼1 ½gið^yÞ \u0005 sið^yÞPo\u0001 ¼ N\u00051=2 X N i¼1 ½giðyoÞ \u0005 siðyoÞPo\u0001 þ opð1Þ which means these standardized partial sums have the same asymptotic distribution. Letting ^P 1 X N i¼1 ^si^s0 i !\u00051 X N i¼1 ^si^g0 i ! it is easily seen that plim ^P ¼ Po under standard regularity conditions. Therefore, the asymptotic variance of N\u00051=2 PN i¼1½gið^yÞ \u0005 sið^yÞPo\u0001 ¼ N\u00051=2 PN i¼1 gið^yÞ is con- sistently estimated by N\u00051 PN i¼1ð^gi \u0005 ^si ^PÞð^gi \u0005 ^si ^PÞ0. When we construct the qua- dratic form, we get the Newey-Tauchen-White (NTW) statistic, NTW ¼ X N i¼1 gið^yÞ \" #0 X N i¼1 ð^gi \u0005 ^si ^PÞð^gi \u0005 ^si ^PÞ0 \" #\u00051 X N i¼1 gið^yÞ \" # ð13:41Þ This statistic was proposed independently by Newey (1985) and Tauchen (1985), and is an extension of White’s (1982a) information matrix (IM) test statistic. For computational purposes it is useful to note that equation (13.41) is identical to N \u0005 SSR0 ¼ NR2 0 from the regression 1 on ^s0 i; ^g0 i; i ¼ 1; 2; . . . ; N ð13:42Þ where SSR0 is the usual sum of squared residuals. Under the null that the density is correctly speciﬁed, NTW is distributed asymptotically as w2 Q, assuming that gðw; yÞ contains Q nonredundant moment conditions. Unfortunately, the outer product form of regression (13.42) means that the statistic can have poor ﬁnite sample properties. In particular applications—such as nonlinear least squares, binary response analysis, and Poisson regression, to name a few—it is best to use forms of test statistics based on the expected Hessian. We gave the regression-based test for NLS in equation (12.72), and we will see other examples in later chapters. For the information matrix test statistic, Davidson and MacKinnon (1992) have suggested an alternative form of the IM statistic that appears to have better ﬁnite sample properties. Example 13.2 (continued): To test the speciﬁcation of the conditional mean for Poission regression, we might take gðw;yÞ ¼ expðxyÞx0½y\u0005expðxyÞ\u0001 ¼ expðxyÞsðw; yÞ, Chapter 13 400", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 412, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p413::c0", "text": "where the score is given by equation (13.19). If Eðy j xÞ ¼ expðxyoÞ then E½gðw; yoÞ j x\u0001 ¼ expðxyoÞE½sðw; yoÞ j x\u0001 ¼ 0. To test the Poisson variance assumption, Varðy j xÞ ¼ Eðy j xÞ ¼ expðxyoÞ, g can be of the form gðw; yÞ ¼ aðx; yÞf½y\u0005expðxyÞ\u00012 \u0005expðxyÞg, where aðx; yÞ is a Q \u0003 1 vector. If the Poisson assumption is true, then u ¼ y \u0005 expðxyoÞ has a zero conditional mean and Eðu2 j xÞ ¼ Varðy j xÞ ¼ expðxyoÞ. It fol- lows that E½gðw; yoÞ j x\u0001 ¼ 0. Example 13.2 contains examples of what are known as conditional moment tests. As the name suggests, the idea is to form orthogonality conditions based on some key conditional moments, usually the conditional mean or conditional variance, but sometimes conditional probabilities or higher order moments. The tests for nonlinear regression in Chapter 12 can be viewed as conditional moment tests, and we will see several other examples in Part IV. For reasons discussed earlier, we will avoid computing the tests using regression (13.42) whenever possible. See Newey (1985), Tauchen (1985), and Pagan and Vella (1989) for general treatments and applications of conditional moment tests. White’s (1982a) information matrix test can often be viewed as a conditional moment test; see Hall (1987) for the linear regression model and White (1994) for a general treatment. 13.8 Partial Likelihood Methods for Panel Data and Cluster Samples Up to this point we have assumed that the parametric model for the density of y given x is correctly speciﬁed. This assumption is fairly general because x can contain any observable variable. The leading case occurs when x contains variables we view as exogenous in a structural model. In other cases, x will contain variables that are endogenous in a structural model, but putting them in the conditioning set and ﬁnd- ing the new conditional density makes estimation of the structural parameters easier. For studying various panel data models, for estimation using cluster samples, and for various other applications, we need to relax the assumption that the full condi- tional density of y given x is correctly speciﬁed. In some examples, such a model is too complicated. Or, for robustness reasons, we do not wish to fully specify the den- sity of y given x. 13.8.1 Setup for Panel Data For panel data applications we let y denote a T \u0003 1 vector, with generic element yt. Thus, yi is a T \u0003 1 random draw vector from the cross section, with tth element yit. As always, we are thinking of T small relative to the cross section sample size. With a Maximum Likelihood Methods 401", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 413, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p414::c0", "text": "slight notational change we can replace yit with, say, a G-vector for each t, an ex- tension that allows us to cover general systems of equations with panel data. For some vector xt containing any set of observable variables, let Dðyt j xtÞ denote the distribution of yt given xt. The key assumption is that we have a correctly speci- ﬁed model for the density of yt given xt; call it ftðyt j xt; yÞ, t ¼ 1; 2; . . . ; T. The vector xt can contain anything, including conditioning variables zt, lags of these, and lagged values of y. The vector y consists of all parameters appearing in ft for any t; some or all of these may appear in the density for every t, and some may appear only in the density for a single time period. What distinguishes partial likelihood from maximum likelihood is that we do not assume that Y T t¼1 Dðyit j xitÞ ð13:43Þ is a conditional distribution of the vector yi given some set of conditioning variables. In other words, even though ftðyt j xt; yoÞ is the correct density for yit given xit for each t, the product of these is not (necessarily) the density of yi given some conditioning vari- ables. Usually, we specify ftðyt j xt; yÞ because it is the density of interest for each t. We deﬁne the partial log likelihood for each observation i as liðyÞ 1 X T t¼1 log ftðyit j xit; yÞ ð13:44Þ which is the sum of the log likelihoods across t. What makes partial likelihood methods work is that yo maximizes the expected value of equation (13.44) provided we have the densities ftðyt j xt; yÞ correctly speciﬁed. By the Kullback-Leibler information inequality, yo maximizes E½log ftðyit j xit; yÞ\u0001 over Y for each t, so yo also maximizes the sum of these over t. As usual, identiﬁca- tion requires that yo be the unique maximizer of the expected value of equation (13.44). It is su‰cient that yo uniquely maximizes E½log ftðyit j xit; yÞ\u0001 for each t, but this assumption is not necessary. The partial maximum likelihood estimator (PMLE) ^y solves max y A Y X N i¼1 X T t¼1 log ftðyit j xit; yÞ ð13:45Þ and this problem is clearly an M-estimator problem (where the asymptotics are with ﬁxed T and N ! y). Therefore, from Theorem 12.2, the partial MLE is generally consistent provided yo is identiﬁed. Chapter 13 402", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 414, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p415::c0", "text": "It is also clear that the partial MLE will be asymptotically normal by Theorem 12.3 in Section 12.3. However, unless poðy j zÞ ¼ Y T t¼1 ftðyt j xt; yoÞ ð13:46Þ for some subvector z of x, we cannot apply the conditional information matrix equality. A more general asymptotic variance estimator of the type covered in Sec- tion 12.5.1 is needed, and we provide such estimators in the next two subsections. It is useful to discuss at a general level why equation (13.46) does not necessarily hold in a panel data setting. First, suppose xt contains only contemporaneous con- ditioning variables, zt; in particular, xt contains no lagged dependent variables. Then we can always write poðy j zÞ ¼ po 1ðy1 j zÞ \u0004 po 2ðy2 j y1; zÞ \u0004 \u0004 \u0004 po t ðyt j yt\u00051; yt\u00052; . . . ; y1; zÞ \u0004 \u0004 \u0004 po TðyT j yT\u00051; yT\u00052; . . . ; y1; zÞ where po t ðyt j yt\u00051; yt\u00052; . . . ; y1; zÞ is the true conditional density of yt given yt\u00051, yt\u00052; . . . ; y1 and z 1 ðz1; . . . ; zTÞ. (For t ¼ 1, po 1 is the density of y1 given z.) For equation (13.46) to hold, we should have po t ðyt j yt\u00051; yt\u00052; . . . ; y1; zÞ ¼ ftðyt j zt; yoÞ; t ¼ 1; . . . ; T which requires that, once zt is conditioned on, neither past lags of yt nor elements of z from any other time period—past or future—appear in the conditional density po t ðyt j yt\u00051; yt\u00052; . . . ; y1; zÞ. Generally, this requirement is very strong, as it requires a combination of strict exogeneity of zt and the absense of dynamics in po t . Equation (13.46) is more likely to hold when xt contains lagged dependent vari- ables. In fact, if xt contains only lagged values of yt, then poðyÞ ¼ Y T t¼1 ftðyt j xt; yoÞ holds if ftðyt j xt; yoÞ ¼ po t ðyt j yt\u00051; yt\u00052; . . . ; y1Þ for all t (where po 1 is the uncondi- tional density of y1), so that all dynamics are captured by ft. When xt contains some variables zt in addition to lagged yt, equation (13.46) requires that the parametric density captures all of the dynamics—that is, that all lags of yt and zt have been properly accounted for in f ðyt j xt; yoÞ—and strict exogeneity of zt. In most treatments of maximum likelihood estimation of dynamic models con- taining additional exogenous variables, the strict exogeneity assumption is main- Maximum Likelihood Methods 403", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 415, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p416::c0", "text": "tained, often implicitly by taking zt to be nonrandom. In Chapter 7 we saw that strict exogeneity played no role in getting consistent, asymptotically normal estimators in linear panel data models by pooled OLS, and the same is true here. We also allow models where the dynamics have been incompletely speciﬁed. Example 13.3 (Probit with Panel Data): To illustrate the previous discussion, we consider estimation of a panel data binary choice model. The idea is that, for each unit i in the population (individual, ﬁrm, and so on) we have a binary outcome, yit, for each of T time periods. For example, if t represents a year, then yit might indicate whether a person was arrested for a crime during year t. Consider the model in latent variable form: y\u0002 it ¼ xityo þ eit yit ¼ 1½y\u0002 it > 0\u0001 ð13:47Þ eit j xit @ Normalð0; 1Þ The vector xit might contain exogenous variables zit, lags of these, and even lagged yit (not lagged y\u0002 it). Under the assumptions in model (13.47), we have, for each t, Pðyit ¼ 1 j xitÞ ¼ FðxityoÞ, and the density of yit given xit ¼ xt is f ðyt j xtÞ ¼ ½FðxtyoÞ\u0001yt½1 \u0005 FðxtyoÞ\u00011\u0005yt. The partial log likelihood for a cross section observation i is liðyÞ ¼ X T t¼1 fyit log FðxityÞ þ ð1 \u0005 yitÞ log½1 \u0005 FðxityÞ\u0001g ð13:48Þ and the partial MLE in this case—which simply maximizes liðyÞ summed across all i—is the pooled probit estimator. With T ﬁxed and N ! y, this estimator is consis- tent and ﬃﬃﬃﬃ N p -asymptotically normal without any assumptions other than identiﬁca- tion and standard regularity conditions. It is very important to know that the pooled probit estimator works without im- posing additional assumptions on ei ¼ ðei1; . . . ; eiTÞ0. When xit contains only exoge- nous variables zit, it would be standard to assume that eit is independent of zi 1 ðzi1; zi2; . . . ; ziTÞ; t ¼ 1; . . . ; T ð13:49Þ This is the natural strict exogeneity assumption (and is much stronger than simply assuming that eit and zit are independent for each t). The crime example can illustrate how strict exogeneity might fail. For example, suppose that zit measures the amount of time the person has spent in prison prior to the current year. An arrest this year ðyit ¼ 1Þ certainly has an e¤ect on expected future values of zit, so that assumption Chapter 13 404", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 416, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p417::c0", "text": "(13.49) is almost certainly false. Fortunately, we do not need assumption (13.49) to apply partial likelihood methods. A second standard assumption is that the eit, t ¼ 1; 2; . . . ; T are serially indepen- dent. This is especially restrictive in a static model. If we maintain this assumption in addition to assumption (13.49), then equation (13.46) holds (because the yit are then independent conditional on zi) and the partial MLE is a conditional MLE. To relax the assumption that the yit are conditionally independent, we can allow the eit to be correlated across t (still assuming that no lagged dependent variables appear). A common assumption is that ei has a multivariate normal distribution with a general correlation matrix. Under this assumption, we can write down the joint distribution of yi given zi, but it is complicated, and estimation is very computation- ally intensive (for recent discussions, see Keane, 1993, and Hajivassilou and Ruud, 1994). We will cover a special case, the random e¤ects probit model, in Chapter 15. A nice feature of the partial MLE is that ^y will be consistent and asymptotically normal even if the eit are arbitrarily serially correlated. This result is entirely analo- gous to using pooled OLS in linear panel data models when the errors have arbitrary serial correlation. When xit contains lagged dependent variables, model (13.47) provides a way of examining dynamic behavior. Or, perhaps yi;t\u00051 is included in xit as a proxy for unobserved factors, and our focus is on on policy variables in zit. For example, if yit is a binary indicator of employment, yi;t\u00051 might be included as a control when studying the e¤ect of a job training program (which may be a binary element of zit) on the employment probability; this method controls for the fact that participation in job training this year might depend on employment last year, and it captures the fact that employment status is persistent. In any case, provided Pðyit ¼ 1 j xitÞ follows a probit, the pooled probit estimator is consistent and asymptotically normal. The dynamics may or may not be correctly speciﬁed (more on this topic later), and the zit need not be strictly exogenous (so that whether someone participates in job training in year t can depend on the past employment history). 13.8.2 Asymptotic Inference The most important practical di¤erence between conditional MLE and partial MLE is in the computation of asymptotic standard errors and test statistics. In many cases, including the pooled probit estimator, the pooled Poisson estimator (see Problem 13.6), and many other pooled procedures, standard econometrics packages can be used to compute the partial MLEs. However, except under certain assumptions, the usual standard errors and test statistics reported from a pooled analysis are not valid. Maximum Likelihood Methods 405", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 417, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p418::c0", "text": "This situation is entirely analogous to the linear model case in Section 7.8 when the errors are serially correlated. Estimation of the asymptotic variance of the partial MLE is not di‰cult. In fact, we can combine the M-estimation results from Section 12.5.1 and the results of Sec- tion 13.5 to obtain valid estimators. From Theorem 12.3, we have Avar ﬃﬃﬃﬃ N p ð^y \u0005 yoÞ ¼ A\u00051 o BoA\u00051 o , where Ao ¼ \u0005E½‘2 y liðyoÞ\u0001 ¼ \u0005 X T t¼1 E½‘2 y litðyoÞ\u0001 ¼ X T t¼1 E½AitðyoÞ\u0001 Bo ¼ E½siðyoÞsiðyoÞ0\u0001 ¼ E X T t¼1 sitðyoÞ \" # X T t¼1 sitðyoÞ \" #0 ( ) AitðyoÞ ¼ \u0005E½‘2 y litðyoÞ j xit\u0001 sitðyÞ ¼ ‘ylitðyÞ0 There are several important features of these formulas. First, the matrix Ao is just the sum across t of minus the expected Hessian. Second, the matrix Bo generally depends on the correlation between the scores at di¤erent time periods: E½sitðyoÞsirðyoÞ0\u0001, t 0 r. Third, for each t, the conditional information matrix equality holds: AitðyoÞ ¼ E½sitðyoÞsitðyoÞ0 j xit\u0001 However, in general, \u0005E½HiðyoÞ j xi\u0001 0 E½siðyoÞsiðyoÞ0 j xi\u0001 and, more importantly, Bo 0 Ao. Thus, to perform inference in the context of partial MLE, we generally need separate estimates of Ao and Bo. Given the structure of the partial MLE, these are easy to obtain. Three possibilities for Ao are N\u00051 X N i¼1 X T t¼1 \u0005‘2 y litð^yÞ; N\u00051 X N i¼1 X T t¼1 Aitð^yÞ; and N\u00051 X N i¼1 X T t¼1 sitð^yÞsitð^yÞ0 ð13:50Þ The validity of the second of these follows from a standard iterated expectations argument, and the last of these follows from the conditional information matrix equality for each t. In most cases, the second estimator is preferred when it is easy to compute. Since Bo depends on E½sitðyoÞsitðyoÞ0\u0001 as well as cross product terms, there are also at least three estimators available for Bo. The simplest is Chapter 13 406", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 418, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p419::c0", "text": "N\u00051 X N i¼1 ^si^s0 i ¼ N\u00051 X N i¼1 X T t¼1 ^sit^s0 it þ N\u00051 X N i¼1 X T t¼1 X r0t ^sir^s0 it ð13:51Þ where the second term on the right-hand side accounts for possible serial correlation in the score. The ﬁrst term on the right-hand side of equation (13.51) can be replaced by one of the other two estimators in equation (13.50). The asymptotic variance of ^y is estimated, as usual, by ^A\u00051^B^A\u00051=N for the chosen estimators ^A and ^B. The asymptotic standard errors come directly from this matrix, and Wald tests for linear and nonlinear hypotheses can be obtained directly. The robust score statistic dis- cussed in Section 12.6.2 can also be used. When Bo 0 Ao, the likelihood ratio statistic computed after pooled estimation is not valid. Because the CIME holds for each t, Bo ¼ Ao when the scores evaluated at yo are serially uncorrelated, that is, when E½sitðyoÞsirðyoÞ0\u0001 ¼ 0; t 0 r ð13:52Þ When the score is serially uncorrelated, inference is very easy: the usual MLE statis- tics computed from the pooled estimation, including likelihood ratio statistics, are asymptotically valid. E¤ectively, we can ignore the fact that a time dimension is present. The estimator of Avarð^yÞ is just ^A\u00051=N, where ^A is one of the matrices in equation (13.50). Example 13.3 (continued): For the pooled probit example, a simple, general esti- mator of the asymptotic variance is X N i¼1 X T t¼1 Aitð^yÞ \" #\u00051 X N i¼1 sið^yÞsið^yÞ0 \" # X N i¼1 X T t¼1 Aitð^yÞ \" #\u00051 ð13:53Þ where Aitð^yÞ ¼ ffðxit ^yÞg2 x0 itxit Fðxit ^yÞ½1 \u0005 Fðxit ^yÞ\u0001 and siðyÞ ¼ X T t¼1 sitðyÞ ¼ X T t¼1 fðxityÞx0 it½yit \u0005 FðxityÞ\u0001 FðxityÞ½1 \u0005 FðxityÞ\u0001 The estimator (13.53) contains cross product terms of the form sitð^yÞsirð^yÞ0, t 0 r, and so it is fully robust. If the score is serially uncorrelated, then the usual probit standard errors and test statistics from the pooled estimation are valid. We will Maximum Likelihood Methods 407", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 419, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p420::c0", "text": "discuss a su‰cient condition for the scores to be serially uncorrelated in the next subsection. 13.8.3 Inference with Dynamically Complete Models There is a very important case where condition (13.52) holds, in which case all statistics obtained by treating liðyÞ as a standard log likelihood are valid. For any deﬁnition of xt, we say that f ftðyt j xt; yoÞ: t ¼ 1; . . . ; Tg is a dynamically complete conditional density if ftðyt j xt; yoÞ ¼ po t ðyt j xt; yt\u00051; xt\u00051; yt\u00052; . . . ; y1; x1Þ; t ¼ 1; . . . ; T ð13:54Þ In other words, ftðyt j xt; yoÞ must be the conditional density of yt given xt and the entire past of ðxt; ytÞ. When xt ¼ zt for contemporaneous exogenous variables, equation (13.54) is very strong: it means that, once zt is controlled for, no past values of zt or yt appear in the conditional density po t ðyt j zt; yt\u00051; zt\u00051; yt\u00052; . . . ; y1; z1Þ. When xt contains zt and some lags—similar to a ﬁnite distributed lag model—then equation (13.54) is per- haps more reasonable, but it still assumes that lagged yt has no e¤ect on yt once current and lagged zt are controlled for. That assumption (13.54) can be false is analogous to the omnipresence of serial correlation in static and ﬁnite distributed lag regression models. One important feature of dynamic completeness is that it does not require strict exogeneity of zt [since only current and lagged xt appear in equation (13.54)]. Dynamic completeness is more likely to hold when xt contains lagged dependent variables. The issue, then, is whether enough lags of yt (and zt) have been included in xt to fully capture the dynamics. For example, if xt 1 ðzt; yt\u00051Þ, then equation (13.54) means that, along with zt, only one lag of yt is needed to capture all of the dynamics. Showing that condition (13.52) holds under dynamic completeness is easy. First, for each t, E½sitðyoÞ j xit\u0001 ¼ 0, since ftðyt j xt; yoÞ is a correctly speciﬁed conditional density. But then, under assumption (13.54), E½sitðyoÞ j xit; yi;t\u00051; . . . ; yi1; xi1\u0001 ¼ 0 ð13:55Þ Now consider the expected value in condition (13.52) for r < t. Since sirðyoÞ is a function of ðxir; yirÞ, which is in the conditioning set (13.55), the usual iterated expectations argument shows that condition (13.52) holds. It follows that, under dy- namic completeness, the usual maximum likelihood statistics from the pooled esti- mation are asymptotically valid. This result is completely analogous to pooled OLS Chapter 13 408", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 420, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p421::c0", "text": "under dynamic completeness of the conditional mean and homoskedasticity (see Section 7.8). If the panel data probit model is dynamically complete, any software package that does standard probit can be used to obtain valid standard errors and test statis- tics, provided the response probability satisﬁes Pðyit ¼ 1 j xitÞ ¼ Pðyit ¼ 1 j xit; yi;t\u00051; xi;t\u00051; . . .Þ. Without dynamic completeness the standard errors and test statistics generally need to be adjusted for serial dependence. Since dynamic completeness a¤ords nontrivial simpliﬁcations, does this fact mean that we should always include lagged values of exogenous and dependent variables until equation (13.54) appears to be satisﬁed? Not necessarily. Static models are sometimes desirable even if they neglect dynamics. For example, suppose that we have panel data on individuals in an occupation where pay is determined partly by cumulative productivity. (Professional athletes and college professors are two ex- amples.) An equation relating salary to the productivity measures, and possibly de- mographic variables, is appropriate. Nothing implies that the equation would be dynamically complete; in fact, past salary could help predict current salary, even after controlling for observed productivity. But it does not make much sense to include past salary in the regression equation. As we know from Chapter 10, a reasonable approach is to include an unobserved e¤ect in the equation, and this does not lead to a model with complete dynamics. See also Section 13.9. We may wish to test the null hypothesis that the density is dynamically complete. White (1994) shows how to test whether the score is serially correlated in a pure time series setting. A similar approach can be used with panel data. A general test for dynamic misspeciﬁcation can be based on the limiting distribution of (the vectoriza- tion of ) N\u00051=2 X N i¼1 X T t¼2 ^sit^s0 i;t\u00051 where the scores are evaluated at the partial MLE. Rather than derive a general sta- tistic here, we will study tests of dynamic completeness in particular applications later (see particularly Chapters 15, 16, and 19). 13.8.4 Inference under Cluster Sampling Partial MLE methods are also useful when using cluster samples. Suppose that, for each group or cluster g, f ðyg j xg; yÞ is a correctly speciﬁed conditional density of yg given xg. Here, i indexes the cluster, and as before we assume a large number of clusters N and relatively small group sizes, Gi. The primary issue is that the yig might Maximum Likelihood Methods 409", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 421, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p422::c0", "text": "be correlated within a cluster, possibly through unobserved cluster e¤ects. A partial MLE of yo is deﬁned exactly as in the panel data case, except that t is replaced with g and T is replaced with Gi for each i; for example, equation (13.44) becomes liðyÞ 1 PGi g¼1 log f ðyig j xig; yÞ. Obtaining the partial MLE is usually much easier than spec- ifying (or deriving) the joint distribution of yi conditional on xi for each cluster i and employing MLE (which must recognize that the cluster observations cannot be identically distributed if the cluster sizes di¤er). In addition to allowing the yig to be arbitrarily dependent within a cluster, the partial MLE does not require Dðyig j xi1; . . . ; xiGiÞ ¼ Dðyig j xigÞ. But we need to compute the robust variance matrix estimator as in Section 13.8.2, along with robust test statistics. The quasi-likelihood ratio statistic is not valid unless Dðyig j xiÞ ¼ Dðyig j xigÞ and the yig are independent within each cluster, conditional on xi. We can use partial MLE analysis to test for peer e¤ects in cluster samples, as dis- cussed brieﬂy in Section 11.5 for linear models. For example, some elements of xig might be averages of explanatory variables for other units (say, people) in the cluster. Therefore, we might specify a model fgðyg j zg; wðgÞ; yÞ (for example, a probit model), where wðgÞ represents average characteristics of other people (or units) in the same cluster. The pooled partial MLE analysis is consistent and asymptotically normal, but the variance matrix must be corrected for additional within-cluster dependence. 13.9 Panel Data Models with Unobserved E¤ects As we saw in Chapters 10 and 11, linear unobserved e¤ects panel data models play an important role in modern empirical research. Nonlinear unobserved e¤ects panel data models are becoming increasingly more important. Although we will cover particular models in Chapters 15, 16, and 19, it is useful to have a general treatment. 13.9.1 Models with Strictly Exogenous Explanatory Variables For each i, let fðyit; xitÞ: t ¼ 1; 2; . . . ; Tg be a random draw from the cross section, where yit and xit can both be vectors. Associated with each cross section unit i is unobserved heterogeneity, ci, which could be a vector. We assume interest lies in the distribution of yit given ðxit; ciÞ. The vector xit can contain lags of contemporaneous variables, say zit [for example, xit ¼ ðzit; zi;t\u00051; zi;t\u00052Þ\u0001, or even leads of zit [for ex- ample, xit ¼ ðzit; zi;tþ1Þ\u0001, but not lags of yit. Whatever the lag structure, we let t ¼ 1 denote the ﬁrst time period available for estimation. Let ftðyt j xt; c; yÞ denote a correctly speciﬁed density for each t. A key assumption on xit is analogous to the strict exogeneity assumption for linear unobserved e¤ects Chapter 13 410", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 422, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p423::c0", "text": "models: Dðyit j xi; ciÞ ¼ Dðyit j xit; ciÞ, which means that only contemporaneous xit matters once ci is also conditioned on. (Whether or not xit contains lagged zit, strict exogeneity conditonal on ci rules out certain kinds of feedback from yit to zi;tþh, h > 0.) In many cases we want to allow ci and xi to be dependent. A general approach to estimating yo (and other quantities of interest) is to model the distribution of ci given xi. [In Chapters 15 and 19 we cover some important models where yo can be con- sistently estimated without making any assumptions about Dðci j xiÞ.] Let hðc j x; dÞ be a correctly speciﬁed density for ci given xi ¼ x. There are two common ways to proceed. First, we can make the additional as- sumption that, conditional on ðxi; ciÞ, the yit are independent. Then, the joint density of ðyi1; . . . ; yiTÞ, given ðxi; ciÞ, is Y T t¼1 ftðyt j xit; ci; yÞ We cannot use this density directly to estimate yo because we do not observe the outcomes ci. Instead, we can use the density of ci given xi to integrate out the de- pendence on c. The density of yi given xi is ð RJ Y T t¼1 ftðyt j xit; c; yoÞ \" # hðc j xi; doÞ dc ð13:56Þ where J is the dimension of c and hðc j x; dÞ is the correctly speciﬁed model for the density of ci given xi ¼ x. For concreteness, we assume that c is a continuous random vector. For each i, the log-likelihood function is log ð RJ Y T t¼1 ftðyit j xit; c; yoÞ \" # hðc j xi; doÞ dc ( ) ð13:57Þ [It is important to see that expression (13.57) does not depend on the ci; c has been integrated out.] Assuming identiﬁcation and standard regularity conditions, we can consistently estimate yo and do by conditional MLE, where the asymptotics are for ﬁxed T and N ! y. The CMLE is ﬃﬃﬃﬃ N p -asymptotically normal. Another approach is often simpler and places no restrictions on the joint distribu- tion of the yit [conditional on ðxi; ciÞ]. For each t, we can obtain the density of yit given xi: ð RJ½ ftðyt j xit; c; yoÞ\u0001hðc j xi; doÞ dc Maximum Likelihood Methods 411", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 423, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p424::c0", "text": "Now the problem becomes one of partial MLE. We estimate yo and do by maximizing X N i¼1 X T t¼1 log ð RJ½ ftðyit j xit; c; yÞ\u0001hðc j xi; dÞ dc \u0002 \u0007 ð13:58Þ (Actually, using PMLE, yo and do are not always separately identiﬁed, although in- teresting functions of them are. We will see examples in Chapters 15 and 16.) Across time, the scores for each i will necessarily be serially correlated because the yit are dependent when we condition only on xi, and not also on ci. Therefore, we must make inference robust to serial dependence, as in Section 13.8.2. In Chapter 15, we will study both the conditional MLE and partial MLE approaches for unobserved e¤ects probit models. 13.9.2 Models with Lagged Dependent Variables Now assume that we are interested in modeling Dðyit j zit; yi;t\u00051; ciÞ where, for sim- plicity, we include only contemporaneous conditioning variables, zit, and only one lag of yit. Adding lags (or even leads) of zit or more lags of yit requires only a notational change. A key assumption is that we have the dynamics correctly speciﬁed and that zi ¼ fzi1; . . . ; ziTg is appropriately strictly exogenous (conditional on ci). These assump- tions are both captured by Dðyit j zit; yi;t\u00051; ciÞ ¼ Dðyit j zi; yi;t\u00051; . . . ; yi0; ciÞ ð13:59Þ We assume that ftðyt j zt; yt\u00051; c; yÞ is a correctly speciﬁed density for the conditional distribution on the left-hand side of equation (13.59). Given strict exogeneity of fzit: t ¼ 1; . . . ; Tg and dynamic completeness, the density of ðyi1; . . . ; yiTÞ given ðzi ¼ z; yi0 ¼ y0; ci ¼ cÞ is Y T t¼1 ftðyt j zt; yt\u00051; c; yoÞ ð13:60Þ (By convention, yi0 is the ﬁrst observation on yit.) Again, to estimate yo, we integrate c out of this density. To do so, we specify a density for ci given zi and the initial value yi0 (sometimes called the initial condition). Let hðc j z; y0; dÞ denote the model for this conditional density. Then, assuming that we have this model correctly specifed, the density of ðyi1; . . . ; yiTÞ given ðzi ¼ z; yi0 ¼ y0Þ is ð RJ Y T t¼1 ftðyt j zt; yt\u00051; c; yoÞ \" # hðc j z; y0; doÞ dc ð13:61Þ Chapter 13 412", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 424, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p425::c0", "text": "which, for each i, leads to the log-likelihood function conditional on ðzi; yi0Þ: log ð RJ Y T t¼1 ftðyit j zit; yi;t\u00051; c; yÞ \" # hðc j zi; yi0; dÞ dc ( ) ð13:62Þ We sum expression (13.62) across i ¼ 1; . . . ; N and maximize with respect to y and d to obtain the CMLEs. Provided all functions are su‰ciently di¤erentiable and iden- tiﬁcation holds, the conditional MLEs are consistent and ﬃﬃﬃﬃ N p -asymptotically normal, as usual. Because we have fully speciﬁed the conditional density of ðyi1; . . . ; yiTÞ given ðzi; yi0Þ, the general theory of conditional MLE applies directly. [The fact that the distribution of yi0 given zi would typically depend on yo has no bearing on the con- sistency of the CMLE. The fact that we are conditioning on yi0, rather than basing the analysis on Dðyi0; yi1; . . . ; yiT j ziÞ, means that we are generally sacriﬁcing e‰- ciency. But by conditioning on yi0 we do not have to ﬁnd Dðyi0 j ziÞ, something which is very di‰cult if not impossible.] The asymptotic variance of ð^y0; ^d0Þ0 can be esti- mated by any of the formulas in equation (13.32) (properly modiﬁed to account for estimation of yo and do). A weakness of the CMLE approach is that we must specify a density for ci given ðzi; yi0Þ, but this is a price we pay for estimating dynamic, nonlinear models with unobserved e¤ects. The alternative of treating the ci as parameters to estimate— which is, unfortunately, often labeled the ‘‘ﬁxed e¤ects’’ approach—does not lead to consistent estimation of yo. In any application, several issues need to be addressed. First, when are the param- eters identiﬁed? Second, what quantities are we interested in? As we cannot observe ci, we typically want to average out ci when obtaining partial e¤ects. Wooldridge (2000e) shows that average partial e¤ects are generally identiﬁed under the assump- tions that we have made. Finally, obtaining the CMLE can be very di‰cult compu- tationally, as can be obtaining the asymptotic variance estimates in equation (13.32). If ci is a scalar, estimation is easier, but there is still a one-dimensional integral to approximate for each i. In Chapters 15, 16, and 19 we will see that, under reasonable assumptions, standard software can be used to estimate dynamic models with unob- served e¤ects, including e¤ects that are averaged across the distribution of heteroge- neity. See also Problem 13.11 for application to a dynamic linear model. 13.10 Two-Step MLE Consistency and asymptotic normality results are also available for two-step maxi- mum likelihood estimators and two-step partial maximum likelihood estimators; we Maximum Likelihood Methods 413", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 425, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p426::c0", "text": "focus on the former for concreteness. Let the conditional density be f ð\u0004 j xi; yo; goÞ, where go is an R \u0003 1 vector of additional parameters. A preliminary estimator of go, say ^g, is plugged into the log-likelihood function, and ^y solves max y A Y X N i¼1 log f ðyi j xi; y; ^gÞ Consistency follow from results for two-step M-estimators. The practical limitation is that log f ðyi j xi; y; gÞ is continuous on Y \u0003 G and that yo and go are identiﬁed. Asymptotic normality of the two-step MLE follows directly from the results on two-step M-estimation in Chapter 12. As we saw there, in general the asymptotic variance of ﬃﬃﬃﬃ N p ð^y \u0005yoÞ depends on the asymptotic variance of ﬃﬃﬃﬃ N p ð^g\u0005goÞ [see equa- tion (12.41)], so we need to know the estimation problem solved by ^g. In some cases estimation of go can be ignored. An important case is where the expected Hessian, deﬁned with respect to y and g, is block diagonal [the matrix Fo in equation (12.36) is zero in this case]. It can also hold for some values of yo, which is important for testing certain hypotheses. We will encounter several examples in Part IV. Problems 13.1. If f ðy j x; yÞ is a correctly speciﬁed model for the density of yi given xi, does yo solve maxy A Y E½ f ðyi j xi; yÞ\u0001? 13.2. Suppose that for a random sample, yi j xi @ Normal½mðxi; boÞ; s2 o\u0001, where mðx; bÞ is a function of the K-vector of explanatory variables x and the P \u0003 1 param- eter vector b. Recall that Eðyi j xiÞ ¼ mðxi; boÞ and Varðyi j xiÞ ¼ s2 o. a. Write down the conditional log-likelihood function for observation i. Show that the CMLE of bo, ^b, solves the problem minb PN i¼1½yi \u0005 mðxi; bÞ\u00012. In other words, the CMLE for bo is the nonlinear least squares estimator. b. Let y 1 ðb 0s2Þ0 denote the ðP þ 1Þ \u0003 1 vector of parameters. Find the score of the log likelihood for a generic i. Show directly that E½siðyoÞ j xi\u0001 ¼ 0. What features of the normal distribution do you need in order to show that the conditional expectation of the score is zero? c. Use the ﬁrst-order condition to ﬁnd ^s2 in terms of ^b. d. Find the Hessian of the log-likelihood function with respect to y. e. Show directly that \u0005E½HiðyoÞ j xi\u0001 ¼ E½siðyoÞsiðyoÞ0 j xi\u0001. Chapter 13 414", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 426, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p427::c0", "text": "f. Write down the estimated asymptotic variance of ^b, and explain how to obtain the asymptotic standard errors. 13.3. Consider a general binary response model Pðyi ¼ 1 j xiÞ ¼ Gðxi; yoÞ, where Gðx; yÞ is strictly between zero and one for all x and y. Here, x and y need not have the same dimension; let x be a K-vector and y a P-vector. a. Write down the log likelihood for observation i. b. Find the score for each i. Show directly that E½siðyoÞ j xi\u0001 ¼ 0. c. When Gðx; yÞ ¼ F½xb þ d1ðxbÞ2 þ d2ðxbÞ3\u0001, ﬁnd the LM statistic for testing H0: do1 ¼ 0; do2 ¼ 0. 13.4. In the Newey-Tauchen-White speciﬁcation-testing context, explain why we can take gðw; yÞ ¼ aðx; yÞsðw; yÞ, where aðx; yÞ is essentially any scalar function of x and y. 13.5. In the context of CMLE, consider a reparameterization of the kind in Section 12.6.2: f ¼ gðyÞ, where the Jacobian of g, GðyÞ, is continuous and nonsingular for all y A Y. Let sg i ðfÞ ¼ sg i ½gðyÞ\u0001 denote the score of the log likelihood in the reparam- eterized model; thus, from Section 12.6.2, sg i ðfÞ ¼ ½GðyÞ0\u0001\u00051siðyÞ. a. Using the conditional information matrix equality, ﬁnd Ag iðfoÞ 1 E½sg i ðfoÞsg i ðfoÞ0 j xi\u0001 in terms of GðyoÞ and AiðyoÞ 1 E½siðyoÞsiðyoÞ0 j xi\u0001. b. Show that ~Ag i ¼ ~G0\u00051 ~Ai ~G\u00051, where these are all evaluated at the restricted esti- mate, ~y. c. Use part b to show that the expected Hessian form of the LM statistic is invariant to reparameterization. 13.6. Suppose that for a panel data set with T time periods, yit given xit has a Poisson distribution with mean expðxityoÞ, t ¼ 1; . . . ; T. a. Do you have enough information to construct the joint distribution of yi given xi? Explain. b. Write down the partial log likelihood for each i and ﬁnd the score, siðyÞ. c. Show how to estimate Avarð^yÞ; it should be of the form (13.53). d. How does the estimator of Avarð^yÞ simplify if the conditional mean is dynami- cally complete? 13.7. Suppose that you have two parametric models for conditional densities: gðy1 j y2; x; yÞ and hðy2 j x; yÞ; not all elements of y need to appear in both densities. Denote the true value of y by yo. Maximum Likelihood Methods 415", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 427, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p428::c0", "text": "a. What is the joint density of ðy1; y2Þ given x? How would you estimate yo given a random sample on ðx; y1; y2Þ? b. Suppose now that a random sample is not available on all variables. In particular, y1 is observed only when ðx; y2Þ satisﬁes a known rule. For example, when y2 is binary, y1 is observed only when y2 ¼ 1. We assume ðx; y2Þ is always observed. Let r2 be a binary variable equal to one if y1 is observed and zero otherwise. A partial MLE is obtained by deﬁning liðyÞ ¼ ri2 log gðyi1 j yi2; xi; yÞ þ log hðyi2 j xi; yÞ 1 ri2li1ðyÞ þ li2ðyÞ for each i. This formulation ensures that ﬁrst part of li only enters the estimation when yi1 is observed. Verify that yo maximizes E½liðyÞ\u0001 over Y. c. Show that \u0005E½HiðyoÞ\u0001 ¼ E½siðyoÞsiðyoÞ0\u0001, even though the problem is not a true conditional MLE problem (and therefore a conditional information matrix equality does not hold). d. Argue that a consistent estimator of Avar ﬃﬃﬃﬃ N p ð^y \u0005 yoÞ is N\u00051 X N i¼1 ðri2 ^Ai1 þ ^Ai2Þ \" #\u00051 where Ai1ðyoÞ ¼ \u0005E½‘2 y li1ðyoÞ j yi2; xi\u0001, Ai2ðyoÞ ¼ \u0005E½‘2 y li2ðyoÞ j xi\u0001, and ^y replaces yo in obtaining the estimates. 13.8. Consider a probit model with an unobserved explanatory variable v, Pðy ¼ 1 j x; z; vÞ ¼ Fðxdo þ rovÞ but where v depends on observable variables w and z and a vector of parameters go: v ¼ w \u0005 zgo. Assume that Eðv j x; zÞ ¼ 0; this assumption implies, among other things, that go can be consistently estimated by the OLS regression of wi on zi, using a random sample. Deﬁne ^vi 1 wi \u0005 zi^g. Let ^y ¼ ð^d0; ^rÞ0 be the two-step probit esti- mator from probit of yi on xi, ^vi. a. Using the results from Section 12.5.2, show how to consistently estimate Avar ﬃﬃﬃﬃ N p ð^y \u0005 yoÞ. b. Show that, when ro ¼ 0, the usual probit asymptotic variance estimator is valid. That is, valid inference is obtained for ðd0 o; roÞ0 by ignoring the ﬁrst-stage estimation. c. How would you test H0: ro ¼ 0? 13.9. Let fyt: t ¼ 0; 1; . . . ; Tg be an observable time series representing a popula- tion, where we use the convention that t ¼ 0 is the ﬁrst time period for which y is Chapter 13 416", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 428, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p429::c0", "text": "observed. Assume that the sequence follows a Markov process: Dðyt j yt\u00051; yt\u00052; . . . y0Þ ¼ Dðyt j yt\u00051Þ for all t b 1. Let ftðyt j yt\u00051; yÞ denote a correctly speciﬁed model for the density of yt given yt\u00051, t b 1, where yo is the true value of y. a. Show that, to obtain the joint distribution of ðy0; y2; . . . ; yTÞ, you need to cor- rectly model the density of y0. b. Given a random sample of size N from the population, that is, ðyi0; yi1; . . . ; yiTÞ for each i, explain how to consistently etimate yo without modeling Dðy0Þ. c. How would you estimate the asymptotic variance of the estimator from part b? Be speciﬁc. 13.10. Let y be a G \u0003 1 random vector with elements yg, g ¼ 1; 2; . . . ; G. These could be di¤erent response variables for the same cross section unit or responses at di¤erent points in time. Let x be a K-vector of observed conditioning variables, and let c be an unobserved conditioning variable. Let fgð\u0004 j x; cÞ denote the density of yg given ðx; cÞ. Further, assume that the y1; y2; . . . ; yG are independent conditional on ðx; cÞ: a. Write down the joint density of y given ðx; cÞ. b. Let hð\u0004 j xÞ be the density of c given x. Find the joint density of y given x. c. If each fgð\u0004 j x; cÞ is known up to a Pg-vector of parameters gg o and hð\u0004 j xÞ is known up to an M-vector do, ﬁnd the log likelihood for any random draw ðxi; yiÞ from the population. d. Is there a relationship between this setup and a linear SUR model? 13.11. Consider the dynamic, linear unobserved e¤ects model yit ¼ ryi;t\u00051 þ ci þ eit; t ¼ 1; 2; . . . ; T Eðeit j yi;t\u00051; yi;t\u00052; . . . ; yi0; ciÞ ¼ 0 In Section 11.1.1 we discussed estimation of r by instrumental variables methods after di¤erencing. The deﬁciencies of the IV approach for large r may be overcome by applying the conditional MLE methods in Section 13.9.2. a. Make the stronger assumption that yit j ðyi;t\u00051; yi;t\u00052; . . . ; yi0; ciÞ is normally dis- tributed with mean ryi;t\u00051 þ ci and variance s2 e . Find the density of ðyi1; . . . ; yiTÞ given ðyi0; ciÞ. Is it a good idea to use the log of this density, summed across i, to estimate r and s2 e along with the ‘‘ﬁxed e¤ects’’ ci? b. If ci j yi0 @ Normalða0 þ a1yi0; s2 a Þ, where s2 a 1 VarðaiÞ and ai 1 ci \u0005 a0 \u0005 a1yi0, write down the density of ðyi1; . . . ; yiTÞ given yi0. How would you estimate r, a0, a1, s2 e , and s2 a ? Maximum Likelihood Methods 417", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 429, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p430::c0", "text": "c. Under the same assumptions in parts a and b, extend the model to yit ¼ ryi;t\u00051 þ ci þ dciyi;t\u00051 þ eit. Explain how to estimate the parameters of this model, and pro- pose a consistent estimator of the average partial e¤ect of the lag, r þ dEðciÞ. d. Now extend part b to the case where zitb is added to the conditional mean func- tion, where the zit are strictly exogenous conditional on ci. Assume that ci j yi0; zi @ Normalða0 þ a1yi0 þ zid; s2 a Þ, where zi is the vector of time averages. Appendix 13A In this appendix we cover some important properties of conditional distributions and conditional densities. Billingsley (1979) is a good reference for this material. For random vectors y A Y H RG and x A X H RK, the conditional distribution of y given x always exists and is denoted Dðy j xÞ. For each x this distribution is a proba- bility measure and completely describes the behavior of the random vector y once x takes on a particular value. In econometrics, we almost always assume that this distribution is described by a conditional density, which we denote by pð\u0004 j xÞ. The density is with respect to a measure deﬁned on the support Y of y. A conditional den- sity makes sense only when this measure does not change with the value of x. In practice, this assumption is not very restrictive, as it means that the nature of y is not dramatically di¤erent for di¤erent values of x. Let n be this measure on RJ. If Dðy j xÞ is discrete, n can be the counting measure and all integrals are sums. If Dðy j xÞ is absolutely continuous, then n is the familiar Lebesgue measure appearing in elementary integration theory. In some cases, Dðy j xÞ has both discrete and con- tinuous characteristics. The important point is that all conditional probabilities can be obtained by inte- gration: Pðy A A j x ¼ xÞ ¼ ð A pðy j xÞnðdyÞ where y is the dummy argument of integration. When y is discrete, taking on the values y1, y2; . . . ; then pð\u0004 j xÞ is a probability mass function and Pðy ¼ yj j x ¼ xÞ ¼ pðyj j xÞ, j ¼ 1; 2; . . . : Suppose that f and g are nonnegative functions on RM, and deﬁne Sf 1 fz A RM: f ðzÞ > 0g. Assume that 1 ¼ ð Sf f ðzÞnðdzÞ b ð Sf gðzÞnðdzÞ ð13:63Þ Chapter 13 418", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 430, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p431::c0", "text": "where n is a measure on RM. The equality in expression (13.63) implies that f is a density on RM, while the inequality holds if g is also a density on RM. An important result is that Iðf ; gÞ 1 ð Sf log½ f ðzÞ=gðzÞ\u0001 f ðzÞnðdzÞ b 0 ð13:64Þ [Note that Iðf ; gÞ ¼ y is allowed; one case where this result can occur is f ðzÞ > 0 but gðzÞ ¼ 0 for some z. Also, the integrand is not deﬁned when f ðzÞ ¼ gðzÞ ¼ 0, but such values of z have no e¤ect because the integrand receives zero weight in the in- tegration.] The quantity Ið f ; gÞ is called the Kullback-Leibler information criterion (KLIC). Another way to state expression (13.64) is Eflog½ f ðzÞ\u0001g b Eflog½gðzÞ\u0001g ð13:65Þ where z A Z H RM is a random vector with density f. Conditional MLE relies on a conditional version of inequality (13.63): property CD.1: Let y A Y H RG and x A X H RK be random vectors. Let pð\u0004 j \u0004Þ denote the conditional density of y given x. For each x, let YðxÞ 1 fy: pðy j xÞ > 0g be the conditional support of y, and let n be a measure that does not depend on x. Then for any other function gð\u0004 j xÞ b 0 such that 1 ¼ ð YðxÞ pðy j xÞnðdyÞ b ð YðxÞ gðy j xÞnðdyÞ the conditional KLIC is nonnegative: Ixðp; gÞ 1 ð YðxÞ log½ pðy j xÞ=gðy j xÞ\u0001pðy j xÞnðdyÞ b 0 That is, Eflog½ pðy j xÞ\u0001 j xg b Eflog½gðy j xÞ\u0001 j xg for any x A X. The proof uses the conditional Jensen’s inequality (Property CE.7 in Chapter 2). See Manski (1988, Section 5.1). property CD.2: For random vectors y, x, and z, let pðy j x; zÞ be the conditional density of y given ðx; zÞ and let pðx j zÞ denote the conditional density of x given z. Then the density of ðy; xÞ given z is pðy; x j zÞ ¼ pðy j x; zÞpðx j zÞ where the script variables are placeholders. Maximum Likelihood Methods 419", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 431, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p432::c0", "text": "property CD.3: For random vectors y, x, and z, let pðy j x; zÞ be the conditional density of y given ðx; zÞ, let pðy j xÞ be the conditional density of y given x, and let pðz j xÞ denote the conditional density of z given x with respect to the measure nðdzÞ. Then pðy j xÞ ¼ ð Z pðy j x; zÞpðz j xÞnðdzÞ In other words, we can obtain the density of y given x by integrating the density of y given the larger conditioning set, ðx; zÞ, against the density of z given x. property CD.4: Suppose that the random variable, u, with cdf, F, is independent of the random vector x. Then, for any function aðxÞ of x, P½u a aðxÞ j x\u0001 ¼ F½aðxÞ\u0001: Chapter 13 420", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 432, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p433::c0", "text": "14 Generalized Method of Moments and Minimum Distance Estimation In Chapter 8 we saw how the generalized method of moments (GMM) approach to estimation can be applied to multiple-equation linear models, including systems of equations, with exogenous or endogenous explanatory variables, and to panel data models. In this chapter we extend GMM to nonlinear estimation problems. This setup allows us to treat various e‰ciency issues that we have glossed over until now. We also cover the related method of minimum distance estimation. Because the asymptotic analysis has many features in common with Chapters 8 and 12, the anal- ysis is not quite as detailed here as in previous chapters. A good reference for this material, which ﬁlls in most of the gaps left here, is Newey and McFadden (1994). 14.1 Asymptotic Properties of GMM Let fwi A RM: i ¼ 1; 2; . . .g denote a set of independent, identically distributed ran- dom vectors, where some feature of the distribution of wi is indexed by the P \u0001 1 parameter vector y. The assumption of identical distribution is mostly for notational convenience; the following methods apply to independently pooled cross sections without modiﬁcation. We assume that for some function gðwi; yÞ A RL, the parameter yo A Y H RP sat- isﬁes the moment assumptions E½gðwi; yoÞ\u0002 ¼ 0 ð14:1Þ As we saw in the linear case, where gðwi; yÞ was of the form Z0 iðyi \u0003 XiyÞ, a minimal requirement for these moment conditions to identify yo is L b P. If L ¼ P, then the analogy principle suggests estimating yo by setting the sample counterpart, N\u00031 PN i¼1 gðwi; yÞ, to zero. In the linear case, this step leads to the instrumental vari- ables estimator [see equation (8.22)]. When L > P, we can choose ^y to make the sample average close to zero in an appropriate metric. A generalized method of moments (GMM) estimator, ^y, minimizes a quadratic form in PN i¼1 gðwi; yÞ: min y A Y X N i¼1 gðwi; yÞ \" #0 ^X X N i¼1 gðwi; yÞ \" # ð14:2Þ where ^X is an L \u0001 L symmetric, positive semideﬁnite weighting matrix. Consistency of the GMM estimator follows along the lines of consistency of the M-estimator in Chapter 12. Under standard moment conditions, N\u00031 PN i¼1 gðwi; yÞ satisﬁes the uniform law of large numbers (see Theorem 12.1). If, ^X ! p Xo, where Xo is an L \u0001 L positive deﬁnite matrix, then the random function", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 433, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p434::c0", "text": "QNðyÞ 1 N\u00031 X N i¼1 gðwi; yÞ \" #0 ^X N\u00031 X N i¼1 gðwi; yÞ \" # ð14:3Þ converges uniformly in probability to fE½gðwi; yÞ\u0002g0XofE½gðwi; yÞ\u0002g ð14:4Þ Because Xo is positive deﬁnite, yo uniquely minimizes expression (14.4). For com- pleteness, we summarize with a theorem containing regularity conditions: theorem 14.1 (Consistency of GMM): Assume that (a) Y is compact; (b) for each y A Y, gð\u0004 ; yÞ is Borel measurable on W; (c) for each w A W, gðw; \u0004Þ is continuous on Y; (d) jgjðw; yÞj a bðwÞ for all y A Y and j ¼ 1; . . . ; L, where bð\u0004Þ is a nonnegative function on W such that E½bðwÞ\u0002 < y; (e) ^X ! p Xo, an L \u0001 L positive deﬁnite matrix; and (f ) yo is the unique solution to equation (14.1). Then a random vector ^y exists that solves problem (14.2), and ^y ! p yo. If we assume only that Xo is positive semideﬁnite, then we must directly assume that yo is the unique minimizer of expression (14.4). Occasionally this generality is useful, but we will not need it. Under the assumption that gðw; \u0004Þ is continuously di¤erentiable on intðYÞ, yo A intðYÞ, and other standard regularity conditions, we can easily derive the limiting distribution of the GMM estimator. The ﬁrst-order condition for ^y can be written as X N i¼1 ‘ygðwi; ^yÞ \" #0 ^X X N i¼1 gðwi; ^yÞ \" # 1 0 ð14:5Þ Deﬁne the L \u0001 P matrix Go 1 E½‘ygðwi; yoÞ\u0002 ð14:6Þ which we assume to have full rank P. This assumption essentially means that the moment conditions (14.1) are nonredundant. Then, by the WLLN and CLT, N\u00031 X N i¼1 ‘ygðwi; yoÞ ! p Go and N\u00031=2 X N i¼1 gðwi; yoÞ ¼ Opð1Þ ð14:7Þ respectively. Let giðyÞ 1 gðwi; yÞ. A mean value expansion of PN i¼1 gðwi; ^yÞ about yo, appropriate standardizations by the sample size, and replacing random averages with their plims gives 0 ¼ G0 oXoN\u00031=2 X N i¼1 giðyoÞ þ Ao ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ þ opð1Þ ð14:8Þ Chapter 14 422", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 434, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p435::c0", "text": "where Ao 1 G0 oXoGo ð14:9Þ Since Ao is positive deﬁnite under the given assumptions, we have ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ ¼ \u0003A\u00031 o G0 oXoN\u00031=2 X N i¼1 giðyoÞ þ opð1Þ ! d Normalð0; A\u00031 o BoA\u00031 o Þ ð14:10Þ where Bo 1 G0 oXoLoXoGo ð14:11Þ and Lo 1 E½giðyoÞgiðyoÞ0\u0002 ¼ Var½giðyoÞ\u0002 ð14:12Þ Expression (14.10) gives the inﬂuence function representation for the GMM estima- tor, and it also gives the limiting distribution of the GMM estimator. We summarize with a theorem, which is essentially given by Newey and McFadden (1994, Theorem 3.4): theorem 14.2 (Asymptotic Normality of GMM): In addition to the assumptions in Theorem 14.1, assume that (a) yo is in the interior of Y; (b) gðw; \u0004Þ is continuously di¤erentiable on the interior of Y for all w A W; (c) each element of gðw; yoÞ has ﬁnite second moment; (d) each element of ‘ygðw; yÞ is bounded in absolute value by a function bðwÞ, where E½bðwÞ\u0002 < y; and (e) Go in expression (14.6) has rank P. Then expression (14.10) holds, and so Avarð^yÞ ¼ A\u00031 o BoA\u00031 o =N. Estimating the asymptotic variance of the GMM estimator is easy once ^y has been obtained. A consistent estimator of Lo is given by ^L 1 N\u00031 X N i¼1 gið^yÞgið^yÞ0 ð14:13Þ and Avarð^yÞ is estimated as ^A\u00031^B^A\u00031=N, where ^A 1 ^G0^X^G; ^B 1 ^G0^X^L^X^G ð14:14Þ and ^G 1 N\u00031 X N i¼1 ‘ygið^yÞ ð14:15Þ Generalized Method of Moments and Minimum Distance Estimation 423", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 435, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p436::c0", "text": "As in the linear case in Section 8.3.3, an optimal weighting matrix exists for the given moment conditions: ^X should be a consistent estimator of L\u00031 o . When Xo ¼ L\u00031 o , Bo ¼ Ao and Avar ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ ¼ ðG0 oL\u00031 o GoÞ\u00031. Thus the di¤erence in asymptotic variances between the general GMM estimator and the estimator with plim ^X ¼ L\u00031 o is ðG0 oXoGoÞ\u00031ðG0 oXoLoXoGoÞðG0 oXoGoÞ\u00031 \u0003 ðG0 oL\u00031 o GoÞ\u00031 ð14:16Þ This expression can be shown to be positive semideﬁnite using the same argument as in Chapter 8 (see Problem 8.5). In order to obtain an asymptotically e‰cient GMM estimator we need a prelimi- nary estimator of yo in order to obtain ^L. Let ^^y^y be such an estimator, and deﬁne ^L as in expression (14.13) but with ^^y^y in place of ^y. Then, an e‰cient GMM estimator [given the function gðw; yÞ] solves min y A Y X N i¼1 gðwi; yÞ \" #0 ^L\u00031 X N i¼1 gðwi; yÞ \" # ð14:17Þ and its asymptotic variance is estimated as Av^arð^yÞ ¼ ð^G0 ^L\u00031 ^GÞ\u00031=N ð14:18Þ As in the linear case, an optimal GMM estimator is called the minimum chi-square estimator because N\u00031=2 X N i¼1 gið^yÞ \" #0 ^L\u00031 X N i¼1 N\u00031=2gið^yÞ \" # ð14:19Þ has a limiting chi-square distribution with L \u0003 P degrees of freedom under the con- ditions of Theorem 14.2. Therefore, the value of the objective function (properly standardized by the sample size) can be used as a test of any overidentifying restric- tions in equation (14.1) when L > P. If statistic (14.19) exceeds the relevant critical value in a w2 L\u0003P distribution, then equation (14.1) must be rejected: at least some of the moment conditions are not supported by the data. For the linear model, this is the same statistic given in equation (8.49). As always, we can test hypotheses of the form H0: cðyoÞ ¼ 0, where cðyÞ is a Q \u0001 1 vector, Q a P, by using the Wald approach and the appropriate variance matrix estimator. A statistic based on the di¤erence in objective functions is also available if the minimum chi-square estimator is used so that Bo ¼ Ao. Let ~y denote the solution to problem (14.17) subject to the restrictions cðyÞ ¼ 0, and let ^y denote the unrestricted estimator solving problem (14.17); importantly, these both use the same weighting Chapter 14 424", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 436, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p437::c0", "text": "matrix ^L\u00031. Typically, ^L is obtained from a ﬁrst-stage, unrestricted estimator. Assuming that the constraints can be written in implicit form and satisfy the condi- tions discussed in Section 12.6.2, the GMM distance statistic (or GMM criterion function statistic) has a limiting w2 Q distribution: X N i¼1 gið~yÞ \" #0 ^L\u00031 X N i¼1 gið~yÞ \" # \u0003 X N i¼1 gið^yÞ \" #0 ^L\u00031 X N i¼1 gið^yÞ \" # ( ) =N ! d w2 Q ð14:20Þ When applied to linear GMM problems, we obtain the statistic in equation (8.45). One nice feature of expression (14.20) is that it is invariant to reparameterization of the null hypothesis, just as the quasi-LR statistic is invariant for M-estimation. Therefore, we might prefer statistic (14.20) over the Wald statistic (8.48) for testing nonlinear restrictions in linear models. Of course, the computation of expression (14.20) is more di‰cult because we would actually need to carry out estimation sub- ject to nonlinear restrictions. A nice application of the GMM methods discussed in this section is two-step esti- mation procedures, which arose in Chapters 6, 12, and 13. Suppose that the estimator ^y—it could be an M-estimator or a GMM estimator—depends on a ﬁrst-stage esti- mator, ^g. A uniﬁed approach to obtaining the asymptotic variance of ^y is to stack the ﬁrst-order conditions for ^y and ^g into the same function gð\u0004Þ. This is always possible for the estimators encountered in this book. For example, if ^g is an M-estimator solving PN i¼1 sðwi; ^gÞ ¼ 0, and ^y is a two-step M-estimator solving X N i¼1 hðwi; ^y; ^gÞ\u0002 ¼ 0 ð14:21Þ then we can obtain the asymptotic variance of ^y by deﬁning gðw; y; gÞ ¼ hðw; y; gÞ sðw; gÞ \u0002 \u0003 and applying the GMM formulas. The ﬁrst-order condition for the full GMM prob- lem reproduces the ﬁrst-order conditions for each estimator separately. In general, either ^g, ^y, or both might themselves be GMM estimators. Then, stacking the orthogonality conditions into one vector can simplify the derivation of the asymptotic variance of the second-step estimator ^y while also ensuring e‰cient estimation when the optimal weighting matrix is used. Finally, sometimes we want to know whether adding additional moment con- ditions does not improve the e‰ciency of the minimum chi-square estimator. (Adding Generalized Method of Moments and Minimum Distance Estimation 425", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 437, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p438::c0", "text": "additional moment conditions can never reduce asymptotic e‰ciency, provided an e‰cient weighting matrix is used.) In other words, if we start with equation (14.1) but add new moments of the form E½hðw; yoÞ\u0002 ¼ 0, when does using the extra moment conditions yield the same asymptotic variance as the original moment conditions? Breusch, Qian, Schmidt, and Wyhowski (1999) prove some general redundancy results for the minimum chi-square estimator. Qian and Schmidt (1999) study the problem of adding moment conditions that do not depend on unknown parameters, and they characterize when such moment conditions improve e‰ciency. 14.2 Estimation under Orthogonality Conditions In Chapter 8 we saw how linear systems of equations can be estimated by GMM under certain orthogonality conditions. In general applications, the moment con- ditions (14.1) almost always arise from assumptions that disturbances are uncorre- lated with exogenous variables. For a G \u0001 1 vector rðwi; yÞ and a G \u0001 L matrix Zi, assume that yo satisﬁes E½Z0 irðwi; yoÞ\u0002 ¼ 0 ð14:22Þ The vector function rðwi; yÞ can be thought of as a generalized residual function. The matrix Zi is usually called the matrix of instruments. Equation (14.22) is a special case of equation (14.1) with gðwi; yÞ 1 Z0 irðwi; yÞ. In what follows, write riðyÞ 1 rðwi; yÞ. Identiﬁcation requires that yo be the only y A Y such that equation (14.22) holds. Condition e of the asymptotic normality result Theorem 14.2 requires that rank E½Z0 i‘yriðyoÞ\u0002 ¼ P (necessary is L b P). Thus, while Zi must be orthogonal to riðyoÞ, Zi must be su‰ciently correlated with the G \u0001 P Jacobian, ‘yriðyoÞ. In the linear case where rðwi; yÞ ¼ yi \u0003 Xiy, this requirement reduces to EðZ0 iXiÞ having full column rank, which is simply Assumption SIV.2 in Chapter 8. Given the instruments Zi, the e‰cient estimator can be obtained as in Section 14.1. A preliminary estimator ^^y^y is usually obtained with ^X 1 N\u00031 X N i¼1 Z0 iZi !\u00031 ð14:23Þ so that ^^y^y solves min y A Y X N i¼1 Z0 iriðyÞ \" #0 N\u00031 X N i¼1 Z0 iZi \" #\u00031 X N i¼1 Z0 iriðyÞ \" # ð14:24Þ Chapter 14 426", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 438, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p439::c0", "text": "The solution to problem (14.24) is called the nonlinear system 2SLS estimator; it is an example of a nonlinear instrumental variables estimator. From Section 14.1, we know that the nonlinear system 2SLS estimator is guaran- teed to be the e‰cient GMM estimator if for some s2 o > 0, E½Z0 iriðyoÞriðyoÞ0Zi\u0002 ¼ s2 o EðZ0 iZiÞ Generally, this is a strong assumption. Instead, we can obtain the minimum chi-square estimator by obtaining ^L ¼ N\u00031 X N i¼1 Z0 irið^^y^yÞrið^^y^yÞ0Zi ð14:25Þ and using this in expression (14.17). In some cases more structure is available that leads to a three-stage least squares estimator. In particular, suppose that E½Z0 iriðyoÞriðyoÞ0Zi\u0002 ¼ EðZ0 iWoZiÞ ð14:26Þ where Wo is the G \u0001 G matrix Wo ¼ E½riðyoÞriðyoÞ0\u0002 ð14:27Þ When E½riðyoÞ\u0002 ¼ 0, as is almost always the case under assumption (14.22), Wo is the variance matrix of riðyoÞ. As in Chapter 8, assumption (14.26) is a kind of system homoskedasticity assumption. By iterated expectations, a su‰cient condition for assumption (14.26) is E½riðyoÞriðyoÞ0 j Zi\u0002 ¼ Wo ð14:28Þ However, assumption (14.26) can hold in cases where assumption (14.28) does not. If assumption (14.26) holds, then Lo can be estimated as ^L ¼ N\u00031 X N i¼1 Z0 i ^WZi ð14:29Þ where ^W ¼ N\u00031 X N i¼1 rið^^y^yÞrið^^y^yÞ ð14:30Þ and ^^y^y is a preliminary estimator. The resulting GMM estimator is usually called the nonlinear 3SLS (N3SLS) estimator. The name is a holdover from the traditional Generalized Method of Moments and Minimum Distance Estimation 427", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 439, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p440::c0", "text": "3SLS estimator in linear systems of equations; there are not really three estimation steps. We should remember that nonlinear 3SLS is generally ine‰cient when as- sumption (14.26) fails. The Wald statistic and the QLR statistic can be computed as in Section 14.1. In addition, a score statistic is sometimes useful. Let ~~y~y be a preliminary ine‰cient esti- mator with Q restrictions imposed. The estimator ~~y~y would usually come from prob- lem (14.24) subject to the restrictions cðyÞ ¼ 0. Let ~L be the estimated weighting matrix from equation (14.25) or (14.29), based on ~~y~y. Let ~y be the minimum chi- square estimator using weighting matrix ~L\u00031. Then the score statistic is based on the limiting distribution of the score of the unrestricted objective function evaluated at the restricted estimates, properly standardized: N\u00031 X N i¼1 Z0 i‘yrið~yÞ \" #0 ~L\u00031 N\u00031=2 X N i¼1 Z0 irið~yÞ \" # ð14:31Þ Let ~si 1 ~G0 ~L\u00031Z0 i~ri, where ~G is the ﬁrst matrix in expression (14.31), and let so i 1 G0 oL\u00031 o Z0 iro i . Then, following the proof in Section 12.6.2, it can be shown that equa- tion (12.67) holds with Ao 1 G0 oL\u00031 o Go. Further, since Bo ¼ Ao for the minimum chi- square estimator, we obtain LM ¼ X N i¼1 ~si !0 ~A\u00031 X N i¼1 ~si ! =N ð14:32Þ where ~A ¼ ~G0 ~L\u00031 ~G. Under H0 and the usual regularity conditions, LM has a limit- ing w2 Q distribution. 14.3 Systems of Nonlinear Equations A leading application of the results in Section 14.2 is to estimation of the parameters in an implicit set of nonlinear equations, such as a nonlinear simultaneous equations model. Partition wi as yi A RJ, xi A RK and, for h ¼ 1; . . . ; G, suppose we have q1ðyi; xi; yo1Þ ¼ ui1 ... qGðyi; xi; yoGÞ ¼ uiG ð14:33Þ where yoh is a Ph \u0001 1 vector of parameters. As an example, write a two-equation SEM in the population as Chapter 14 428", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 440, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p441::c0", "text": "y1 ¼ x1d1 þ g1 yg2 2 þ u1 ð14:34Þ y2 ¼ x2d2 þ g3 y1 þ u2 ð14:35Þ (where we drop ‘‘o’’ to index the parameters). This model, unlike those covered in Section 9.5, is nonlinear in the parameters as well as the endogenous variables. Nev- ertheless, assuming that Eðug j xÞ ¼ 0, g ¼ 1; 2, the parameters in the system can be estimated by GMM by deﬁning q1ðy; x; y1Þ ¼ y1 \u0003 x1d1 \u0003 g1 yg2 2 and q2ðy; x; y2Þ ¼ y2 \u0003 x2d2 \u0003 g3 y1. Generally, the equations (14.33) need not actually determine yi given the exoge- nous variables and disturbances; in fact, nothing requires J ¼ G. Sometimes equations (14.33) represent a system of orthogonality conditions of the form E½qgðy; x; yogÞ j x\u0002 ¼ 0, g ¼ 1; . . . ; G. We will see an example later. Denote the P \u0001 1 vector of all parameters by yo, and the parameter space by Y H RP. To identify the parameters we need the errors uih to satisfy some orthogonality conditions. A general assumption is, for some subvector xih of xi, Eðuih j xihÞ ¼ 0; h ¼ 1; 2; . . . ; G ð14:36Þ This allows elements of xi to be correlated with some errors, a situation that some- times arises in practice (see, for example, Chapter 9 and Wooldridge, 1996). Under assumption (14.36), let zih 1 fhðxihÞ be a 1 \u0001 Lh vector of possibly nonlinear func- tions of xi. If there are no restrictions on the yoh across equations we should have Lh b Ph so that each yoh is identiﬁed. By iterated expectations, for all h ¼ 1; . . . ; G, Eðz0 ihuihÞ ¼ 0 ð14:37Þ provided appropriate moments exist. Therefore, we obtain a set of orthogonality conditions by deﬁning the G \u0001 L matrix Zi as the block diagonal matrix with zig in the gth block: Zi 1 zi1 0 0 \u0004 \u0004 \u0004 0 0 zi2 0 \u0004 \u0004 \u0004 0 .. . .. . 0 0 0 \u0004 \u0004 \u0004 ziG 2 6664 3 7775 ð14:38Þ where L 1 L1 þ L2 þ \u0004 \u0004 \u0004 þ LG. Letting rðwi; yÞ 1 qðyi; xi; yÞ 1 ½qi1ðy1Þ; . . . ; qiGðyGÞ\u00020, equation (14.22) holds under assumption (14.36). When there are no restrictions on the yg across equations and Zi is chosen as in matrix (14.38), the system 2SLS estimator reduces to the nonlinear 2SLS (N2SLS) estimator (Amemiya, 1974) equation by equation. That is, for each h, the N2SLS estimator solves Generalized Method of Moments and Minimum Distance Estimation 429", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 441, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p442::c0", "text": "min yh X N i¼1 z0 ihqihðyhÞ \" #0 N\u00031 X N i¼1 z0 ihzih !\u00031 X N i¼1 z0 ihqihðyhÞ \" # ð14:39Þ Given only the orthogonality conditions (14.37), the N2SLS estimator is the e‰cient estimator of yoh if Eðu2 ihz0 ihzihÞ ¼ s2 ohEðz0 ihzihÞ ð14:40Þ where s2 oh 1 Eðu2 ihÞ; su‰cient for condition (14.40) is Eðu2 ih j xihÞ ¼ s2 oh. Let ^^y^yh denote the N2SLS estimator. Then a consistent estimator of s2 oh is ^s2 h 1 N\u00031 X N i¼1 ^^u^u2 ih ð14:41Þ where ^^u^uih 1 qhðyi; xi; ^^y^yhÞ are the N2SLS residuals. Under assumptions (14.37) and (14.40), the asymptotic variance of ^^y^yh is estimated as ^s2 h X N i¼1 z0 ih‘yhqihð^^y^yhÞ \" #0 X N i¼1 z0 ihzih !\u00031 X N i¼1 z0 ih‘yhqihð^^y^yhÞ \" # 8 < : 9 = ; \u00031 ð14:42Þ where ‘yhqihð^^y^yhÞ is the 1 \u0001 Ph gradient. If assumption (14.37) holds but assumption (14.40) does not, the N2SLS estimator is still ﬃﬃﬃﬃ N p -consistent, but it is not the e‰cient estimator that uses the orthogonality condition (14.37) whenever Lh > Ph [and expression (14.42) is no longer valid]. A more e‰cient estimator is obtained by solving min yh X N i¼1 z0 ihqihðyhÞ \" #0 N\u00031 X N i¼1 ^^u^u2 ihz0 ihzih !\u00031 X N i¼1 z0 ihqihðyhÞ \" # with asymptotic variance estimated as X N i¼1 z0 ih‘yhqihð^^y^yhÞ \" #0 X N i¼1 ^^u^u2 ihz0 ihzih !\u00031 X N i¼1 z0 ih‘yhqihð^^y^yhÞ \" # 8 < : 9 = ; \u00031 This estimator is asymptotically equivalent to the N2SLS estimator if assumption (14.40) happens to hold. Rather than focus on one equation at a time, we can increase e‰ciency if we esti- mate the equations simultaneously. One reason for doing so is to impose cross equation restrictions on the yoh. The system 2SLS estimator can be used for these Chapter 14 430", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 442, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p443::c0", "text": "purposes, where Zi generally has the form (14.38). But this estimator does not exploit correlation in the errors uig and uih in di¤erent equations. The e‰cient estimator that uses all orthogonality conditions in equation (14.37) is just the GMM estimator with ^L given by equation (14.25), where rið^^y^yÞ is the G \u0001 1 vector of system 2SLS residuals, ^^u^ui. In other words, the e‰cient GMM estimator solves min y A Y X N i¼1 Z0 iqiðyÞ \" #0 N\u00031 X N i¼1 Z0 i^^u^ui^^u^u0 iZi !\u00031 X N i¼1 Z0 iqiðyÞ \" # ð14:43Þ The asymptotic variance of ^y is estimated as X N i¼1 Z0 i‘yqið^yÞ \" #0 X N i¼1 Z0 i^^u^ui^^u^u0 iZi !\u00031 X N i¼1 Z0 i‘yqið^yÞ \" # 8 < : 9 = ; \u00031 Because this is the e‰cient GMM estimator, the QLR statistic can be used to test hypotheses about yo. The Wald statistic can also be applied. Under the homoskedasticity assumption (14.26) with riðyoÞ ¼ ui, the nonlinear 3SLS estimator, which solves min y A Y X N i¼1 Z0 iqiðyÞ \" #0 N\u00031 X N i¼1 Z0 i ^WZi !\u00031 X N i¼1 Z0 iqiðyÞ \" # is e‰cient, and its asymptotic variance is estimated as X N i¼1 Z0 i‘yrið^yÞ \" #0 X N i¼1 Z0 i ^WZi !\u00031 X N i¼1 Z0 i‘yrið^yÞ \" # 8 < : 9 = ; \u00031 The N3SLS estimator is used widely for systems of the form (14.33), but, as we dis- cussed in Section 9.6, there are many cases where assumption (14.26) must fail when di¤erent instruments are needed for di¤erent equations. As an example, we show how a hedonic price system ﬁts into this framework. Consider a linear demand and supply system for G attributes of a good or service (see Epple, 1987; Kahn and Lang, 1988; and Wooldridge, 1996). The demand and supply system is written as demandg ¼ h1g þ wa1g þ x1b1g þ u1g; g ¼ 1; . . . ; G supplyg ¼ h2g þ wa2g þ x2b2g þ u2g; g ¼ 1; . . . ; G Generalized Method of Moments and Minimum Distance Estimation 431", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 443, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p444::c0", "text": "where w ¼ ðw1; . . . ; wGÞ is the 1 \u0001 G vector of attribute prices. The demand equations usually represent an individual or household; the supply equations can represent an individual, ﬁrm, or employer. There are several tricky issues in estimating either the demand or supply function for a particular g. First, the attribute prices wg are not directly observed. What is usually observed are the equilibrium quantities for each attribute and each cross section unit i; call these qig, g ¼ 1; . . . ; G. (In the hedonic systems literature these are often denoted zig, but we use qig here because they are endogenous variables, and we have been using zi to denote exogenous variables.) For example, the qig can be fea- tures of a house, such as size, number of bathrooms, and so on. Along with these features we observe the equilibrium price of the good, pi, which we assume follows a quadratic hedonic price function: pi ¼ g þ qic þ qiPq0 i=2 þ xi3d þ xi3Gq0 i þ ui3 ð14:44Þ where xi3 is a vector of variables that a¤ect pi, P is a G \u0001 G symmetric matrix, and G is a G \u0001 G matrix. A key point for identifying the demand and supply functions is that wi ¼ qpi=qqi, which, under equation (14.44), becomes wi ¼ qiP þ xi3G, or wig ¼ qipg þ xi3gg for each g. By substitution, the equilibrium estimating equations can be written as equation (14.44) plus qig ¼ h1g þ ðqiP þ xi3GÞa1g þ xi1b1g þ ui1g; g ¼ 1; . . . ; G ð14:45Þ qig ¼ h2g þ ðqiP þ xi3GÞa2g þ xi2b2g þ ui2g; g ¼ 1; . . . ; G ð14:46Þ These two equations are linear in qi; xi1; xi2, and xi3 but nonlinear in the parameters. Let ui1 be the G \u0001 1 vector of attribute demand disturbances and ui2 the G \u0001 1 vector of attribute supply disturbances. What are reasonable assumptions about ui1; ui2, and ui3? It is almost always assumed that equation (14.44) represents a con- ditional expectation with no important unobserved factors; this assumption means Eðui3 j qi; xiÞ ¼ 0, where xi contains all elements in xi1; xi2, and xi3. The properties of ui1 and ui2 are more subtle. It is clear that these cannot be uncorrelated with qi, and so equations (14.45) and (14.46) contain endogenous explanatory variables if P 0 0. But there is another problem, pointed out by Bartik (1987), Epple (1987), and Kahn and Lang (1988): because of matching that happens between individual buyers and sellers, xi2 is correlated with ui1, and xi1 is correlated with ui2. Consequently, what would seem to be the obvious IVs for the demand equations (14.45)—the factors shifting the supply curve—are endogenous to equation (14.45). Fortunately, all is not lost: if xi3 contains exogenous factors that a¤ect pi but do not appear in the struc- Chapter 14 432", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 444, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p445::c0", "text": "tural demand and supply functions, we can use these as instruments in both the de- mand and supply equations. Speciﬁcally, we assume Eðui1 j xi1; xi3Þ ¼ 0; Eðui2 j xi2; xi3Þ ¼ 0; Eðui3 j qi; xiÞ ¼ 0 ð14:47Þ Common choices for xi3 are geographical or industry dummy indicators (for exam- ple, Montgomery, Shaw, and Benedict, 1992; Hagy, 1998), where the assumption is that the demand and supply functions do not change across region or industry but the type of matching does, and therefore pi can di¤er systematically across region or in- dustry. Bartik (1987) discusses how a randomized experiment can be used to create the elements of xi3. For concreteness, let us focus on estimating the set of demand functions. If P ¼ 0, so that the quadratic in qi does not appear in equation (14.44), a simple two-step procedure is available: (1) estimate equation (14.44) by OLS, and obtain ^wig ¼ ^cg þ xi3^gg for each i and g; (2) run the regression qig on 1, ^wi; xi1; i ¼ 1; . . . ; N. Under assumptions (14.47) and identiﬁcation assumptions, this method produces ﬃﬃﬃﬃ N p - consistent, asymptotically normal estimators of the parameters in demand equation g. Because the second regression involves generated regressors, the standard errors and test statistics should be adjusted. It is clear that, without restrictions on a1g, the order condition necessary for iden- tifying the demand parameters is that the dimension of xi3, say K3, must exceed G. If K3 < G then E½ðwi; xi1Þ0ðwi; xi1Þ\u0002 has less than full rank, and the OLS rank condition fails. If we make exclusion restrictions on a1g, fewer elements are needed in xi3. In the case that only wig appears in the demand equation for attribute g, xi3 can be a scalar, provided its interaction with qig in the hedonic price system is signiﬁcant ðggg 0 0Þ. Checking the analogue of the rank condition in general is somewhat complicated; see Epple (1987) for discussion. When wi ¼ qiP þ xi3G, wi is correlated with ui1g, so we must modify the two-step procedure. In the second step, we can use instruments for ^wi and perform 2SLS rather than OLS. Assuming that xi3 has enough elements, the demand equations are still identiﬁed. If only wig appears in demandig, su‰cient for identiﬁcation is that an ele- ment of xi3 appears in the linear projection of wig on xi1, xi3. This assumption can hold even if xi3 has only a single element. For the matching reasons we discussed previously, xi2 cannot be used as instruments for ^wi in the demand equation. Whether P ¼ 0 or not, more e‰cient estimators are obtained from the full demand system and the hedonic price function. Write q0 i ¼ h1 þ ðqiP þ xi3GÞA1 þ xi1B1 þ ui1 Generalized Method of Moments and Minimum Distance Estimation 433", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 445, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p446::c0", "text": "along with equation (14.44). Then ðxi1; xi3Þ (and functions of these) can be used as instruments in any of the G demand equations, and ðqi; xiÞ act as IVs in equation (14.44). (It may be that the supply function is not even speciﬁed, in which case xi contains only xi1 and xi3.) A ﬁrst-stage estimator is the nonlinear system 2SLS esti- mator. Then the system can be estimated by the minimum chi-square estimator that solves problem (14.43). When restricting attention to demand equations plus the hedonic price equation, or supply equations plus the hedonic price equation, nonlinear 3SLS is e‰cient under certain assumptions. If the demand and supply equations are estimated together, the key assumption (14.26) that makes nonlinear 3SLS asymp- totically e‰cient cannot be expected to hold; see Wooldridge (1996) for discussion. If one of the demand functions is of primary interest, it may make sense to estimate it along with equation (14.44), by GMM or nonlinear 3SLS. If the demand functions are written in inverse form, the resulting system is linear in the parameters, as shown in Wooldridge (1996). 14.4 Panel Data Applications As we saw in Chapter 11, system IV methods are needed in certain panel data con- texts. In the current case, our interest is in nonlinear panel data models that cannot be estimated using linear methods. We hold o¤ on discussing nonlinear panel data models explicitly containing unobserved e¤ects until Part IV. One increasingly popular use of panel data is to test rationality in economic models of individual, family, or ﬁrm behavior (see, for example, Shapiro, 1984; Zeldes, 1989; Keane and Runkle, 1992; Shea, 1995). For a random draw from the population we assume that T time periods are available. Suppose that an economic theory implies that E½rtðwt; yoÞ j wt\u00031; . . . ; w1Þ ¼ 0; t ¼ 1; . . . ; T ð14:48Þ where, for simplicity, rt is a scalar. These conditional moment restrictions are often implied by rational expectations, under the assumption that the decision horizon is the same length as the sampling period. For example, consider a standard life-cycle model of consumption. Let cit denote consumption of family i at time t, let hit denote taste shifters, let do denote the common rate of time preference, and let a j it denote the return for family i from holding asset j from period t \u0003 1 to t. Under the assumption that utility is given by uðcit; yitÞ ¼ expðhitboÞc1\u0003lo it =ð1 \u0003 loÞ ð14:49Þ the Euler equation is Chapter 14 434", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 446, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p447::c0", "text": "E½ð1 þ a j itÞðcit=ci;t\u00031Þ\u0003lo j Ii;t\u00031\u0002 ¼ ð1 þ doÞ\u00031 expðxitboÞ ð14:50Þ where Iit is family i’s information set at time t and xit 1 hi;t\u00031 \u0003 hit; equation (14.50) assumes that hit \u0003 hi;t\u00031 A Ii;t\u00031, an assumption which is often reasonable. Given equation (14.50), we can deﬁne a residual function for each t: ritðyÞ ¼ ð1 þ a j itÞðcit=ci;t\u00031Þ\u0003l \u0003 expðxitbÞ ð14:51Þ where ð1 þ dÞ\u00031 is absorbed in an intercept in xit. Let wit contain cit, ci;t\u00031, ait, and xit. Then condition (14.48) holds, and lo and bo can be estimated by GMM. Returning to condition (14.48), valid instruments at time t are functions of infor- mation known at time t \u0003 1: zt ¼ ftðwt\u00031; . . . ; w1Þ ð14:52Þ The T \u0001 1 residual vector is rðw; yÞ ¼ ½r1ðw1; yÞ; . . . ; rTðwT; yÞ\u00020, and the matrix of instruments has the same form as matrix (14.38) for each i (with G ¼ T). Then, the minimum chi-square estimator can be obtained after using the system 2SLS estima- tor, although the choice of instruments is a nontrivial matter. A common choice is linear and quadratic functions of variables lagged one or two time periods. Estimation of the optimal weighting matrix is somewhat simpliﬁed under the con- ditional moment restrictions (14.48). Recall from Section 14.2 that the optimal esti- mator uses the inverse of a consistent estimator of Lo ¼ E½Z0 iriðyoÞriðyoÞ0Zi\u0002. Under condition (14.48), this matrix is block diagonal. Dropping the i subscript, the ðs; tÞ block is E½rsðyoÞrtðyoÞz0 szt\u0002. For concreteness, assume that s < t. Then zt; zs, and rsðyoÞ are all functions of wt\u00031; wt\u00032; . . . ; w1. By iterated expectations it follows that E½rsðyoÞrtðyoÞz0 szt\u0002 ¼ EfrsðyoÞz0 sztE½rtðyoÞ j wt\u00031; . . . ; w1\u0002g ¼ 0 and so we only need to estimate the diagonal blocks of E½Z0 iriðyoÞriðyoÞ0Zi\u0002: N\u00031 X N i¼1 ^^r^r2 it z0 itzit ð14:53Þ is a consistent estimator of the tth block, where the ^^r^rit are obtained from an ine‰cient GMM estimator. In cases where the data frequency does not match the horizon relevant for decision making, the optimal matrix does not have the block diagonal form: some o¤-diagonal blocks will be nonzero. See Hansen (1982) for the pure time series case. Ahn and Schmidt (1995) apply nonlinear GMM methods to estimate the linear, unobserved e¤ects AR(1) model. Some of the orthogonality restrictions they use are nonlinear in the parameters of interest. In Part IV we will cover nonlinear panel data Generalized Method of Moments and Minimum Distance Estimation 435", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 447, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p448::c0", "text": "models with unobserved e¤ects. For the consumption example, we would like to allow for a family-speciﬁc rate of time preference, as well as unobserved family tastes. Orthogonality conditions can often be obtained in such cases, but they are not as straightforward to obtain as in the previous example. 14.5 E‰cient Estimation In Chapter 8 we obtained the e‰cient weighting matrix for GMM estimation of linear models, and we extended that to nonlinear models in Section 14.1. In Chapter 13 we asserted that maximum likelihood estimation has some important e‰ciency properties. We are now in a position to study a framework that allows us to show the e‰ciency of an estimator within a particular class of estimators, and also to ﬁnd e‰cient estimators within a stated class. Our approach is essentially that in Newey and McFadden (1994, Section 5.3), although we will not use the weakest possible assumptions. Bates and White (1993) proposed a very similar framework and also considered time series problems. 14.5.1 A General E‰ciency Framework Most estimators in econometrics—and all of the ones we have studied—are ﬃﬃﬃﬃ N p - asymptotically normal, with variance matrices of the form V ¼ A\u00031E½sðwÞsðwÞ0\u0002ðA0Þ\u00031 ð14:54Þ where, in most cases, sðwÞ is the score of an objective function (evaluated at yo) and A is the expected value of the Jacobian of the score, again evaluated at yo. (We suppress an ‘‘o’’ subscript here, as the value of the true parameter is irrelevant.) All M-estimators with twice continuously di¤erentiable objective functions (and even some without) have variance matrices of this form, as do GMM estimators. The fol- lowing lemma is a useful su‰cient condition for showing that one estimator is more e‰cient than another. lemma 14.1 (Relative E‰ciency): Let ^y1 and ^y2 be two ﬃﬃﬃﬃ N p -asymptotically normal estimators of the P \u0001 1 parameter vector yo, with asymptotic variances of the form (14.54) (with appropriate subscripts on A, s, and V). If for some r > 0, E½s1ðwÞs1ðwÞ0\u0002 ¼ rA1 ð14:55Þ E½s2ðwÞs1ðwÞ0\u0002 ¼ rA2 ð14:56Þ then V2 \u0003 V1 is positive semideﬁnite. Chapter 14 436", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 448, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p449::c0", "text": "The proof of Lemma 14.1 is given in the chapter appendix. Condition (14.55) is essentially the generalized information matrix equality (GIME) we introduced in Section 12.5.1 for the estimator ^y1. Notice that A1 is necessarily symmetric and positive deﬁnite under condition (14.55). Condition (14.56) is new. In most cases, it says that the expected outer product of the scores s2 and s1 equals the expected Jacobian of s2 (evaluated at yo). In Section 12.5.1 we claimed that the GIME plays a role in e‰ciency, and Lemma 14.1 shows that it does so. Verifying the conditions of Lemma 14.1 is also very convenient for constructing simple forms of the Hausman (1978) statistic in a variety of contexts. Provided that the two estimators are jointly asymptotically normally distributed—something that is almost always true when each is ﬃﬃﬃﬃ N p -asymptotically normal, and that can be veriﬁed by stacking the ﬁrst-order representations of the estimators—assumptions (14.55) and (14.56) imply that the asymptotic covariance between ﬃﬃﬃﬃ N p ð^y2 \u0003 yoÞ and ﬃﬃﬃﬃ N p ð^y1 \u0003 yoÞ is A\u00031 2 Eðs2s0 1ÞA\u00031 1 ¼ A\u00031 2 ðrA2ÞA\u00031 1 ¼ rA\u00031 1 ¼ Avar½ ﬃﬃﬃﬃ N p ð^y1 \u0003 yoÞ\u0002. In other words, the asymptotic covariance between the ( ﬃﬃﬃﬃ N p -scaled) estimators is equal to the asymptotic variance of the e‰cient estimator. This equality implies that Avar½ ﬃﬃﬃﬃ N p ð^y2 \u0003 ^y1Þ\u0002 ¼ V2 þ V1 \u0003 C \u0003 C0 ¼ V2 þ V1 \u0003 2V1 ¼ V2 \u0003 V1, where C is the asymptotic covariance. If V2 \u0003 V1 is actually positive deﬁnite (rather than just positive semideﬁnite), then ½ ﬃﬃﬃﬃ N p ð^y2 \u0003 ^y1Þ\u00020ð^V2 \u0003 ^V1Þ\u00031½ ﬃﬃﬃﬃ N p ð^y2 \u0003 ^y1Þ\u0002 @ a w2 P under the assumptions of Lemma 14.1, where ^Vg is a consistent estimator of Vg, g ¼ 1; 2. Statistically signiﬁcant di¤er- ences between ^y2 and ^y1 signal some sort of model misspeciﬁcation. (See Section 6.2.1, where we discussed this form of the Hausman test for comparing 2SLS and OLS to test whether the explanatory variables are exogenous.) If assumptions (14.55) and (14.56) do not hold, this standard form of the Hausman statistic is invalid. Given Lemma 14.1, we can state a condition that implies e‰ciency of an estimator in an entire class of estimators. It is useful to be somewhat formal in deﬁning the relevant class of estimators. We do so by introducing an index, t. For each t in an index set, say, T, the estimator ^yt has an associated st and At such that the asymp- totic variance of ﬃﬃﬃﬃ N p ð^yt \u0003 yoÞ has the form (14.54). The index can be very abstract; it simply serves to distinguish di¤erent ﬃﬃﬃﬃ N p -asymptotically normal estimators of yo. For example, in the class of M-estimators, the set T consists of objective functions qð\u0004 ; \u0004Þ such that yo uniquely minimizes E½qðw; yÞ\u0002 over Y, and q satisﬁes the twice con- tinuously di¤erentiable and bounded moment assumptions imposed for asymptotic normality. For GMM with given moment conditions, T is the set of all L \u0001 L posi- tive deﬁnite matrices. We will see another example in Section 14.5.3. Lemma 14.1 immediately implies the following theorem. theorem 14.3 (E‰ciency in a Class of Estimators): Let f^yt: t A Tg be a class of ﬃﬃﬃﬃ N p -asymptotically normal estimators with variance matrices of the form (14.54). If Generalized Method of Moments and Minimum Distance Estimation 437", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 449, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p450::c0", "text": "for some t\u0005 A T and r > 0 E½stðwÞst \u0005ðwÞ0\u0002 ¼ rAt; all t A T ð14:57Þ then ^yt \u0005 is asymptotically relatively e‰cient in the class f^yt: t A Tg. This theorem has many applications. If we specify a class of estimators by deﬁning the index set T, then the estimator ^yt \u0005 is more e‰cient than all other estimators in the class if we can show condition (14.57). [A partial converse to Theorem 14.3 also holds; see Newey and McFadden (1994, Section 5.3).] This is not to say that ^yt \u0005 is necessarily more e‰cient than all possible ﬃﬃﬃﬃ N p -asymptotically normal estimators. If there is an estimator that falls outside of the speciﬁed class, then Theorem 14.3 does not help us to compare it with ^yt \u0005. In this sense, Theorem 14.3 is a more general (and asymptotic) version of the Gauss-Markov theorem from linear regression analysis: while the Gauss-Markov theorem states that OLS has the smallest variance in the class of linear, unbiased estimators, it does not allow us to compare OLS to unbiased estimators that are not linear in the vector of observations on the dependent variable. 14.5.2 E‰ciency of MLE Students of econometrics are often told that the maximum likelihood estimator is ‘‘e‰cient.’’ Unfortunately, in the context of conditional MLE from Chapter 13, the statement of e‰ciency is usually ambiguous; Manski (1988, Chapter 8) is a notable exception. Theorem 14.3 allows us to state precisely the class of estimators in which the conditional MLE is relatively e‰cient. As in Chapter 13, we let Eyð\u0004 j xÞ denote the expectation with respect to the conditional density f ðy j x; yÞ. Consider the class of estimators solving the ﬁrst-order condition N\u00031 X N i¼1 gðwi; ^yÞ 1 0 ð14:58Þ where the P \u0001 1 function gðw; yÞ such that Ey½gðw; yÞ j x\u0002 ¼ 0; all x A X; all y A Y ð14:59Þ In other words, the class of estimators is indexed by functions g satisfying a zero conditional moment restriction. We assume the standard regularity conditions from Chapter 12; in particular, gðw; \u0004Þ is continuously di¤erentiably on the interior of Y. As we showed in Section 13.7, functions g satisfying condition (14.59) generally have the property \u0003E½‘ygðw; yoÞ j x\u0002 ¼ E½gðw; yoÞsðw; yoÞ0 j x\u0002 Chapter 14 438", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 450, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p451::c0", "text": "where sðw; yÞ is the score of log f ðy j x; yÞ (as always, we must impose certain regu- larity conditons on g and log f ). If we take the expectation of both sides with respect to x, we obtain condition (14.57) with r ¼ 1, At ¼ E½‘ygðw; yoÞ\u0002, and st \u0005ðwÞ ¼ \u0003sðw; yoÞ. It follows from Theorem 14.3 that the conditional MLE is e‰cient in the class of estimators solving equation (14.58), where gð\u0004Þ satisﬁes condition (14.59) and appropriate regularity conditions. Recall from Section 13.5.1 that the asymp- totic variance of the (centered and standardized) CMLE is fE½sðw; yoÞsðw; yoÞ0\u0002g\u00031. This is an example of an e‰ciency bound because no estimator of the form (14.58) under condition (14.59) can have an asymptotic variance smaller than fE½sðw; yoÞsðw; yoÞ0\u0002g\u00031 (in the matrix sense). When an estimator from this class has the same asymptotic variance as the CMLE, we way it achieves the e‰ciency bound. It is important to see that the e‰ciency of the conditional MLE in the class of estimators solving equation (14.58) under condition (14.59) does not require x to be ancillary for yo: except for regularity conditions, the distribution of x is essentially unrestricted, and could depend on yo. Conditional MLE simply ignores information on yo that might be contained in the distribution of x, but so do all other estimators that are based on condition (14.59). By choosing x to be empty, we conclude that the unconditional MLE is e‰cient in the class of estimators based on equation (14.58) with Ey½gðw; yÞ\u0002 ¼ 0, all y A Y. This is a very broad class of estimators, including all of the estimators requiring condition (14.59): if a function g satisﬁes condition (14.59), it has zero unconditional mean, too. Consequently, the unconditional MLE is generally more e‰cient than the condi- tional MLE. This e‰ciency comes at the price of having to model the joint density of ðy; xÞ, rather than just the conditional density of y given x. And, if our model for the density of x is incorrect, the unconditional MLE generally would be inconsistent. When is CMLE as e‰cient as unconditional MLE for estimating yo? Assume that the model for the joint density of ðx; yÞ can be expressed as f ðy j x; yÞhðx; dÞ, where y is the parameter vector of interest, and hðx; doÞ is the marginal density of x for some vector do. Then, if d does not depend on y in the sense that ‘yhðx; dÞ ¼ 0 for all x and d, x is ancillary for yo. In fact, the CMLE is identical to the unconditional MLE. If d depends on y, the term ‘y log½hðx; dÞ\u0002 generally contains information for estimating yo, and unconditional MLE will be more e‰cient than CMLE. 14.5.3 E‰cient Choice of Instruments under Conditional Moment Restrictions We can also apply Theorem 14.3 to ﬁnd the optimal set of instrumental variables under general conditional moment restrictions. For a G \u0001 1 vector rðwi; yÞ, where wi A RM, yo is said to satisfy conditional moment restrictions if E½rðwi; yoÞ j xi\u0002 ¼ 0 ð14:60Þ Generalized Method of Moments and Minimum Distance Estimation 439", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 451, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p452::c0", "text": "where xi A RK is a subvector of wi. Under assumption (14.60), the matrix Zi appearing in equation (14.22) can be any function of xi. For a given matrix Zi, we obtain the e‰cient GMM estimator by using the e‰cient weighting matrix. However, unless Zi is the optimal set of instruments, we can generally obtain a more e‰cient estimator by adding any nonlinear function of xi to Zi. Because the list of potential IVs is endless, it is useful to characterize the optimal choice of Zi. The solution to this problem is now pretty well known, and it can be obtained by applying Theorem 14.3. Let WoðxiÞ 1 Var½rðwi; yoÞ j xi\u0002 ð14:61Þ be the G \u0001 G conditional variance of riðyoÞ given xi, and deﬁne RoðxiÞ 1 E½‘yrðwi; yoÞ j xi\u0002 ð14:62Þ Problem 14.3 asks you to verify that the optimal choice of instruments is Z\u0005ðxiÞ 1 WoðxiÞ\u00031RoðxiÞ ð14:63Þ The optimal instrument matrix is always G \u0001 P, and so the e‰cient method of moments estimator solves X N i¼1 Z\u0005ðxiÞ0rið^yÞ ¼ 0 There is no need to use a weighting matrix. Incidentally, by taking gðw; yÞ 1 Z\u0005ðxÞ0rðw; yÞ, we obtain a function g satisfying condition (14.59). From our discus- sion in Section 14.5.2, it follows immediately that the conditional MLE is no less e‰cient than the optimal IV estimator. In practice, Z\u0005ðxiÞ is never a known function of xi. In some cases the function RoðxiÞ is a known function of xi and yo and can be easily estimated; this statement is true of linear SEMs under conditional mean assumptions (see Chapters 8 and 9) and of multivariate nonlinear regression, which we cover later in this subsection. Rarely do moment conditions imply a parametric form for WoðxiÞ, but sometimes homo- skedasticity is assumed: E½riðyoÞriðyoÞ j xi\u0002 ¼ Wo ð14:64Þ and Wo is easily estimated as in equation (14.30) given a preliminary estimate of yo. Since both WoðxiÞ and RoðxiÞ must be estimated, we must know the asymptotic properties of GMM with generated instruments. Under conditional moment restric- tions, generated instruments have no e¤ect on the asymptotic variance of the GMM estimator. Thus, if the matrix of instruments is Zðxi; goÞ for some unknown parame- Chapter 14 440", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 452, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p453::c0", "text": "ter vector go, and ^g is an estimator such that ﬃﬃﬃﬃ N p ð^g \u0003 goÞ ¼ Opð1Þ, then the GMM estimator using the generated instruments ^Zi 1 Zðxi; ^gÞ has the same limiting dis- tribution as the GMM estimator using instruments Zðxi; goÞ (using any weighting matrix). This result follows from a mean value expansion, using the fact that the de- rivative of each element of Zðxi; gÞ with respect to g is orthogonal to riðyoÞ under condition (14.60): N\u00031=2 X N i¼1 ^Z0 irið^yÞ ¼ N\u00031=2 X N i¼1 ZiðgoÞ0riðyoÞ þ E½ZiðgoÞ0RoðxiÞ\u0002 ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ þ opð1Þ ð14:65Þ The right-hand side of equation (14.65) is identical to the expansion with ^Zi replaced with ZiðgoÞ. Assuming now that ZiðgoÞ is the matrix of e‰cient instruments, the asymptotic variance of the e‰cient estimator is Avar ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ ¼ fE½RoðxiÞ0WoðxiÞ\u00031RoðxiÞ\u0002g\u00031 ð14:66Þ as can be seen from Section 14.1 by noting that Go ¼ E½RoðxiÞ0WoðxiÞ\u00031RoðxiÞ\u0002 and Lo ¼ G\u00031 o when the instruments are given by equation (14.63). Equation (14.66) is another example of an e‰ciency bound, this time under the conditional moment restrictions (14.54). What we have shown is that any GMM es- timator has variance matrix that di¤ers from equation (14.66) by a positive semi- deﬁnite matrix. Chamberlain (1987) has shown more: any estimator that uses only condition (14.60) and satisﬁes regularity conditions has variance matrix no smaller than equation (14.66). Estimation of RoðxiÞ generally requires nonparametric methods. Newey (1990) describes one approach. Essentially, regress the elements of ‘yrið^^y^yÞ on polynomial functions of xi (or other functions with good approximating properties), where ^^y^y is an initial estimate of yo. The ﬁtted values from these regressions can be used as the elements of ^Ri. Other nonparametric approaches are available. See Newey (1990, 1993) for details. Unfortunately, we need a fairly large sample size in order to apply such methods e¤ectively. As an example of ﬁnding the optimal instruments, consider the problem of esti- mating a conditional mean for a vector yi: Eðyi j xiÞ ¼ mðxi; yoÞ ð14:67Þ Then the residual function is rðwi; yÞ 1 yi \u0003 mðxi; yÞ and WoðxiÞ ¼ Varðyi j xiÞ; therefore, the optimal instruments are ZoðxiÞ 1 WoðxiÞ\u00031‘ymðxi; yoÞ. This is an im- Generalized Method of Moments and Minimum Distance Estimation 441", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 453, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p454::c0", "text": "portant example where RoðxiÞ ¼ \u0003‘ymðxi; yoÞ is a known function of xi and yo. If the homoskedasticity assumption Varðyi j xiÞ ¼ Wo ð14:68Þ holds, then the e‰cient estimator is easy to obtain. First, let ^^y^y be the multivariate nonlinear least squares (MNLS) estimator, which solves miny A Y PN i¼1½yi \u0003mðxi; yÞ\u00020 \u0004 ½yi \u0003 mðxi; yÞ\u0002. As discussed in Problem 12.11, the MNLS estimator is generally consistent and ﬃﬃﬃﬃ N p -asymptotic normal. Deﬁne the residuals ^^u^ui 1 yi \u0003 mðxi; ^^y^yÞ, and deﬁne a consistent estimator of Wo by ^W ¼ N\u00031 PN i¼1 ^^u^ui^^u^u0 i. An e‰cient estimator, ^y, solves X N i¼1 ‘ymðxi; ^^y^yÞ0 ^W\u00031½yi \u0003 mðxi; ^yÞ\u0002 ¼ 0 and the asymptotic variance of ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ is fE½‘ymiðyoÞ0W\u00031 o ‘ymiðyoÞ\u0002g\u00031. An asymptotically equivalent estimator is the nonlinear SUR estimator described in Problem 12.7. In either case, the estimator of Avarð^yÞ under assumption (14.68) is Av^arð^yÞ ¼ X N i¼1 ‘ymið^yÞ0 ^W\u00031‘ymið^yÞ \" #\u00031 Because the nonlinear SUR estimator is a two-step M-estimator and Bo ¼ Ao (in the notation of Chapter 12), the simplest forms of tests statistics are valid. If assumption (14.68) fails, the nonlinear SUR estimator is consistent, but robust inference should be used because Ao 0 Bo. And, the estimator is no longer e‰cient. 14.6 Classical Minimum Distance Estimation We end this chapter with a brief treatment of classical minimum distance (CMD) estimation. This method has features in common with GMM, and often it is a con- venient substitute for GMM. Suppose that the P \u0001 1 parameter vector of interest, yo, which often consists of parameters from a structural model, is known to be related to an S \u0001 1 vector of reduced form parameters, po, where S > P. In particular, po ¼ hðyoÞ for a known, continuously di¤erentiable function h: RP ! RS, so that h maps the structural parameters into the reduced form parameters. CMD estimation of yo entails ﬁrst estimating po by ^p, and then choosing an esti- mator ^y of yo by making the distance between ^p and hð^yÞ as small as possible. As with GMM estimation, we use a weighted Euclidean measure of distance. While a Chapter 14 442", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 454, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p455::c0", "text": "CMD estimator can be deﬁned for any positive semideﬁnite weighting matrix, we consider only the e‰cient CMD estimator given our choice of ^p. As with e‰cient GMM, the CMD estimator that uses the e‰cient weighting matrix is also called the minimum chi-square estimator. Assuming that for an S \u0001 S positive deﬁnite matrix Xo ﬃﬃﬃﬃ N p ð^p \u0003 poÞ @ a Normalð0; XoÞ ð14:69Þ it turns out that an e‰cient CMD estimator solves min y A Yf^p \u0003 hðyÞg0^X\u00031f^p \u0003 hðyÞg ð14:70Þ where plimN!y ^X ¼ Xo. In other words, an e‰cient weighting matrix is the inverse of any consistent estimator of Avar ﬃﬃﬃﬃ N p ð^p \u0003 poÞ. We can easily derive the asymptotic variance of ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ. The ﬁrst-order con- dition for ^y is Hð^yÞ0^X\u00031f^p \u0003 hð^yÞg 1 0 ð14:71Þ where HðyÞ 1 ‘yhðyÞ is the S \u0001 P Jacobian of hðyÞ. Since hðyoÞ ¼ po and ﬃﬃﬃﬃ N p fhð^yÞ \u0003 hðyoÞg ¼ HðyoÞ ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ þ opð1Þ by a standard mean value expansion about yo, we have 0 ¼ Hð^yÞ0^X\u00031f ﬃﬃﬃﬃ N p ð^p \u0003 poÞ \u0003 HðyoÞ ﬃﬃﬃﬃ N p ð^y \u0003 yoÞg þ opð1Þ ð14:72Þ Because Hð\u0004Þ is continuous and ^y ! p yo, Hð^yÞ ¼ HðyoÞ þ opð1Þ; by assumption ^X ¼ Xo þ opð1Þ. Therefore, HðyoÞ0X\u00031 o HðyoÞ ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ ¼ HðyoÞ0X\u00031 o ﬃﬃﬃﬃ N p ð^p \u0003 poÞ þ opð1Þ By assumption (14.69) and the asymptotic equivalence lemma, HðyoÞ0X\u00031 o HðyoÞ ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ @ a Normal½0; HðyoÞ0X\u00031 o HðyoÞ\u0002 and so ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ @ a Normal½0; ðH0 oX\u00031 o HoÞ\u00031\u0002 ð14:73Þ provided that Ho 1 HðyoÞ has full-column rank P, as will generally be the case when yo is identiﬁed and hð\u0004Þ contains no redundancies. The appropriate estimator of Av^arð^yÞ is Av^arð^yÞ 1 ð ^H0^X\u00031 ^HÞ\u00031=N ¼ ð ^H0½Av^arð^pÞ\u0002\u00031 ^HÞ\u00031 ð14:74Þ Generalized Method of Moments and Minimum Distance Estimation 443", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 455, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p456::c0", "text": "The proof that ^X\u00031 is the optimal weighting matrix in expression (14.70) is very similar to the derivation of the optimal weighting matrix for GMM. (It can also be shown by applying Theorem 14.3.) We will simply call the e‰cient estimator the CMD estimator, where it is understood that we are using the e‰cient weighting matrix. There is another e‰ciency issue that arises when more than one ﬃﬃﬃﬃ N p -asymptotically normal estimator for po is available: Which estimator of po should be used? Let ^y be the estimator based on ^p, and let ~y be the estimator based on another estimator, ~p. You are asked to show in Problem 14.6 that Avar ﬃﬃﬃﬃ N p ð~y \u0003 yoÞ \u0003 Avar ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ is p.s.d. whenever Avar ﬃﬃﬃﬃ N p ð~p \u0003 poÞ \u0003 Avar ﬃﬃﬃﬃ N p ð^p \u0003 poÞ is p.s.d. In other words, we should use the most e‰cient estimator of po to obtain the most e‰cient estimator of yo. A test of overidentifying restrictions is immediately available after estimation, be- cause, under the null hypothesis po ¼ hðyoÞ, N½^p \u0003 hð^yÞ\u00020^X\u00031½^p \u0003 hð^yÞ\u0002 @ a w2 S\u0003P ð14:75Þ To show this result, we use ﬃﬃﬃﬃ N p ½^p \u0003 hð^yÞ\u0002 ¼ ﬃﬃﬃﬃ N p ð^p \u0003 poÞ \u0003 Ho ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ þ opð1Þ ¼ ﬃﬃﬃﬃ N p ð^p \u0003 poÞ \u0003 HoðH0 oX\u00031 o HoÞ\u00031H0 oX\u00031 o ﬃﬃﬃﬃ N p ð^p \u0003 poÞ þ opð1Þ ¼ ½IS \u0003 HoðH0 oX\u00031 o HoÞ\u00031H0 oX\u00031 o \u0002 ﬃﬃﬃﬃ N p ð^p \u0003 poÞ þ opð1Þ Therefore, up to opð1Þ, X\u00031=2 o ﬃﬃﬃﬃ N p f^p \u0003 hð^yÞg ¼ ½IS \u0003 X\u00031=2 o HoðH0 oX\u00031 o HoÞ\u00031H0 oX\u00031=2 o \u0002Z 1 MoZ where Z 1 X\u00031=2 o ﬃﬃﬃﬃ N p ð^p \u0003 poÞ ! d Normalð0; ISÞ. But Mo is a symmetric idempotent matrix with rank S \u0003 P, so f ﬃﬃﬃﬃ N p ½^p \u0003 hð^yÞ\u0002g0X\u00031 o f ﬃﬃﬃﬃ N p ½^p \u0003 hð^yÞ\u0002g @ a w2 S\u0003P. Because ^X is consistent for Xo, expression (14.75) follows from the asymptotic equivalence lemma. The statistic can also be expressed as f^p \u0003 hð^yÞg0½Av^arð^pÞ\u0002\u00031f^p \u0003 hð^yÞg ð14:76Þ Testing restrictions on yo is also straightforward, assuming that we can express the restrictions as yo ¼ dðaoÞ for an R \u0001 1 vector ao, R < P. Under these restrictions, po ¼ h½dðaoÞ\u0002 1 gðaoÞ. Thus, ao can be estimated by minimum distance by solving problem (14.70) with a in place of y and gðaÞ in place of hðyÞ. The same estimator ^X should be used in both minimization problems. Then it can be shown (under interi- ority and di¤erentiability) that Chapter 14 444", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 456, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p457::c0", "text": "N½^p \u0003 gð^aÞ\u00020^X\u00031½^p \u0003 gð^aÞ\u0002 \u0003 N½^p \u0003 hð^yÞ\u00020^X\u00031½^p \u0003 hð^yÞ\u0002 @ a w2 P\u0003R ð14:77Þ when the restrictions on yo are true. To illustrate the application of CMD estimation, we reconsider Chamberlain’s (1982, 1984) approach to linear, unobserved e¤ects panel data models. (See Section 11.3.2 for the GMM approach.) The key equations are yit ¼ c þ xi1l1 þ \u0004 \u0004 \u0004 þ xitðb þ ltÞ þ \u0004 \u0004 \u0004 þ xiTlT þ vit ð14:78Þ where EðvitÞ ¼ 0; Eðx0 ivitÞ ¼ 0; t ¼ 1; 2; . . . ; T ð14:79Þ (For notational simplicity we do not index the true parameters by ‘‘o’’.) Equation (14.78) embodies the restrictions on the ‘‘structural’’ parameters y 1 ðc; l0 1; . . . ; l0 T; b 0Þ0, a ð1 þ TK þ KÞ \u0001 1 vector. To apply CMD, write yit ¼ pt0 þ xipt þ vit; t ¼ 1; . . . ; T so that the vector p is Tð1 þ TKÞ \u0001 1. When we impose the restrictions, pt0 ¼ c, pt ¼ ½l0 1; l0 2; . . . ; ðb þ ltÞ0; . . . ; l0 T\u00020; t ¼ 1; . . . ; T Therefore, we can write p ¼ Hy for a ðT þ T 2KÞ \u0001 ð1 þ TK þ KÞ matrix H. When T ¼ 2, p can be written with restrictions imposed as p ¼ ðc; b 0 þ l0 1; l0 2; c; l0 1; b 0 þ l0 2Þ0, and so H ¼ 1 0 0 0 0 IK 0 IK 0 0 IK 0 1 0 0 0 0 IK 0 0 0 0 IK IK 2 666666664 3 777777775 The CMD estimator can be obtained in closed form, once we have ^p; see Problem 14.7 for the general case. How should we obtain ^p, the vector of estimates without the restrictions imposed? There is really only one way, and that is OLS for each time period. Condition (14.79) ensures that OLS is consistent and ﬃﬃﬃﬃ N p -asymptotically normal. Why not use a system method, in particular, SUR? For one thing, we cannot generally assume that vi sat- isﬁes the requisite homoskedasticity assumption that ensures that SUR is more e‰- cient than OLS equation by equation; see Section 11.3.2. Anyway, because the same Generalized Method of Moments and Minimum Distance Estimation 445", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 457, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p458::c0", "text": "regressors appear in each equation and no restrictions are imposed on the pt, OLS and SUR are identical. Procedures that might use nonlinear functions of xi as instruments are not allowed under condition (14.79). The estimator ^X of Avar ﬃﬃﬃﬃ N p ð^p \u0003 pÞ is the robust asymptotic variance for system OLS from Chapter 7: ^X 1 N\u00031 X N i¼1 X0 iXi !\u00031 N\u00031 X N i¼1 X0 i^vi^v0 i Xi ! N\u00031 X N i¼1 X0 iXi !\u00031 ð14:80Þ where Xi ¼ IT n ð1; xiÞ is T \u0001 ðT þ T 2KÞ and ^vi is the vector of OLS residuals; see also equation (7.26). Given the linear model with an additive unobserved e¤ect, the overidentiﬁcation test statistic (14.75) in Chamberlain’s setup is a test of the strict exogeneity assump- tion. Essentially, it is a test of whether the leads and lags of xt appearing in each time period are due to a time-constant unobserved e¤ect ci. The number of overidentifying restrictions is ðT þ T 2KÞ \u0003 ð1 þ TK þ KÞ. Perhaps not surprisingly, the minimum distance approach to estimating y is asymptotically equivalent to the GMM proce- dure we described in Section 11.3.2, as can be reasoned from the work of Angrist and Newey (1991). One hypothesis of interest concerning y is that lt ¼ 0, t ¼ 1; . . . ; T. Under this hypothesis, the random e¤ects assumption that the unobserved e¤ect ci is uncorre- lated with xit for all t holds. We discussed a test of this assumption in Chapter 10. A more general test is available in the minimum distance setting. First, estimate a 1 ðc; b 0Þ0 by minimum distance, using ^p and ^X in equation (14.80). Second, com- pute the test statistic (14.77). Chamberlain (1984) gives an empirical example. Minimum distance methods can be applied to more complicated panel data models, including some of the duration models that we cover in Chapter 20. (See Han and Hausman, 1990.) Van der Klaauw (1996) uses minimum distance estimation in a complicated dynamic model of labor force participation and marital status. Problems 14.1. Consider the system in equations (14.34) and (14.35). a. How would you estimate equation (14.35) using single-equation methods? Give a few possibilities, ranging from simple to more complicated. State any additional assumptions relevant for estimating asymptotic variances or for e‰ciency of the various estimators. Chapter 14 446", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 458, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p459::c0", "text": "b. Is equation (14.34) identiﬁed if g1 ¼ 0? c. Now suppose that g3 ¼ 0, so that the parameters in equation (14.35) can be con- sistently estimated by OLS. Let ^y2 be the OLS ﬁtted values. Explain why nonlinear least squares estimation of y1 ¼ x1d1 þ g1^yg2 2 þ error does not consistently estimate d1, g1, and g2 when g1 0 0 and g2 0 1. 14.2. Consider the following labor supply function nonlinear in parameters: hours ¼ z1d1 þ g1ðwager1 \u0003 1Þ=r1 þ u1; Eðu1 j zÞ ¼ 0 where z1 contains unity and z is the full set of exogenous variables. a. Show that this model contains the level-level and level-log models as special cases. [Hint: For w > 0, ðwr \u0003 1Þ=r ! logðwÞ as r ! 0.] b. How would you test H0: g1 ¼ 0? (Be careful here; r1 cannot be consistently esti- mated under H0.) c. Assuming that g1 0 0, how would you estimate this equation if Varðu1 j zÞ ¼ s2 1? What if Varðu1 j zÞ is not constant? d. Find the gradient of the residual function with respect to d1, g1, and r1. [Hint: Recall that the derivative of wr with respect to r is wr logðwÞ.] e. Explain how to obtain the score test of H0: r1 ¼ 1. 14.3. Use Theorem 14.3 to show that the optimal instrumental variables based on the conditional moment restrictions (14.60) are given by equation (14.63). 14.4. a. Show that, under Assumptions WNLS.1–WNLS.3 in Chapter 12, the weighted NLS estimator has asymptotic variance equal to that of the e‰cient IV es- timator based on the orthogonality condition E½ðyi \u0003 mðxi; boÞÞ j xi\u0002 ¼ 0. b. When does the nonlinear least squares estimator of bo achieve the e‰ciency bound derived in part a? c. Suppose that, in addition to Eðy j xÞ ¼ mðx; boÞ, you use the restriction Varðy j xÞ ¼ s2 o for some s2 o > 0. Write down the two conditional moment restrictions for esti- mating bo and s2 o . What are the e‰cient instrumental variables? 14.5. Write down y, p, and the matrix H such that p ¼ Hy in Chamberlain’s approach to unobserved e¤ects panel data models when T ¼ 3. 14.6. Let ^p and ~p be two consistent estimators of po, with Avar ﬃﬃﬃﬃ N p ð^p \u0003 poÞ ¼ Xo and Avar ﬃﬃﬃﬃ N p ð~p \u0003 poÞ ¼ Lo. Let ^y be the CMD estimator based on ^p, and let ~y be Generalized Method of Moments and Minimum Distance Estimation 447", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 459, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p460::c0", "text": "the CMD estimator based on ~p, where po ¼ hðyoÞ. Show that, if Lo \u0003 Xo is positive semideﬁnite, then so is Avar ﬃﬃﬃﬃ N p ð~y \u0003 yoÞ \u0003 Avar ﬃﬃﬃﬃ N p ð^y \u0003 yoÞ. (Hint: Twice use the fact that, for two positive deﬁnite matrices A and B, A \u0003 B is p.s.d. if and only if B\u00031 \u0003 A\u00031 is p.s.d.) 14.7. Show that when the mapping from yo to po is linear, po ¼ Hyo for a known S \u0001 P matrix H with rankðHÞ ¼ P, the CMD estimator ^y is ^y ¼ ðH0^X\u00031HÞ\u00031H0^X\u00031^p ð14:81Þ Equation (14.81) looks like a generalized least squares (GLS) estimator of ^p on H using variance matrix ^X, and this apparent similarity has prompted some to call the minimum chi-square estimator a ‘‘generalized least squares’’ (GLS) estimator. Unfortunately, the association between CMD and GLS is misleading because ^p and H are not data vectors whose row dimension, S, grows with N. The asymptotic properties of the minimum chi-square estimator do not follow from those of GLS. 14.8. In Problem 13.9, suppose you model the unconditional distribution of y0 as f0ðy0; yÞ, which depends on at least some elements of y appearing in ftðyt j yt\u00031; yÞ. Discuss the pros and cons of using f0ðy0; yÞ in a maximum likelihood analysis along with ftðyt j yt\u00031; yÞ, t ¼ 1; 2; . . . ; T. 14.9. Verify that, for the linear unobserved e¤ects model under Assumptions RE.1– RE.3, the conditions of Lemma 14.1 hold for the ﬁxed e¤ects ð^y2Þ and the ran- dom e¤ects ð^y1Þ estimators, with r ¼ s2 u . [Hint: For clarity, it helps to introduce a cross section subscript, i. Then A1 ¼ Eð\u0001X0 i \u0001XiÞ, where \u0001Xi ¼ Xi \u0003 ljTxi; A2 ¼ Eð€X0 i €XiÞ, where €Xi ¼ Xi \u0003 jTxi; si1 ¼ \u0001X0 iri, where ri ¼ vi \u0003 ljTvi; and si2 ¼ €X0 iui; see Chapter 10 for further notation. You should show that €X0 iui ¼ €X0 iri and then €X0 i \u0001Xi ¼ €X0 i €Xi.] Appendix 14A Proof of Lemma 14.1: Given condition (14.55), A1 ¼ ð1=rÞEðs1s0 1Þ, a P \u0001 P sym- metric matrix, and V1 ¼ A\u00031 1 Eðs1s0 1ÞA\u00031 1 ¼ r2½Eðs1s0 1Þ\u0002\u00031 where we drop the argument w for notational simplicity. Next, under condition (14.56), A2 ¼ ð1=rÞEðs0 2s1Þ, and so V2 ¼ A\u00031 2 Eðs2s0 2ÞðA0 2Þ\u00031 ¼ r2½Eðs2s0 1Þ\u0002\u00031Eðs2s0 2Þ½Eðs1s0 2Þ\u0002\u00031 Chapter 14 448", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 460, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p461::c0", "text": "Now we use the standard result that V2 \u0003 V1 is positive semideﬁnite if and only if V\u00031 1 \u0003 V\u00031 2 is p.s.d. But, dropping the term r2 (which is simply a positive constant), we have V\u00031 1 \u0003 V\u00031 2 ¼ Eðs1s0 1Þ \u0003 Eðs1s0 2Þ½Eðs2s0 2Þ\u0002\u00031Eðs2s0 1Þ 1 Eðr1r0 1Þ where r1 is the P \u0001 1 population residual from the population regression s1 on s2. As Eðr1r0 1Þ is necessarily p.s.d., this step completes the proof. Generalized Method of Moments and Minimum Distance Estimation 449", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 461, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p462::c0", "text": "IV NONLINEAR MODELS AND RELATED TOPICS We now apply the general methods of Part III to study speciﬁc nonlinear models that often arise in applications. Many nonlinear econometric models are intended to ex- plain limited dependent variables. Roughly, a limited dependent variable is a variable whose range is restricted in some important way. Most variables encountered in economics are limited in range, but not all require special treatment. For example, many variables—wage, population, and food consumption, to name just a few—can only take on positive values. If a strictly positive variable takes on numerous values, special econometric methods are rarely called for. Often, taking the log of the vari- able and then using a linear model su‰ces. When the variable to be explained, y, is discrete and takes on a ﬁnite number of values, it makes little sense to treat it as an approximately continuous variable. Dis- creteness of y does not in itself mean that a linear model for Eðy j xÞ is inappropriate. However, in Chapter 15 we will see that linear models have certain drawbacks for modeling binary responses, and we will treat nonlinear models such as probit and logit. We also cover basic multinomial response models in Chapter 15, including the case when the response has a natural ordering. Other kinds of limited dependent variables arise in econometric analysis, especially when modeling choices by individuals, families, or ﬁrms. Optimizing behavior often leads to corner solutions for some nontrivial fraction of the population. For example, during any given time, a fairly large fraction of the working age population does not work outside the home. Annual hours worked has a population distribution spread out over a range of values, but with a pileup at the value zero. While it could be that a linear model is appropriate for modeling expected hours worked, a linear model will likely lead to negative predicted hours worked for some people. Taking the nat- ural log is not possible because of the corner solution at zero. In Chapter 16 we will discuss econometric models that are better suited for describing these kinds of limited dependent variables. We treat the problem of sample selection in Chapter 17. In many sample selection contexts the underlying population model is linear, but nonlinear econometric meth- ods are required in order to correct for nonrandom sampling. Chapter 17 also covers testing and correcting for attrition in panel data models, as well as methods for dealing with stratiﬁed samples. In Chapter 18 we provide a modern treatment of switching regression models and, more generally, random coe‰cient models with endogenous explanatory variables. We focus on estimating average treatment e¤ects. We treat methods for count-dependent variables, which take on nonnegative inte- ger values, in Chapter 19. An introduction to modern duration analysis is given in Chapter 20.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 462, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p463::c0", "text": "15 Discrete Response Models 15.1 Introduction In qualitative response models, the variable to be explained, y, is a random variable taking on a ﬁnite number of outcomes; in practice, the number of outcomes is usually small. The leading case occurs where y is a binary response, taking on the values zero and one, which indicate whether or not a certain event has occurred. For example, y ¼ 1 if a person is employed, y ¼ 0 otherwise; y ¼ 1 if a family contributes to charity during a particular year, y ¼ 0 otherwise; y ¼ 1 if a ﬁrm has a particular type of pension plan, y ¼ 0 otherwise. Regardless of the deﬁnition of y, it is traditional to refer to y ¼ 1 as a success and y ¼ 0 as a failure. As in the case of linear models, we often call y the explained variable, the response variable, the dependent variable, or the endogenous variable; x 1 ðx1; x2; . . . ; xKÞ is the vector of explanatory variables, regressors, independent variables, exogenous variables, or covariates. In binary response models, interest lies primarily in the response probability, pðxÞ 1 Pðy ¼ 1 j xÞ ¼ Pðy ¼ 1 j x1; x2; . . . ; xKÞ ð15:1Þ for various values of x. For example, when y is an employment indicator, x might contain various individual characteristics such as education, age, marital status, and other factors that a¤ect employment status, such as a binary indicator variable for participation in a recent job training program, or measures of past criminal behavior. For a continuous variable, xj, the partial e¤ect of xj on the response probability is qPðy ¼ 1 j xÞ qxj ¼ qpðxÞ qxj ð15:2Þ When multiplied by Dxj, equation (15.2) gives the approximate change in Pðy ¼ 1 j xÞ when xj increases by Dxj, holding all other variables ﬁxed (for ‘‘small’’ Dxj). Of course if, say, x1 1 z and x2 1 z2 for some variable z (for example, z could be work experience), we would be interested in qpðxÞ=qz. If xK is a binary variable, interest lies in pðx1; x2; . . . ; xK\u00011; 1Þ \u0001 pðx1; x2; . . . ; xK\u00011; 0Þ ð15:3Þ which is the di¤erence in response probabilities when xK ¼ 1 and xK ¼ 0. For most of the models we consider, whether a variable xj is continuous or discrete, the partial e¤ect of xj on pðxÞ depends on all of x. In studying binary response models, we need to recall some basic facts about Bernoulli (zero-one) random variables. The only di¤erence between the setup here", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 463, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p464::c0", "text": "and that in basic statistics is the conditioning on x. If Pðy ¼ 1 j xÞ ¼ pðxÞ then Pðy ¼ 0 j xÞ ¼ 1 \u0001 pðxÞ, Eðy j xÞ ¼ pðxÞ, and Varðy j xÞ ¼ pðxÞ½1 \u0001 pðxÞ\u0002. 15.2 The Linear Probability Model for Binary Response The linear probability model (LPM) for binary response y is speciﬁed as Pðy ¼ 1 j xÞ ¼ b0 þ b1x1 þ b2x2 þ \u0003 \u0003 \u0003 þ bKxK ð15:4Þ As usual, the xj can be functions of underlying explanatory variables, which would simply change the interpretations of the bj. Assuming that x1 is not functionally re- lated to the other explanatory variables, b1 ¼ qPðy ¼ 1 j xÞ=qx1. Therefore, b1 is the change in the probability of success given a one-unit increase in x1. If x1 is a binary explanatory variable, b1 is just the di¤erence in the probability of success when x1 ¼ 1 and x1 ¼ 0, holding the other xj ﬁxed. Using functions such as quadratics, logarithms, and so on among the independent variables causes no new di‰culties. The important point is that the bj now measure the e¤ects of the explanatory variables xj on a particular probability. Unless the range of x is severely restricted, the linear probability model cannot be a good description of the population response probability Pðy ¼ 1 j xÞ. For given values of the population parameters bj, there would usually be feasible values of x1; . . . ; xK such that b0 þ xb is outside the unit interval. Therefore, the LPM should be seen as a convenient approximation to the underlying response probability. What we hope is that the linear probability approximates the response probability for common values of the covariates. Fortunately, this often turns out to be the case. In deciding on an appropriate estimation technique, it is useful to derive the con- ditional mean and variance of y. Since y is a Bernoulli random variable, these are simply Eðy j xÞ ¼ b0 þ b1x1 þ b2x2 þ \u0003 \u0003 \u0003 þ bKxK ð15:5Þ Varðy j xÞ ¼ xbð1 \u0001 xbÞ ð15:6Þ where xb is shorthand for the right-hand side of equation (15.5). Equation (15.5) implies that, given a random sample, the OLS regression of y on 1; x1; x2; . . . ; xK produces consistent and even unbiased estimators of the bj. Equation (15.6) means that heteroskedasticity is present unless all of the slope co- e‰cients b1; . . . ; bK are zero. A nice way to deal with this issue is to use standard heteroskedasticity-robust standard errors and t statistics. Further, robust tests of multiple restrictions should also be used. There is one case where the usual F statistic Chapter 15 454", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 464, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p465::c0", "text": "can be used, and that is to test for joint signiﬁcance of all variables (leaving the con- stant unrestricted). This test is asymptotically valid because Varðy j xÞ is constant under this particular null hypothesis. Since the form of the variance is determined by the model for Pðy ¼ 1 j xÞ, an asymptotically more e‰cient method is weighted least squares (WLS). Let ^b be the OLS estimator, and let ^yi denote the OLS ﬁtted values. Then, provided 0 < ^yi < 1 for all observations i, deﬁne the estimated standard deviation as ^si 1 ½ ^yið1 \u0001 ^yiÞ\u00021=2. Then the WLS estimator, b \u0004, is obtained from the OLS regression yi=^si on 1=^si; xi1=^si; . . . ; xiK=^si; i ¼ 1; 2; . . . ; N ð15:7Þ The usual standard errors from this regression are valid, as follows from the treat- ment of weighted least squares in Chapter 12. In addition, all other testing can be done using F statistics or LM statistics using weighted regressions. If some of the OLS ﬁtted values are not between zero and one, WLS analysis is not possible without ad hoc adjustments to bring deviant ﬁtted values into the unit in- terval. Further, since the OLS ﬁtted value ^yi is an estimate of the conditional proba- bility Pðyi ¼ 1 j xiÞ, it is somewhat awkward if the predicted probability is negative or above unity. Aside from the issue of ﬁtted values being outside the unit interval, the LPM implies that a ceteris paribus unit increase in xj always changes Pðy ¼ 1 j xÞ by the same amount, regardless of the initial value of xj. This implication cannot literally be true because continually increasing one of the xj would eventually drive Pðy ¼ 1 j xÞ to be less than zero or greater than one. Even with these weaknesses, the LPM often seems to give good estimates of the partial e¤ects on the response probability near the center of the distribution of x. (How good they are can be determined by comparing the coe‰cients from the LPM with the partial e¤ects estimated from the nonlinear models we cover in Section 15.3.) If the main purpose is to estimate the partial e¤ect of xj on the response probability, averaged across the distribution of x, then the fact that some predicted values are outside the unit interval may not be very important. The LPM need not provide very good estimates of partial e¤ects at extreme values of x. Example 15.1 (Married Women’s Labor Force Participation): We use the data from MROZ.RAW to estimate a linear probability model for labor force participation (inlf ) of married women. Of the 753 women in the sample, 428 report working non- zero hours during the year. The variables we use to explain labor force participation are age, education, experience, nonwife income in thousands (nwifeinc), number of children less than six years of age (kidslt6), and number of kids between 6 and 18 Discrete Response Models 455", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 465, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p466::c0", "text": "inclusive (kidsge6); 606 women report having no young children, while 118 report having exactly one young child. The usual OLS standard errors are in parentheses, while the heteroskedasticity-robust standard errors are in brackets: i^nlf ¼ :586 ð:154Þ ½:151\u0002 \u0001 :0034 ð:0014Þ ½:0015\u0002 nwifeinc þ :038 ð:007Þ ½:007\u0002 educ þ :039 ð:006Þ ½:006\u0002 exper \u0001 :00060 ð:00018Þ ½:00019\u0002 exper2 \u0001 :016 ð:002Þ ½:002\u0002 age \u0001 :262 ð:034Þ ½:032\u0002 kidslt6 þ :013 ð:013Þ ½:013\u0002 kidsge6 N ¼ 753; R2 ¼ :264 With the exception of kidsge6, all coe‰cients have sensible signs and are statistically signiﬁcant; kidsge6 is neither statistically signiﬁcant nor practically important. The coe‰cient on nwifeinc means that if nonwife income increases by 10 ($10,000), the probability of being in the labor force is predicted to fall by .034. This is a small e¤ect given that an increase in income by $10,000 in 1975 dollars is very large in this sam- ple. (The average of nwifeinc is about $20,129 with standard deviation $11,635.) Having one more small child is estimated to reduce the probability of inlf ¼ 1 by about .262, which is a fairly large e¤ect. Of the 753 ﬁtted probabilities, 33 are outside the unit interval. Rather than using some adjustment to those 33 ﬁtted values and applying weighted least squares, we just use OLS and report heteroskedasticity-robust standard errors. Interestingly, these di¤er in practically unimportant ways from the usual OLS standard errors. The case for the LPM is even stronger if most of the xj are discrete and take on only a few values. In the previous example, to allow a diminishing e¤ect of young children on the probability of labor force participation, we can break kidslt6 into three binary indicators: no young children, one young child, and two or more young children. The last two indicators can be used in place of kidslt6 to allow the ﬁrst young child to have a larger e¤ect than subsequent young children. (Interestingly, when this method is used, the marginal e¤ects of the ﬁrst and second young children are virtually the same. The estimated e¤ect of the ﬁrst child is about \u0001.263, and the additional reduction in the probability of labor force participation for the next child is about \u0001.274.) In the extreme case where the model is saturated—that is, x contains dummy vari- ables for mutually exclusive and exhaustive categories—the linear probability model Chapter 15 456", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 466, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p467::c0", "text": "is completely general. The ﬁtted probabilities are simply the average yi within each cell deﬁned by the di¤erent values of x; we need not worry about ﬁtted probabilities less than zero or greater than one. See Problem 15.1. 15.3 Index Models for Binary Response: Probit and Logit We now study binary response models of the form Pðy ¼ 1 j xÞ ¼ GðxbÞ 1 pðxÞ ð15:8Þ where x is 1 \u0005 K, b is K \u0005 1, and we take the ﬁrst element of x to be unity. Examples where x does not contain unity are rare in practice. For the linear probability model, GðzÞ ¼ z is the identity function, which means that the response probabilities cannot be between 0 and 1 for all x and b. In this section we assume that Gð\u0003Þ takes on values in the open unit interval: 0 < GðzÞ < 1 for all z A R. The model in equation (15.8) is generally called an index model because it restricts the way in which the response probability depends on x: pðxÞ is a function of x only through the index xb ¼ b1 þ b2x2 þ \u0003 \u0003 \u0003 þ bKxK. The function G maps the index into the response probability. In most applications, G is a cumulative distribution function (cdf ), whose speciﬁc form can sometimes be derived from an underlying economic model. For example, in Problem 15.2 you are asked to derive an index model from a utility-based model of charitable giving. The binary indicator y equals unity if a family contributes to charity and zero otherwise. The vector x contains family characteristics, income, and the price of a charitable contribution (as determined by marginal tax rates). Under a normality assumption on a particular unobservable taste variable, G is the standard normal cdf. Index models where G is a cdf can be derived more generally from an underlying latent variable model, as in Example 13.1: y\u0004 ¼ xb þ e; y ¼ 1½y\u0004 > 0\u0002 ð15:9Þ where e is a continuously distributed variable independent of x and the distribution of e is symmetric about zero; recall from Chapter 13 that 1½\u0003\u0002 is the indicator function. If G is the cdf of e, then, because the pdf of e is symmetric about zero, 1 \u0001 Gð\u0001zÞ ¼ GðzÞ for all real numbers z. Therefore, Pðy ¼ 1 j xÞ ¼ Pðy\u0004 > 0 j xÞ ¼ Pðe > \u0001xb j xÞ ¼ 1 \u0001 Gð\u0001xbÞ ¼ GðxbÞ which is exactly equation (15.8). Discrete Response Models 457", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 467, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p468::c0", "text": "There is no particular reason for requiring e to be symmetrically distributed in the latent variable model, but this happens to be the case for the binary response models applied most often. In most applications of binary response models, the primary goal is to explain the e¤ects of the xj on the response probability Pðy ¼ 1 j xÞ. The latent variable formu- lation tends to give the impression that we are primarily interested in the e¤ects of each xj on y\u0004. As we will see, the direction of the e¤ects of xj on Eðy\u0004 j xÞ ¼ xb and on Eðy j xÞ ¼ Pðy ¼ 1 j xÞ ¼ GðxbÞ are the same. But the latent variable y\u0004 rarely has a well-deﬁned unit of measurement (for example, y\u0004 might be measured in utility units). Therefore, the magnitude of bj is not especially meaningful except in special cases. The probit model is the special case of equation (15.8) with GðzÞ 1 FðzÞ 1 ð z \u0001y fðvÞ dv ð15:10Þ where fðzÞ is the standard normal density fðzÞ ¼ ð2pÞ\u00011=2 expð\u0001z2=2Þ ð15:11Þ The probit model can be derived from the latent variable formulation when e has a standard normal distribution. The logit model is a special case of equation (15.8) with GðzÞ ¼ LðzÞ 1 expðzÞ=½1 þ expðzÞ\u0002 ð15:12Þ This model arises from the model (15.9) when e has a standard logistic distribution. The general speciﬁcation (15.8) allows us to cover probit, logit, and a number of other binary choice models in one framework. In fact, in what follows we do not even need G to be a cdf, but we do assume that GðzÞ is strictly between zero and unity for all real numbers z. In order to successfully apply probit and logit models, it is important to know how to interpret the bj on both continuous and discrete explanatory variables. First, if xj is continuous, qpðxÞ qxj ¼ gðxbÞbj; where gðzÞ 1 dG dz ðzÞ ð15:13Þ Therefore, the partial e¤ect of xj on pðxÞ depends on x through gðxbÞ. If Gð\u0003Þ is a strictly increasing cdf, as in the probit and logit cases, gðzÞ > 0 for all z. Therefore, Chapter 15 458", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 468, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p469::c0", "text": "the sign of the e¤ect is given by the sign of bj. Also, the relative e¤ects do not depend on x: for continuous variables xj and xh, the ratio of the partial e¤ects is constant and given by the ratio of the corresponding coe‰cients: qpðxÞ=qxj qpðxÞ=qxh ¼ bj=bh. In the typical case that g is a symmetric density about zero, with unique mode at zero, the largest e¤ect is when xb ¼ 0. For example, in the probit case with gðzÞ ¼ fðzÞ, gð0Þ ¼ fð0Þ ¼ 1= ﬃﬃﬃﬃﬃ 2p p A:399. In the logit case, gðzÞ ¼ expðzÞ=½1 þ expðzÞ\u00022, and so gð0Þ ¼ :25. If xK is a binary explanatory variable, then the partial e¤ect from changing xK from zero to one, holding all other variables ﬁxed, is simply Gðb1 þ b2x2 þ \u0003 \u0003 \u0003 þ bK\u00011xK\u00011 þ bKÞ \u0001 Gðb1 þ b2x2 þ \u0003 \u0003 \u0003 þ bK\u00011xK\u00011Þ ð15:14Þ Again, this expression depends on all other values of the other xj. For example, if y is an employment indicator and xj is a dummy variable indicating participation in a job training program, then expression (15.14) is the change in the probability of em- ployment due to the job training program; this depends on other characteristics that a¤ect employability, such as education and experience. Knowing the sign of bK is enough to determine whether the program had a positive or negative e¤ect. But to ﬁnd the magnitude of the e¤ect, we have to estimate expression (15.14). We can also use the di¤erence in expression (15.14) for other kinds of discrete variables (such as number of children). If xK denotes this variable, then the e¤ect on the probability of xK going from cK to cK þ 1 is simply G½b1 þ b2x2 þ \u0003 \u0003 \u0003 þ bK\u00011xK\u00011 þ bKðcK þ 1Þ\u0002 \u0001 Gðb1 þ b2x2 þ \u0003 \u0003 \u0003 þ bK\u00011xK\u00011 þ bKcKÞ ð15:15Þ It is straightforward to include standard functional forms among the explanatory variables. For example, in the model Pðy ¼ 1 j zÞ ¼ G½b0 þ b1z1 þ b2z2 1 þ b3 logðz2Þ þ b4z3\u0002 the partial e¤ect of z1 on Pðy ¼ 1 j zÞ is qPðy ¼ 1 j zÞ=qz1 ¼ gðxbÞðb1 þ 2b2z1Þ, where xb ¼ b0 þ b1z1 þ b2z2 1 þ b3 logðz2Þ þ b4z3. It follows that if the quadratic in z1 has a hump shape or a U shape, the turning point in the response probability is jb1=ð2b2Þj [because gðxbÞ > 0\u0002. Also, qPðy ¼ 1 j zÞ=q logðz2Þ ¼ gðxbÞb3, and so gðxbÞðb3=100Þ is the approximate change in Pðy ¼ 1 j zÞ given a 1 percent increase in z2. Models with interactions among explanatory variables, including interactions between dis- crete and continuous variables, are handled similarly. When measuring e¤ects of discrete variables, we should use expression (15.15). Discrete Response Models 459", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 469, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p470::c0", "text": "15.4 Maximum Likelihood Estimation of Binary Response Index Models Assume we have N independent, identically distributed observations following the model (15.8). Since we essentially covered the case of probit in Chapter 13, the dis- cussion here will be brief. To estimate the model by (conditional) maximum likeli- hood, we need the log-likelihood function for each i. The density of yi given xi can be written as f ðy j xi; bÞ ¼ ½GðxibÞ\u0002y½1 \u0001 GðxibÞ\u00021\u0001y; y ¼ 0; 1 ð15:16Þ The log-likelihood for observation i is a function of the K \u0005 1 vector of parameters and the data ðxi; yiÞ: liðbÞ ¼ yi log½GðxibÞ\u0002 þ ð1 \u0001 yiÞ log½1 \u0001 GðxibÞ\u0002 ð15:17Þ (Recall from Chapter 13 that, technically speaking, we should distinguish the ‘‘true’’ value of beta, bo, from a generic value. For conciseness we do not do so here.) Restricting Gð\u0003Þ to be strictly between zero and one ensures that liðbÞ is well deﬁned for all values of b. As usual, the log likelihood for a sample size of N is LðbÞ ¼ PN i¼1 liðbÞ, and the MLE of b, denoted ^b, maximizes this log likelihood. If Gð\u0003Þ is the standard normal cdf, then ^b is the probit estimator; if Gð\u0003Þ is the logistic cdf, then ^b is the logit esti- mator. From the general maximum likelihood results we know that ^b is consistent and asymptotically normal. We can also easily estimate the asymptotic variance ^b. We assume that Gð\u0003Þ is twice continuously di¤erentiable, an assumption that is usually satisﬁed in applications (and, in particular, for probit and logit). As before, the function gðzÞ is the derivative of GðzÞ. For the probit model, gðzÞ ¼ fðzÞ, and for the logit model, gðzÞ ¼ expðzÞ=½1 þ expðzÞ\u00022. Using the same calculations for the probit example as in Chapter 13, the score of the conditional log likelihood for observation i can be shown to be siðbÞ 1 gðxibÞx0 i½yi \u0001 GðxibÞ\u0002 GðxibÞ½1 \u0001 GðxibÞ\u0002 ð15:18Þ Similarly, the expected value of the Hessian conditional on xi is \u0001E½HiðbÞ j xi\u0002 ¼ ½gðxibÞ\u00022x0 ixi fGðxibÞ½1 \u0001 GðxibÞ\u0002g 1 Aðxi; bÞ ð15:19Þ which is a K \u0005 K positive semideﬁnite matrix for each i. From the general condi- tional MLE results in Chapter 13, Avarð ^bÞ is estimated as Chapter 15 460", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 470, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p471::c0", "text": "Av^arð ^bÞ 1 X N i¼1 ½gðxi ^bÞ\u00022x0 ixi Gðxi ^bÞ½1 \u0001 Gðxi ^bÞ\u0002 ( )\u00011 1 ^V ð15:20Þ In most cases the inverse exists, and when it does, ^V is positive deﬁnite. If the matrix in equation (15.20) is not invertible, then perfect collinearity probably exists among the regressors. As usual, we treat ^b as being normally distributed with mean zero and variance matrix in equation (15.20). The (asymptotic) standard error of ^bj is the square root of the jth diagonal element of ^V. These can be used to construct t statistics, which have a limiting standard normal distribution, and to construct approximate conﬁdence intervals for each population parameter. These are reported with the estimates for packages that perform logit and probit. We discuss multiple hypothesis testing in the next section. Some packages also compute Huber-White standard errors as an option for probit and logit analysis, using the general M-estimator formulas; see, in particular, equa- tion (12.49). While the robust variance matrix is consistent, using it in place of the usual estimator means we must think that the binary response model is incorrectly speciﬁed. Unlike with nonlinear regression, in a binary response model it is not pos- sible to correctly specify Eðy j xÞ but to misspecify Varðy j xÞ. Once we have speciﬁed Pðy ¼ 1 j xÞ, we have speciﬁed all conditional moments of y given x. In Section 15.8 we will see that, when using binary response models with panel data or cluster samples, it is sometimes important to compute variance matrix esti- mators that are robust to either serial dependence or within-group correlation. But this need arises as a result of dependence across time or subgroup, and not because the response probability is misspeciﬁed. 15.5 Testing in Binary Response Index Models Any of the three tests from general MLE analysis—the Wald, LR, or LM test—can be used to test hypotheses in binary response contexts. Since the tests are all asymptotically equivalent under local alternatives, the choice of statistic usually depends on computa- tional simplicity (since ﬁnite sample comparisons must be limited in scope). In the fol- lowing subsections we discuss some testing situations that often arise in binary choice analysis, and we recommend particular tests for their computational advantages. 15.5.1 Testing Multiple Exclusion Restrictions Consider the model Pðy ¼ 1 j x; zÞ ¼ Gðxb þ zgÞ ð15:21Þ Discrete Response Models 461", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 471, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p472::c0", "text": "where x is 1 \u0005 K and z is 1 \u0005 Q. We wish to test the null hypothesis H0: g ¼ 0, so we are testing Q exclusion restrictions. The elements of z can be functions of x, such as quadratics and interactions—in which case the test is a pure functional form test. Or, the z can be additional explanatory variables. For example, z could contain dummy variables for occupation or region. In any case, the form of the test is the same. Some packages, such as Stata, compute the Wald statistic for exclusion restrictions using a simple command following estimation of the general model. This capability makes it very easy to test multiple exclusion restrictions, provided the dimension of ðx; zÞ is not so large as to make probit estimation di‰cult. The likelihood ratio statistic is also easy to use. Let Lur denote the value of the log- likelihood function from probit of y on x and z (the unrestricted model), and let Lr denote the value of the likelihood function from probit of y on x (the restricted model). Then the likelihood ratio test of H0: g ¼ 0 is simply 2ðLur \u0001 LrÞ, which has an asymptotic w2 Q distribution under H0. This is analogous to the usual F statistic in OLS analysis of a linear model. The score or LM test is attractive if the unrestricted model is di‰cult to estimate. In this section, let ^b denote the restricted estimator of b, that is, the probit or logit estimator with z excluded from the model. The LM statistic using the estimated expected hessian, ^Ai [see equation (15.20) and Section 12.6.2], can be shown to be numerically identical to the following: (1) Deﬁne ^ui 1 yi \u0001 Gðxi ^bÞ, ^Gi 1 Gðxi ^bÞ, and ^gi 1 gðxi ^bÞ. These are all obtainable after estimating the model without z. (2) Use all N observations to run the auxiliary OLS regression ^ui ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ^Gið1 \u0001 ^GiÞ q on ^gi ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ^Gið1 \u0001 ^GiÞ q xi; ^gi ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ^Gið1 \u0001 ^GiÞ q zi ð15:22Þ The LM statistic is equal to the explained sum of squares from this regression. A test that is asymptotically (but not numerically) equivalent is NR2 u, where R2 u is the uncentered R-squared from regression (15.22). The LM procedure is rather easy to remember. The term ^gixi is the gradient of the mean function Gðxib þ zigÞ with respect to b, evaluated at b ¼ ^b and g ¼ 0. Simi- larly, ^gizi is the gradient of Gðxib þ zigÞ with respect to g, again evaluated at b ¼ ^b and g ¼ 0. Finally, under H0: g ¼ 0, the conditional variance of ui given ðxi; ziÞ is GðxibÞ½1 \u0001 GðxibÞ\u0002; therefore, ½ ^Gið1 \u0001 ^GiÞ\u00021=2 is an estimate of the conditional stan- dard deviation of ui. The dependent variable in regression (15.22) is often called a standardized residual because it is an estimate of ui=½Gið1 \u0001 GiÞ\u00021=2, which has unit conditional (and unconditional) variance. The regressors are simply the gradient of the conditional mean function with respect to both sets of parameters, evaluated under Chapter 15 462", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 472, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p473::c0", "text": "H0, and weighted by the estimated inverse conditional standard deviation. The ﬁrst set of regressors in regression (15.22) is 1 \u0005 K and the second set is 1 \u0005 Q. Under H0, LM @ w2 Q. The LM approach can be an attractive alternative to the LR statistic if z has large dimension, since with many explanatory variables probit can be di‰cult to estimate. 15.5.2 Testing Nonlinear Hypotheses about b For testing nonlinear restrictions on b in equation (15.8), the Wald statistic is com- putationally the easiest because the unrestricted estimator of b, which is just probit or logit, is easy to obtain. Actually imposing nonlinear restrictions in estimation— which is required to apply the score or likelihood ratio methods—can be di‰cult. However, we must also remember that the Wald statistic for testing nonlinear restric- tions is not invariant to reparameterizations, whereas the LM and LR statistics are. (See Sections 12.6 and 13.6; for the LM statistic, we would always use the expected Hessian.) Let the restictions on b be given by H0: cðbÞ ¼ 0, where cðbÞ is a Q \u0005 1 vector of possibly nonlinear functions satisfying the di¤erentiability and rank requirements from Chapter 13. Then, from the general MLE analysis, the Wald statistic is simply W ¼ cð ^bÞ0½‘bcð ^bÞ^V‘bcð ^bÞ0\u0002\u00011cð ^bÞ ð15:23Þ where ^V is given in equation (15.20) and ‘bcð ^bÞ is the Q \u0005 K Jacobian of cðbÞ evalu- ated at ^b. 15.5.3 Tests against More General Alternatives In addition to testing for omitted variables, sometimes we wish to test the probit or logit model against a more general functional form. When the alternatives are not standard binary response models, the Wald and LR statistics are cumbersome to apply, whereas the LM approach is convenient because it only requires estimation of the null model. As an example of a more complicated binary choice model, consider the latent variable model (15.9) but assume that e j x @ Normal½0; expð2x1dÞ\u0002, where x1 is 1 \u0005 K1 subset of x that excludes a constant and d is a K1 \u0005 1 vector of additional param- eters. (In many cases we would take x1 to be all nonconstant elements of x.) There- fore, there is heteroskedasticity in the latent variable model, so that e is no longer independent of x. The standard deviation of e given x is simply expðx1dÞ. Deﬁne r ¼ e=expðx1dÞ, so that r is independent of x with a standard normal distribution. Then Discrete Response Models 463", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 473, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p474::c0", "text": "Pðy ¼ 1 j xÞ ¼ Pðe > \u0001xb j xÞ ¼ P½expð\u0001x1dÞe > \u0001expð\u0001x1dÞxb\u0002 ¼ P½r > \u0001expð\u0001x1dÞxb\u0002 ¼ F½expð\u0001x1dÞxb\u0002 ð15:24Þ The partial e¤ects of xj on Pðy ¼ 1 j xÞ are much more complicated in equation (15.24) than in equation (15.8). When d ¼ 0, we obtain the standard probit model. Therefore, a test of the probit functional form for the response probability is a test of H0: d ¼ 0. To obtain the LM test of d ¼ 0 in equation (15.24), it is useful to derive the LM test for an index model against a more general alternative. Consider Pðy ¼ 1 j xÞ ¼ mðxb; x; dÞ ð15:25Þ where d is a Q \u0005 1 vector of parameters. We wish to test H0: d ¼ d0, where d0 is often (but not always) a vector of zeros. We assume that, under the null, we obtain a standard index model (probit or logit, usually): GðxbÞ ¼ mðxb; x; d0Þ ð15:26Þ In the previous example, Gð\u0003Þ ¼ Fð\u0003Þ, d0 ¼ 0, and mðxb; x; dÞ ¼ F½expð\u0001x1dÞxb\u0002. Let ^b be the probit or logit estimator of b obtained under d ¼ d0. Deﬁne ^ui 1 yi \u0001 Gðxi ^bÞ, ^Gi 1 Gðxi ^bÞ, and ^gi 1 gðxi ^bÞ. The gradient of the mean function mðxib; xi; dÞ with respect to b, evaluated at d0, is simply gðxibÞxi. The only other piece we need is the gradient of mðxib; xi; dÞ with respect to d, evaluated at d0. Denote this 1 \u0005 Q vector as ‘dmðxib; xi; d0Þ. Further, set ‘d ^mi 1 ‘dmðxi ^b; xi; d0Þ. The LM statistic can be obtained as the explained sum of squares or NR2 u from the regression ^ui ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ^Gið1 \u0001 ^GiÞ q on ^gi ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ^Gið1 \u0001 ^GiÞ q xi; ‘d ^mi ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ^Gið1 \u0001 ^GiÞ q ð15:27Þ which is quite similar to regression (15.22). The null distribution of the LM statistic is w2 Q, where Q is the dimension of d. When applying this test to the preceding probit example, we have only ‘d ^mi left to compute. But mðxib; xi; dÞ ¼ F½expð\u0001xi1dÞxib\u0002, and so ‘dmðxib; xi; dÞ ¼ \u0001ðxibÞ expð\u0001xi1dÞxi1f½expð\u0001xi1dÞxib\u0002 When evaluated at b ¼ ^b and d ¼ 0 (the null value), we get ‘d ^mi ¼ \u0001ðxi ^bÞfðxi ^bÞxi1 1 \u0001ðxi ^bÞ ^fixi1, a 1 \u0005 K1 vector. Regression (15.27) becomes ^ui ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ^Fið1 \u0001 ^FiÞ q on ^fi ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ^Fið1 \u0001 ^FiÞ q xi; ðxi ^bÞ ^fi ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ^Fið1 \u0001 ^FiÞ q xi1 ð15:28Þ Chapter 15 464", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 474, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p475::c0", "text": "(We drop the minus sign because it does not a¤ect the value of the explained sum of squares or R2 u.) Under the null hypothesis that the probit model is correctly speciﬁed, LM @ w2 K1. This statistic is easy to compute after estimation by probit. For a one-degree-of-freedom test regardless of the dimension of xi, replace the last term in regression (15.28) with ðxi ^bÞ2 ^fi= ^Fið1 \u0001 ^FiÞ ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ p , and then the explained sum of squares is distributed asymptotically as w2 1. See Davidson and MacKinnon (1984) for further examples. 15.6 Reporting the Results for Probit and Logit Several statistics should be reported routinely in any probit or logit (or other binary choice) analysis. The ^bj, their standard errors, and the value of the likelihood func- tion are reported by all software packages that do binary response analysis. The ^bj give the signs of the partial e¤ects of each xj on the response probability, and the statistical signiﬁcance of xj is determined by whether we can reject H0: bj ¼ 0. One measure of goodness of ﬁt that is usually reported is the percent correctly predicted. For each i, we compute the predicted probability that yi ¼ 1, given the explanatory variables, xi. If Gðxi ^bÞ > :5, we predict yi to be unity; if Gðxi ^bÞ a :5, yi is predicted to be zero. The percentage of times the predicted yi matches the actual yi is the percent correctly predicted. In many cases it is easy to predict one of the out- comes and much harder to predict another outcome, in which case the percent cor- rectly predicted can be misleading as a goodness-of-ﬁt statistic. More informative is to compute the percent correctly predicted for each outcome, y ¼ 0 and y ¼ 1. The overall percent correctly predicted is a weighted average of the two, with the weights being the fractions of zero and one outcomes, respectively. Problem 15.7 provides an illustration. Various pseudo R-squared measures have been proposed for binary response. McFadden (1974) suggests the measure 1 \u0001 Lur=Lo, where Lur is the log-likelihood function for the estimated model and Lo is the log-likelihood function in the model with only an intercept. Because the log likelihood for a binary response model is always negative, jLurj a jLoj, and so the pseudo R-squared is always between zero and one. Alternatively, we can use a sum of squared residuals measure: 1 \u0001 SSRur= SSRo, where SSRur is the sum of squared residuals ^ui ¼ yi \u0001 Gðxi ^bÞ and SSRo is the total sum of squares of yi. Several other measures have been suggested (see, for ex- ample, Maddala, 1983, Chapter 2), but goodness of ﬁt is not as important as statis- tical and economic signiﬁcance of the explanatory variables. Estrella (1998) contains a recent comparison of goodness-of-ﬁt measures for binary response. Discrete Response Models 465", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 475, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p476::c0", "text": "Often we want to estimate the e¤ects of the variables xj on the response proba- bilities Pðy ¼ 1 j xÞ. If xj is (roughly) continuous then D^Pðy ¼ 1 j xÞA½gðx ^bÞ^bj\u0002Dxj ð15:29Þ for small changes in xj. (As usual when using calculus, the notion of ‘‘small’’ here is somewhat vague.) Since gðx^bÞ depends on x, we must compute gðx ^bÞ at interesting values of x. Often the sample averages of the xj’s are plugged in to get gðx ^bÞ. This factor can then be used to adjust each of the ^bj (at least those on continuous vari- ables) to obtain the e¤ect of a one-unit increase in xj. If x contains nonlinear functions of some explanatory variables, such as natural logs or quadratics, there is the issue of using the log of the average versus the average of the log (and similarly with qua- dratics). To get the e¤ect for the ‘‘average’’ person, it makes more sense to plug the averages into the nonlinear functions, rather than average the nonlinear functions. Software packages (such as Stata with the dprobit command) necessarily average the nonlinear functions. Sometimes minimum and maximum values of key variables are used in obtaining gðx ^bÞ, so that we can see how the partial e¤ects change as some elements of x get large or small. Equation (15.29) also suggests how to roughly compare magnitudes of the probit and logit estimates. If x ^b is close to zero for logit and probit, the scale factor we use can be gð0Þ. For probit, gð0ÞA:4, and for logit, gð0Þ ¼ :25. Thus the logit estimates can be expected to be larger by a factor of about :4=:25 ¼ 1:6. Alternatively, multiply the logit estimates by .625 to make them comparable to the probit estimates. In the linear probability model, gð0Þ is unity, and so logit estimates should be divided by four to compare them with LPM estimates, while probit estimates should be divided by 2.5 to make them roughly comparable to LPM estimates. More accurate com- parisons are obtained by using the scale factors gðx ^bÞ for probit and logit. Of course, one of the potential advantages of using probit or logit is that the partial e¤ects vary with x, and it is of some interest to compute gðx ^bÞ at values of x other than the sample averages. If, say, x2 is a binary variable, it perhaps makes more sense to plug in zero or one for x2, rather than x2 (which is the fraction of ones in the sample). Putting in the averages for the binary variables means that the e¤ect does not really correspond to a particular individual. But often the results are similar, and the choice is really based on taste. To obtain standard errors of the partial e¤ects in equation (15.29) we use the delta method. Consider the case j ¼ K for notational simplicity, and for given x, deﬁne dK ¼ bKgðxbÞ ¼ qPðy ¼ 1 j xÞ=qxK. Write this relation as dK ¼ hðbÞ to denote that this is a (nonlinear) function of the vector b. We assume x1 ¼ 1. The gradient of hðbÞ is Chapter 15 466", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 476, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p477::c0", "text": "‘bhðbÞ ¼ bK dg dz ðxbÞ; bKx2 dg dz ðxbÞ; . . . ; bKxK\u00011 dg dz ðxbÞ; bKxK dg dz ðxbÞ þ gðxbÞ \u0002 \u0003 where dg=dz is simply the derivative of g with respect to its argument. The delta method implies that the asymptotic variance of ^dK is estimated as ½‘bhð ^bÞ\u0002^V½‘bhð ^bÞ\u00020 ð15:30Þ where ^V is the asymptotic variance estimate of ^b. The asymptotic standard error of ^dK is simply the square root of expression (15.30). This calculation allows us to ob- tain a large-sample conﬁdence interval for ^dK. The program Stata does this calcula- tion for the probit model using the dprobit command. If xK is a discrete variable, then we can estimate the change in the predicted prob- ability in going from cK to cK þ 1 as ^dK ¼ G½ ^b1 þ ^b2x2 þ \u0003 \u0003 \u0003 þ ^bK\u00011xK\u00011 þ ^bKðcK þ 1Þ\u0002 \u0001 Gð ^b1 þ ^b2x2 þ \u0003 \u0003 \u0003 þ ^bK\u00011xK\u00011 þ ^bKcKÞ ð15:31Þ In particular, when xK is a binary variable, set cK ¼ 0. Of course, the other xj’s can be evaluated anywhere, but the use of sample averages is typical. The delta method can be used to obtain a standard error of equation (15.31). For probit, Stata does this calculation when xK is a binary variable. Usually the calculations ignore the fact that xj is an estimate of EðxjÞ in applying the delta method. If we are truly interested in bKgðmxbÞ, the estimation error in x can be accounted for, but it makes the calculation more complicated, and it is unlikely to have a large e¤ect. An alternative way to summarize the estimated marginal e¤ects is to estimate the average value of bKgðxbÞ across the population, or bKE½gðxbÞ\u0002. A consistent estima- tor is ^bK N\u00011 X N i¼1 gðxi ^bÞ \" # ð15:32Þ when xK is continuous or N\u00011 X N i¼1 ½Gð ^b1 þ ^b2xi2 þ \u0003 \u0003 \u0003 þ ^bK\u00011xi;K\u00011 þ ^bKÞ \u0001 Gð ^b1 þ ^b2xi2 þ \u0003 \u0003 \u0003 þ ^bK\u00011xi;K\u00011Þ\u0002 ð15:33Þ if xK is binary. The delta method can be used to obtain an asymptotic standard error of expression (15.32) or (15.33). Costa (1995) is a recent example of average e¤ects obtained from expression (15.33). Discrete Response Models 467", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 477, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p478::c0", "text": "Example 15.2 (Married Women’s Labor Force Participation): We now estimate logit and probit models for women’s labor force participation. For comparison we report the linear probability estimates. The results, with standard errors in parenthe- ses, are given in Table 15.1 (for the LPM, these are heteroskedasticity-robust). The estimates from the three models tell a consistent story. The signs of the co- e‰cients are the same across models, and the same variables are statistically signiﬁ- cant in each model. The pseudo R-squared for the LPM is just the usual R-squared reported for OLS; for logit and probit the pseudo R-squared is the measure based on the log likelihoods described previously. In terms of overall percent correctly pre- dicted, the models do equally well. For the probit model, it correctly predicts ‘‘out of the labor force’’ about 63.1 percent of the time, and it correctly predicts ‘‘in the labor force’’ about 81.3 percent of the time. The LPM has the same overall percent cor- rectly predicted, but there are slight di¤erences within each outcome. As we emphasized earlier, the magnitudes of the coe‰cients are not directly com- parable across the models. Using the rough rule of thumb discussed earlier, we can Table 15.1 LPM, Logit, and Probit Estimates of Labor Force Participation Dependent Variable: inlf Independent Variable LPM (OLS) Logit (MLE) Probit (MLE) nwifeinc \u0001.0034 (.0015) \u0001.021 (.008) \u0001.012 (.005) educ .038 (.007) .221 (.043) .131 (.025) exper .039 (.006) .206 (.032) .123 (.019) exper2 \u0001.00060 (.00019) \u0001.0032 (.0010) \u0001.0019 (.0006) age \u0001.016 (.002) \u0001.088 (.015) \u0001.053 (.008) kidslt6 \u0001.262 (.032) \u00011.443 (0.204) \u0001.868 (.119) kidsge6 .013 (.013) .060 (.075) .036 (.043) constant .586 (.151) .425 (.860) .270 (.509) Number of observations 753 753 753 Percent correctly predicted 73.4 73.6 73.4 Log-likelihood value — \u0001401.77 \u0001401.30 Pseudo R-squared .264 .220 .221 Chapter 15 468", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 478, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p479::c0", "text": "divide the logit estimates by four and the probit estimates by 2.5 to make all estimates comparable to the LPM estimates. For example, for the coe‰cients on kidslt6, the scaled logit estimate is about \u0001.361, and the scaled probit estimate is about \u0001.347. These are larger in magnitude than the LPM estimate (for reasons we will soon dis- cuss). The scaled coe‰cient on educ is .055 for logit and .052 for probit. If we evaluate the standard normal probability density function, fð ^b0 þ ^b1x1 þ \u0003 \u0003 \u0003 þ ^bkxkÞ, at the average values of the independent variables in the sample (including the average of exper2), we obtain about .391; this value is close enough to .4 to make the rough rule of thumb for scaling the probit coe‰cients useful in obtaining the e¤ects on the response probability. In other words, to estimate the change in the re- sponse probability given a one-unit increase in any independent variable, we multiply the corresponding probit coe‰cient by .4. The biggest di¤erence between the LPM model on one hand, and the logit and probit models on the other, is that the LPM assumes constant marginal e¤ects for educ, kidslt6, and so on, while the logit and probit models imply diminishing mar- ginal magnitudes of the partial e¤ects. In the LPM, one more small child is estimated to reduce the probability of labor force participation by about .262, regardless of how many young children the woman already has (and regardless of the levels of the other dependent variables). We can contrast this ﬁnding with the estimated marginal e¤ect from probit. For concreteness, take a woman with nwifeinc ¼ 20:13, educ ¼ 12:3, exper ¼ 10:6, age ¼ 42:5—which are roughly the sample averages—and kidsge6 ¼ 1. What is the estimated fall in the probability of working in going from zero to one small child? We evaluate the standard normal cdf, Fð ^b0 þ ^b1x1 þ \u0003 \u0003 \u0003 þ ^bkxkÞ with kidslt6 ¼ 1 and kidslt6 ¼ 0, and the other independent variables set at the values given. We get roughly :373 \u0001 :707 ¼ \u0001:334, which means that the labor force par- ticipation probability is about .334 lower when a woman has one young child. This is not much di¤erent from the scaled probit coe‰cient of \u0001.347. If the woman goes from one to two young children, the probability falls even more, but the marginal e¤ect is not as large: :117 \u0001 :373 ¼ \u0001:256. Interestingly, the estimate from the linear probability model, which we think can provide a good estimate near the average values of the covariates, is in fact between the probit estimated partial e¤ects starting from zero and one children. Binary response models apply with little modiﬁcation to independently pooled cross sections or to other data sets where the observations are independent but not necessarily identically distributed. Often year or other time-period dummy variables are included to account for aggregate time e¤ects. Just as with linear models, probit can be used to evaluate the impact of certain policies in the context of a natural ex- periment; see Problem 15.13. An application is given in Gruber and Poterba (1994). Discrete Response Models 469", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 479, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p480::c0", "text": "15.7 Speciﬁcation Issues in Binary Response Models We now turn to several issues that can arise in applying binary response models to economic data. All of these topics are relevant for general index models, but features of the normal distribution allow us to obtain concrete results in the context of probit models. Therefore, our primary focus is on probit models. 15.7.1 Neglected Heterogeneity We begin by studying the consequences of omitting variables when those omitted variables are independent of the included explanatory variables. This is also called the neglected heterogeneity problem. The (structural) model of interest is Pðy ¼ 1 j x; cÞ ¼ Fðxb þ gcÞ ð15:34Þ where x is 1 \u0005 K with x1 1 1 and c is a scalar. We are interested in the partial e¤ects of the xj on the probability of success, holding c (and the other elements of x) ﬁxed. We can write equation (15.34) in latent variable form as y\u0004 ¼ xb þ gc þ e, where y ¼ 1½y\u0004 > 0\u0002 and e j x; c @ Normalð0; 1Þ. Because x1 ¼ 1, EðcÞ ¼ 0 without loss of generality. Now suppose that c is independent of x and c @ Normalð0; t2Þ. [Remember, this assumption is much stronger than Covðx; cÞ ¼ 0 or even Eðc j xÞ ¼ 0: under indepen- dence, the distribution of c given x does not depend on x.] Given these assumptions, the composite term, gc þ e, is independent of x and has a Normalð0; g2t2 þ 1Þ dis- tribution. Therefore, Pðy ¼ 1 j xÞ ¼ Pðgc þ e > \u0001xb j xÞ ¼ Fðxb=sÞ ð15:35Þ where s2 1 g2t2 þ 1. It follows immediately from equation (15.35) that probit of y on x consistently estimates b=s. In other words, if ^b is the estimator from a probit of y on x, then plim ^bj ¼ bj=s. Because s ¼ ðg2t2 þ 1Þ1=2 > 1 (unless g ¼ 0 or t2 ¼ 0Þ, jbj=sj < jbjj. The attenuation bias in estimating bj in the presence of neglected heterogeneity has prompted statements of the following kind: ‘‘In probit analysis, neglected heteroge- neity is a much more serious problem than in linear models because, even if the omitted heterogeneity is independent of x, the probit coe‰cients are inconsistent.’’ We just derived that probit of y on x consistently estimates b=s rather than b, so the statement is technically correct. However, we should remember that, in nonlinear models, we usually want to estimate partial e¤ects and not just parameters. For the purposes of obtaining the directions of the e¤ects or the relative e¤ects of the ex- planatory variables, estimating b=s is just as good as estimating b. Chapter 15 470", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 480, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p481::c0", "text": "For continuous xj, we would like to estimate qPðy ¼ 1 j x; cÞ=qxj ¼ bjfðxb þ gcÞ ð15:36Þ for various values of x and c. Because c is not observed, we cannot estimate g. Even if we could estimate g, c almost never has meaningful units of measurement—for example, c might be ‘‘ability,’’ ‘‘health,’’ or ‘‘taste for saving’’—so it is not obvious what values of c we should plug into equation (15.36). Nevertheless, c is normalized so that EðcÞ ¼ 0, so we may be interested in equation (15.36) evaluated at c ¼ 0, which is simply bjfðxbÞ. What we consistently estimate from the probit of y on x is ðbj=sÞfðxb=sÞ ð15:37Þ This expression shows that, if we are interested in the partial e¤ects evaluated at c ¼ 0, then probit of y on x does not do the trick. An interesting fact about expres- sion (15.37) is that, even though bj=s is closer to zero than bj, fðxb=sÞ is larger than fðxbÞ because fðzÞ increases as jzj ! 0, and s > 1. Therefore, for estimating the partial e¤ects in equation (15.36) at c ¼ 0, it is not clear for what values of x an attenuation bias exists. With c having a normal distribution in the population, the partial e¤ect evaluated at c ¼ 0 describes only a small fraction of the population. [Technically, Pðc ¼ 0Þ ¼ 0.] Instead, we can estimate the average partial e¤ect (APE), which we introduced in Section 2.2.5. The APE is obtained, for given x, by averaging equation (15.36) across the distribution of c in the population. For emphasis, let xo be a given value of the explanatory variables (which could be, but need not be, the mean value). When we plug xo into equation (15.36) and take the expected value with respect to the distri- bution of c, we get E½bjfðxob þ gcÞ\u0002 ¼ ðbj=sÞfðxob=sÞ ð15:38Þ In other words, probit of y on x consistently estimates the average partial e¤ects, which is usually what we want. The result in equation (15.38) follows from the general treatment of average partial e¤ects in Section 2.2.5. In the current setup, there are no extra conditioning variables, w, and the unobserved heterogeneity is independent of x. It follows from equation (2.35) that the APE with respect to xj, evaluated at xo, is simply qEðy j xoÞ=qxj. But from the law of iterated expectations, Eðy j xÞ ¼ Ec½Fðxb þ gcÞ\u0002 ¼ Fðxb=sÞ, where Ecð\u0003Þ denotes the expectation with respect to the distribution of c. The derivative of Fðxb=sÞ with respect to xj is ðbj=sÞfðxb=sÞ, which is what we wanted to show. The bottom line is that, except in cases where the magnitudes of the bj in equation (15.34) have some meaning, omitted heterogeneity in probit models is not a problem Discrete Response Models 471", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 481, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p482::c0", "text": "when it is independent of x: ignoring it consistently estimates the average partial e¤ects. Of course, the previous arguments hinge on the normality of c and the probit structural equation. If the structural model (15.34) were, say, logit and if c were normally distributed, we would not get a probit or logit for the distribution of y given x; the response probability is more complicated. The lesson from Section 2.2.5 is that we might as well work directly with models for Pðy ¼ 1 j xÞ because partial e¤ects of Pðy ¼ 1 j xÞ are always the average of the partial e¤ects of Pðy ¼ 1 j x; cÞ over the distribution of c. If c is correlated with x or is otherwise dependent on x [for example, if Varðc j xÞ depends on x], then omission of c is serious. In this case we cannot get consistent estimates of the average partial e¤ects. For example, if c j x @ Normalðxd; h2Þ, then probit of y on x gives consistent estimates of ðb þ gdÞ=r, where r2 ¼ g2h2 þ 1. Un- less g ¼ 0 or d ¼ 0, we do not consistently estimate b=s. This result is not surprising given what we know from the linear case with omitted variables correlated with the xj. We now study what can be done to account for endogenous variables in probit models. 15.7.2 Continuous Endogenous Explanatory Variables We now explicitly allow for the case where one of the explanatory variables is cor- related with the error term in the latent variable model. One possibility is to estimate a linear probability model by 2SLS. This procedure is relatively easy and might pro- vide a good estimate of the average e¤ect. If we want to estimate a probit model with an endogenous explanatory variables, we must make some fairly strong assumptions. In this section we consider the case of a continuous endogenous explanatory variable. Write the model as y\u0004 1 ¼ z1d1 þ a1y2 þ u1 ð15:39Þ y2 ¼ z1d21 þ z2d22 þ v2 ¼ zd2 þ v2 ð15:40Þ y1 ¼ 1½y\u0004 1 > 0\u0002 ð15:41Þ where ðu1; v2Þ has a zero mean, bivariate normal distribution and is independent of z. Equation (15.39), along with equation (15.41), is the structural equation; equa- tion (15.40) is a reduced form for y2, which is endogenous if u1 and v2 are correlated. If u1 and v2 are independent, there is no endogeneity problem. Because v2 is nor- mally distributed, we are assuming that y2 given z is normal; thus y2 should have features of a normal random variable. (For example, y2 should not be a discrete variable.) Chapter 15 472", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 482, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p483::c0", "text": "The model is applicable when y2 is correlated with u1 because of omitted variables or measurement error. It can also be applied to the case where y2 is determined jointly with y1, but with a caveat. If y1 appears on the right-hand side in a linear structural equation for y2, then the reduced form for y2 cannot be found with v2 having the stated properties. However, if y\u0004 1 appears in a linear structural equation for y2, then y2 has the reduced form given by equation (15.40); see Maddala (1983, Chapter 7) for further discussion. The normalization that gives the parameters in equation (15.39) an average partial e¤ect interpretation, at least in the omitted variable and simultaneity contexts, is Varðu1Þ ¼ 1, just as in a probit model with all explanatory variables exogenous. To see this point, consider the outcome on y1 at two di¤erent outcomes of y2, say y2 and y2 þ 1. Holding the observed exogenous factors ﬁxed at z1, and holding u1 ﬁxed, the di¤erence in responses is 1½z1d1 þ a1ðy2 þ 1Þ þ u1 b 0\u0002 \u0001 1½z1d1 þ a1y2 þ u1 b 0\u0002 (This di¤erence can take on the values \u00011, 0, and 1.) Because u1 is unobserved, we cannot estimate the di¤erence in responses for a given population unit. Nevertheless, if we average across the distribution of u1, which is Normalð0; 1Þ, we obtain F½z1d1 þ a1ðy2 þ 1Þ\u0002 \u0001 Fðz1d1 þ a1y2Þ Therefore, d1 and a1 are the parameters appearing in the APE. [Alternatively, if we begin by allowing s2 1 ¼ Varðu1Þ > 0 to be unrestricted, the APE would depend on d1=s1 and a1=s1, and so we should just rescale u1 to have unit variance. The variance and slope parameters are not separately identiﬁed, anyway.] The proper normali- zation for Varðu1Þ should be kept in mind, as two-step procedures, which we cover in the following paragraphs, only consistently estimate d1 and a1 up to scale; we have to do a little more work to obtain estimates of the APE. If y2 is a mismeasured variable, we apparently cannot estimate the APE of interest: we would like to estimate the change in the response probability due to a change in y\u0004 2, but, without further as- sumptions, we can only estimate the e¤ect of changing y2. The most useful two-step approach is due to Rivers and Vuong (1988), as it leads to a simple test for endogeneity of y2. To derive the procedure, ﬁrst note that, under joint normality of ðu1; v2Þ, with Varðu1Þ ¼ 1, we can write u1 ¼ y1v2 þ e1 ð15:42Þ where y1 ¼ h1=t2 2, h1 ¼ Covðv2; u1Þ, t2 2 ¼ Varðv2Þ, and e1 is independent of z and v2 (and therefore of y2). Because of joint normality of ðu1; v2Þ, e1 is also normally Discrete Response Models 473", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 483, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p484::c0", "text": "distributed with Eðe1Þ ¼ 0 and Varðe1Þ ¼ Varðu1Þ \u0001 h2 1=t2 2 ¼ 1 \u0001 r2 1, where r1 ¼ Corrðv2; u1Þ. We can now write y\u0004 1 ¼ z1d1 þ a1y2 þ y1v2 þ e1 ð15:43Þ e1 j z; y2; v2 @ Normalð0; 1 \u0001 r2 1Þ ð15:44Þ A standard calculation shows that Pðy1 ¼ 1 j z; y2; v2Þ ¼ F½ðz1d1 þ a1y2 þ y1v2Þ=ð1 \u0001 r2 1Þ1=2\u0002 Assuming for the moment that we observe v2, then probit of y1 on z1, y2, and v2 con- sistently estimates dr1 1 d1=ð1\u0001 r2 1Þ1=2, ar1 1 a1=ð1\u0001 r2 1Þ1=2, and yr1 1 y1=ð1\u0001 r2 1Þ1=2. Notice that because r2 1 < 1, each scaled coe‰cient is greater than its unscaled coun- terpart unless y2 is exogenous ðr1 ¼ 0Þ. Since we do not know d2, we must ﬁrst estimate it, as in the following procedure: Procedure 15.1: (a) Run the OLS regression y2 on z and save the residuals ^v2. (b) Run the probit y1 on z1, y2, ^v2 to get consistent estimators of the scaled co- e‰cients dr1, ar1, and yr1: A nice feature of Procedure 15.1 is that the usual probit t statistic on ^v2 is a valid test of the null hypothesis that y2 is exogenous, that is, H0: y1 ¼ 0. If y1 0 0, the usual probit standard errors and test statistics are not strictly valid, and we have only estimated d1 and a1 up to scale. The asymptotic variance of the two-step estimator can be derived using the M-estimator results in Section 12.5.2; see also Rivers and Vuong (1988). Under H0: y1 ¼ 0, e1 ¼ u1, and so the distribution of v2 plays no role under the null. Therefore, the test of exogeneity is valid without assuming normality or homo- skedasticity of v2, and it can be applied very broadly, even if y2 is a binary variable. Unfortunately, if y2 and u1 are correlated, normality of v2 is crucial. Example 15.3 (Testing for Exogeneity of Education in the Women’s LFP Model): We test the null hypothesis that educ is exogenous in the married women’s labor force participation equation. We ﬁrst obtain the reduced form residuals, ^v2, from regressing educ on all exogenous variables, including motheduc, fatheduc, and huseduc. Then, we add ^v2 to the probit from Example 15.2. The t statistic on ^v2 is only .867, which is weak evidence against the null hypothesis that educ is exogenous. As always, this conclusion hinges on the assumption that the instruments for educ are themselves exogenous. Even when y1 0 0, it turns out that we can consistently estimate the average partial e¤ects after the two-stage estimation. We simply apply the results from Section 2.2.5. Chapter 15 474", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 484, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p485::c0", "text": "To see how, write y1 ¼ 1½z1d1 þ a1y2 þ u1 > 0\u0002, where, in the notation of Section 2.2.5, q 1 u1, x 1 ðz1; y2Þ, and w 1 v2 (a scalar in this case). Because y1 is a deter- ministic function of ðz1; y2; u1Þ, v2 is trivially redundant in Eðy1 j z1; y2; u1Þ, and so equation (2.34) holds. Further, as we have already used, u1 given ðz1; y2; v2Þ is inde- pendent of ðz1; y2Þ, and so equation (2.33) holds as well. It follows from Section 2.2.5 that the APEs are obtained by taking derivatives (or di¤erences) of Ev2½Fðz1dr1 þ ar1 y2 þ yr1v2Þ\u0002 ð15:45Þ where we still use the r subscript to denote the scaled coe‰cients. But we computed exactly this kind of expectation in Section 15.7.1. The same reasoning gives Ev2½Fðz1dr1 þ ar1 y2 þ yr1v2Þ\u0002 ¼ Fðz1dy1 þ ay1 y2Þ where dy1 1 dr1=ðy2 r1t2 2 þ 1Þ1=2 and ay1 1 ar1=ðy2 r1t2 2 þ 1Þ1=2, where t2 2 ¼ Varðv2Þ. Therefore, for any ðz1; y2Þ, a consistent estimator of expression (15.45) is Fðz1 ^dy1 þ ^ay1 y2Þ ð15:46Þ where ^dy1 1 ^dr1=ð^y2 r1^t2 2 þ 1Þ1=2 and ^ay1 1 ^ar1=ð^y2 r1^t2 2 þ 1Þ1=2. Note that ^t2 2 is the usual error variance estimator from the ﬁrst-stage regression of y2 on z. Expression (15.46) implies a very simple way to obtain the estimated APEs after the second-stage probit. We simply divide each coe‰cient by the factor ð^y2 r1^t2 2 þ 1Þ1=2 before computing derivatives or di¤erences with respect to the elements of ðz1; y2Þ. Unfortunately, be- cause the APEs depend on the parameters in a complicated way—and the asymptotic variance of ð^d0 r1; ^ar1; ^yr1Þ0 is already complicated because of the two-step estimation— standard errors for the APEs would be very di‰cult to come by using the delta method. An alternative method for estimating the APEs does not exploit the normality assumption for v2. By the usual uniform weak law of large numbers argument—see Lemma 12.1—a consistent estimator of expression (15.45) for any ðz1; y2Þ is obtained by replacing unknown parameters by consistent estimators: N\u00011 X N i¼1 Fðz1 ^dr1 þ ^ar1 y2 þ ^yr1^vi2Þ ð15:47Þ where the ^vi2 are the ﬁrst-stage OLS residuals from regressing yi2 on zi, i ¼ 1; . . . ; N. This approach provides a di¤erent strategy for estimating APEs: simply compute partial e¤ects with respect to z1 and y2 after the second-stage estimation, but then average these across the ^vi2 in the sample. Rather than use a two-step procedure, we can estimate equations (15.39)–(15.41) by conditional maximum likelihood. To obtain the joint distribution of ðy1; y2Þ, Discrete Response Models 475", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 485, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p486::c0", "text": "conditional on z, recall that f ðy1; y2 j zÞ ¼ f ðy1 j y2; zÞf ðy2 j zÞ ð15:48Þ (see Property CD.2 in Appendix 13A). Since y2 j z @ Normalðzd2; t2 2Þ, the density f ðy2 j zÞ is easy to write down. We can also derive the conditional density of y1 given ðy2; zÞ. Since v2 ¼ y2 \u0001 zd2 and y1 ¼ 1½y\u0004 1 > 0\u0002, Pðy1 ¼ 1 j y2; zÞ ¼ F z1d1 þ a1y2 þ ðr1=t2Þðy2 \u0001 zd2Þ ð1 \u0001 r2 1Þ1=2 \" # ð15:49Þ where we have used the fact that y1 ¼ r1=t2. Let w denote the term in inside Fð\u0003Þ in equation (15.49). Then we have derived f ðy1; y2 j zÞ ¼ fFðwÞgy1f1 \u0001 FðwÞg1\u0001y1ð1=t2Þf½ðy2 \u0001 zd2Þ=t2\u0002 and so the log likelihood for observation i (apart from terms not depending on the parameters) is yi1 log FðwiÞ þ ð1 \u0001 yi1Þ log½1 \u0001 FðwiÞ\u0002 \u0001 1 2 logðt2 2Þ \u0001 1 2 ðyi2 \u0001 zid2Þ2=t2 2 ð15:50Þ where we understand that wi depends on the parameters ðd1; a1; r1; d2; t2Þ: wi 1 ½zi1d1 þ a1yi2 þ ðr1=t2Þðyi2 \u0001 zid2Þ\u0002=ð1 \u0001 r2 1Þ1=2 Summing expression (15.50) across all i and maximizing with respect to all param- eters gives the MLEs of d1, a1, r1, d2, t2 2. The general theory of conditional MLE applies, and so standard errors can be obtained using the estimated Hessian, the estimated expected Hessian, or the outer product of the score. Maximum likelihood estimation has some decided advantages over two-step pro- cedures. First, MLE is more e‰cient than any two-step procedure. Second, we get direct estimates of d1 and a1, the parameters of interest for computing partial e¤ects. Evans, Oates, and Schwab (1992) study peer e¤ects on teenage behavior using the full MLE. Testing that y2 is exogenous is easy once the MLE has been obtained: just test H0: r1 ¼ 0 using an asymptotic t test. We could also use a likelihood ratio test. The drawback with the MLE is computational. Sometimes it can be di‰cult to get the iterations to converge, as ^r1 sometimes tends toward 1 or \u00011. Comparing the Rivers-Vuong approach to the MLE shows that the former is a limited information procedure. Essentially, Rivers and Vuong focus on f ðy1 j y2; zÞ, where they replace the unknown d2 with the OLS estimator ^d2 (and they ignore the rescaling problem by taking e1 in equation (15.43) to have unit variance). MLE esti- Chapter 15 476", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 486, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p487::c0", "text": "mates the parameters using the information in f ðy1 j y2; zÞ and f ðy2 j zÞ simulta- neously. For the initial test of whether y2 is exogenous, the Rivers-Vuong approach has signiﬁcant computational advantages. If exogeneity is rejected, it is probably worth doing MLE. Another beneﬁt of the maximum likelihood approach for this and related problems is that it forces discipline on us in coming up with consistent estimation procedures and correct standard errors. It is easy to abuse two-step procedures if we are not careful in deriving estimating equations. With MLE, although it can be di‰cult to derive joint distributions of the endogenous variables given the exogenous variables, we know that, if the underlying distributional assumptions hold, consistent and e‰- cient estimators are obtained. 15.7.3 A Binary Endogenous Explanatory Variable We now consider the case where the probit model contains a binary explanatory variable that is endogenous. The model is y1 ¼ 1½z1d1 þ a1y2 þ u1 > 0\u0002 ð15:51Þ y2 ¼ 1½zd2 þ v2 > 0\u0002 ð15:52Þ where ðu1; v2Þ is independent of z and distributed as bivariate normal with mean zero, each has unit variance, and r1 ¼ Corrðu1; v2Þ. If r1 0 0, then u1 and y2 are corre- lated, and probit estimation of equation (15.51) is inconsistent for d1 and a1. As discussed in Section 15.7.2, the normalization Varðu1Þ ¼ 1 is the proper one for computing average partial e¤ects. Often, the e¤ect of y2 is of primary interest, espe- cially when y2 indicates participation in some sort of program, such as job training, and the binary outcome y1 might denote employment status. The average treatment e¤ect (for a given value of z1) is Fðz1d1 þ a1Þ \u0001 Fðz1d1Þ. To derive the likelihood function, we again need the joint distribution of ðy1; y2Þ given z, which we obtain from equation (15.48). To obtain Pðy1 ¼ 1 j y2; zÞ, ﬁrst note that Pðy1 ¼ 1 j v2; zÞ ¼ F½ðz1d1 þ a1y2 þ r1v2Þ=ð1 \u0001 r2 1Þ1=2\u0002 ð15:53Þ Since y2 ¼ 1 if and only if v2 > \u0001zd2, we need a basic fact about truncated normal distributions: If v2 has a standard normal distribution and is independent of z, then the density of v2 given v2 > \u0001zd2 is fðv2Þ=Pðv2 > \u0001zd2Þ ¼ fðv2Þ=Fðzd2Þ ð15:54Þ Therefore, Discrete Response Models 477", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 487, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p488::c0", "text": "Pðy1 ¼ 1 j y2 ¼ 1; zÞ ¼ E½Pðy1 ¼ 1 j v2; zÞ j y2 ¼ 1; z\u0002 ¼ EfF½ðz1d1 þ a1y2 þ r1v2Þ=ð1 \u0001 r2 1Þ1=2\u0002 j y2 ¼ 1; zg ¼ 1 Fðzd2Þ ðy \u0001zd2 F½ðz1d1 þ a1y2 þ r1v2Þ=ð1 \u0001 r2 1Þ1=2\u0002fðv2Þ dv2 ð15:55Þ where v2 in the integral is a dummy argument of integration. Of course Pðy1 ¼ 0 j y2 ¼ 1; zÞ is just one minus equation (15.55). Similarly, Pðy1 ¼ 1 j y2 ¼ 0; zÞ is 1 1 \u0001 Fðzd2Þ ð\u0001zd2 \u0001y F½ðz1d1 þ a1y2 þ r1v2Þ=ð1 \u0001 r2 1Þ1=2\u0002fðv2Þ dv2 ð15:56Þ Combining the four possible outcomes of ðy1; y2Þ, along with the probit model for y2, and taking the log gives the log-likelihood function for maximum likelihood analysis. It is messy but certainly doable. Evans and Schwab (1995) use the MLE approach to study the causal e¤ects of attending a Catholic high school on the probability of attending college, allowing the Catholic high school indicator to be correlated with unobserved factors that a¤ect college attendence. As an IV they use a binary variable indicating whether a student is Catholic. Because the MLE is nontrivial to compute, it is tempting to use some seemingly ‘‘obvious’’ two-step procedures. As an example, we might try to inappropriately mimic 2SLS. Since Eðy2 j zÞ ¼ Fðzd2Þ and d2 is consistently estimated by probit of y2 on z, it is tempting to estimate d1 and a1 from the probit of y1 on z, ^F2, where ^F2 1 Fðz^d2Þ. This approach does not produce consistent estimators, for the same reasons the forbidden regression discussed in Section 9.5 for nonlinear simultaneous equa- tions models does not. For this two-step procedure to work, we would have to have Pðy1 ¼ 1 j zÞ ¼ F½z1d1 þ a1Fðzd2Þ\u0002. But Pðy1 ¼ 1 j zÞ ¼ Eðy1 j zÞ ¼ Eð1½z1d1 þ a1y2 þ u1 > 0\u0002 j zÞ, and since the indicator function 1½\u0003\u0002 is nonlinear, we cannot pass the expected value through. If we were to compute the correct (complicated) formula for Pðy1 ¼ 1 j zÞ, plug in ^d2, and then maximize the resulting binary response log likeli- hood, then the two-step approach would produce consistent estimators. But full maximum likelihood is easier and more e‰cient. As mentioned in the previous subsection, we can use the Rivers-Vuong approach to test for exogeneity of y2. This has the virtue of being simple, and, if the test fails to reject, we may not need to compute the MLE. A more e‰cient test is the score test of H0: r1 ¼ 0, and this does not require estimation of the full MLE. Chapter 15 478", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 488, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p489::c0", "text": "15.7.4 Heteroskedasticity and Nonnormality in the Latent Variable Model In applying the probit model it is easy to become confused about the problems of heteroskedasticity and nonnormality. The confusion stems from a failure to distin- guish between the underlying latent variable formulation, as in the model (15.9), and the response probability in equation (15.8). As we have emphasized throughout this chapter, for most purposes we want to estimate Pðy ¼ 1 j xÞ. The latent variable for- mulation is convenient for certain manipulations, but we are rarely interested in Eðy\u0004 j xÞ. [One case in which Eðy\u0004 j xÞ is of interest is covered in Problem 15.16.] Once we focus on Pðy ¼ 1 j xÞ, we can easily see why we should not attempt to compare heteroskedasticity in the latent variable model (15.9) with the consequences of heteroskedasticity in a standard linear regression model. Heteroskedasticity in Varðe j xÞ entirely changes the functional form for Pðy ¼ 1 j xÞ ¼ Eðy j xÞ. While the statement ‘‘probit will be inconsistent for b when e is heteroskedastic’’ is correct, it largely misses the point. In most probit applications, it makes little sense to care about consistent estimation of b when Pðy ¼ 1 j xÞ 0 FðxbÞ. (Section 15.7.5 contains a di¤erent perspective.) It is easy to construct examples where the partial e¤ect of a variable on Pðy ¼ 1 j xÞ has the sign opposite to that of its coe‰cient in the latent variable formulation. For example, let x1 be a positive, continuous variable, and write the latent variable model as y\u0004 ¼ b0 þ b1x1 þ e, e j x1 @ Normalð0; x2 1Þ. The binary response is deﬁned as y ¼ 1½y\u0004 > 0\u0002. A simple calculation shows that Pðy ¼ 1 j x1Þ ¼ Fðb0=x1 þ b1Þ, and so qPðy ¼ 1 j x1Þ=qx1 ¼ \u0001ðb0=x2 1Þfðb0=x1 þ b1Þ. If b0 > 0 and b1 > 0, then qPðy ¼ 1 j x1Þ=qx1 and b1 have opposite signs. The problem is fairly clear: while the latent variable model has a conditional mean that is linear in x1, the response prob- ability depends on 1=x1. If the latent variable model is correct, we should just do probit of y on 1 and 1=x1. Nonnormality in the latent error e means that GðzÞ 0 FðzÞ, and therefore Pðy ¼ 1 j xÞ 0 FðxbÞ. Again, this is a functional form problem in the response probability, and it should be treated as such. As an example, suppose that the true model is logit, but we estimate probit. We are not going to consistently estimate b in Pðy ¼ 1 j xÞ ¼ LðxbÞ—in fact, Table 15.1 shows that the logit estimates are generally much larger (roughly 1.6 times as large)—because of the di¤erent scalings inherent in the probit and logit functions. But inconsistent estimation of b is practically irrele- vant: probit might provide very good estimates of the partial e¤ects, qPðy ¼ 1 j xÞ=qxj, even though logit is the correct model. In Example 15.2, the estimated partial e¤ects are very similar for logit and probit. Discrete Response Models 479", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 489, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p490::c0", "text": "Relaxing distributional assumptions on e in the model (15.9) can be useful for obtaining more ﬂexible functional forms for Pðy ¼ 1 j xÞ, as we saw in equation (15.24). Replacing FðzÞ with some function Gðz; gÞ, where g is a vector of parameters, is a good idea, especially when it nests the standard normal cdf. [Moon (1988) covers some interesting possibilities in the context of logit models, including asymmetric cumulative distribution functions.] But it is important to remember that these are just ways of generalizing functional form, and they may be no better than directly speci- fying a more ﬂexible functional form for the response probability, as in McDonald (1996). When di¤erent functional forms are used, parameter estimates across di¤er- ent models should not be the basis for comparison: in most cases, it makes sense only to compare the estimated response probabilities at various values of x and goodness of ﬁt, such as the values of the log-likelihood function. (For an exception, see Prob- lem 15.16.) 15.7.5 Estimation under Weaker Assumptions Probit, logit, and the extensions of these mentioned in the previous subsection are all parametric models: Pðy ¼ 1 j xÞ depends on a ﬁnite number of parameters. There have been many recent advances in estimation of binary response models that relax parametric assumptions on Pðy ¼ 1 j xÞ. We brieﬂy discuss some of those here. If we are interested in estimating the directions and relative sizes of the partial e¤ects, and not the response probabilities, several approaches are possible. Ruud (1983) obtains conditions under which we can estimate the slope parameters, call these b, up to scale—that is, we can consistently estimate tb for some unknown constant t— even though we misspecify the function Gð\u0003Þ. Ruud (1986) shows how to exploit these results to consistently estimate the slope parameters up to scale fairly generally. An alternative approach is to recognize that we do not know the function Gð\u0003Þ, but the response probability has the index form in equation (15.8). This arises from the latent variable formulation (15.9) when e is independent of x but the distribution of e is not known. There are several semiparametric estimators of the slope parameters, up to scale, that do not require knowledge of G. Under certain restrictions on the function G and the distribution of x, the semiparametric estimators are consistent and ﬃﬃﬃﬃ N p -asymptotically normal. See, for example, Stoker (1986); Powell, Stock, and Stoker (1989); Ichimura (1993); Klein and Spady (1993); and Ai (1997). Powell (1994) contains a recent survey of these methods. Once ^b is obtained, the function G can be consistently estimated (in a sense we cannot make precise here, as G is part of an inﬁnite dimensional space). Thus, the response probabilities, as well as the partial e¤ects on these probabilities, can be consistently estimated for unknown G. Obtaining ^G requires nonparametric regression Chapter 15 480", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 490, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p491::c0", "text": "of yi on xi ^b, where ^b are the scaled slope estimators. Accessible treatments of the methods used are contained in Stoker (1992), Powell (1994), and Ha¨rdle and Linton (1994). Remarkably, it is possible to estimate b up to scale without assuming that e and x are independent in the model (15.9). In the speciﬁcation y ¼ 1½xb þ e > 0\u0002, Manski (1975, 1988) shows how to consistently estimate b, subject to a scaling, under the assumption that the median of e given x is zero. Some mild restrictions are needed on the distribution of x; the most important of these is that at least one element of x with nonzero coe‰cient is essentially continuous. This allows e to have any distribution, and e and x can be dependent; for example, Varðe j xÞ is unrestricted. Manski’s esti- mator, called the maximum score estimator, is a least absolute deviations estimator. Since the median of y given x is 1½xb > 0\u0002, the maximum score estimator solves min b X N i¼1 jyi \u0001 1½xib > 0\u0002j over all b with, say, b 0b ¼ 1, or with some element of b ﬁxed at unity if the corre- sponding xj is known to appear in Medðy j xÞ. fA normalization is needed because if Medðy j xÞ ¼ 1½xb > 0\u0002 then Medðy j xÞ ¼ 1½xðtbÞ > 0\u0002 for any t > 0.g The resulting estimator is consistent—for a recent proof see Newey and McFadden (1994)—but its limiting distribution is nonnormal. In fact, it converges to its limiting distribution at rate N 1=3. Horowitz (1992) proposes a smoothed version of the maximum score esti- mator that converges at a rate close to ﬃﬃﬃﬃ N p . The maximum score estimator’s strength is that it consistently estimates b up to scale in cases where the index model (15.8) does not hold. In a sense, this is also the estimator’s weakness, because it is not intended to deliver estimates of the response probabilities Pðy ¼ 1 j xÞ. In some cases we might only want to know the relative e¤ects of each xj on an underlying utility di¤erence or unobserved willingness to pay ðy\u0004Þ, and the maximum score estimator is well suited for that purpose. However, for most policy purposes we want to know the magnitude of the change in Pðy ¼ 1 j xÞ for a given change in xj. As illustrated by the heteroskedasticity example in the pre- vious subsection, where Varðe j x1Þ ¼ x2 1, it is possible for bj and qPðy ¼ 1 j xÞ=qxj to have opposite signs. More generally, for any variable y, it is possible that xj has a positive e¤ect on Medðy j xÞ but a negative e¤ect on Eðy j xÞ, or vice versa. This possibility raises the issue of what should be the focus, the median or the mean. For binary response, the conditional mean is the response probability. It is also possible to estimate the parameters in a binary response model with endogenous explanatory variables without knowledge of Gð\u0003Þ. Lewbel (1998) con- Discrete Response Models 481", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 491, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p492::c0", "text": "tains some recent results. Apparently, methods for estimating average partial e¤ects with endogenous explanatory variables and unknown Gð\u0003Þ are not yet available. 15.8 Binary Response Models for Panel Data and Cluster Samples When analyzing binary responses in the context of panel data, it is often useful to begin with a linear model with an additive, unobserved e¤ect, and then, just as in Chapters 10 and 11, use the within transformation or ﬁrst di¤erencing to remove the unobserved e¤ect. A linear probability model for binary outcomes has the same problems as in the cross section case. In fact, it is probably less appealing for unobserved e¤ects models, as it implies the unnatural restrictions xitb U ci U 1 \u0001 xitb; t ¼ 1; . . . ; T; on the unobserved e¤ects. In this section we discuss probit and logit models that can incorporate unobserved e¤ects. 15.8.1 Pooled Probit and Logit In Section 13.8 we used a probit model to illustrate partial likelihood methods with panel data. Naturally, we can use logit or any other binary response function as well. Suppose the model is Pðyit ¼ 1 j xitÞ ¼ GðxitbÞ; t ¼ 1; 2; . . . ; T ð15:57Þ where Gð\u0003Þ is a known function taking on values in the open unit interval. As we discussed in Chapter 13, xit can contain a variety of factors, including time dummies, interactions of time dummies with time-constant or time-varying variables, and lagged dependent variables. In specifying the model (15.57) we have not assumed nearly enough to obtain the distribution of yi 1 ðyi1; . . . ; yiTÞ given xi ¼ ðxi1; . . . ; xiTÞ. Nevertheless, we can ob- tain a ﬃﬃﬃﬃ N p -consistent estimator of b by maximizing the partial log-likelihood function X N i¼1 X T t¼1 fyit log GðxitbÞ þ ð1 \u0001 yitÞ log½1 \u0001 GðxitbÞ\u0002g which is simply an exercise in pooled estimation. Without further assumptions, a robust variance matrix estimator is needed to account for serial correlation in the scores across t; see equation (13.53) with ^b in place of ^y and G in place of F. Wald and score statistics can be computed as in Chapter 12. In the case that the model (15.57) is dynamically complete, that is, Pðyit ¼ 1 j xit; yi;t\u00011; xi;t\u00011; . . .Þ ¼ Pðyit ¼ 1 j xitÞ ð15:58Þ Chapter 15 482", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 492, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p493::c0", "text": "inference is considerably easier: all the usual statistics from a probit or logit that pools observations and treats the sample as a long independent cross section of size NT are valid, including likelihood ratio statistics. Remember, we are deﬁnitely not assuming independence across t (for example, xit can contain lagged dependent vari- ables). Dynamic completeness implies that the scores are serially uncorrelated across t, which is the key condition for the standard inference procedures to be valid. (See the general treatment in Section 13.8.) To test for dynamic completeness, we can always add a lagged dependent variable and possibly lagged explanatory variables. As an alternative, we can derive a simple one-degree-of-freedom test that works regardless of what is in xit. For concreteness, we focus on the probit case; other index models are handled similarly. Deﬁne uit 1 yit \u0001 FðxitbÞ, so that, under assumption (15.58), Eðuit j xit; yi;t\u00011; xi;t\u00011; . . .Þ ¼ 0, all t. It follows that uit is uncorrelated with any function of the variables ðxit; yi;t\u00011; xi;t\u00011; . . .Þ, including ui;t\u00011. By studying equation (13.53), we can see that it is serial correlation in the uit that makes the usual inference procedures invalid. Let ^uit ¼ yit \u0001 Fðxit ^bÞ. Then a simple test is available by using pooled probit to estimate the artiﬁcial model ‘‘Pðyit ¼ 1 j xit; ^ui;t\u00011Þ ¼ Fðxitb þ g1^ui;t\u00011Þ’’ ð15:59Þ using time periods t ¼ 2; . . . ; T. The null hypothesis is H0: g1 ¼ 0. If H0 is rejected, then so is assumption (15.58). This is a case where under the null hypothesis, the es- timation of b required to obtain ^ui;t\u00011 does not a¤ect the limiting distribution of any of the usual test statistics, Wald, LR, or LM, of H0: g1 ¼ 0. The Wald statistic, that is, the t statistic on ^g1, is the easiest to obtain. For the LM and LR statistics we must be sure to drop the ﬁrst time period in estimating the restricted model ðg1 ¼ 0Þ. 15.8.2 Unobserved E¤ects Probit Models under Strict Exogeneity A popular model for binary outcomes with panel data is the unobserved e¤ects probit model. The main assumption of this model is Pðyit ¼ 1 j xi; ciÞ ¼ Pðyit ¼ 1 j xit; ciÞ ¼ Fðxitb þ ciÞ; t ¼ 1; . . . ; T ð15:60Þ where ci is the unobserved e¤ect and xi contains xit for all t. The ﬁrst equality says that xit is strictly exogenous conditional on ci: once ci is conditioned on, only xit appears in the response probability at time t. This rules out lagged dependent vari- ables in xit, as well as certain kinds of explanatory variables whose future move- ments depend on current and past outcomes on y. (Strict exogeneity also requires that we have enough lags of explanatory variables if there are distributed lag e¤ects.) The Discrete Response Models 483", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 493, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p494::c0", "text": "second equality is the standard probit assumption, with ci appearing additively in the index inside Fð\u0003Þ. Many analyses are not convincing unless xit contains a full set of time dummies. In addition to assumption (15.60), a standard assumption is that the outcomes are independent conditional on ðxi; ciÞ: yi1; . . . ; yiT are independent conditional on ðxi; ciÞ ð15:61Þ Because of the presence of ci, the yit are dependent across t conditional only on the observables, xi. [Assumption (15.61) is analogous to the linear model assumption that, conditional on ðxi; ciÞ, the yit are serially uncorrelated; see Assumption FE.3 in Chapter 10.] Under assumptions (15.60) and (15.61), we can derive the density of ðyi1; . . . ; yiTÞ conditional on ðxi; ciÞ: f ðy1; . . . ; yT j xi; ci; bÞ ¼ Y T t¼1 f ðyt j xit; ci; bÞ ð15:62Þ where f ðyt j xt; c; bÞ ¼ Fðxtb þ cÞyt½1 \u0001 Fðxtb þ cÞ\u00021\u0001yt. Ideally, we could estimate the quantities of interest without restricting the relationship between ci and the xit. In this spirit, a ﬁxed e¤ects probit analysis treats the ci as parameters to be estimated along with b, as this treatment obviates the need to make assumptions about the distribution of ci given xi. The log-likelihood function is PN i¼1 liðci; bÞ, where liðci; bÞ is the log of equation (15.62) evaluated at the yit. Unfortunately, in addition to being computationally di‰cult, estimation of the ci along with b introduces an incidental parameters problem. Unlike in the linear case, where estimating the ci along with b leads to the ﬃﬃﬃﬃ N p -consistent ﬁxed e¤ects estimator of b, in the present case estimating the ci (N of them) along with b leads to inconsistent estimation of b with T ﬁxed and N ! y. We discuss the incidental parameters problem in more detail for the unob- served e¤ects logit model in Section 15.8.3. The name ‘‘ﬁxed e¤ects probit’’ to describe the model where the ci are ﬁxed parameters is somewhat unfortunate. As we saw with linear models, and, as we will see with the logit model in the next subsection and with count data models in Chapter 19, in some cases we can consistently estimate the parameters b without specifying a distribution for ci given xi. This ability is the hallmark of a ﬁxed e¤ects analysis for most microeconometric applications. By contrast, treating the ci as parameters to estimate can lead to potentially serious biases. Here we follow the same approach adopted for linear models: we always treat ci as an unobservable random variable drawn along with ðxi; yiÞ. The question is, Under what additional assumptions can we consistently estimate parameters, as Chapter 15 484", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 494, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p495::c0", "text": "well as interesting partial e¤ects? Unfortunately, for the unobserved e¤ects probit model, we must make an assumption about the relationship between ci and xi. The traditional random e¤ects probit model adds, to assumptions (15.60) and (15.61), the assumption ci j xi @ Normalð0; s2 c Þ ð15:63Þ This is a strong assumption, as it implies that ci and xi are independent and that ci has a normal distribution. It is not enough to assume that ci and xi are uncorrelated, or even that Eðci j xiÞ ¼ 0. The assumption EðciÞ ¼ 0 is without loss of generality provided xit contains an intercept, as it always should. Before we discuss estimation of the random e¤ects probit model, we should be sure we know what we want to estimate. As in Section 15.7.1, consistent estimation of b means that we can consistently estimate the partial e¤ects of the elements of xt on the response probability Pðyt ¼ 1 j xt; cÞ at the average value of c in the population, c ¼ 0. (We can also estimate the relative e¤ects of any two elements of xt for any value of c, as the relative e¤ects do not depend on c.) For the reasons discussed in Section 15.7.1, average partial e¤ects (APEs) are at least as useful. Since ci @ Normalð0; s2 c Þ, the APE for a continuous xtj is ½bj=ð1 þ s2 c Þ1=2\u0002f½xtb=ð1 þ s2 c Þ1=2\u0002, just as in equation (15.38). Therefore, we only need to estimate bc 1 b=ð1 þ s2 c Þ1=2 to estimate the APEs, for either continuous or discrete explanatory variables. (In other branches of applied statistics, such as biostatistics and education, the coef- ﬁcients indexing the APEs—bc in our notation—are called the population-averaged parameters.) Under assumptions (15.60), (15.61), and (15.63), a conditional maximum likeli- hood approach is available for estimating b and s2 c . This is a special case of the approach in Section 13.9. Because the ci are not observed, they cannot appear in the likelihood function. Instead, we ﬁnd the joint distribution of ðyi1; . . . ; yiTÞ condi- tional on xi, a step that requires us to integrate out ci. Since ci has a Normalð0; s2 c Þ distribution, f ðy1; . . . ; yT j xi; yÞ ¼ ðy \u0001y Y T t¼1 f ðyt j xit; c; bÞ \" # ð1=scÞfðc=scÞ dc ð15:64Þ where f ðyt j xt; c; bÞ ¼ Fðxtb þ cÞyt½1 \u0001 Fðxtb þ cÞ\u00021\u0001yt and y contains b and s2 c . Plugging in yit for all t and taking the log of equation (15.64) gives the conditional log likelihood liðyÞ for each i. The log-likelihood function for the entire sample of size N can be maximized with respect to b and s2 c (or b and sc) to obtain ﬃﬃﬃﬃ N p -consistent asymptotically normal estimators; Butler and Mo‰tt (1982) describe a procedure for Discrete Response Models 485", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 495, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p496::c0", "text": "approximating the integral in equation (15.64). The conditional MLE in this context is typically called the random e¤ects probit estimator, and the theory in Section 13.9 can be applied directly to obtain asymptotic standard errors and test statistics. Since b and s2 c can be estimated, the partial e¤ects at c ¼ 0 as well as the average partial e¤ects can be estimated. Since the variance of the idiosyncratic error in the latent variable model is unity, the relative importance of the unobserved e¤ect is measured as r ¼ s2 c =ðs2 c þ 1Þ, which is also the correlation between the composite latent error, say, ci þ eit, across any two time periods. Many random e¤ects probit routines report ^r and its standard error; these statistics lead to an easy test for the presence of the unobserved e¤ect. Assumptions (15.61) and (15.63) are fairly strong, and it is possible to relax them. First consider relaxing assumption (15.61). One useful observation is that, under assumptions (15.60) and (15.63) only, Pðyit ¼ 1 j xiÞ ¼ Pðyit ¼ 1 j xitÞ ¼ FðxitbcÞ ð15:65Þ where bc ¼ b=ð1 þ s2 c Þ1=2. Therefore, just as in Section 15.8.1, we can estimate bc from pooled probit of yit on xit, t ¼ 1; . . . ; T, i ¼ 1; . . . ; N, meaning that we directly estimate the average partial e¤ects. If ci is truly present, fyit: t ¼ 1; . . . ; Tg will not be independent conditional on xi. Robust inference is needed to account for the serial dependence, as discussed in Section 15.8.1. An alternative to simply calculating robust standard errors for ^bc after pooled probit, or using the full random e¤ects assumptions and obtaining the MLE, is what is called the generalized estimating equations (GEE) approach (see Zeger, Liang, and Albert, 1988). In the GEE approach to unobserved e¤ects binary response models, the response probabilities are speciﬁed conditional only on xi, with the result that we have Eðyit j xitÞ ¼ Eðyit j xiÞ for all t. [Unfortunately, the model is then called a population-averaged model. As we just saw, we can estimate the population-averaged parameters, or APEs, even if we have used random e¤ects probit estimation. It is best to think of assumption (15.60) as the unobserved e¤ects model of interest. The rele- vant question is, What e¤ects are we interested in, and how can we consistently esti- mate them under various assumptions? We are usually interested in averaging across the distribution of ci, but the basic model has not changed.] Next, a model for the T \u0005 T matrix Wi 1 Varðyi j xiÞ is speciﬁed, and this depends on b as well as some additional parameters, say d. After estimating b and d in a ﬁrst step, ^Wi is obtained, and the GEE estimate of b is a multivariate weighted nonlinear least squares (MWNLS) estimator. (See Problem 12.11.) As the model for Varðyi j xiÞ is often chosen for convenience, it can be misspeciﬁed. The MWNLS estimator is still con- Chapter 15 486", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 496, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p497::c0", "text": "sistent and asymptotically normal because Eðyi j xiÞ is correctly speciﬁed under assumptions (15.60) and (15.63)—we do not need assumption (15.61)—but a robust variance matrix is needed (see Problem 12.11). Even with a misspeciﬁed variance function, the MWNLS estimator is likely to be more e‰cient than pooled probit but less e‰cient than the random e¤ects MLE under the full set of random e¤ects assumptions. Another way to relax assumption (15.61) is to assume a particular correlation structure and then use full conditional maximum likelihood. For example, for each t write the latent variable model as y\u0004 it ¼ xitb þ ci þ eit; yit ¼ 1½y\u0004 it > 0\u0002 ð15:66Þ and assume that the T \u0005 1 vector ei is multivariate normal, with unit variances, but unrestricted correlation matrix. This assumption, along with assumptions (15.60) and (15.63), fully characterizes the distribution of yi given xi. However, even for moderate T the computation of the CMLE can be very di‰cult. Recent advances in simulation methods of estimation make it possible to estimate such models for fairly large T; see, for example, Keane (1993) and Geweke and Keane (in press). The pooled probit procedure that we have described is valid for estimating bc, the vector that indexes the average partial e¤ects, regardless of the serial dependence in feitg, but it is ine‰- cient relative to the full CMLE. As in the linear case, in many applications the point of introducing the unobserved e¤ect, ci, is to explicitly allow unobservables to be correlated with some elements of xit. Chamberlain (1980) allowed for correlation between ci and xi by assuming a conditional normal distribution with linear expectation and constant variance. A Mundlak (1978) version of Chamberlain’s assumption is ci j xi @ Normalðc þ xix; s2 a Þ ð15:67Þ where xi is the average of xit, t ¼ 1; . . . ; T and s2 a is the variance of ai in the equation ci ¼ c þ xix þ ai. (In other words, s2 a is the conditional variance of ci, which is assumed not to depend on xi.) Chamberlain (1980) allowed more generality by having xi, the vector of all explanatory variables across all time periods, in place of xi. We will work with assumption (15.67), as it conserves on parameters; the more general model requires a simple notational change. Chamberlain (1980) called model (15.60) under assumption (15.67) a random e¤ects probit model, so we refer to the model as Chamberlain’s random e¤ects probit model. While assumption (15.67) is re- strictive in that it speciﬁes a distribution for ci given xi, it at least allows for some dependence between ci and xi. Discrete Response Models 487", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 497, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p498::c0", "text": "As in the linear case, we can only estimate the e¤ects of time-varying elements in xit. In particular, xit should no longer contain a constant, as that would be indistin- guishable from c in assumption (15.67). If our original model contains a time-constant explanatory variable, say wi, it can be included among the explanatory variables, but we cannot distinguish its e¤ect from ci unless we assume that the coe‰cient for wi in x is zero. (That is, unless we assume that ci is partially uncorrelated with wi.) Time dummies, which do not vary across i, are omitted from xi. If assumptions (15.60), (15.61), and (15.67) hold, estimation of b, c, x, and s2 a is straightforward because we can write the latent variable as y\u0004 it ¼ c þ xitb þ xix þ ai þ eit, where the eit are independent Normalð0; 1Þ variates [conditional on ðxi; aiÞ], and ai j xi @ Normalð0; s2 a Þ. In other words, by adding xi to the equation for each time period, we arrive at a traditional random e¤ects probit model. (The variance we estimate is s2 a rather than s2 c , but, as we will see, this suits our purposes nicely.) Adding xi as a set of controls for unobserved heterogeneity is very intuitive: we are estimating the e¤ect of changing xitj but holding the time average ﬁxed. A test of the usual RE probit model is easily obtained as a test of H0: x ¼ 0. Estimation can be carried out using standard random e¤ects probit software. Given estimates of c and x, we can estimate EðciÞ ¼ c þ EðxiÞx by ^c þ x^x, where x is the sample average of xi. Therefore, for any vector xt, we can estimate the response probability at EðciÞ as Fð ^c þ xt ^b þ x^xÞ. Taking di¤erences or derivatives (with respect to the elements of xt) allows us to estimate the partial e¤ects on the response probabilities for any value of xt. (We will show how to estimate the average partial e¤ects shortly.) If we drop assumption (15.61), we can still estimate scaled versions of c, b, and x. Under assumptions (15.60) and (15.67) we have Pðyit ¼ 1 j xiÞ ¼ F½ðc þ xitb þ xixÞ \u0003 ð1 þ s2 a Þ\u00011=2\u0002 1 Fðca þ xitba þ xixaÞ ð15:68Þ where the a subscript means that a parameter vector has been multiplied by ð1 þ s2 a Þ\u00011=2. It follows immediately that ca, ba, and xa can be consistently estimated using a pooled probit analysis of yit on 1, xit, xi, t ¼ 1; . . . ; T, i ¼ 1; . . . ; N. Because the yit will be dependent condition on xi, inference that is robust to arbitrary time dependence is required. Conveniently, once we have estimated ca, ba, and xa, we can estimate the average partial e¤ects. (We could apply the results from Section 2.2.5 here, but a direct argument is instructive.) To see how, we need to average Pðyt ¼ 1 j xt ¼ xo; ciÞ across the distribution of ci; that is, we need to ﬁnd E½Pðyt ¼ 1 j xt ¼ xo; ciÞ\u0002 ¼ E½Fðxob þ ciÞ\u0002 for any given value xo of the explanatory variables. (In what follows, xo is a non- Chapter 15 488", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 498, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p499::c0", "text": "random vector of numbers that we choose as interesting values of the explanatory variables. For emphasis, we include an i subscript on the random variables appearing in the expectations.) Writing ci ¼ c þ xix þ ai and using iterated expectations, we have E½Fðc þ xob þ xix þ aiÞ\u0002 ¼ E½EfFðc þ xob þ xix þ aiÞ j xig\u0002 [where the ﬁrst expectation is with respect to ðxi; aiÞ]. Using the same argument from Section 15.7.1, E½Fðcþxob þxixþaiÞjxi\u0002 ¼ F½fcþxob þ xixg \u0003 ð1þs2 a Þ\u00011=2\u0002 ¼ Fðca þxoba þxixaÞ, and so E½Fðxob þ ciÞ\u0002 ¼ E½Fðca þ xoba þ xixaÞ\u0002 ð15:69Þ Because the only random variable in the right-hand-side expectation is xi, a consis- tent estimator of the right-hand side of equation (15.66) is simply N\u00011 X N i¼1 Fð ^ca þ xo ^ba þ xi ^xaÞ ð15:70Þ Average partial e¤ects can be estimated by evaluating expression (15.70) at two di¤erent values for xo and forming the di¤erence, or, for continuous variable xj, by using the average across i of ^bajfð ^ca þ xo ^ba þ xi ^xaÞ to get the approximate APE of a one-unit increase in xj. See also Chamberlain [1984, equation (3.4)]. If we use Chamberlain’s more general version of assumption (15.67), xi replaces xi everywhere. [Incidentally, the focus on the APEs raises an interesting, and apparently open, ques- tion: How does treating the ci’s as parameters to estimate—in a ‘‘ﬁxed e¤ects probit’’ analysis—a¤ect estimation of the APEs? Given ^ci, i ¼ 1; . . . ; N and ^b, the APEs could be based on N\u00011 PN i¼1 Fð^ci þ xo ^bÞ. Even though ^b does not consistently esti- mate b and the ^ci are estimates of the incidental parameters, it could be that the resulting estimates of the APEs have reasonable properties.] Under assumption (15.60) and the more general version of assumption (15.67), Chamberlain (1980) suggested a minimum distance approach analogous to the linear case (see Section 14.6). Namely, obtain ^pt for each t by running a cross-sectional probit of yit on 1, xi, i ¼ 1; . . . ; N. The mapping from the structural parameters ya 1 ðca; b 0 a; x0 aÞ0 to the vector p is exactly as in the linear case (see Section 14.6). The variance matrix estimator for ^p is obtained by pooling all T probits and computing the robust variance matrix estimator in equation (13.53), with ^y replaced by ^p; see also Chamberlain (1984, Section 4.5). The minimum distance approach leads to a straightforward test of H0: xa ¼ 0, which is a test of assumption (15.63) that does not impose assumption (15.61). All of the previous estimation methods hinge crucially on the strict exogeneity of fxit: t ¼ 1; . . . ; Tg, conditional on ci. As mentioned earlier, this assumption rules out Discrete Response Models 489", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 499, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p500::c0", "text": "lagged dependent variables, a case we consider explicitly in Section 15.8.4. But there are other cases where strict exogeneity is questionable. For example, suppose that yit is an employment indicator for person i in period t and wit is measure of recent arrests. It is possible that whether someone is employed this period has an e¤ect on future arrests. If so, then shocks that a¤ect employment status could be correlated with future arrests, and such correlation would violate strict exogeneity. Whether this situation is empirically important is largely unknown. On the one hand, correcting for an explanatory variable that is not strictly exoge- nous is quite di‰cult in nonlinear models; Wooldridge (2000c) suggests one possible approach. On the other hand, obtaining a test of strict exogeneity is fairly easy. Let wit denote a 1 \u0005 G subset of xit that we suspect of failing the strict exogeneity re- quirement. Then a simple test is to add wi;tþ1 as an additional set of covariates; under the null hypothesis, wi;tþ1 should be insigniﬁcant. In implementing this test, we can use either random e¤ects probit or pooled probit, where, in either case, we lose the last time period. (In the pooled probit case, we should use a fully robust Wald or score test.) We should still obtain xi from all T time periods, as the test is either based on the distribution of ðyi1; . . . ; yi;T\u00011Þ given ðxi1; . . . ; xiTÞ (random e¤ects probit) or on the marginal distributions of yit given ðxi1; . . . ; xiTÞ, t ¼ 1; . . . ; T \u0001 1 (pooled probit). If the test does not reject, it provides at least some justiﬁcation for the strict exogeneity assumption. 15.8.3 Unobserved E¤ects Logit Models under Strict Exogeneity The unobserved e¤ects probit models of the previous subsection have logit counter- parts. If we replace the standard normal cdf F in assumption (15.60) with the logistic function L, and also maintain assumptions (15.61) and (15.63), we arrive at what is usually called the random e¤ects logit model. This model is not as attractive as the random e¤ects probit model because there are no simple estimators available. The normal distribution, with its property that linear combinations of normals are nor- mally distributed, facilitates the pooled probit, random e¤ects, and minimum distance estimation approaches. By contrast, in the random e¤ects logit model, Pðyit ¼ 1 j xiÞ has no simple form: integrating the logit response Lðxtb þ cÞ with respect to the normal density ð1=scÞfðc=scÞ yields no simple functional form. This statement is also true of other popular continuous distributions for c. There is one important advantage of the unobserved e¤ects logit model over the probit model: under assumptions (15.60) (with F replaced by L) and (15.61), it is possible to obtain a ﬃﬃﬃﬃ N p -consistent estimator of b without any assumptions about how ci is related to xi. Chapter 15 490", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 500, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p501::c0", "text": "How can we allow ci and xi to be arbitrarily related in the unobserved e¤ects logit model? In the linear case we used the FE or FD transformation to eliminate ci from the estimating equation. It turns out that a similar strategy works in the logit case, although the argument is more subtle. What we do is ﬁnd the joint distribution of yi 1 ðyi1; . . . ; yiTÞ0 conditional on xi; ci, and ni 1 PT t¼1 yit. It turns out that this conditional distribution does not depend on ci, so that it is also the distribution of yi given xi and ni. Therefore, we can use standard conditional maximum likelihood methods to estimate b. (The fact that we can ﬁnd a conditional distribution that does not depend on the ci is a feature of the logit functional form. Unfortunately, the same argument does not work for the unobserved e¤ects probit model.) First consider the T ¼ 2 case, where ni takes a value in f0; 1; 2g. Intuitively, the conditional distribution of ðyi1; yi2Þ0 given ni cannot be informative for b when ni ¼ 0 or ni ¼ 2 because these values completely determine the outcome on yi. However, for ni ¼ 1, Pðyi2 ¼ 1 j xi; ci; ni ¼ 1Þ ¼ Pðyi2 ¼ 1; ni ¼ 1 j xi; ciÞ=Pðni ¼ 1 j xi; ciÞ ¼ Pðyi2 ¼ 1 j xi; ciÞPðyi1 ¼ 0 j xi; ciÞ=fPðyi1 ¼ 0; yi2 ¼ 1 j xi; ciÞ þ Pðyi1 ¼ 1; yi2 ¼ 0 j xi; ciÞg ¼ Lðxi2b þ ciÞ½1 \u0001 Lðxi1b þ ciÞ\u0002=f½1 \u0001 Lðxi1b þ ciÞ\u0002Lðxi2b þ ciÞ þ Lðxi1b þ ciÞ½1 \u0001 Lðxi2b þ ciÞ\u0002g ¼ L½ðxi2 \u0001 xi1Þb\u0002 Similarly, Pðyi1 ¼ 1 j xi; ci; ni ¼ 1Þ ¼ L½\u0001ðxi2 \u0001 xi1Þb\u0002 ¼ 1 \u0001 L½ðxi2 \u0001 xi1Þb\u0002. The conditional log likelihood for observation i is liðbÞ ¼ 1½ni ¼ 1\u0002ðwi log L½ðxi2 \u0001 xi1Þb\u0002 þ ð1 \u0001 wiÞ logf1 \u0001 L½ðxi2 \u0001 xi1Þb\u0002gÞ ð15:71Þ where wi ¼ 1 if ðyi1 ¼ 0; yi2 ¼ 1Þ and wi ¼ 0 if ðyi1 ¼ 1; yi2 ¼ 0Þ. The conditional MLE is obtained by maximizing the sum of the liðbÞ across i. The indicator function 1½ni ¼ 1\u0002 selects out the observations for which ni ¼ 1; as stated earlier, observations for which ni ¼ 0 or ni ¼ 2 do not contribute to the log likelihood. Interestingly, equa- tion (15.71) is just a standard cross-sectional logit of wi on ðxi2 \u0001 xi1Þ using the obser- vations for which ni ¼ 1. (This approach is analogous to di¤erencing in the linear case with T ¼ 2.) The conditional MLE from equation (15.71) is usually called the ﬁxed e¤ects logit estimator. We must emphasize that the FE logit estimator does not arise by treating the ci as parameters to be estimated along with b. (This fact is confusing, as the FE probit estimator does estimate the ci along with b.) As shown recently by Abrevaya Discrete Response Models 491", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 501, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p502::c0", "text": "(1997), the MLE of b that is obtained by maximizing the log likelihood over b and the ci has probability limit 2b. (This ﬁnding extends a simple example due to Andersen, 1970; see also Hsiao, 1986, Section 7.3.) Sometimes the conditional MLE is described as ‘‘conditioning on the unobserved e¤ects in the sample.’’ This description is misleading. What we have done is found a conditional density—which describes the subpopulation with ni ¼ 1—that depends only on observable data and the parameter b. For general T the log likelihood is more complicated, but it is tractable. First, Pðyi1 ¼ y1; . . . ; yiT ¼ yT j xi; ci; ni ¼ nÞ ¼ Pðyi1 ¼ y1; . . . ; yiT ¼ yT j xi; ciÞ=Pðni ¼ n j xi; ciÞ ð15:72Þ and the numerator factors as Pðyi1 ¼ y1 j xi; ciÞ \u0003 \u0003 \u0003 PðyiT ¼ yT j xi; ciÞ by the condi- tional independence assumption. The denominator is the complicated part, but it is easy to describe: Pðni ¼ n j xi; ciÞ is the sum of the probabilities of all possible out- comes of yi such that ni ¼ n. Using the speciﬁc form of the logit function we can write liðbÞ ¼ log exp X T t¼1 yitxitb ! X a A Ri exp X T t¼1 atxitb ! \" #\u00011 8 < : 9 = ; ð15:73Þ where Ri is the subset of RT deﬁned as fa A RT: at A f0; 1g and PT t¼1 at ¼ nig. The log likelihood summed across i can be used to obtain a ﬃﬃﬃﬃ N p -asymptotically normal estimator of b, and all inference follows from conditional MLE theory. Observations for which equation (15.72) is zero or unity—and which therefore do not depend on b—drop out of LðbÞ. See Chamberlain (1984). The ﬁxed e¤ects logit estimator ^b immediately gives us the e¤ect of each element of xt on the log-odds ratio, logfLðxtb þ cÞ=½1 \u0001 Lðxtb þ cÞ\u0002g ¼ xtb þ c. Unfortunately, we cannot estimate the partial e¤ects on the response probabilities unless we plug in a value for c. Because the distribution of ci is unrestricted—in particular, EðciÞ is not necessarily zero—it is hard to know what to plug in for c. In addition, we cannot estimate average partial e¤ects, as doing so would require ﬁnding E½Lðxtb þ ciÞ\u0002, a task that apparently requires specifying a distribution for ci. The conditional logit approach also has the drawback of apparently requiring the conditional independence assumption (15.61) for consistency. As we saw in Section 15.8.2, if we are willing to make the normality assumption (15.67), the probit approach allows unrestricted serial dependence in yit even after conditioning on xi and ci. This possibility may be especially important when several time periods are available. Chapter 15 492", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 502, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p503::c0", "text": "15.8.4 Dynamic Unobserved E¤ects Models Dynamic models that also contain unobserved e¤ects are important in testing theories and evaluating policies. Here we cover one class of models that illustrates the impor- tant points for general dynamic models and is of considerable interest in its own right. Suppose we date our observations starting at t ¼ 0, so that yi0 is the ﬁrst obser- vation on y. For t ¼ 1; . . . ; T we are interested in the dynamic unobserved e¤ects model Pðyit ¼ 1 j yi;t\u00011; . . . ; yi0; zi; ciÞ ¼ Gðzitd þ ryi;t\u00011 þ ciÞ ð15:74Þ where zit is a vector of contemporaneous explanatory variables, zi ¼ ðzi1; . . . ; ziTÞ, and G can be the probit or logit function. There are several important points about this model. First, the zit are assumed to satisfy a strict exogeneity assumption (con- ditional on ci), since zi appears in the conditioning set on the left-hand side of equa- tion (15.74), but only zit appears on the right-hand side. Second, the probability of success at time t is allowed to depend on the outcome in t \u0001 1 as well as unobserved heterogeneity, ci. We saw the linear version in Section 11.1.1. Of particular interest is the hypothesis H0: r ¼ 0. Under this null, the response probability at time t does not depend on past outcomes once ci (and zi) have been controlled for. Even if r ¼ 0, Pðyit ¼ 1 j yi;t\u00011; ziÞ 0 Pðyit ¼ 1 j ziÞ owing to the presence of ci. But economists are interested in whether there is state dependence—that is, r 0 0 in equation (15.74)— after controlling for the unobserved heterogeneity, ci. We might also be interested in the e¤ects of zt, as it may contain policy variables. Then, equation (15.74) simply captures the fact that, in addition to an unobserved e¤ect, behavior may depend on past observed behavior. How can we estimate d and r in equation (15.74), in addition to quantities such as average partial e¤ects? First, we can always write f ðy1; y2; . . . ; yT j y0; z; c; bÞ ¼ Y T t¼1 f ðyt j yt\u00011; . . . y1; y0; zt; c; bÞ ¼ Y T t¼1 Gðztd þ ryt\u00011 þ cÞyt½1 \u0001 Gðztd þ ryt\u00011 þ cÞ\u00021\u0001yt ð15:75Þ With ﬁxed-T asymptotics, this density, because of the unobserved e¤ect c, does not allow us to construct a log-likelihood function that can be used to estimate b con- sistently. Just as in the case with strictly exogenous explanatory variables, treating the Discrete Response Models 493", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 503, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p504::c0", "text": "ci as parameters to be estimated does not result in consistent estimators of d and r as N ! y. In fact, the simulations in Heckman (1981) show that the incidental parameters problem is even more severe in dynamic models. What we should do is integrate out the unobserved e¤ect c, as we discussed generally in Section 13.9.2. Our need to integrate c out of the distribution raises the issue of how we treat the initial observations, yi0; this is usually called the initial conditions problem. One pos- sibility is to treat each yi0 as a nonstochastic starting position for each i. Then, if ci is assumed to be independent of zi (as in a pure random e¤ects environment), equation (15.75) can be integrated against the density of c to obtain the density of ðy1; y2; . . . ; yTÞ given z; this density also depends on y0 through f ðy1 j y0; c; z1; bÞ. We can then apply conditional MLE. Although treating the yi0 as nonrandom simpliﬁes estima- tion, it is undesirable because it e¤ectively means that ci and yi0 are independent, a very strong assumption. Another possibility is to ﬁrst specify a density for yi0 given ðzi; ciÞ and to multiply this density by equation (15.75) to obtain f ðy0; y1; y2; . . . ; yT j z; c; b; gÞ. Next, a density for ci given zi can be speciﬁed. Finally, f ðy0; y1; y2; . . . ; yT j z; c; b; gÞ is inte- grated against the density hðc j z; aÞ to obtain the density of ðyi0; yi1; yi2; . . . ; yiTÞ given zi. This density can then be used in an MLE analysis. The problem with this approach is that ﬁnding the density of yi0 given ðzi; ciÞ is very di‰cult, if not impos- sible, even if the process is assumed to be in equilibrium. For discussion, see Hsiao (1986, Section 7.4). Heckman (1981) suggests approximating the conditional density of yi0 given ðzi; ciÞ and then specifying a density for ci given zi. For example, we might assume that yi0 follows a probit model with success probability Fðh þ zip þ gciÞ and specify the den- sity of ci given zi as normal. Once these two densities are given, they can be multi- plied by equation (15.75), and c can be integrated out to approximate the density of ðyi0; yi1; yi2; . . . ; yiTÞ given zi; see Hsiao (1986, Section 7.4). Heckman’s (1981) approach attempts to ﬁnd or approximate the joint distribution of ðyi0; yi1; yi2; . . . ; yiTÞ given zi. We discussed an alternative approach in Section 13.9.2: obtain the joint distribution of ðyi1; yi2; . . . ; yiTÞ conditional on ðyi0; ziÞ. This allows us to remain agnostic about the distribution of yi0 given ðzi; ciÞ, which is the primary source of di‰culty in Heckman’s approach. If we can ﬁnd the density of ðyi1; yi2; . . . ; yiTÞ given ðyi0; ziÞ, in terms of b and other parameters, then we can use standard conditional maximum likelihood methods: we are simply conditioning on yi0 in addition to zi. It is important to see that using the density of ðyi1; yi2; . . . ; yiTÞ given ðyi0; ziÞ is not the same as treating yi0 as nonrandom. Indeed, the model with ci independent of yi0, given zi, is a special case. Chapter 15 494", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 504, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p505::c0", "text": "To obtain f ðy1; y2; . . . ; yT j yi0; ziÞ, we need to propose a density for ci given ðyi0; ziÞ. This approach is very much like Chamberlain’s (1980) approach to static probit models with unobserved e¤ects, except that we now condition on yi0 as well. [Since the density of ci given zi is not restricted by the speciﬁcation (15.75), our choice of the density of ci given ðyi0; ziÞ is not logically restricted in any way.] Given a density hðc j y0; z; gÞ, which depends on a vector of parameters g, we have f ðy1; y2; . . . ; yT j y0; z; yÞ ¼ ðy \u0001y f ðy1; y2; . . . ; yT j y0; z; c; bÞhðc j y0; z; gÞ dc See Property CD.2 in Chapter 13. The integral can be replaced with a weighted average if the distribution of c is discrete. When G ¼ F in the model (15.74)—the leading case—a very convenient choice for hðc j y0; z; gÞ is Normalðc þ x0 yi0 þ zix; s2 a Þ, which follows by writing ci ¼ c þ x0 yi0 þ zix þ ai, where ai @ Normalð0; s2 a Þ and independent of ðyi0; ziÞ. Then we can write yit ¼ 1½c þ zitd þ ryi;t\u00011 þ x0 yi0 þ zix þ ai þ eit > 0\u0002 so that yit given ðyi;t\u00011; . . . ; yi0; zi; aiÞ follows a probit model and ai given ðyi0; ziÞ is distributed as Normalð0; s2 a Þ. Therefore, the density of ðyi1; . . . ; yiTÞ given ðyi0; ziÞ has exactly the form in equation (15.64), where xit ¼ ð1; zit; yi;t\u00011; yi0; ziÞ and with a and sa replacing c and sc, respectively. Conveniently, this result means that we can use standard random e¤ects probit software to estimate c, d, r, x0, x, and s2 a : we simply expand the list of explanatory variables to include yi0 and zi in each time period. (The approach that treats yi0 and zi as ﬁxed omits yi0 and zi in each time period.) It is simple to test H0: r ¼ 0, which means there is no state dependence once we control for an unobserved e¤ect. Average partial e¤ects can be estimated as in Chamberlain’s unobserved e¤ects probit model: for given values of ztðzoÞ and yt\u00011ðyo \u00011Þ, E½Fðzod þ ryo \u00011 þ ciÞ\u0002 is con- sistently estimated by N\u00011 PN i¼1 Fð ^ca þ zo ^da þ ^ra yo \u00011 þ ^xa0 yi0 þ zi ^xaÞ, where the a subscript denotes multiplication by ð1 þ ^s2 aÞ\u00011=2, and ^c, ^d, ^r, ^x0, ^x, and ^s2 a are the conditional MLEs. See Wooldridge (2000e) for additional details. A mean value ex- pansion can be used to obtain asymptotic standard errors for the APEs, or a boot- strapping approach, as described in Section 12.8.2, can be used. 15.8.5 Semiparametric Approaches Under strict exogeneity of the explanatory variables, it is possible to consistently es- timate b up to scale under very weak assumptions. Manski (1987) derives an objec- tive function that identiﬁes b up to scale in the T ¼ 2 case when ei1 and ei2 in the Discrete Response Models 495", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 505, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p506::c0", "text": "model (15.66) are identically distributed conditional on ðxi1; xi2; ciÞ and xit is strictly exogenous. The estimator is the maximum score estimator applied to the di¤erences Dyi and Dxi. As in the cross-sectional case, it is not known how to estimate the average response probabilities. Honore´ and Kyriazidou (2000a) show how to estimate the parameters in the unobserved e¤ects logit model with a lagged dependent variable and strictly exoge- nous explanatory variables without making distributional assumptions about the unobserved e¤ect. Unfortunately, the estimators, which are consistent and asymp- totically normal, do not generally converge at the usual ﬃﬃﬃﬃ N p rate. In addition, as with many semiparametric approaches, discrete explanatory variables such as time dum- mies are ruled out, and it is not possible to estimate the average partial e¤ects. See also Arellano and Honore´ (in press). 15.8.6 Cluster Samples In Section 13.8 we noted how partial MLE methods can be applied to cluster sam- ples, and binary choice models are no exception. For cluster i and unit g, we might specify Pðyig ¼ 1 j xigÞ ¼ FðxigbÞ, and then estimate b using a pooled probit analysis. (Replacing F with L gives pooled logit.) A robust variance matrix is needed to ac- count for any within-cluster correlation due, say, to unobserved cluster e¤ects. The formula is given in equation (13.53), except that g replaces t, and Gi, the size of cluster i, replaces T everywhere. This estimator is valid as N ! y with Gi ﬁxed. We can also test for peer group e¤ects by including among xig the average (or other summary statistics) of variables within the same cluster. In this scenario there are almost certainly unobserved cluster e¤ects, so statistics robust to intercluster correlation should be computed. An alternative to pooled probit or logit is to use an unobserved e¤ect framework explicitly. For example, we might have Pðyig ¼ 1 j xi; ciÞ ¼ Fðxigb þ ciÞ, where ci is an unobserved cluster e¤ect. If observations are assumed independent within cluster conditional on ðxi; ciÞ, and if ci is independent of xi, then the random e¤ects probit MLE is easily modiﬁed: just use equation (15.64) with t ¼ g and T ¼ Gi. The fact that the observations are no longer identically distributed across i has no practical implications. Allowing ci and xi to be correlated in the context of cluster sampling is easy if we maintain assumption (15.67) regardless of the cluster size. The details are essentially the same as the panel data case. When Gi ¼ 2 for all i, the ﬁxed e¤ects logit approach is straightforward. Geronimus and Korenman (1992) use sister pairs to determine the e¤ects of teenage motherhood on subsequent economic outcomes. When the outcome is binary (such as an employ- Chapter 15 496", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 506, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p507::c0", "text": "ment indicator), Geronimus and Korenman allow for an unobserved family e¤ect by applying ﬁxed e¤ects logit. 15.9 Multinomial Response Models 15.9.1 Multinomial Logit The logit model for binary outcomes extends to the case where the unordered response has more than two outcomes. Examples of unordered multinomial responses include occupational choice, choice of health plan, and transportation mode for commuting to work. In each case, an individual chooses one alternative from the group of choices, and the labeling of the choices is arbitrary. Let y denote a random variable taking on the values f0; 1; . . . ; Jg for J a positive integer, and let x denote a set of conditioning variables. For example, if y denotes occupational choice, x can contain things like education, age, gender, race, and marital status. As usual, ðxi; yiÞ is a random draw from the population. As in the binary response case, we are interested in how ceteris paribus changes in the elements of x a¤ect the response probabilities, Pðy ¼ j j xÞ, j ¼ 0; 1; 2; . . . ; J. Since the probabilities must sum to unity, Pðy ¼ 0 j xÞ is determined once we know the probabilities for j ¼ 1; . . . ; J. Let x be a 1 \u0005 K vector with ﬁrst-element unity. The multinomial logit (MNL) model has response probabilities Pðy ¼ j j xÞ ¼ expðxbjÞ= 1 þ X J h¼1 expðxbhÞ \" # ; j ¼ 1; . . . ; J ð15:76Þ where bj is K \u0005 1, j ¼ 1; . . . ; J. Because the response probabilities must sum to unity, Pðy ¼ 0 j xÞ ¼ 1= 1 þ X J h¼1 expðxbhÞ \" # When J ¼ 1, b1 is the K \u0005 1 vector of unknown parameters, and we get the binary logit model. The partial e¤ects for this model are complicated. For continuous xk, we can write qPðy ¼ j j xÞ qxk ¼ Pðy ¼ j j xÞ bjk \u0001 X J h¼1 bhk expðxbhÞ \" # =gðx; bÞ ( ) ð15:77Þ where bhk is the kth element of bh and gðx; bÞ ¼ 1 þ PJ h¼1 expðxbhÞ. Equation Discrete Response Models 497", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 507, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p508::c0", "text": "(15.77) shows that even the direction of the e¤ect is not determined entirely by bjk. A simpler interpretation of bj is given by pjðx; bÞ=p0ðx; bÞ ¼ expðxbjÞ; j ¼ 1; 2; . . . ; J ð15:78Þ where pjðx; bÞ denotes the response probability in equation (15.76). Thus the change in pjðx; bÞ=p0ðx; bÞ is approximately bjk expðxbjÞDxk for roughly continuous xk. Equivalently, the log-odds ratio is linear in x: log½ pjðx; bÞ=p0ðx; bÞ\u0002 ¼ xbj. This result extends to general j and h: log½ pjðx; bÞ=phðx; bÞ\u0002 ¼ xðbj \u0001 bhÞ. Here is another useful fact about the multinomial logit model. Since Pðy ¼ j or y ¼ h j xÞ ¼ pjðx; bÞ þ phðx; bÞ, Pðy ¼ j j y ¼ j or y ¼ h; xÞ ¼ pjðx; bÞ=½ pjðx; bÞ þ phðx; bÞ\u0002 ¼ L½xðbj \u0001 bhÞ\u0002 where Lð\u0003Þ is the logistic function. In other words, conditional on the choice being either j or h, the probability that the outcome is j follows a standard logit model with parameter vector bj \u0001 bh. Since we have fully speciﬁed the density of y given x, estimation of the MNL model is best carried out by maximum likelihood. For each i the conditional log likelihood can be written as liðbÞ ¼ X J j¼0 1½yi ¼ j\u0002 log½ pjðxi; bÞ\u0002 where the indicator function selects out the appropriate response probability for each observation i. As usual, we estimate b by maximizing PN i¼1 liðbÞ. McFadden (1974) has shown that the log-likelihood function is globally concave, and this fact makes the maximization problem straightforward. The conditions needed to apply Theorems 13.1 and 13.2 for consistency and asymptotic normality are broadly appli- cable; see McFadden (1984). Example 15.4 (School and Employment Decisions for Young Men): The data KEANE.RAW (a subset from Keane and Wolpin, 1997) contains employment and schooling history for a sample of men for the years 1981 to 1987. We use the data for 1987. The three possible outcomes are enrolled in school (status ¼ 0), not in school and not working (status ¼ 1), and working (status ¼ 2). The explanatory variables are education, a quadratic in past work experience, and a black binary indicator. The base category is enrolled in school. Out of 1,717 observations, 99 are enrolled in school, 332 are at home, and 1,286 are working. The results are given in Table 15.2. Another year of education reduces the log-odds between at home and enrolled in school by \u0001.674, and the log-odds between at home and enrolled in school is .813 Chapter 15 498", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 508, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p509::c0", "text": "higher for black men. The magnitudes of these coe‰cients are di‰cult to interpret. Instead, we can either compute partial e¤ects, as in equation (15.77), or compute di¤erences in probabilities. For example, consider two black men, each with ﬁve years of experience. A black man with 16 years of education has an employment proba- bility that is .042 higher than a man with 12 years of education, and the at-home probability is .072 lower. (Necessarily, the in-school probability is .030 higher for the man with 16 years of education.) These results are easily obtained by comparing ﬁtted probabilities after multinomial logit estimation. The experience terms are each insigniﬁcant in the home column, but the Wald test for joint signiﬁcance of exper and exper2 gives p-value ¼ .047, and so they are jointly signiﬁcant at the 5 percent level. We would probably leave their coe‰cients un- restricted in b1 rather than setting them to zero. The ﬁtted probabilities can be used for prediction purposes: for each observation i, the outcome with the highest estimated probability is the predicted outcome. This can be used to obtain a percent correctly predicted, by category if desired. For the pre- vious example, the overall percent correctly predicted is almost 80 percent, but the model does a much better job of predicting that a man is employed (95.2 percent correct) than in school (12.1 percent) or at home (39.2 percent). Table 15.2 Multinomial Logit Estimates of School and Labor Market Decisions Dependent Variable: status Explanatory Variable home (status ¼ 1) work (status ¼ 2) educ \u0001.674 (.070) \u0001.315 (.065) exper \u0001.106 (.173) .849 (.157) exper2 \u0001.013 (.025) \u0001.077 (.023) black .813 (.303) .311 (.282) constant 10.28 (1.13) 5.54 (1.09) Number of observations 1,717 Percent correctly predicted 79.6 Log-likelihood value \u0001907.86 Pseudo R-squared .243 Discrete Response Models 499", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 509, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p510::c0", "text": "15.9.2 Probabilistic Choice Models McFadden (1974) showed that a model closely related to the multinomial logit model can be obtained from an underlying utility comparison. Suppose that, for a random draw i from the underlying population (usually, but not necessarily, individuals), the utility from choosing alternative j is y\u0004 ij ¼ xijb þ aij; j ¼ 0; . . . ; J ð15:79Þ where aij; j ¼ 0; 1; 2; . . . ; J are unobservables a¤ecting tastes. Here, xij is a 1 \u0005 K vector that di¤ers across alternatives and possibly across individuals as well. For ex- ample, xij might contain the commute time for individual i using transportation mode j, or the co-payment required by health insurance plan j (which may or may not di¤er by individual). For reasons we will see, xij cannot contain elements that vary only across i and not j; in particular, xij does not contain unity. We assume that the ðJ þ 1Þ-vector ai is independent of xi, which contains fxij: j ¼ 0; . . . ; Jg. Let yi denote the choice of individual i that maximizes utility: yi ¼ argmaxðy\u0004 i0; y\u0004 i2; . . . ; y\u0004 iJÞ so that yi takes on a value in f0; 1; . . . ; Jg. As shown by McFadden (1974), if the aij, j ¼ 0; . . . ; J are independently distributed with cdf FðaÞ ¼ exp½\u0001expð\u0001aÞ\u0002—the type I extreme value distribution—then Pðyi ¼ j j xiÞ ¼ expðxijbÞ= X J h¼0 expðxihbÞ \" # ; j ¼ 0; . . . ; J ð15:80Þ The response probabilities in equation (15.80) constitute what is usually called the conditional logit model. Dropping the subscript i and di¤erentiating shows that the marginal e¤ects are given by qpjðxÞ=qxjk ¼ pjðxÞ½1 \u0001 pjðxÞ\u0002bk; j ¼ 0; . . . ; J; k ¼ 1; . . . ; K ð15:81Þ and qpjðxÞ=qxhk ¼ \u0001pjðxÞphðxÞbk; j 0 h; k ¼ 1; . . . ; K ð15:82Þ where pjðxÞ is the response probability in equation (15.80) and bk is the kth element of b. As usual, if the xj contain nonlinear functions of underlying explanatory vari- ables, this fact will be reﬂected in the partial derivatives. The conditional logit and multinomial logit models have similar response proba- bilities, but they di¤er in some important respects. In the MNL model, the condi- Chapter 15 500", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 510, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p511::c0", "text": "tioning variables do not change across alternative: for each i, xi contains variables speciﬁc to the individual but not to the alternatives. This model is appropriate for problems where characteristics of the alternatives are unimportant or are not of in- terest, or where the data are simply not available. For example, in a model of occu- pational choice, we do not usually know how much someone could make in every occupation. What we can usually collect data on are things that a¤ect individual productivity and tastes, such as education and past experience. The MNL model allows these characteristics to have di¤erent e¤ects on the relative probabilities be- tween any two choices. The conditional logit model is intended speciﬁcally for problems where consumer or ﬁrm choices are at least partly made based on observable attributes of each alter- native. The utility level of each choice is assumed to be a linear function in choice attributes, xij, with common parameter vector b. This turns out to actually contain the MNL model as a special case by appropriately choosing xij. Suppose that wi is a vector of individual characteristics and that Pðyi ¼ j j wiÞ follows the MNL in equa- tion (15.76) with parameters dj, j ¼ 1; . . . ; J. We can cast this model as the condi- tional logit model by deﬁning xij ¼ ðd1jwi; d2jwi; . . . ; dJjwiÞ, where djh is a dummy variable equal to unity when j ¼ h, and b ¼ ðd0 1; . . . ; d0 JÞ0. Consequently, some authors refer to the conditional logit model as the multinomial logit model, with the understanding that alternative-speciﬁc characteristics are allowed in the response probability. Empirical applications of the conditional logit model often include individual- speciﬁc variables by allowing them to have separate e¤ects on the latent utilities. A general model is y\u0004 ij ¼ zijg þ widj þ aij; j ¼ 0; 1; . . . ; J with d0 ¼ 0 as a normalization, where zij varies across j and possibly i. If dj ¼ d for all j, then wid drops out of all response probabilities. The conditional logit model is very convenient for modeling probabilistic choice, but it has some limitations. An important restriction is pjðxjÞ=phðxhÞ ¼ expðxjbÞ=expðxhbÞ ¼ exp½ðxj \u0001 xhÞb\u0002 ð15:83Þ so that relative probabilities for any two alternatives depend only on the attributes of those two alternatives. This is called the independence from irrelevant alternatives (IIA) assumption because it implies that adding another alernative or changing the characteristics of a third alternative does not a¤ect the relative odds between alter- natives j and h. This implication is implausible for applications with similar alterna- Discrete Response Models 501", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 511, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p512::c0", "text": "tives. A well-known example is due to McFadden (1974). Consider commuters initially choosing between two modes of transportation, car and red bus. Suppose that a consumer chooses between the buses with equal probability, .5, so that the ratio in equation (15.83) is unity. Now suppose a third mode, blue bus, is added. Assuming bus commuters do not care about the color of the bus, consumers will choose be- tween these with equal probability. But then IIA implies that the probability of each mode is 1 3; therefore, the fraction of commuters taking a car would fall from 1 2 to 1 3, a result that is not very realistic. This example is admittedly extreme—in practice, we would lump the blue bus and red bus into the same category, provided there are no other di¤erences—but it indicates that the IIA property can impose unwanted restric- tions in the conditional logit model. Hausman and McFadden (1984) o¤er tests of the IIA assumption based on the observation that, if the conditional logit model is true, b can be consistently esti- mated by conditional logit by focusing on any subset of alternatives. They apply the Hausman principle that compares the estimate of b using all alternatives to the esti- mate using a subset of alternatives. Several models that relax the IIA assumption have been suggested. In the context of the random utility model the IIA assumption comes about because the faij: j ¼ 0; 1; . . . ; Jg are assumed to be independent Wiebull random variables. A more ﬂexible assumption is that ai has a multivariate normal distribution with arbitrary corre- lations between aij and aih, all j 0 h. The resulting model is called the multinomial probit model. [In keeping with the spirit of the previous names, conditional probit model is a better name, which is used by Hausman and Wise (1978) but not by many others.] Theoretically, the multinomial probit model is attractive, but it has some practical limitations. The response probabilities are very complicated, involving a ðJ þ 1Þ- dimensional integral. This complexity not only makes it di‰cult to obtain the partial e¤ects on the response probabilities, but also makes maximum likelihood infeasible for more than about ﬁve alternatives. For details, see Maddala (1983, Chapter 3) and Amemiya (1985, Chapter 9). Hausman and Wise (1978) contain an application to transportation mode for three alternatives. Recent advances on estimation through simulation make multinomial probit esti- mation feasible for many alternatives. See Hajivassilou and Ruud (1994) and Keane (1993) for recent surveys of simulation estimation. Keane and Mo‰tt (1998) apply simulation methods to structural multinomial response models, where the econometric model is obtained from utility maximization subject to constraints. Keane and Mo‰tt study the tax e¤ects of labor force participation allowing for participation in multiple welfare programs. Chapter 15 502", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 512, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p513::c0", "text": "A di¤erent approach to relaxing IIA is to specify a hierarchical model. The most popular of these is called the nested logit model. McFadden (1984) gives a detailed treatment of these and other models; here we illustrate the basic approach where there are only two hierarchies. Suppose that the total number of alternatives can be put into S groups of similar alternatives, and let Gs denote the number of alternatives within group s. Thus the ﬁrst hierarchy corresponds to which of the S groups y falls into, and the second cor- responds to the actual alternative within each group. McFadden (1981) studied the model Pðy A Gs j xÞ ¼ as X j A Gs expðr\u00011 s xjbÞ \" #rs ( ), X S r¼1 ar X j A Gr expðr\u00011 r xjbÞ \" #rr ( ) ð15:84Þ and Pðy ¼ j j y A Gs; xÞ ¼ expðr\u00011 s xjbÞ= X h A Gs expðr\u00011 s xhbÞ \" # ð15:85Þ where equation (15.84) is deﬁned for s ¼ 1; 2; . . . ; S while equation (15.85) is deﬁned for j A Gs and s ¼ 1; 2; . . . ; S; of course, if j B Gs, Pðy ¼ j j y A Gs; xÞ ¼ 0. This model requires a normalization restriction, usually a1 ¼ 1. Equation (15.84) gives the prob- ability that the outcome is in group s (conditional on x); then, conditional on y A Gs, equation (15.85) gives the probability of choosing alternative j within Gs. The re- sponse probability Pðy ¼ j j xÞ, which is ultimately of interest, is obtained by multi- plying equations (15.84) and (15.85). This model can be derived by specifying a particular joint distribution for ai in equation (15.79); see Amemiya (1985, p. 303). Equation (15.85) implies that, conditional on choosing group s, the response probabilities take a conditional logit form with parameter vector r\u00011 s b. This suggests a natural two-step estimation procedure. First, estimate ls 1 r\u00011 s b, s ¼ 1; 2; . . . ; S, by applying conditional logit analysis separately to each of the groups. Then, plug the ^ls into equation (15.84) and estimate as, s ¼ 2; . . . ; S and rs, s ¼ 1; . . . ; S by maximizing the log-likelihood function X N i¼1 X S s¼1 1½yi A Gs\u0002 log½qsðxi; ^l; a; rÞ\u0002 where qsðx; l; a; rÞ is the probability in equation (15.84) with ls ¼ r\u00011 s b. This two- step conditional MLE is consistent and ﬃﬃﬃﬃ N p -asymptotically normal under general Discrete Response Models 503", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 513, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p514::c0", "text": "conditions, but the asymptotic variance needs to be adjusted for the ﬁrst-stage esti- mation of the ls; see Chapters 12 and 13 for more on two-step estimators. Of course, we can also use full maximum likelihood. The log likelihood for obser- vation i can be written as liðb; a; rÞ ¼ X S s¼1 ð1½yi A Gs\u0002flog½qsðxi; b; a; rÞ\u0002 þ 1½yi ¼ j\u0002 log½ psjðxi; b; rsÞ\u0002gÞ ð15:86Þ where qsðxi; b; a; rÞ is the probability in equation (15.84) and psjðxi; b; rsÞ is the probability in equation (15.85). The regularity conditions for MLE are satisﬁed under weak assumptions. When as ¼ 1 and rs ¼ 1 for all s, the nested logit model reduces to the conditional logit model. Thus, a test of IIA (as well as the other assumptions underlying the CL model) is a test of H0: a2 ¼ \u0003 \u0003 \u0003 ¼ aS ¼ r1 ¼ \u0003 \u0003 \u0003 ¼ rS ¼ 1. McFadden (1987) suggests a score test, which only requires estimation of the conditional logit model. Often special cases of the model are used, such as setting each as to unity and estimating the rs. In his study of community choice and type of dwelling within a community, McFadden (1978) imposes this restriction along with rs ¼ r for all s, so that the model has only one more parameter than the conditional logit model. This approach allows for correlation among the aj for j belonging to the same community group, but the correlation is assumed to be the same for all communities. Higher-level nested-logit models are covered in McFadden (1984) and Amemiya (1985, Chapter 9). 15.10 Ordered Response Models 15.10.1 Ordered Logit and Ordered Probit Another kind of multinomial response is an ordered response. As the name suggests, if y is an ordered response, then the values we assign to each outcome are no longer arbitrary. For example, y might be a credit rating on a scale from zero to six, with y ¼ 6 representing the highest rating and y ¼ 0 the lowest rating. The fact that six is a better rating than ﬁve conveys useful information, even though the credit rating itself only has ordinal meaning. For example, we cannot say that the di¤erence between four and two is somehow twice as important as the di¤erence between one and zero. Let y be an ordered response taking on the values f0; 1; 2; . . . ; J} for some known integer J. The ordered probit model for y (conditional on explanatory variables x) can be derived from a latent variable model. Assume that a latent variable y\u0004 is deter- mined by Chapter 15 504", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 514, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p515::c0", "text": "y\u0004 ¼ xb þ e; e j x @ Normalð0; 1Þ ð15:87Þ where b is K \u0005 1 and, for reasons to be seen, x does not contain a constant. Let a1 < a2 < \u0003 \u0003 \u0003 < aJ be unknown cut points (or threshold parameters), and deﬁne y ¼ 0 if y\u0004 a a1 y ¼ 1 if a1 < y\u0004 a a2 .. . y ¼ J if y\u0004 > aJ ð15:88Þ For example, if y takes on the values 0, 1, and 2, then there are two cut points, a1 and a2. Given the standard normal assumption for e, it is straightforward to derive the conditional distribution of y given x; we simply compute each response probability: Pðy ¼ 0 j xÞ ¼ Pðy\u0004 a a1 j xÞ ¼ Pðxb þ e a a1 j xÞ ¼ Fða1 \u0001 xbÞ Pðy ¼ 1 j xÞ ¼ Pða1 < y\u0004 a a2 j xÞ ¼ Fða2 \u0001 xbÞ \u0001 Fða1 \u0001 xbÞ ... Pðy ¼ J \u0001 1 j xÞ ¼ PðaJ\u00011 < y\u0004 a aJ j xÞ ¼ FðaJ \u0001 xbÞ \u0001 FðaJ\u00011 \u0001 xbÞ Pðy ¼ J j xÞ ¼ Pðy\u0004 > aJ j xÞ ¼ 1 \u0001 FðaJ \u0001 xbÞ You can easily verify that these sum to unity. When J ¼ 1 we get the binary probit model: Pðy ¼ 1 j xÞ ¼ 1 \u0001 Pðy ¼ 0 j xÞ ¼ 1 \u0001 Fða1 \u0001 xbÞ ¼ Fðxb \u0001 a1Þ, and so \u0001a1 is the intercept inside F. It is for this reason that x does not contain an intercept in this formulation of the ordered probit model. (When there are only two outcomes, zero and one, we set the single cut point to zero and estimate the intercept; this approach leads to the standard probit model.) The parameters a and b can be estimated by maximum likelihood. For each i, the log-likelihood function is liða; bÞ ¼ 1½yi ¼ 0\u0002 log½Fða1 \u0001 xibÞ\u0002 þ 1½yi ¼ 1\u0002 log½Fða2 \u0001 xibÞ \u0001 Fða1 \u0001 xibÞ\u0002 þ \u0003 \u0003 \u0003 þ 1½yi ¼ J\u0002 log½1 \u0001 FðaJ \u0001 xibÞ\u0002 ð15:89Þ This log-likelihood function is well behaved, and many statistical packages routinely estimate ordered probit models. Other distribution functions can be used in place of F. Replacing F with the logit function, L, gives the ordered logit model. In either case we must remember that b, by Discrete Response Models 505", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 515, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p516::c0", "text": "itself, is of limited interest. In most cases we are not interested in Eðy\u0004 j xÞ ¼ xb, as y\u0004 is an abstract construct. Instead, we are interested in the response probabilities Pðy ¼ j j xÞ, just as in the ordered response case. For the ordered probit model qp0ðxÞ=qxk ¼ \u0001bkfða1 \u0001 xbÞ, qpJðxÞ=qxk ¼ bkfðaJ \u0001 xbÞ qpjðxÞ=qxk ¼ bk½fðaj\u00011 \u0001 xbÞ \u0001 fðaj \u0001 xbÞ\u0002; 0 < j < J and the formulas for the ordered logit model are similar. In making comparisons across di¤erent models—in particular, comparing ordered probit and ordered logit— we must remember to compare estimated response probabilities at various values of x, such as x; the ^b are not directly comparable. In particular, the ^aj are important determinants of the magnitudes of the estimated probabilities and partial e¤ects. (Therefore, treatments of ordered probit that refer to the aj as ancillary, or second- ary, parameters are misleading.) While the direction of the e¤ect of xk on the probabilities Pðy ¼ 0 j xÞ and Pðy ¼ J j xÞ is unambiguously determined by the sign of bk, the sign of bk does not always determine the direction of the e¤ect for the intermediate outcomes, 1; 2; . . . ; J \u0001 1. To see this point, suppose there are three possible outcomes, 0, 1, and 2, and that bk > 0. Then qp0ðxÞ=qxk < 0 and qp2ðxÞ=qxk > 0, but qp1ðxÞ=qxk could be either sign. If ja1 \u0001 xbj < ja2 \u0001 xbj, the scale factor, fða1 \u0001 xbÞ \u0001 fða2 \u0001 xbÞ, is positive; otherwise it is negative. (This conclusion follows because the standard normal pdf is symmetric about zero, reaches its maximum at zero, and declines monotonically as its argument increases in absolute value.) As with multinomial logit, for ordered responses we can compute the percent cor- rectly predicted, for each outcome as well as overall: our prediction for y is simply the outcome with the highest probability. Ordered probit and logit can also be applied when y is given quantitative meaning but we wish to acknowledge the discrete, ordered nature of the response. For exam- ple, suppose that individuals are asked to give one of three responses on how their pension funds are invested: ‘‘mostly bonds,’’ ‘‘mixed,’’ and ‘‘mostly stocks.’’ One possibility is to assign these outcomes as 0, 1, 2 and apply ordered probit or ordered logit to estimate the e¤ects of various factors on the probability of each outcome. Instead, we could assign the percent invested in stocks as, say 0, 50, and 100, or 25, 50, and 75. For estimating the probabilities of each category it is irrelevant how we assign the percentages as long as the order is preserved. However, if we give quanti- tative meaning to y, the expected value of y has meaning. We have Eðy j xÞ ¼ a0Pðy ¼ a0 j xÞ þ a1Pðy ¼ a1 j xÞ þ \u0003 \u0003 \u0003 þ aJPðy ¼ aJ j xÞ Chapter 15 506", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 516, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p517::c0", "text": "where a0 < a1 < \u0003 \u0003 \u0003 < aJ are the J values taken on by y. Once we have estimated the response probabilities by ordered probit or ordered logit, we can easily estimate Eðy j xÞ for any value of x, for example, x. Estimates of the expected values can be compared at di¤erent values of the explanatory variables to obtain partial e¤ects for discrete xj. Example 15.5 (Asset Allocation in Pension Plans): The data in PENSION.RAW are a subset of data used by Papke (1998) in assessing the impact of allowing indi- viduals to choose their own allocations on asset allocation in pension plans. Initially, Papke codes the responses ‘‘mostly bonds,’’ ‘‘mixed,’’ and ‘‘mostly stocks’’ as 0, 50, and 100, and uses a linear regression model estimated by OLS. The binary explana- tory variable choice is unity if the person has choice in how his or her pension fund is invested. Controlling for age, education, gender, race, marital status, income (via a set of dummy variables), wealth, and whether the plan is proﬁt sharing, gives the OLS estimate ^bchoice ¼ 12:05 (se ¼ 6.30), where N ¼ 194. This result means that, other things equal, a person having choice has about 12 percentage points more assets in stocks. The ordered probit coe‰cient on choice is .371 (se ¼ .184). The magnitude of the ordered probit coe‰cient does not have a simple interpretation, but its sign and sta- tistical signiﬁcance agree with the linear regression results. (The estimated cut points are ^a1 ¼ \u00013:087 and ^a2 ¼ \u00012:054.) To get an idea of the magnitude of the estimated e¤ect of choice on the expected percent in stocks, we can estimate Eðy j xÞ with choice ¼ 1 and choice ¼ 0, and obtain the di¤erence. However, we need to choose values for the other regressors. For illustration, suppose the person is 60 years old, has 13.5 years of education (roughly the averages in the sample), is a single, nonblack male, has annual income between $50,000 and $75,000, and had wealth in 1989 of $200,000 (also close to the sample average). Then, for choice ¼ 1, ^Eðpctstck j xÞA 50:4, and with choice ¼ 0, ^Eðpctstck j xÞA37:6. The di¤erence, 12.8, is remarkably close to the linear model estimate of the e¤ect on choice. For ordered probit, the percentages correctly predicted for each category are 51.6 (mostly bonds), 43.1 (mixed), and 37.9 (mostly stocks). The overall percentage cor- rectly predicted is about 44.3. The speciﬁcation issues discussed in Section 15.7 for binary probit have analogues for ordered probit. The presence of normally distributed unobserved heterogeneity that is independent of x does not cause any problems when average partial e¤ects are the focus. We can test for continuous endogenous variables in a manner very similar to the Rivers and Vuong (1988) procedure for binary probit, and maximum likeli- Discrete Response Models 507", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 517, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p518::c0", "text": "hood estimation is possible if we make a distributional assumption for the endoge- nous explanatory variable. Heteroskedasticity in the latent error e in equation (15.87) changes the form of the response probabilities and, therefore, Eðy j xÞ when y has quantitative meaning. If the heteroskedasticity is modeled, for example, as expðx1d1Þ where x1 is a subset of x, then maximum likelihood can be used to estimate b, a and d1. However, as with the probit case, we must compute the partial e¤ects on the response probabilities in comparing di¤erent models. It does not make sense to compare estimates of b with and without heteroskedasticity. Score tests for heteroskedasticity are also easily derived along the lines of Section 15.5.3. Similar comments hold for deviations from normality in the latent variable model. Unobserved e¤ects ordered probit models can be handled by adapting Chamber- lain’s approach for binary probit in Section 15.8.2. The latent variable model can be written as y\u0004 it ¼ xitb þ ci þ eit; eit j xi @ Normalð0; 1Þ; t ¼ 1; . . . ; T and yit ¼ 0 if y\u0004 it a a1, yit ¼ 1 if a1 < y\u0004 it a a2, and so on. Certain embellishments are possible, such as letting the aj change over time. Assumption (15.67) allows estima- tion of the average partial e¤ects by using a pooled ordered probit of yit on 1, xit, xi. A full conditional MLE analysis is possible when we add assumption (15.61). The details are very similar to the probit case and are omitted. 15.10.2 Applying Ordered Probit to Interval-Coded Data The ordered probit model can be modiﬁed to apply to a very di¤erent situation. When the quantitative outcome we would like to explain is grouped into intervals, we say that we have interval-coded data. Speciﬁcally, suppose that y\u0004 is a variable with quantitative meaning, such as family wealth, and we are interested in estimating the model Eðy\u0004 j xÞ ¼ xb, where x1 ¼ 1. (Therefore, y\u0004 is no longer a vague, latent vari- able.) If we observed y\u0004 we would just use OLS to estimate b. However, because we only observe whether, say, wealth falls into one of several cells, we have a data-coding problem. We can still consistently estimate b if we make a distributional assumption. Let a1 < a2 < \u0003 \u0003 \u0003 < aJ denote the known cell limits, and deﬁne y as in equations (15.88), but with aj replacing the unknown parameter aj. Because our interest is now in the linear model for Eðy\u0004 j xÞ, we replace the standard normal assumption in equation (15.87) with the assumption y\u0004 j x @ Normalðxb; s2Þ, where s2 ¼ Varðy\u0004 j xÞ is assumed not to depend on x. The parameters of b and s2 can be estimated by maximum likelihood by deﬁning the log likelihood for observation i as in equa- tion (15.89), but with ðb; s2Þ replacing ða; bÞ, and ðaj \u0001 xbÞ=s replacing ðaj \u0001 xbÞ. Chapter 15 508", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 518, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p519::c0", "text": "(Remember, now we do not estimate the cut points, as these are set by the data col- lection scheme.) Many ordered probit software routines assume that our purpose in estimating an ordered probit model is to model a qualitative, ordered response. This assumption means that the cut points are always estimated and that s2 is normalized to be one, but these characteristics are not what we want in interval coding applications. For- tunately, some econometrics packages have a feature for interval regression, which is exactly ordered probit with the cut points ﬁxed and with b and s2 estimated by maximum likelihood. When applying ordered probit to interval regression, it is important to remember that the bj are interpretable as if we had observed y\u0004 i for each i and estimated Eðy\u0004 j xÞ ¼ xb by OLS. Our ability to estimate the partial e¤ects of the xj is due to the strong assumption that y\u0004 given x satisﬁes the classical linear model assumptions; without these assumptions, the ordered probit estimator of b would be inconsistent. A simpler method, which sometimes works well for approximating the partial e¤ects on y\u0004, is to deﬁne an artiﬁcial dependent variable. For each observation i, wi is the midpoint of the reported interval. If all we know is that y\u0004 i > aJ, so that the response is in the cell unbounded from above, we might set wi equal to aJ, or some value above aJ, perhaps based on an estimated cell average from aggregate data or other data sets. (For example, if aJ ¼ 250,000 and we are modeling wealth, we might be able to ﬁnd from a separate data source the average wealth for people with wealth above $250,000.) Once we have deﬁned wi for each observation i, an OLS regression of wi on xi might approximately estimate the bj. If the variable we would like to explain, y\u0004, is strictly positive, it often makes sense to estimate the model logðy\u0004Þ ¼ xb þ u. Of course, this transformation changes the cell limits to logðajÞ. Problems 15.1. Suppose that y is a binary outcome and d1; d2; . . . ; dM are dummy variables for exhaustive and mutually exclusive categories; that is, each person in the popula- tion falls into one and only one category. a. Show that the ﬁtted values from the regression (without an intercept) yi on d1i; d2i; . . . ; dMi; i ¼ 1; 2; . . . ; N are always in the unit interval. In particular, carefully describe the coe‰cient on each dummy variable and the ﬁtted value for each i. Discrete Response Models 509", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 519, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p520::c0", "text": "b. What happens if yi is regressed on M linearly independent, linear combinations of d1i; . . . ; dMi, for example, 1, d2i; d3i; . . . ; dMi? 15.2. Suppose that family i chooses annual consumption ci (in dollars) and chari- table contributions qi (in dollars) to solve the problem max c;q c þ ai logð1 þ qÞ subject to c þ piq a mi; c; q b 0 where mi is income of family i, pi is the price of one dollar of charitable contributions —where pi < 1 because of the tax deductability of charitable contributions, and this price di¤ers across families because of di¤erent marginal tax rates and di¤erent state tax codes—and ai b 0 determines the marginal utility of charitable contributions. Take mi and pi as exogenous to the family in this problem. a. Show that the optimal solution is qi ¼ 0 if ai a pi, and qi ¼ ai=pi \u0001 1 if ai > pi. b. Deﬁne yi ¼ 1 if qi > 0 and yi ¼ 0 if qi ¼ 0, and suppose that ai ¼ expðzig þ viÞ, where zi is a J-vector of observable family traits and vi is unobservable. Assume that vi is independent of ðzi; mi; piÞ and vi=s has symmetric distribution function Gð\u0003Þ, where s2 ¼ VarðviÞ. Show that Pðyi ¼ 1 j zi; mi; piÞ ¼ G½ðzig \u0001 log piÞ=s\u0002 so that yi follows an index model. 15.3. Let z1 be a vector of variables, let z2 be a continuous variable, and let d1 be a dummy variable. a. In the model Pðy ¼ 1 j z1; z2Þ ¼ Fðz1d1 þ g1z2 þ g2z2 2Þ ﬁnd the partial e¤ect of z2 on the response probability. How would you estimate this partial e¤ect? b. In the model Pðy ¼ 1 j z1; z2; d1Þ ¼ Fðz1d1 þ g1z2 þ g2d1 þ g3z2d1Þ ﬁnd the partial e¤ect of z2. How would you measure the e¤ect of d1 on the response probability? How would you estimate these e¤ects? c. Describe how you would obtain the standard errors of the estimated partial e¤ects from parts a and b. Chapter 15 510", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 520, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p521::c0", "text": "15.4. Evaluate the following statement: ‘‘Estimation of a linear probability model is more robust than probit or logit because the LPM does not assume homoskedasticity or a distributional assumption.’’ 15.5. Consider the probit model Pðy ¼ 1 j z; qÞ ¼ Fðz1d1 þ g1z2qÞ where q is independent of z and distributed as Normalð0; 1Þ; the vector z is observed but the scalar q is not. a. Find the partial e¤ect of z2 on the response probability, namely, qPðy ¼ 1 j z; qÞ qz2 b. Show that Pðy ¼ 1 j zÞ ¼ F½z1d1=ð1 þ g2 1z2 2Þ1=2\u0002. c. Deﬁne r1 1 g2 1. How would you test H0: r1 ¼ 0? d. If you have reason to believe r1 > 0, how would you estimate d1 along with r1? 15.6. Consider taking a large random sample of workers at a given point in time. Let sicki ¼ 1 if person i called in sick during the last 90 days, and zero otherwise. Let zi be a vector of individual and employer characteristics. Let cigsi be the number of cigarettes individual i smokes per day (on average). a. Explain the underlying experiment of interest when we want to examine the e¤ects of cigarette smoking on workdays lost. b. Why might cigsi be correlated with unobservables a¤ecting sicki? c. One way to write the model of interest is Pðsick ¼ 1 j z; cigs; q1Þ ¼ Fðz1d1 þ g1cigs þ q1Þ where z1 is a subset of z and q1 is an unobservable variable that is possibly correlated with cigs. What happens if q1 is ignored and you estimate the probit of sick on z1, cigs? d. Can cigs have a conditional normal distribution in the population? Explain. e. Explain how to test whether cigs is exogenous. Does this test rely on cigs having a conditional normal distribution? f. Suppose that some of the workers live in states that recently implemented no- smoking laws in the workplace. Does the presence of the new laws suggest a good IV candidate for cigs? Discrete Response Models 511", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 521, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p522::c0", "text": "15.7. Use the data in GROGGER.RAW for this question. a. Deﬁne a binary variable, say arr86, equal to unity if a man was arrested at least once during 1986, and zero otherwise. Estimate a linear probability model relating arr86 to pcnv, avgsen, tottime, ptime86, inc86, black, hispan, and born60. Report the usual and heteroskedasticity-robust standard errors. What is the estimated e¤ect on the probability of arrest if pcnv goes from .25 to .75? b. Test the joint signiﬁcance of avgsen and tottime, using a nonrobust and robust test. c. Now estimate the model by probit. At the average values of avgsen, tottime, inc86, and ptime86 in the sample, and with black ¼ 1, hispan ¼ 0, and born60 ¼ 1, what is the estimated e¤ect on the probability of arrest if pcnv goes from .25 to .75? Compare this result with the answer from part a. d. For the probit model estimated in part c, obtain the percent correctly predicted. What is the percent correctly predicted when narr86 ¼ 0? When narr86 ¼ 1? What do you make of these ﬁndings? e. In the probit model, add the terms pcnv2, ptime862, and inc862 to the model. Are these individually or jointly signiﬁcant? Describe the estimated relationship between the probability of arrest and pcnv. In particular, at what point does the probability of conviction have a negative e¤ect on probability of arrest? 15.8. Use the data set BWGHT.RAW for this problem. a. Deﬁne a binary variable, smokes, if the woman smokes during pregnancy. Esti- mate a probit model relating smokes to motheduc, white, and logðfamincÞ. At white ¼ 0 and faminc evaluated at the average in the sample, what is the estimated di¤erence in the probability of smoking for a woman with 16 years of education and one with 12 years of education? b. Do you think faminc is exogenous in the smoking equation? What about motheduc? c. Assume that motheduc and white are exogenous in the probit from part a. Also assume that fatheduc is exogenous to this equation. Estimate the reduced form for logðfamincÞ to see if fatheduc is partially correlated with logð famincÞ. d. Test the null hypothesis that logðfamincÞ is exogenous in the probit from part a. 15.9. Assume that the binary variable y follows a linear probability model. a. Write down the log-likelihood function for observation i. b. Why might maximum likelihood estimation of the LPM be di‰cult? Chapter 15 512", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 522, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p523::c0", "text": "c. Assuming that you can estimate the LPM by MLE, explain why it is valid, as a model selection device, to compare the log likelihood from the LPM with that from logit or probit. 15.10. Suppose you wish to use goodness-of-ﬁt measures to compare the LPM with a model such as logit or probit, after estimating the LPM by ordinary least squares. The usual R-squared from OLS estimation measures the proportion of the variance in y that is explained by ^Pðy ¼ 1 j xÞ ¼ x ^b. a. Explain how to obtain a comparable R-squared measured for the general index model Pðy ¼ 1 j xÞ ¼ GðxbÞ. b. Compute the R-squared measures using the data in CRIME.RAW, where the dependent variable is arr86 and the explanatory variables are pcnv, pcnv2, avgsen, tottime, ptime86, ptime862, inc86, inc862, black, hispan, and born60. Are the R- squareds substantially di¤erent? 15.11. List assumptions under which the pooled probit estimator is a conditional MLE based on the distribution of yi given xi, where yi is the T \u0005 1 vector of binary outcomes and xi is the vector of all explanatory variables across all T time periods. 15.12. Find Pðyi1 ¼ 1; yi2 ¼ 0; yi3 ¼ 0 j xi; ci; ni ¼ 1Þ in the ﬁxed e¤ects logit model with T ¼ 3. 15.13. Suppose that you have a control group, A, and a treatment group, B, and two periods of data. Between the two years, a new policy is implemented that a¤ects group B; see Section 6.3.1. a. If your outcome variable is binary (for example, an employment indicator), and you have no covariates, how would you estimate the e¤ect of the policy? b. If you have covariates, write down a probit model that allows you to estimate the e¤ect of the policy change. Explain in detail how you would estimate this e¤ect. c. How would you get an asymptotic 95 percent conﬁdence interval for the estimate in part b? 15.14. Use the data in PENSION.RAW for this example. a. Estimate a linear model for pctstck, where the explanatory variables are choice, age, educ, female, black, married, ﬁnc25; . . . ; ﬁnc101, wealth89, and prftshr. Why might you compute heteroskedasticity-robust standard errors? b. The sample contains separate observations for some husband-wife pairs. Compute standard errors of the estimates from the model in part a that account for the cluster Discrete Response Models 513", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 523, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p524::c0", "text": "correlation within family. (These should also be heteroskedasticity-robust.) Do the standard errors di¤er much from the usual OLS standard errors, or from the heteroskedasticity-robust standard errors? c. Estimate the model from part a by ordered probit. Estimate Eðpctstck j xÞ for a single, nonblack female with 12 years of education who is 60 years old. Assume she has net worth (in 1989) equal to $150,000 and earns $45,000 a year, and her plan is not proﬁt sharing. Compare this with the estimate of Eðpctstck j xÞ from the linear model. d. If you want to choose between the linear model and ordered probit based on how well each estimates Eðy j xÞ, how would you proceed? 15.15. Suppose that you are hired by a university to estimate the e¤ect of drug usage on college grade point average of undergraduates. The survey data given to you had students choose a range of grade point averages: less than 1.0, 1.0 to 1.5, and so on, with the last interval being 3.5 to 4.0. You have data on family background variables, drug usage, and standardized test scores such as the SAT or ACT. What approach would you use? Provide enough detail so that someone can implement your suggested method. 15.16. Let wtpi denote the willingness of person i from a population to pay for a new public project, such as a new park or the widening of an existing highway. You are interested in the e¤ects of various socioeconomic variables on wtp, and you specify the population model wtp ¼ xb þ u, where x is 1 \u0005 K and Eðu j xÞ ¼ 0. Rather than observe wtpi, each person in the sample is presented with a cost of the project, ri. At this cost the person either favors or does not favor the project. Let yi ¼ 1 if person i favors the project and zero otherwise. a. Assume that yi ¼ 1 if and only if wtpi > ri. If ui is independent of ðxi; riÞ and is distributed as Normalð0; s2Þ, ﬁnd Pðyi ¼ 1 j xi; riÞ. In particular, show that this probability follows a probit model with parameters depending on b and s. b. Let ^g be the K \u0005 1 vector of estimates on x, and let ^d be the coe‰cient on ri, from the probit of yi on xi, ri. Given these estimates, how would you estimate b and s? c. How would you estimate b and s directly? d. Now suppose ui is independent of ðxi; riÞ with cdf Gð\u0003 ; dÞ, where d is an R \u0005 1 vector of parameters. Write down the log likelihood for observation i as a function of b and d. e. Does it make sense to compare the estimates of b for di¤erent choices of Gð\u0003 ; dÞ in part d? Explain. Chapter 15 514", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 524, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p525::c0", "text": "15.17. Let y1; y2; . . . ; yG be a set of discrete outcomes representing a population. These could be outcomes for the same individual, family, ﬁrm, and so on. Some entries could be binary outcomes, others might be ordered outcomes. For a vector of conditioning variables x and unobserved heterogeneity c, assume that y1; y2; . . . ; yG are independent conditional on ðx; cÞ, where fgð\u0003 j x; c; gg oÞ is the density of yg given ðx; cÞ, where gg o is a Pg-vector of parameters. For example, if y1 is a binary outcome, f1ð\u0003 j x; c; g1 oÞ might represent a probit model with response probability Fðxg1 o þ cÞ. a. Write down the density of y ¼ ðy1; y2; . . . ; yGÞ given ðx; cÞ. b. Let hð\u0003 j x; doÞ be the density of c given x, where do is a vector of parameters. Find the density of y given x. Are the yg independent conditional on x? Explain. c. Find the log likelihood for any random draw ðxi; yiÞ. 15.18. Consider Chamberlain’s random e¤ects probit model under assumptions (15.60) and (15.61), but replace assumption (15.67) with ci j xi @ Normal½c þ xix; s2 a expðxilÞ\u0002 so that ci given xi has exponential heteroskedasticity. a. Find Pðyit ¼ 1 j xi; aiÞ, where ai ¼ ci \u0001 Eðci j xiÞ. Does this probability di¤er from the probability under assumption (15.67)? Explain. b. Derive the log-likelihood function by ﬁrst ﬁnding the density of ðyi1; . . . ; yiTÞ given xi. Does it have similarities with the log-likelihood function under assumption (15.67)? c. Assuming you have estimated b, c, x, s2 a , and l by CMLE, how would you esti- mate the average partial e¤ects? fHint: First show that E½Fðxob þ c þ xix þ aiÞ j xi\u0002 ¼ Fðfxob þ c þ xixg=f1 þ s2 a expðxilÞg1=2Þ, and then use the appropriate average across i.g 15.19. Use the data in KEANE.RAW for this question, and restrict your attention to black men who are in the sample all 11 years. a. Use pooled probit to estimate the model Pðemployit ¼ 1 j employi;t\u00011Þ ¼ Fðd0 þ remployi;t\u00011Þ. What assumption is needed to ensure that the usual standard errors and test statistics from pooled probit are asymptotically valid? b. Estimate Pðemployt ¼ 1 j employt\u00011 ¼ 1Þ and Pðemployt ¼ 1 j employt\u00011 ¼ 0Þ. Ex- plain how you would obtain standard errors of these estimates. c. Add a full set of year dummies to the analysis in part a, and estimate the proba- bilities in part b for 1987. Are there important di¤erences with the estimates in part b? Discrete Response Models 515", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 525, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p526::c0", "text": "d. Now estimate a dynamic unobserved e¤ects model using the method described in Section 15.8.4. In particular, add employi;81 as an additional explanatory variable, and use random e¤ects probit software. Use a full set of year dummies. e. Is there evidence of state dependence, conditional on ci? Explain. f. Average the estimated probabilities across employi;81 to get the average partial e¤ect for 1987. Compare the estimates with the e¤ects estimated in part c. 15.20. A nice feature of the Rivers and Vuong (1988) approach to estimating probit models with endogenous explanatory variables—see Section 15.7.2—is that it im- mediately extends to models containing any nonlinear functions of the endogenous explanatory variables. Suppose that the model is y\u0004 1 ¼ z1d1 þ gðy2Þa1 þ u1 along with equations (15.40) and (15.41) and the assumption that ðu1; v2Þ is inde- pendent of z and bivariate normal. Here, gðy2Þ is a row vector of functions of y2; for example, gðy2Þ ¼ ðy2; y2 2Þ. Show that Pðy1 ¼ 1 j z; v2Þ ¼ Ff½z1d1 þ gðy2Þa1 þ y1v2\u0002=ð1 \u0001 r2 1Þ1=2g so that Procedure 15.1 goes through with the minor notational change that gðy2Þ replaces y2 in step b; step a is unchanged. Chapter 15 516", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 526, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p527::c0", "text": "16 Corner Solution Outcomes and Censored Regression Models 16.1 Introduction and Motivation In this chapter we cover a class of models traditionally called censored regression models. Censored regression models generally apply when the variable to be explained is partly continuous but has positive probability mass at one or more points. In order to apply these methods e¤ectively, we must understand that the statistical model underlying censored regression analysis applies to problems that are conceptually very di¤erent. For the most part, censored regression applications can be put into one of two categories. In the ﬁrst case there is a variable with quantitative meaning, call it y\u0001, and we are interested in the population regression Eðy\u0001 j xÞ. If y\u0001 and x were ob- served for everyone in the population, there would be nothing new: we could use standard regression methods (ordinary or nonlinear least squares). But a data prob- lem arises because y\u0001 is censored above or below some value; that is, it is not ob- servable for part of the population. An example is top coding in survey data. For example, assume that y\u0001 is family wealth, and, for a randomly drawn family, the actual value of wealth is recorded up to some threshold, say, $200,000, but above that level only the fact that wealth was more than $200,000 is recorded. Top coding is an example of data censoring, and is analogous to the data-coding problem we dis- cussed in Section 15.10.2 in connection with interval regression. Example 16.1 (Top Coding of Wealth): In the population of all families in the United States, let wealth\u0001 denote actual family wealth, measured in thousands of dollars. Suppose that wealth\u0001 follows the linear regression model Eðwealth\u0001 j xÞ ¼ xb, where x is a 1 \u0002 K vector of conditioning variables. However, we observe wealth\u0001 only when wealth\u0001 a 200. When wealth\u0001 is greater than 200 we know that it is, but we do not know the actual value of wealth. Deﬁne observed wealth as wealth ¼ minðwealth\u0001; 200Þ The deﬁnition wealth ¼ 200 when wealth\u0001 > 200 is arbitrary, but it is useful for deﬁning the statistical model that follows. To estimate b we might assume that wealth\u0001 given x has a homoskedastic normal distribution. In error form, wealth\u0001 ¼ xb þ u; u j x @ Normalð0; s2Þ This is a strong assumption about the conditional distribution of wealth\u0001, something we could avoid entirely if wealth\u0001 were not censored above 200. Under these as- sumptions we can write recorded wealth as wealth ¼ minð200; xb þ uÞ ð16:1Þ", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 527, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p528::c0", "text": "Data censoring also arises in the analysis of duration models, a topic we treat in Chapter 20. A second kind of application of censored regression models appears more often in econometrics and, unfortunately, is where the label ‘‘censored regression’’ is least appropriate. To describe the situation, let y be an observable choice or outcome describing some economic agent, such as an individual or a ﬁrm, with the following characteristics: y takes on the value zero with positive probability but is a continuous random variable over strictly positive values. There are many examples of variables that, at least approximately, have these features. Just a few examples include amount of life insurance coverage chosen by an individual, family contributions to an indi- vidual retirement account, and ﬁrm expenditures on research and development. In each of these examples we can imagine economic agents solving an optimization problem, and for some agents the optimal choice will be the corner solution, y ¼ 0. We will call this kind of response variable a corner solution outcome. For corner solu- tion outcomes, it makes more sense to call the resulting model a corner solution model. Unfortunately, the name ‘‘censored regression model’’ appears to be ﬁrmly entrenched. For corner solution applications, we must understand that the issue is not data observability: we are interested in features of the distribution of y given x, such as Eðy j xÞ and Pðy ¼ 0 j xÞ. If we are interested only in the e¤ect of the xj on the mean response, Eðy j xÞ, it is natural to ask, Why not just assume Eðy j xÞ ¼ xb and apply OLS on a random sample? Theoretically, the problem is that, when y b 0, Eðy j xÞ cannot be linear in x unless the range of x is fairly limited. A related weakness is that the model implies constant partial e¤ects. Further, for the sample at hand, predicted values for y can be negative for many combinations of x and b. These are very sim- ilar to the shortcomings of the linear probability model for binary responses. We have already seen functional forms that ensure that Eðy j xÞ is positive for all values of x and parameters, the leading case being the exponential function, Eðy j xÞ ¼ expðxbÞ. [We cannot use logðyÞ as the dependent variable in a linear re- gression because logð0Þ is undeﬁned.] We could then estimate b using nonlinear least squares (NLS), as in Chapter 12. Using an exponential conditional mean function is a reasonable strategy to follow, as it ensures that predicted values are positive and that the parameters are easy to interpret. However, it also has limitations. First, if y is a corner solution outcome, Varðy j xÞ is probably heteroskedastic, and so NLS could be ine‰cient. While we may be able to partly solve this problem using weighted NLS, any model for the conditional variance would be arbitrary. Probably a more important criticism is that we would not be able to measure the e¤ect of each xj on other features of the distribution of y given x. Two that are commonly of Chapter 16 518", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 528, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p529::c0", "text": "interest are Pðy ¼ 0 j xÞ and Eðy j x; y > 0Þ. By deﬁnition, a model for Eðy j xÞ does not allow us to estimate other features of the distribution. If we make a full distribu- tional assumption for y given x, we can estimate any feature of the conditional dis- tribution. In addition, we will obtain e‰cient estimates of quantities such as Eðy j xÞ. The following example shows how a simple economic model leads to an econo- metric model where y can be zero with positive probability and where the conditional expectation Eðy j xÞ is not a linear function of parameters. Example 16.2 (Charitable Contributions): Problem 15.1 shows how to derive a probit model from a utility maximization problem for charitable giving, using utility function utiliðc; qÞ ¼ c þ ai logð1 þ qÞ, where c is annual consumption, in dollars, and q is annual charitable giving. The variable ai determines the marginal utility of giving for family i. Maximizing subject to the budget constraint ci þ piqi ¼ mi (where mi is family income and pi is the price of a dollar of charitable contributions) and the in- equality constraint c, q b 0, the solution qi is easily shown to be qi ¼ 0 if ai=pi a 1 and qi ¼ ai=pi \u0003 1 if ai=pi > 1. We can write this relation as 1 þ qi ¼ maxð1; ai=piÞ. If ai ¼ expðzig þ uiÞ, where ui is an unobservable independent of ðzi; pi; miÞ and nor- mally distributed, then charitable contributions are determined by the equation logð1 þ qiÞ ¼ max½0; zig \u0003 logðpiÞ þ ui\u0004 ð16:2Þ Comparing equations (16.2) and (16.1) shows that they have similar statistical structures. In equation (16.2) we are taking a maximum, and the lower threshold is zero, whereas in equation (16.1) we are taking a minimum with an upper threshold of 200. Each problem can be transformed into the same statistical model: for a ran- domly drawn observation i from the population, y\u0001 i ¼ xib þ ui; ui j xi @ Normalð0; s2Þ ð16:3Þ yi ¼ maxð0; y\u0001 i Þ ð16:4Þ These equations constitute what is known as the standard censored Tobit model (after Tobin, 1956) or type I Tobit model (which is from Amemiya’s 1985 taxonomy). This is the canonical form of the model in the sense that it is the form usually studied in methodological papers, and it is the default model estimated by many software packages. The charitable contributions example immediately ﬁts into the standard censored Tobit framework by deﬁning xi ¼ ½zi; logðpiÞ\u0004 and yi ¼ logð1 þ qiÞ. This particular transformation of qi and the restriction that the coe‰cient on logðpiÞ is \u00031 depend critically on the utility function used in the example. In practice, we would probably take yi ¼ qi and allow all parameters to be unrestricted. Corner Solution Outcomes and Censored Regression Models 519", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 529, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p530::c0", "text": "The wealth example can be cast as equations (16.3) and (16.4) after a simple transformation: \u0003ðwealthi \u0003 200Þ ¼ maxð0; \u0003200 \u0003 xib \u0003 uiÞ and so the intercept changes, and all slope coe‰cients have the opposite sign from equation (16.1). For data-censoring problems, it is easier to study the censoring scheme directly, and many econometrics packages support various kinds of data censoring. Problem 16.3 asks you to consider general forms of data censoring, including the case when the censoring point can change with observation, in which case the model is often called the censored normal regression model. (This label properly emphasizes the data-censoring aspect.) For the population, we write the standard censored Tobit model as y\u0001 ¼ xb þ u; u j x @ Normalð0; s2Þ ð16:5Þ y ¼ maxð0; y\u0001Þ ð16:6Þ where, except in rare cases, x contains unity. As we saw from the two previous examples, di¤erent features of this model are of interest depending on the type of application. In examples with true data censoring, such as Example 16.1, the vector b tells us everything we want to know because Eðy\u0001 j xÞ ¼ xb is of interest. For corner solution outcomes, such as Example 16.2, b does not give the entire story. Usually, we are interested in Eðy j xÞ or Eðy j x; y > 0Þ. These certainly depend on b, but in a nonlinear fashion. For the statistical model (16.5) and (16.6) to make sense, the variable y\u0001 should have characteristics of a normal random variable. In data censoring cases this re- quirement means that the variable of interest y\u0001 should have a homoskedastic nor- mal distribution. In some cases the logarithmic transformation can be used to make this assumption more plausible. Example 16.1 might be one such case if wealth is positive for all families. See also Problems 16.1 and 16.2. In corner solution examples, the variable y should be (roughly) continuous when y > 0. Thus the Tobit model is not appropriate for ordered responses, as in Section 15.10. Similarly, Tobit should not be applied to count variables, especially when the count variable takes on only a small number of values (such as number of patents awarded annually to a ﬁrm or the number of times someone is arrested during a year). Poisson regression models, a topic we cover in Chapter 19, are better suited for analyzing count data. For corner solution outcomes, we must avoid placing too much emphasis on the latent variable y\u0001. Most of the time y\u0001 is an artiﬁcial construct, and we are not interested in Eðy\u0001 j xÞ. In Example 16.2 we derived the model for charitable con- Chapter 16 520", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 530, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p531::c0", "text": "tributions using utility maximization, and a latent variable never appeared. Viewing y\u0001 as something like ‘‘desired charitable contributions’’ can only sow confusion: the variable of interest, y, is observed charitable contributions. 16.2 Derivations of Expected Values In corner solution applications such as the charitable contributions example, interest centers on probabilities or expectations involving y. Most of the time we focus on the expected values Eðy j x; y > 0Þ and Eðy j xÞ. Before deriving these expectations for the Tobit model, it is interesting to derive an inequality that bounds Eðy j xÞ from below. Since the function gðzÞ 1 maxð0; zÞ is convex, it follows from the conditional Jensen’s inequality (see Appendix 2A) that Eðy j xÞ b max½0; Eðy\u0001 j xÞ\u0004. This condition holds when y\u0001 has any distribution and for any form of Eðy\u0001 j xÞ. If Eðy\u0001 j xÞ ¼ xb, then Eðy j xÞ b maxð0; xbÞ ð16:7Þ which is always nonnegative. Equation (16.7) shows that Eðy j xÞ is bounded from below by the larger of zero and xb. When u is independent of x and has a normal distribution, we can ﬁnd an explicit expression for Eðy j xÞ. We ﬁrst derive Pðy > 0 j xÞ and Eðy j x; y > 0Þ, which are of interest in their own right. Then, we use the law of iterated expectations to obtain Eðy j xÞ: Eðy j xÞ ¼ Pðy ¼ 0 j xÞ \u0005 0 þ Pðy > 0 j xÞ \u0005 Eðy j x; y > 0Þ ¼ Pðy > 0 j xÞ \u0005 Eðy j x; y > 0Þ ð16:8Þ Deriving Pðy > 0 j xÞ is easy. Deﬁne the binary variable w ¼ 1 if y > 0, w ¼ 0 if y ¼ 0. Then w follows a probit model: Pðw ¼ 1 j xÞ ¼ Pðy\u0001 > 0 j xÞ ¼ Pðu > \u0003xb j xÞ ¼ Pðu=s > \u0003xb=sÞ ¼ Fðxb=sÞ ð16:9Þ One implication of equation (16.9) is that g 1 b=s, but not b and s separately, can be consistently estimated from a probit of w on x. To derive Eðy j x; y > 0Þ, we need the following fact about the normal distribution: if z @ Normalð0; 1Þ, then, for any constant c, Eðz j z > cÞ ¼ fðcÞ 1 \u0003 FðcÞ Corner Solution Outcomes and Censored Regression Models 521", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 531, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p532::c0", "text": "where fð\u0005Þ is the standard normal density function. fThis is easily shown by noting that the density of z given z > c is fðzÞ=½1 \u0003 FðcÞ\u0004, z > c, and then integrating zfðzÞ from c to y.g Therefore, if u @ Normalð0; s2Þ, then Eðu j u > cÞ ¼ sE u s \u0001\u0001\u0001\u0001 u s > c s \u0002 \u0003 ¼ s fðc=sÞ 1 \u0003 Fðc=sÞ \u0004 \u0005 We can use this equation to ﬁnd Eðy j x; y > 0Þ when y follows a Tobit model: Eðy j x; y > 0Þ ¼ xb þ Eðu j u > \u0003xbÞ ¼ xb þ s fðxb=sÞ Fðxb=sÞ \u0004 \u0005 ð16:10Þ since 1 \u0003 Fð\u0003xb=sÞ ¼ Fðxb=sÞ. Although it is not obvious from looking at equation (16.10), the right-hand side is positive for any values of x and b; this statement must be true by equations (16.7) and (16.8). For any c the quantity lðcÞ 1 fðcÞ=FðcÞ is called the inverse Mills ratio. Thus, Eðy j x; y > 0Þ is the sum of xb and s times the inverse Mills ratio evaluated at xb=s. If xj is a continuous explanatory variable, then qEðy j x; y > 0Þ qxj ¼ bj þ bj dl dc ðxb=sÞ \u0004 \u0005 assuming that xj is not functionally related to other regressors. By di¤erentiating lðcÞ ¼ fðcÞ=FðcÞ, it can be shown that dl dc ðcÞ ¼ \u0003lðcÞ½c þ lðcÞ\u0004, and therefore qEðy j x; y > 0Þ qxj ¼ bjf1 \u0003 lðxb=sÞ½xb=s þ lðxb=sÞ\u0004g ð16:11Þ This equation shows that the partial e¤ect of xj on Eðy j x; y > 0Þ is not entirely de- termined by bj; there is an adjustment factor multiplying bj, the term in f \u0005 g, that depends on x through the index xb=s. We can use the fact that if z @ Normalð0; 1Þ, then Varðz j z > \u0003cÞ ¼ 1 \u0003 lðcÞ½c þ lðcÞ\u0004 for any c A R, which implies that the adjust- ment factor in equation (16.11), call it yðxb=sÞ ¼ f1 \u0003 lðxb=sÞ½xb=s þ lðxb=sÞ\u0004g, is strictly between zero and one. Therefore, the sign of bj is the same as the sign of the partial e¤ect of xj. Other functional forms are easily handled. Suppose that x1 ¼ logðz1Þ (and that this is the only place z1 appears in x). Then qEðy j x; y > 0Þ qz1 ¼ ðb1=z1Þyðxb=sÞ ð16:12Þ Chapter 16 522", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 532, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p533::c0", "text": "where b1 now denotes the coe‰cient on logðz1Þ. Or, suppose that x1 ¼ z1 and x2 ¼ z2 1. Then qEðy j x; y > 0Þ qz1 ¼ ðb1 þ 2b2z1Þyðxb=sÞ where b1 is the coe‰cient on z1 and b2 is the coe‰cient on z2 1. Interaction terms are handled similarly. Generally, we compute the partial e¤ect of xb with respect to the variable of interest and multiply this by the factor yðxb=sÞ. All of the usual economic quantities such as elasticities can be computed. The elasticity of y with respect to x1, conditional on y > 0, is qEðy j x; y > 0Þ qx1 \u0005 x1 Eðy j x; y > 0Þ ð16:13Þ and equations (16.11) and (16.10) can be used to ﬁnd the elasticity when x1 appears in levels form. If z1 appears in logarithmic form, the elasticity is obtained simply as q log Eðy j x; y > 0Þ=q logðz1Þ. If x1 is a binary variable, the e¤ect of interest is obtained as the di¤erence between Eðy j x; y > 0Þ with x1 ¼ 1 and x1 ¼ 0. Other discrete variables (such as number of children) can be handled similarly. We can also compute Eðy j xÞ from equation (16.8): Eðy j xÞ ¼ Pðy > 0 j xÞ \u0005 Eðy j x; y > 0Þ ¼ Fðxb=sÞ½xb þ slðxb=sÞ\u0004 ¼ Fðxb=sÞxb þ sfðxb=sÞ ð16:14Þ We can ﬁnd the partial derivatives of Eðy j xÞ with respect to continuous xj using the chain rule. In examples where y is some quantity chosen by individuals (labor supply, charitable contributions, life insurance), this derivative accounts for the fact that some people who start at y ¼ 0 may switch to y > 0 when xj changes. Formally, qEðy j xÞ qxj ¼ qPðy > 0 j xÞ qxj \u0005 Eðy j x; y > 0Þ þ Pðy > 0 j xÞ \u0005 qEðy j x; y > 0Þ qxj ð16:15Þ This decomposition is attributed to McDonald and Mo‰tt (1980). Because Pðy > 0 j xÞ ¼ Fðxb=sÞ, qPðy > 0 j xÞ=qxj ¼ ðbj=sÞfðxb=sÞ. If we plug this along with equa- tion (16.11) into equation (16.15), we get a remarkable simpliﬁcation: qEðy j xÞ qxj ¼ Fðxb=sÞbj ð16:16Þ The estimated scale factor for a given x is Fðx^b=^sÞ. This scale factor has a very in- teresting interpretation: Fðx^b=^sÞ ¼ ^Pðy > 0 j xÞ; that is, Fðx^b=^sÞ is the estimated Corner Solution Outcomes and Censored Regression Models 523", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 533, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p534::c0", "text": "probability of observing a positive response given x. If Fðx ^b=^sÞ is close to one, then it is unlikely we observe yi ¼ 0 when xi ¼ x, and the adjustment factor becomes unimportant. In practice, a single adjustment factor is obtained as Fðx ^b=^sÞ, where x denotes the vector of mean values. If the estimated probability of a positive response is close to one at the sample means of the covariates, the adjustment factor can be ignored. In most interesting Tobit applications, Fðx ^b=^sÞ is notably less than unity. For discrete variables or for large changes in continuous variables, we can compute the di¤erence in Eðy j xÞ at di¤erent values of x. [Incidentally, equations (16.11) and (16.16) show that s is not a ‘‘nuisance parameter,’’ as it is sometimes called in Tobit applications: s plays a crucial role in estimating the partial e¤ects of interest in corner solution applications.] Equations (16.9), (16.11), and (16.14) show that, for continuous variables xj and xh, the relative partial e¤ects on Pðy > 0 j xÞ, Eðy j x; y > 0Þ, and Eðy j xÞ are all equal to bj=bh (assuming that bh 0 0). This fact can be a limitation of the Tobit model, something we take up further in Section 16.7. By taking the log of equation (16.8) and di¤erentiating, we see that the elasticity (or semielasticity) of Eðy j xÞ with respect to any xj is simply the sum of the elasticities (or semielasticities) of Fðxb=sÞ and Eðy j x; y > 0Þ, each with respect to xj. 16.3 Inconsistency of OLS We can use the previous expectation calculations to show that OLS using the entire sample or OLS using the subsample for which yi > 0 are both (generally) inconsistent estimators of b. First consider OLS using the subsample with strictly positive yi. From equation (16.10) we can write yi ¼ xib þ slðxib=sÞ þ ei ð16:17Þ Eðei j xi; yi > 0Þ ¼ 0 ð16:18Þ which implies that Eðei j xi; li; yi > 0Þ ¼ 0, where li 1 lðxib=sÞ. It follows that if we run OLS of yi on xi using the sample for which yi > 0, we e¤ectively omit the vari- able li. Correlation between li and xi in the selected subpopulation results in incon- sistent estimation of b. The inconsistency of OLS restricted to the subsample with yi > 0 is especially un- fortunate in the case of true data censoring. Restricting the sample to yi > 0 means we are only using the data on uncensored observations. In the wealth top coding ex- ample, this restriction means we drop all people whose wealth is at least $200,000. In a duration application—see Problem 16.1 and Chapter 20—it would mean using Chapter 16 524", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 534, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p535::c0", "text": "only observations with uncensored durations. It would be convenient if OLS using only the uncensored observations were consistent for b, but such is not the case. From equation (16.14) it is also pretty clear that regressing yi on xi using all of the data will not consistently estimate b: Eðy j xÞ is nonlinear in x, b, and s, so it would be a ﬂuke if a linear regression consistently estimated b. There are some interesting theoretical results about how the slope coe‰cients in b can be estimated up to scale using one of the two OLS regressions that we have dis- cussed. Therefore, each OLS coe‰cient is inconsistent by the same multiplicative factor. This fact allows us—both in data-censoring applications and corner solution applications—to estimate the relative e¤ects of any two explanatory variables. The assumptions made to derive such results are very restrictive, and they generally rule out discrete and other discontinuous regressors. [Multivariate normality of ðx; y\u0001Þ is su‰cient.] The arguments, which rely on linear projections, are elegant—see, for ex- ample, Chung and Goldberger (1984)—but such results have questionable practical value. The previous discussion does not mean a linear regression of yi on xi is uninfor- mative. Remember that, whether or not the Tobit model holds, we can always write the linear projection of y on x as Lðy j xÞ ¼ xg for g ¼ ½Eðx0xÞ\u0004\u00031Eðx0 yÞ, under the mild restriction that all second moments are ﬁnite. It is possible that gj approximates the e¤ect of xj on Eðy j xÞ when x is near its population mean. Similarly, a linear re- gression of yi on xi, using only observations with yi > 0, might approximate the partial e¤ects on Eðy j x; y > 0Þ near the mean values of the xj. Such issues have not been fully explored in corner solution applications of the Tobit model. 16.4 Estimation and Inference with Censored Tobit Let fðxi; yiÞ: i ¼ 1; 2; . . . Ng be a random sample following the censored Tobit model. To use maximum likelihood, we need to derive the density of yi given xi. We have already shown that f ð0 j xiÞ ¼ Pðyi ¼ 0 j xiÞ ¼ 1 \u0003 Fðxib=sÞ. Further, for y > 0, Pðyi a y j xiÞ ¼ Pðy\u0001 i a y j xiÞ, which implies that f ðy j xiÞ ¼ f \u0001ðy j xiÞ; all y > 0 where f \u0001ð\u0005 j xiÞ denotes the density of y\u0001 i given xi. (We use y as the dummy argument in the density.) By assumption, y\u0001 i j xi @ Normalðxib; s2Þ, so f \u0001ðy j xiÞ ¼ 1 s f½ðy \u0003 xibÞ=s\u0004; \u0003y < y < y Corner Solution Outcomes and Censored Regression Models 525", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 535, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p536::c0", "text": "(As in recent chapters, we will use b and s2 to denote the true values as well as dummy arguments in the log-likelihood function and its derivatives.) We can write the density for yi given xi compactly using the indicator function 1½ \u0005 \u0004 as f ðy j xiÞ ¼ f1 \u0003 Fðxib=sÞg1½y¼0\u0004fð1=sÞf½ðy \u0003 xibÞ=s\u0004g1½y>0\u0004 ð16:19Þ where the density is zero for y < 0. Let y 1 ðb 0; s2Þ0 denote the ðK þ 1Þ \u0002 1 vector of parameters. The conditional log likelihood is liðyÞ ¼ 1½yi ¼ 0\u0004 log½1 \u0003 Fðxib=sÞ\u0004 þ 1½yi > 0\u0004flog f½ðyi \u0003 xibÞ=s\u0004 \u0003 logðs2Þ=2g ð16:20Þ Apart from a constant that does not a¤ect the maximization, equation (16.20) can be written as 1½yi ¼ 0\u0004 log½1 \u0003 Fðxib=sÞ\u0004 \u0003 1½yi > 0\u0004fðyi \u0003 xibÞ2=2s2 þ logðs2Þ=2g Therefore, qliðyÞ=qb ¼ \u00031½yi ¼ 0\u0004fðxib=sÞxi=½1 \u0003 Fðxib=sÞ\u0004 þ 1½yi > 0\u0004ðyi \u0003 xibÞxi=s2 (16.21) qliðyÞ=qs2 ¼ 1½yi ¼ 0\u0004fðxib=sÞðxibÞ=f2s2½1 \u0003 Fðxib=sÞ\u0004g þ 1½yi > 0\u0004fðyi \u0003 xibÞ2=ð2s4Þ \u0003 1=ð2s2Þg ð16:22Þ The second derivatives are complicated, but all we need is Aðxi; yÞ 1 \u0003E½HiðyÞ j xi\u0004. After tedious calculations it can be shown that Aðxi; yÞ ¼ aix0 ixi bix0 i bixi ci \u0004 \u0005 ð16:23Þ where ai ¼ \u0003s\u00032fxigfi \u0003 ½f2 i =ð1 \u0003 FiÞ\u0004 \u0003 Fig bi ¼ s\u00033fðxigÞ2fi þ fi \u0003 ½ðxigÞf2 i =ð1 \u0003 FiÞ\u0004g=2 ci ¼ \u0003s\u00034fðxigÞ3fi þ ðxigÞfi \u0003 ½ðxigÞf2 i =ð1 \u0003 FiÞ\u0004 \u0003 2Fig=4 g ¼ b=s, and fi and Fi are evaluated at xig. This matrix is used in equation (13.32) to obtain the estimate of Avarð^yÞ. See Amemiya (1973) for details. Testing is easily carried out in a standard MLE framework. Single exclusion restrictions are tested using asymptotic t statistics once ^bj and its asymptotic standard error have been obtained. Multiple exclusion restrictions are easily tested using the LR statistic, and some econometrics packages routinely compute the Wald statistic. Chapter 16 526", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 536, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p537::c0", "text": "If the unrestricted model has so many variables that computation becomes an issue, the LM statistic is an attractive alternative. The Wald statistic is the easiest to compute for testing nonlinear restrictions on b, just as in binary response analysis, because the unrestricted model is just standard Tobit. 16.5 Reporting the Results For data censoring applications, the quantities of interest are the ^bj and their stan- dard errors. (We might use these to compute elasticities, and so on.) We interpret the estimated model as if there were no data-censoring problem, because the population model is a linear conditional mean. The value of the log-likelihood function should be reported for any estimated model because of its role in obtaining likelihood ratio statistics. We can test for omitted variables, including nonlinear functions of already included variables, using either t tests or LR tests. All of these rely on the homo- skedastic normal assumption in the underlying population. For corner solution applications, the same statistics can be reported, and, in addi- tion, we should report estimated partial e¤ects on Eðy j x; y > 0Þ and Eðy j xÞ. The formulas for these are given in Section 16.2, where b and s are replaced with their MLEs. Because these estimates depend on x, we must decide at what values of x to report the partial e¤ects or elasticities. As with probit, the average values of x can be used, or, if some elements of x are qualitative variables, we can assign them values of particular interest. For the important elements of x, the partial e¤ects or elasticities can be estimated at a range of values, holding the other elements ﬁxed. For example, if x1 is price, then we can compute equation (16.11) or (16.16), or the corresponding elasticities, for low, medium, and high prices, while keeping all other elements ﬁxed. If x1 is a dummy variable, then we can obtain the di¤erence in estimates with x1 ¼ 1 and x1 ¼ 0, holding all other elements of x ﬁxed. Standard errors of these estimates can be obtained by the delta method, although the calculations can be tedious. Example 16.3 (Annual Hours Equation for Married Women): We use the Mroz (1987) data (MROZ.RAW) to estimate a reduced form annual hours equation for married women. The equation is a reduced form because we do not include hourly wage o¤er as an explanatory variable. The hourly wage o¤er is unlikely to be exog- enous, and, just as importantly, we cannot observe it when hours ¼ 0. We will show how to deal with both these issues in Chapter 17. For now, the explanatory variables are the same ones appearing in the labor force participation probit in Example 15.2. Of the 753 women in the sample, 428 worked for a wage outside the home during the year; 325 of the women worked zero hours. For the women who worked positive Corner Solution Outcomes and Censored Regression Models 527", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 537, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p538::c0", "text": "hours, the range is fairly broad, ranging from 12 to 4,950. Thus, annual hours worked is a reasonable candidate for a Tobit model. We also estimate a linear model (using all 753 observations) by OLS. The results are in Table 16.1. Not surprisingly, the Tobit coe‰cient estimates are the same sign as the corre- sponding OLS estimates, and the statistical signiﬁcance of the estimates is similar. (Possible exceptions are the coe‰cients on nwifeinc and kidsge6, but the t statistics have similar magnitudes.) Second, though it is tempting to compare the magnitudes of the OLS estimates and the Tobit estimates, such comparisons are not very infor- mative. We must not think that, because the Tobit coe‰cient on kidslt6 is roughly twice that of the OLS coe‰cient, the Tobit model somehow implies a much greater response of hours worked to young children. We can multiply the Tobit estimates by the adjustment factors in equations (16.11) and (16.16), evaluated at the estimates and the mean values of the xj (but where we square exper rather than use the average of the exper2 i Þ, to obtain the partial e¤ects on the conditional expectations. The factor in equation (16.11) is about .451. For example, conditional on hours being positive, a year of education (starting from the mean values of all variables) is estimated to increase expected hours by about Table 16.1 OLS and Tobit Estimation of Annual Hours Worked Dependent Variable: hours Independent Variable Linear (OLS) Tobit (MLE) nwifeinc \u00033.45 (2.54) \u00038.81 (4.46) educ 28.76 (12.95) 80.65 (21.58) exper 65.67 (9.96) 131.56 (17.28) exper2 \u0003.700 (.325) \u00031.86 (0.54) age \u000330.51 (4.36) \u000354.41 (7.42) kidslt6 \u0003442.09 (58.85) \u0003894.02 (111.88) kidsge6 \u000332.78 (23.18) \u000316.22 (38.64) constant 1,330.48 (270.78) 965.31 (446.44) Log-likelihood value — \u00033,819.09 R-squared .266 .275 ^s 750.18 1,122.02 Chapter 16 528", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 538, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p539::c0", "text": ".451(80.65)A36.4 hours. Using the approximation for one more young child gives a fall in expected hours by about (.451)(894.02)A403.2. Of course, this ﬁgure does not make sense for a woman working less than 403.2 hours. It would be better to estimate the expected values at two di¤erent values of kidslt6 and form the di¤erence, rather than using the calculus approximation. The factor in equation (16.16), again evaluated at the mean values of the xj, is about .645. This result means that the estimated probability of a woman being in the workforce, at the mean values of the covariates, is about .645. Therefore, the mag- nitudes of the e¤ects of each xj on expected hours—that is, when we account for people who initially do not work, as well as those who are initially working—is larger than when we condition on hours > 0. We can multiply the Tobit coe‰cients, at least those on roughly continuous explanatory variables, by .645 to make them roughly comparable to the OLS estimates in the ﬁrst column. In most cases the estimated Tobit e¤ect at the mean values are signiﬁcantly above the corresponding OLS estimate. For example, the Tobit e¤ect of one more year of education is about .645(80.65)A52.02, which is well above the OLS estimate of 28.76. We have reported an R-squared for both the linear regression model and the Tobit model. The R-squared for OLS is the usual one. For Tobit, the R-squared is the square of the correlation coe‰cient between yi and ^yi, where ^yi ¼ Fðxi ^b=^sÞxi ^b þ ^sfðxi ^b=^sÞ is the estimate of Eðy j x ¼ xiÞ. This statistic is motivated by the fact that the usual R-squared for OLS is equal to the squared correlation between the yi and the OLS ﬁtted values. Based on the R-squared measures, the Tobit conditional mean function ﬁts the hours data somewhat better, although the di¤erence is not overwhelming. However, we should remember that the Tobit estimates are not chosen to maximize an R-squared— they maximize the log-likelihood function—whereas the OLS estimates produce the highest R-squared given the linear functional form for the conditional mean. When two additional variables, the local unemployment rate and a binary city in- dicator, are included, the log likelihood becomes about \u00033,817.89. The likelihood ratio statistic is about 2(3,819.09 \u0003 3,817.89) ¼ 2.40. This is the outcome of a w2 2 variate under H0, and so the p-value is about .30. Therefore, these two variables are jointly insigniﬁcant. 16.6 Speciﬁcation Issues in Tobit Models 16.6.1 Neglected Heterogeneity Suppose that we are initially interested in the model y ¼ maxð0; xb þ gq þ uÞ; u j x; q @ Normalð0; s2Þ ð16:24Þ Corner Solution Outcomes and Censored Regression Models 529", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 539, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p540::c0", "text": "where q is an unobserved variable that is assumed to be independent of x and has a Normalð0; t2Þ distribution. It follows immediately that y ¼ maxð0; xb þ vÞ; v j x @ Normalð0; s2 þ g2t2Þ ð16:25Þ Thus, y conditional on x follows a Tobit model, and Tobit of y on x consistently estimates b and h2 1 s2 þ g2t2. In data-censoring cases we are interested in b; g is of no use without observing q, and g cannot be estimated anyway. We have shown that heterogeneity independent of x and normally distributed has no important con- sequences in data-censoring examples. Things are more complicated in corner solution examples because, at least initially, we are interested in Eðy j x; qÞ or Eðy j x; q; y > 0Þ. As we discussed in Sections 2.2.5 and 15.7.1, we are often interested in the average partial e¤ects (APEs), where, say, Eðy j x; qÞ is averaged over the population distribution of q, and then derivatives or di¤erences with respect to elements of x are obtained. From Section 2.2.5 we know that when the heterogeneity is independent of x, the APEs are obtained by ﬁnding Eðy j xÞ [or Eðy j x; y > 0Þ]. Naturally, these conditional means come from the dis- tribution of y given x. Under the preceding assumptions, it is exactly this distribution that Tobit of y on x estimates. In other words, we estimate the desired quantities— the APEs—by simply ignoring the heterogeneity. This is the same conclusion we reached for the probit model in Section 15.7.1. If q is not normal, then these arguments do not carry over because y given x does not follow a Tobit model. But the ﬂavor of the argument does. A more di‰cult issue arises when q and x are correlated, and we address this in the next subsection. 16.6.2 Endogenous Explanatory Variables Suppose we now allow one of the variables in the Tobit model to be endogenous. The model is y1 ¼ maxð0; z1d1 þ a1y2 þ u1Þ ð16:26Þ y2 ¼ zd2 þ v2 ¼ z1d21 þ z2d22 þ v2 ð16:27Þ where ðu1; v2Þ are zero-mean normally distributed, independent of z. If u1 and v2 are correlated, then y2 is endogenous. For identiﬁcation we need the usual rank condi- tion d22 0 0; Eðz0zÞ is assumed to have full rank, as always. If equation (16.26) represents a data-censoring problem, we are interested, as always, in the parameters, d1 and a1, as these are the parameters of interest in the uncensored population model. For corner solution outcomes, the quantities of interest are more subtle. However, when the endogeneity of y2 is due to omitted variables or simulta- neity, the parameters we need to estimate to obtain average partial e¤ects are d1, a1, Chapter 16 530", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 540, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p541::c0", "text": "and s2 1 ¼ Varðu1Þ. The reasoning is just as for the probit model in Section 15.7.2. Holding other factors ﬁxed, the di¤erence in y1 when y2 changes from y2 to y2 þ 1 is max½0; z1d1 þ a1ðy2 þ 1Þ þ u1\u0004 \u0003 max½0; z1d1 þ a1y2 þ u1\u0004 Averaging this expression across the distribution of u1 gives di¤erences in expecta- tions that have the form (16.14), with x ¼ ½z1; ðy2 þ 1Þ\u0004 in the ﬁrst case, x ¼ ðz1; y2Þ in the second, and s ¼ s1. Importantly, unlike in the data censoring case, we need to estimate s2 1 in order to estimate the partial e¤ects of interest (the APEs). Before estimating this model by maximum likelihood, a procedure that requires obtaining the distribution of ðy1; y2Þ given z, it is convenient to have a two-step procedure that also delivers a simple test for the endogeneity of y2. Smith and Blundell (1986) propose a two-step procedure that is analogous to the Rivers-Vuong method (see Section 15.7.2) for binary response models. Under bivariate normality of ðu1; v2Þ, we can write u1 ¼ y1v2 þ e1 ð16:28Þ where y1 ¼ h1=t2 2, h1 ¼ Covðu1; v2Þ, t2 2 ¼ Varðv2Þ, and e1 is independent of v2 with a zero-mean normal distribution and variance, say, t2 1. Further, because ðu1; v2Þ is in- dependent of z, e1 is independent of ðz; v2Þ. Now, plugging equation (16.28) into equation (16.26) gives y1 ¼ maxð0; z1d1 þ a1 y2 þ y1v2 þ e1Þ ð16:29Þ where e1 j z; v2 @ Normalð0; t2 1Þ. It follows that, if we knew v2, we would just estimate d1, a1, y1, and t2 1 by standard censored Tobit. We do not observe v2 because it depends on the unknown vector d2. However, we can easily estimate d2 by OLS in a ﬁrst stage. The Smith-Blundell procedure is as follows: Procedure 16.1: (a) Estimate the reduced form of y2 by OLS; this step gives ^d2. Deﬁne the reduced-form OLS residuals as ^v2 ¼ y2 \u0003 z^d2. (b) Estimate a standard Tobit of y1 on z1, y2, and ^v2. This step gives consistent estimators of d1, a1, y1, and t2 1. The usual t statistic on ^v2 reported by Tobit provides a simple test of the null H0: y1 ¼ 0, which says that y2 is exogenous. Further, under y1 ¼ 0, e1 ¼ u1, and so normality of v2 plays no role: as a test for endogeneity of y2, the Smith-Blundell approach is valid without any distributional assumptions on the reduced form of y2. Example 16.4 (Testing Exogeneity of Education in the Hours Equation): As an illus- tration, we test for endogeneity of educ in the reduced-form hours equation in Example 16.3. We assume that motheduc, fatheduc, and huseduc are exogenous in the hours Corner Solution Outcomes and Censored Regression Models 531", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 541, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p542::c0", "text": "equation, and so these are valid instruments for educ. We ﬁrst obtain ^v2 as the OLS residuals from estimating the reduced form for educ. When ^v2 is added to the Tobit model in Example 16.3 (without unem and city), its coe‰cient is 39.88 with t statistic ¼ .91. Thus, there is little evidence that educ is endogenous in the equation. The test is valid under the null hypothesis that educ is exogenous even if educ does not have a conditional normal distribution. When y1 0 0, the second-stage Tobit standard errors and test statistics are not asymptotically valid because ^d2 has been used in place of d2. Smith and Blundell (1986) contain formulas for correcting the asymptotic variances; these can be derived using the formulas for two-step M-estimators in Chapter 12. It is easily seen that joint normality of ðu1; v2Þ is not absolutely needed for the procedure to work. It su‰ces that u1 conditional on z and v2 is distributed as Normalðy1v2; t2 1Þ. Still, this is a fairly restrictive assumption. When y1 0 0, the Smith-Blundell procedure does not allow us to estimate s2 1, which is needed to estimate average partial e¤ects in corner solution outcomes. Nevertheless, we can obtain consistent estimates of the average partial e¤ects by using methods similar to those in the probit case. Using the same reasoning in Sec- tion 15.7.2, the APEs are obtained by computing derivatives or di¤erences of Ev2½mðz1d1 þ a1 y2 þ y1v2; t2 1Þ\u0004 ð16:30Þ where mðz; s2Þ 1 Fðz=sÞz þ sfðz=sÞ and Ev2½ \u0005 \u0004 denotes expectation with respect to the distribution of v2. Using the same argument as in Section 16.6.1, expression (16.30) can be written as mðz1d1 þ a1 y2; y2 1t2 2 þ t2 1Þ. Therefore, consistent estimators of the APEs are obtained by taking, with respect to elements of ðz1; y2Þ, derivatives or di¤erences of mðz1 ^d1 þ ^a1 y2; ^y2 1^t2 2 þ ^t2 1Þ ð16:31Þ where all estimates except ^t2 2 come from step b of the Smith-Blundell procedure; ^t2 2 is simply the usual estimate of the error variance from the ﬁrst-stage OLS regression. As in the case of probit, obtaining standard errors for the APEs based on expression (16.31) and the delta method would be quite complicated. An alternative procedure, where mðz1 ^d1 þ ^a1 y2 þ ^y1^vi2; ^t2 1Þ is averaged across i, is also consistent, but it does not exploit the normality of v2. A full maximum likelihood approach avoids the two-step estimation problem. The joint distribution of ðy1; y2Þ given z is most easily found by using f ðy1; y2 j zÞ ¼ f ðy1 j y2; zÞf ðy2 j zÞ ð16:32Þ Chapter 16 532", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 542, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p543::c0", "text": "just as for the probit case in Section 15.7.2. The density f ðy2 j zÞ is Normalðzd2; t2 2Þ. Further, from equation (16.29), y1 given ðy2; zÞ follows a Tobit with latent mean z1d1 þ a1 y2 þ y1v2 ¼ z1d1 þ a1 y2 þ ðh1=t2 2Þðy2 \u0003 zd2Þ and variance t2 1 ¼ s2 1 \u0003 ðh2 1=t2 2Þ, where s2 1 ¼ Varðu1Þ, t2 2 ¼ Varðv2Þ, and h1 ¼ Covðu1; v2Þ. Taking the log of equation (16.32), the log-likelihood function for each i is easily constructed as a function of the parameters ðd1; a1; d2; s2 1; t2 2; h1Þ. The usual coditional maximum likelihood theory can be used for constructing standard errors and test statistics. Once the MLE has been obtained, we can easily test the null hypothesis of exoge- neity of y2 by using the t statistic for ^y1. Because the MLE can be computationally more di‰cult than the Smith-Blundell procedure, it makes sense to use the Smith- Blundell procedure to test for endogeneity before obtaining the MLE. If y2 is a binary variable, then the Smith-Blundell assumptions cannot be expected to hold. Taking equation (16.26) as the structural equation, we could add y2 ¼ 1½zp2 þ v2 > 0\u0004 ð16:33Þ and assume that ðu1; v2Þ has a zero-mean normal distribution and is independent of z; v2 is standard normal, as always. Equation (16.32) can be used to obtain the log like- lihood for each i. Since y2 given z is probit, its density is easy to obtain: f ðy2 j zÞ ¼ Fðzp2Þy2½1 \u0003 Fðzp2Þ\u00041\u0003y2. The hard part is obtaining the conditional density f ðy1 j y2; zÞ, which is done ﬁrst for y2 ¼ 0 and then for y2 ¼ 1; see Problem 16.6. Similar comments hold if y2 given z follows a standard Tobit model. 16.6.3 Heteroskedasticity and Nonnormality in the Latent Variable Model As in the case of probit, both heteroskedasticity and nonnormality result in the Tobit estimator ^b being inconsistent for b. This inconsistency occurs because the derived density of y given x hinges crucially on y\u0001 j x @ Normalðxb; s2Þ. This nonrobustness of the Tobit estimator shows that data censoring can be very costly: in the absence of censoring ðy ¼ y\u0001Þ, b could be consistently estimated under Eðu j xÞ ¼ 0 [or even Eðx0uÞ ¼ 0]. In corner solution applications, we must remember that the presence of hetero- skedasticity or nonnormality in the latent variable model entirely changes the func- tional forms for Eðy j x; y > 0Þ and Eðy j xÞ. Therefore, it does not make sense to focus only on the inconsistency in estimating b. We should study how departures from the homoskedastic normal assumption a¤ect the estimated partial derivatives of the conditional mean functions. Allowing for heteroskedasticity or nonnormality in Corner Solution Outcomes and Censored Regression Models 533", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 543, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p544::c0", "text": "the latent variable model can be useful for generalizing functional form in corner solution applications, and it should be viewed in that light. Speciﬁcation tests can be based on the score approach, where the standard Tobit model is nested in a more general alternative. Tests for heteroskedasticity and non- normality in the latent variable equation are easily constructed if the outer product of the form statistic (see Section 13.6) is used. A useful test for heteroskedasticity is obtained by assuming Varðu j xÞ ¼ s2 expðzdÞ, where z is a 1 \u0002 Q subvector of x (z does not include a constant). The Q restrictions H0: d ¼ 0 can be tested using the LM statistic. The partial derivatives of the log likelihood liðb; s2; dÞ with respect to b and s2, evaluated at d ¼ 0, are given exactly as in equations (16.21) and (16.22). Further, we can show that qli=qd ¼ s2ziðqli=qs2Þ. Thus the outer product of the score statistic is N \u0003 SSR0 from the regression 1 on q^li=qb; q^li=qs2; ^s2ziðq^li=qs2Þ; i ¼ 1; . . . ; N where the derivatives are evaluated at the Tobit estimates (the restricted estimates) and SSR0 is the usual sum of squared residuals. Under H0, N \u0003 SSR0 @ a w2 Q. Unfortunately, as we discussed in Section 13.6, the outer product form of the statistic can reject much too often when the null hypothesis is true. If maximum likelihood estimation of the alternative model is possible, the likelihood ratio statistic is a pref- erable alternative. We can also construct tests of nonnormality that only require standard Tobit esti- mation. The most convenient of these are derived as conditional moment tests, which we discussed in Section 13.7. See Pagan and Vella (1989). It is not too di‰cult to estimate Tobit models with u heteroskedastic if a test reveals such a problem. For data-censoring applications, it makes sense to directly compare the estimates of b from standard Tobit and Tobit with heteroskedasticity. But when Eðy j x; y > 0Þ and Eðy j xÞ are of interest, we should look at estimates of these expectations with and without heteroskedasticity. The partial e¤ects on Eðy j x; y > 0Þ and Eðy j xÞ could be similar even though the estimates of b might be very di¤erent. As a rough idea of the appropriateness of the Tobit model, we can compare the probit estimates, say ^g, to the Tobit estimate of g ¼ b=s, namely, ^b=^s. These will never be identical, but they should not be statistically di¤erent. Statistically signiﬁ- cant sign changes are indications of misspeciﬁcation. For example, if ^gj is positive and signiﬁcant but ^bj is negative and perhaps signiﬁcant, the Tobit model is probably misspeciﬁed. As an illustration, in Example 15.2, we obtained the probit coe‰cient on nwifeinc as \u0003.012, and the coe‰cient on kidslt6 was \u0003.868. When we divide the corresponding Chapter 16 534", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 544, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p545::c0", "text": "Tobit coe‰cients by ^s ¼ 1,122.02, we obtain about \u0003.0079 and \u0003.797, respectively. Though the estimates di¤er somewhat, the signs are the same and the magnitudes are similar. It is possible to form a Hausman statistic as a quadratic form in ð^g \u0003 ^b=^sÞ, but obtaining the appropriate asymptotic variance is somewhat complicated. (See Ruud, 1984, for a formal discussion of this test.) Section 16.7 discusses more ﬂexible models that may be needed for corner solution outcomes. 16.6.4 Estimation under Conditional Median Restrictions It is possible to ﬃﬃﬃﬃ N p -consistently estimate b without assuming a particular distribu- tion for u and without even assuming that u and x are independent. Consider again the latent variable model, but where the median of u given x is zero: y\u0001 ¼ xb þ u; Medðu j xÞ ¼ 0 ð16:34Þ This equation implies that Medðy\u0001 j xÞ ¼ xb, so that the median of y\u0001 is linear in x. If the distribution of u given x is symmetric about zero, then the conditional expec- tation and conditional median of y\u0001 coincide, in which case there is no ambiguity about what we would like to estimate in the case of data censoring. If y\u0001 given x is asymmetric, the median and mean can be very di¤erent. A well-known result in probability says that, if gðyÞ is a nondecreasing function, then Med½gðyÞ\u0004 ¼ g½MedðyÞ\u0004. (The same property does not hold for the expected value.) Then, because y ¼ maxð0; y\u0001Þ is a nondecreasing function, Medðy j xÞ ¼ max½0; Medðy\u0001 j xÞ\u0004 ¼ maxð0; xbÞ ð16:35Þ Importantly, equation (16.35) holds under assumption (16.34) only; no further dis- tributional assumptions are needed. In Chapter 12 we noted that the analogy princi- ple leads to least absolute deviations as the appropriate method for estimating the parameters in a conditional median. Therefore, assumption (16.35) suggests estimat- ing b by solving min b X N i¼1 jyi \u0003 maxð0; xibÞj ð16:36Þ This estimator was suggested by Powell (1984) for the censored Tobit model. Since qðw; bÞ 1 jy \u0003 maxð0; xbÞj is a continuous function of b, consistency of Powell’s es- timator follows from Theorem 12.2 under an appropriate identiﬁcation assumption. Establishing ﬃﬃﬃﬃ N p -asymptotic normality is much more di‰cult because the objective function is not twice continuously di¤erentiable with nonsingular Hessian. Powell (1984, 1994) and Newey and McFadden (1994) contain applicable theorems. Corner Solution Outcomes and Censored Regression Models 535", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 545, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p546::c0", "text": "Powell’s method also applies to corner solution applications, but the di¤erence between the conditional median of y and its conditional expectations becomes cru- cial. As shown in equation (16.35), Medðy j xÞ does not depend on the distribution of u given x, whereas Eðy j xÞ and Eðy j x; y > 0Þ do. Further, the median and mean functions have di¤erent shapes. The conditional median of y is zero for xb a 0, and it is linear in x for xb > 0. (One implication of this fact is that, when using the median for predicting y, the prediction is exact when xi ^b a 0 and yi ¼ 0.) By con- trast, the conditional expectation Eðy j xÞ is never zero and is everywhere a nonlinear function of x. In the standard Tobit speciﬁcation we can also estimate Eðy j x; y > 0Þ and various probabilities. By its nature, the LAD approach does not allow us to do so. We cannot resolve the issue about whether the median or mean is more relevant for determining the e¤ects of the xj on y. It depends on the context and is somewhat a matter of taste. In some cases a quantile other than the median is of interest. Buchinsky and Hahn (1998) show how to estimate the parameters in a censored quantile regression model. It is also possible to estimate Eðy j xÞ and Eðy j x; y > 0Þ without specifying the distribution of u given x using semiparametric methods similar to those used to esti- mate index binary choice models without specifying the index function G. See Powell (1994) for a summary. 16.7 Some Alternatives to Censored Tobit for Corner Solution Outcomes In corner solution applications, an important limitation of the standard Tobit model is that a single mechanism determines the choice between y ¼ 0 versus y > 0 and the amount of y given y > 0. In particular, qPðy > 0 j xÞ=qxj and qEðy j x; y > 0Þ=qxj have the same sign. In fact, in Section 16.2 we showed that the relative e¤ects of continuous explanatory variables on Pðy > 0 j xÞ and Eðy j x; y > 0Þ are identical. Alternatives to censored Tobit have been suggested to allow the initial decision of y > 0 versus y ¼ 0 to be separate from the decision of how much y given that y > 0. These are often called hurdle models or two-tiered models. The hurdle or ﬁrst tier is whether or not to choose positive y. For example, in the charitable contributions ex- ample, family characteristics may di¤erently a¤ect the decision to contribute at all and the decision on how much to contribute. A simple two-tiered model for a corner solution variable is Pðy ¼ 0 j xÞ ¼ 1 \u0003 FðxgÞ ð16:37Þ logðyÞ j ðx; y > 0Þ @ Normalðxb; s2Þ ð16:38Þ Chapter 16 536", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 546, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p547::c0", "text": "The ﬁrst equation dictates the probability that y is zero or positive, and equation (16.38) says that, conditional on y > 0, y j x follows a lognormal distribution. If we deﬁne w ¼ 1½y > 0\u0004 and use f ðy j xÞ ¼ Pðw ¼ 0 j xÞf ðy j x; w ¼ 0Þ þ Pðw ¼ 1 j xÞ f ðy j x; w ¼ 1Þ we obtain f ðy j xÞ ¼ 1½y ¼ 0\u0004½1 \u0003 FðxgÞ\u0004 þ 1½y > 0\u0004FðxgÞf½flogðyÞ \u0003 xbg=s\u0004=ðysÞ since P½y > 0 j x\u0004 ¼ FðxgÞ and f½flogðyÞ \u0003 xbg=s\u0004=ðysÞ is the density of a lognormal random variable. For maximum likelihood analysis, a better way to write the den- sity is f ðy j x; yÞ ¼ ½1 \u0003 FðxgÞ\u00041½y¼0\u0004fFðxgÞf½flogðyÞ \u0003 xbg=s\u0004=ðysÞg1½y>0\u0004 for y b 0. If there are no restrictions on g, b, and s2, then the MLEs are easy to ob- tain: the log-likelihood function for observation i is liðyÞ ¼ 1½yi ¼ 0\u0004 log½1 \u0003 FðxgÞ\u0004 þ 1½yi > 0\u0004flog FðxigÞ \u0003 logðyiÞ \u0003 1 2 logðs2Þ \u0003 1 2 logð2pÞ \u0003 1 2 ½logðyiÞ \u0003 xib\u00042=s2g The MLE of g is simply the probit estimator using w ¼ 1½y > 0\u0004 as the binary re- sponse. The MLE of b is just the OLS estimator from the regression logðyÞ on x using those observations for which y > 0. A consistent estimator of ^s is the usual standard error from this regression. Estimation is very simple because we assume that, condi- tional on y > 0, logðyÞ follows a classical linear model. The expectations Eðyjx; y > 0Þ and Eðy j xÞ are easy to obtain using properties of the lognormal distribution: Eðy j x; y > 0Þ ¼ expðxb þ s2=2Þ; Eðy j xÞ ¼ FðxgÞ expðxb þ s2=2Þ and these are easily estimated given ^b, ^s2, and ^g. We cannot obtain the Tobit model as a special case of the model (16.37) and (16.38) by imposing parameter restrictions, and this inability makes it di‰cult to test the Tobit model against equations (16.37) and (16.38). Vuong (1989) suggests a gen- eral model selection test that can be applied to choose the best-ﬁtting model when the models are nonnested. Essentially, Vuong shows how to test whether one log- likelihood value is signiﬁcantly greater than another, where the null is that they have the same expected value. Cragg (1971) suggests a di¤erent two-tiered model which, unlike equations (16.37) and (16.38), nests the usual Tobit model. Cragg uses the truncated normal distribu- tion in place of the lognormal distribution: Corner Solution Outcomes and Censored Regression Models 537", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 547, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p548::c0", "text": "f ðy j x; y > 0Þ ¼ ½Fðxb=sÞ\u0004\u00031ff½ðy \u0003 xbÞ=s\u0004=sg; y > 0 where the term ½Fðxb=sÞ\u0004\u00031 ensures that the density integrates to unity over y > 0. The density of y given x becomes f ðy j x; yÞ ¼ ½1 \u0003 FðxgÞ\u00041½y¼0\u0004fFðxgÞ½Fðxb=sÞ\u0004\u00031½fðfy \u0003 xbg=sÞ=s\u0004g1½y>0\u0004 This equation is easily seen to yield the standard censored Tobit density when g ¼ b=s. Fin and Schmidt (1984) derive the LM test of this restriction, which allows the Tobit model to be tested against Cragg’s more general alternative. Problem 16.7 asks you to derive the conditional expectations associated with Cragg’s model. It is legitimate to choose between Cragg’s model and the lognormal model in equation (16.38) by using the value of the log-likelihood function. Vuong’s (1989) approach can be used to determine whether the di¤erence in log likelihoods is statistically signiﬁcant. If we are interested primarily in Eðy j xÞ, then we can model Eðy j xÞ directly and use a least squares approach. We discussed the drawbacks of using linear regression methods in Section 16.1. Nevertheless, a linear model for Eðy j xÞ might give good estimates on the partial e¤ects for x near its mean value. In Section 16.1 we also mentioned the possibility of modeling Eðy j xÞ as an ex- ponential function and using NLS or a quasi-MLE procedure (see Chapter 19) without any further assumptions about the distribution of y given x. If a model for Pðy ¼ 0 j xÞ is added, then we can obtain Eðy j x; y > 0Þ ¼ expðxbÞ=½1 \u0003 Pðy ¼ 0 j xÞ\u0004. Such methods are not common in applications, but this neglect could be partly due to confusion about which quantities are of interest for corner solution outcomes. 16.8 Applying Censored Regression to Panel Data and Cluster Samples We now cover Tobit methods for panel data and cluster samples. The treatment is very similar to that for probit models in Section 15.8, and so we make it brief. 16.8.1 Pooled Tobit As with binary response, it is easy to apply pooled Tobit methods to panel data or cluster samples. A panel data model is yit ¼ maxð0; xitb þ uitÞ; t ¼ 1; 2; . . . ; T ð16:39Þ uit j xit @ Normalð0; s2Þ ð16:40Þ This model has several notable features. First, it does not maintain strict exogeneity of xit: uit is independent of xit, but the relationship between uit and xis, t 0 s, is unspeciﬁed. As a result, xit could contain yi;t\u00031 or variables that are a¤ected by Chapter 16 538", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 548, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p549::c0", "text": "feedback. A second important point is that the fuit: t ¼ 1; . . . ; Tg are allowed to be serially dependent, which means that the yit can be dependent after conditioning on the explanatory variables. In short, equations (16.39) and (16.40) only specify a model for Dðyit j xitÞ, and xit can contain any conditioning variables (time dummies, interactions of time dummies with time-constant or time-varying variables, lagged dependent variables, and so on). The pooled estimator maximizes the partial log-likelihood function X N i¼1 X T t¼1 litðb; s2Þ where litðb; s2Þ is the log-likelihood function given in equation (16.20). Computa- tionally, we just apply Tobit to the data set as if it were one long cross section of size NT. However, without further assumptions, a robust variance matrix estimator is needed to account for serial correlation in the score across t; see Sections 13.8.2 and 15.8.1. Robust Wald and score statistics can be computed as in Section 12.6. The same methods work when each i represents a cluster and t is a unit within a cluster; see Section 15.8.6 for the probit case and Section 13.8.4 for the general case. With either panel data or cluster samples, the LR statistic based on the pooled Tobit esti- mation is not generally valid. In the case that the panel data model is dynamically complete, that is, Dðyit j xit; yi;t\u00031; xi;t\u00031; . . .Þ ¼ Dðyit j xitÞ ð16:41Þ inference is considerably easier: all the usual statistics from pooled Tobit are valid, including likelihood ratio statistics. Remember, we are not assuming any kind of in- dependence across t; in fact, xit can contain lagged dependent variables. It just works out that dynamic completeness leads to the same inference procedures one would use on independent cross sections; see the general treatment in Section 13.8. A general test for dynamic completeness can be based on the scores ^sit, as men- tioned in Section 13.8.3, but it is nice to have a simple test that can be computed from pooled Tobit estimation. Under assumption (16.41), variables dated at time t \u0003 1 and earlier should not a¤ect the distribution of yit once xit is conditioned on. There are many possibilities, but we focus on just one here. Deﬁne ri;t\u00031 ¼ 1 if yi;t\u00031 ¼ 0 and ri;t\u00031 ¼ 0 if yi;t\u00031 > 0. Further, deﬁne ^ui;t\u00031 1 yi;t\u00031 \u0003 xi;t\u00031 ^b if yi;t\u00031 > 0. Then es- timate the following (artiﬁcial) model by pooled Tobit: yit ¼ max½0; xitb þ g1ri;t\u00031 þ g2ð1 \u0003 ri;t\u00031Þ^ui;t\u00031 þ errorit\u0004 using time periods t ¼ 2; . . . ; T, and test the joint hypothesis H0: g1 ¼ 0, g2 ¼ 0. Under the null of dynamic completeness, errorit ¼ uit, and the estimation of ui;t\u00031 Corner Solution Outcomes and Censored Regression Models 539", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 549, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p550::c0", "text": "does not a¤ect the limiting distribution of the Wald, LR, or LM tests. In computing either the LR or LM test it is important to drop the ﬁrst time period in estimating the restricted model with g1 ¼ g2 ¼ 0. Since pooled Tobit is used to estimate both the restricted and unrestricted models, the LR test is fairly easy to obtain. In some applications it may be important to allow interactions between time dummies and explanatory variables. We might also want to allow the variance of uit to change over time. In data-censoring cases, where Eðy\u0001 it j xitÞ ¼ xitb is of direct in- terest, allowing changing variances over time could give us greater conﬁdence in the estimate of b. If s2 t ¼ VarðuitÞ, a pooled approach still works, but litðb; s2Þ becomes litðb; s2 t Þ, and special software may be needed for estimation. With true data censoring, it is tricky to allow for lagged dependent variables in xit, because we probably want a linear, AR(1) model for the unobserved outcome, y\u0001 it. But including y\u0001 i;t\u00031 in xit is very di‰cult, because y\u0001 i;t\u00031 is only partially observed. For corner solution applications, it makes sense to include functions of yi;t\u00031 in xit, and this approach is straightforward. 16.8.2 Unobserved E¤ects Tobit Models under Strict Exogeneity Another popular model for Tobit outcomes with panel data is the unobserved e¤ects Tobit model. We can state this model as yit ¼ maxð0; xitb þ ci þ uitÞ; t ¼ 1; 2; . . . ; T ð16:42Þ uit j xi; ci @ Normalð0; s2 uÞ ð16:43Þ where ci is the unobserved e¤ect and xi contains xit for all t. Assumption (16.43) is a normality assumption, but it also imples that the xit are strictly exogenous condi- tional on ci. As we have seen in several contexts, this assumption rules out certain kinds of explanatory variables. If these equations represent a data-censoring problem, then b is of primary interest. In corner solution applications we must be careful to specify what is of interest. Consistent estimation of b and s2 u means we can estimate the partial e¤ects of the elements of xt on Eðyt j xt; c; yt > 0Þ and Eðyt j xt; cÞ for given values of c, using equations (16.11) and (16.14). Under assumption (16.44), which follows, we can es- timate EðciÞ and evaluate the partial e¤ects at the estimated mean value. We will also see how to estimate the average partial e¤ects. Rather than cover a standard random e¤ects version, we consider a more general Chamberlain-like model that allows ci and xi to be correlated. To this end, assume, just as in the probit case, ci j xi @ Normalðc þ xix; s2 aÞ ð16:44Þ Chapter 16 540", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 550, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p551::c0", "text": "where s2 a is the variance of ai in the equation ci ¼ c þ xix þ ai. We could replace xi with xi to be more general, but xi has at most dimension K. (As usual, xit would not include a constant, and time dummies would be excluded from xi because they are already in xit.) Under assumptions (16.42)–(16.44), we can write yit ¼ maxð0; c þ xitb þ xix þ ai þ uitÞ ð16:45Þ uit j xi; ai @ Normalð0; s2 uÞ; t ¼ 1; 2; . . . ; T ð16:46Þ ai j xi @ Normalð0; s2 aÞ ð16:47Þ This formulation is very useful, especially if we assume that, conditional on ðxi; aiÞ [equivalently, conditional on ðxi; ciÞ], the fuitg are serially independent: ðui1; . . . ; uiTÞ are independent given ðxi; aiÞ ð16:48Þ Under assumptions (16.45)–(16.47), we have the random e¤ects Tobit model but with xi as an additional set of time-constant explanatory variables appearing in each time period. Software that estimates a random e¤ects Tobit model will provide ﬃﬃﬃﬃ N p - consistent estimates of c, b, x, s2 u, and s2 a. We can easily test H0: x ¼ 0 as a test of the traditional Tobit random e¤ects model. In data-censoring applications, our interest lies in b, and so—under the maintained assumptions—adding xi to the random e¤ects Tobit model solves the unobserved heterogeneity problem. If xit contains a time-constant variable, say, wi, we will not be able to estimate its e¤ect unless we assume that its coe‰cient in x is zero. But we can still include wi as an explanatory variable to reduce the error variance. For corner solution applications, we can estimate either partial e¤ects evaluated at EðcÞ or average partial e¤ects (APEs). As in Section 16.6.2, it is convenient to deﬁne mðz; s2Þ 1 Fðz=sÞz þ sfðz=sÞ, so that Eðyt j x; cÞ ¼ mðxtb þ c; s2 uÞ. A consistent es- timator of EðciÞ is ^c þ x^x, where x is the sample average of the xi, and so we can consistently estimate partial e¤ects at the mean value by taking derivatives or di¤er- ences of mð ^c þ xt ^b þ x^x; ^s2 uÞ with respect to the elements of xt. Estimating APEs is also relatively simple. APEs (at xt ¼ xo) are obtained by ﬁnd- ing E½mðxob þ ci; s2 uÞ\u0004 and then computing partial derivatives or changes with respect to elements of xo. Since ci ¼ c þ xix þ ai, we have, by iterated expectations, E½mðxob þ ci; s2 uÞ\u0004 ¼ EfE½mðc þ xob þ xix þ ai; s2 uÞ j xi\u0004g ð16:49Þ where the ﬁrst expectation is with respect to the distribution of ci. Since ai and xi are independent and ai @ Normalð0; s2 aÞ, the conditional expectation in equation (16.49) is obtained by integrating mðc þ xob þ xix þ ai; s2 uÞ over ai with respect to the Corner Solution Outcomes and Censored Regression Models 541", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 551, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p552::c0", "text": "Normalð0; s2 aÞ distribution. Since mðc þ xob þ xix þ ai; s2 uÞ is obtained by integrat- ing maxð0; c þ xob þ xix þ ai þ uitÞ with respect to uit over the Normalð0; s2 uÞ distri- bution, it follows that E½mðc þ xob þ xix þ ai; s2 uÞ j xi\u0004 ¼ mðc þ xob þ xix; s2 a þ s2 uÞ ð16:50Þ Therefore, the expected value of equation (16.50) (with respect to the distribution of xi) is consistently estimated as N\u00031 X N i¼1 mð ^c þ xo ^b þ xi ^x; ^s2 a þ ^s2 uÞ ð16:51Þ A similar argument works for Eðyt j x; c; yt > 0Þ: sum ð ^c þ xo ^b þ xi ^xÞ þ ^svl½ð ^c þ xo ^b þ xi ^xÞ=^sv\u0004 in expression (16.51), where lð\u0005Þ is the inverse Mills ratio and ^s2 v ¼ ^s2 a þ ^s2 u. We can relax assumption (16.48) and still obtain consistent, ﬃﬃﬃﬃ N p -asymptotically normal estimates of the APEs. In fact, under assumptions (16.45)–(16.47), we can write yit ¼ maxð0; c þ xitb þ xix þ vitÞ ð16:52Þ vit j xi @ Normalð0; s2 vÞ; t ¼ 1; 2; . . . ; T ð16:53Þ where vit ¼ ai þ uit. Without further assumptions, the vit are arbitrarily serially cor- related, and so maximum likelihood analysis using the density of yi given xi would be computationally demanding. However, we can obtain ﬃﬃﬃﬃ N p -asymptotically normal estimators by a simple pooled Tobit procedure of yit on 1, xit, xi, t ¼ 1; . . . ; T, i ¼ 1; . . . ; N. While we can only estimate s2 v from this procedure, it is all we need—along with ^c, ^b, and ^x—to obtain the average partial e¤ects based on expression (16.51). The robust variance matrix for partial MLE derived in Section 13.8.2 should be used for standard errors and inference. A minimum distance approach, analogous to the probit case discussed in Section 15.8.2, is also available. When we are interested only in b, such as in data-censoring cases or when we are interested in Medðyt j x; cÞ ¼ maxð0; xtb þ cÞ, it is useful to have an estimator of b that does not require distributional assumptions for uit or ci. Honore´ (1992) uses a clever transformation that eliminates ci and provides estimating equations for b. See also Honore´ and Kyriazidou (2000b) and Arellano and Honore´ (in press). 16.8.3 Dynamic Unobserved E¤ects Tobit Models We now turn to a speciﬁc dynamic model yit ¼ maxð0; zitd þ r1 yi;t\u00031 þ ci þ uitÞ ð16:54Þ Chapter 16 542", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 552, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p553::c0", "text": "uit j ðzi; yi;t\u00031; . . . ; yi0; ciÞ @ Normalð0; s2 uÞ; t ¼ 1; . . . ; T ð16:55Þ We can embellish this model in many ways. For example, the lagged e¤ect of yi;t\u00031 can depend on whether yi;t\u00031 is zero or greater than zero. Thus, we might replace r1 yi;t\u00031 by h1ri;t\u00031 þ r1ð1 \u0003 ri;t\u00031Þyi;t\u00031, where rit is a binary variable equal to unity if yit ¼ 0. Or, we can let the variance of uit change over time. The basic approach does not depend on the particular model. The model in equation (16.54) is suitable only for corner solution applications. In data-censoring cases, it makes more sense to have a dynamic linear model y\u0001 it ¼ zitd þ r1 y\u0001 i;t\u00031 þ ci þ uit and then to introduce the data-censoring mechanism for each time period. This approach leads to y\u0001 i;t\u00031 in equation (16.54) and is considerably more di‰cult to handle. The discussion in Section 15.8.4 about how to handle the initial value problem also holds here (see Section 13.9.2 for the general case). A fairly general and tractable approach is to specify a distribution for the unobserved e¤ect, ci, given the initial value, yi0, and the exogenous variables in all time periods, zi. Let hðc j y0; z; gÞ denote such a density. Then the joint density of ðy1; . . . ; yTÞ given ðy0; zÞ is ðy \u0003y Y T t¼1 f ðyt j yt\u00031; . . . y1; y0; z; c; yÞhðc j y0; z; gÞ dc ð16:56Þ where f ðyt j yt\u00031; . . . y1; y0; z; c; yÞ is the censored-at-zero normal distribution with mean ztd þ r1 yt\u00031 þ c and variance s2 u. A natural speciﬁcation for hðc j y0; z; gÞ is Normalðc þ x0 y0 þ zx; s2 aÞ, where s2 a ¼ Varðc j y0; zÞ. This leads to a fairly straight- forward procedure. To see why, write ci ¼ c þ x0 yi0 þ zix þ ai, so that yit ¼ maxð0; c þ zitd þ r1 yi;t\u00031 þ x0 yi0 þ zix þ ai þ uitÞ where the distribution of ai given ðyi0; ziÞ is Normalð0; s2 aÞ, and assumption (16.55) holds with ai replacing ci. The density in expression (16.56) then has the same form as the random e¤ects Tobit model, where the explanatory variables at time t are ðzit; yi;t\u00031; yi0; ziÞ. The inclusion of the initial condition in each time period, as well as the entire vector zi, allows for the unobserved heterogeneity to be correlated with the initial condition and the strictly exogenous variables. Standard software can be used to test for state dependence ðr1 0 0Þ. Average partial e¤ects can be estimated by modiﬁcation of the probit results in Section 15.8.4 and the formulas in Section 16.8.2. See Wooldridge (2000e) for details. Honore´ (1993a) obtains orthogonality conditions that can be used in a method of moments framework to estimate d and r1 in equation (16.54) without making dis- tributional assumptions about ci. The assumptions on uit restrict the dependence across time but do not include distributional assumptions. Because no distributional Corner Solution Outcomes and Censored Regression Models 543", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 553, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p554::c0", "text": "assumptions are made, partial e¤ects on the conditional mean cannot be estimated using Honore´’s approach. Problems 16.1. Let t\u0001 i denote the duration of some event, such as unemployment, measured in continuous time. Consider the following model for t\u0001 i : t\u0001 i ¼ expðxib þ uiÞ; ui j xi @ Normalð0; s2Þ ti ¼ minðt\u0001 i ; cÞ where c > 0 is a known censoring constant. a. Find Pðti ¼ c j xiÞ, that is, the probability that the duration is censored. What happens as c ! y? b. What is the density of logðtiÞ (given xi) when ti < c? Now write down the full density of logðtiÞ given xi. c. Write down the log-likelihood function for observation i. d. Partition b into the K1 \u0002 1 and K2 \u0002 1 vectors b1 and b2. How would you test H0: b2 ¼ 0? Be speciﬁc. e. Obtain the log-likelihood function if the censoring time is potentially di¤erent for each person, so that ti ¼ minðt\u0001 i ; ciÞ, where ci is observed for all i. Assume that ui is independent of ðxi; ciÞ. 16.2. In some occupations, such as major league baseball, salary ﬂoors exist. This situation can be described by the model wage\u0001 ¼ expðxb þ uÞ; u j x @ Normalð0; s2Þ wage ¼ maxðc; wage\u0001Þ where c > 0 is the known salary ﬂoor (the minimum wage), wage\u0001 is the person’s true worth, and x contains productivity and demographic variables. a. Show how to turn this into a standard censored Tobit model. b. Why is Eðwage\u0001 j xÞ, rather than Eðwage\u0001 j x; wage\u0001 > cÞ or Eðwage j xÞ, of interest in this application? 16.3. Suppose that, for a random draw ðxi; yiÞ from the population, yi is a doubly censored variable: Chapter 16 544", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 554, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p555::c0", "text": "y\u0001 i j xi @ Normalðxib; s2Þ yi ¼ a1 if y\u0001 i a a1 yi ¼ y\u0001 i if a1 < y\u0001 i < a2 yi ¼ a2 if y\u0001 i b a2 where xi is 1 \u0002 K, b is K \u0002 1, and a1 < a2 are known censoring constants. This may be a data-censoring problem—for example, y\u0001 may be both top coded and bottom coded in a survey—in which case we are interested in Eðy\u0001 i j xiÞ ¼ xib. Or, yi may be the outcome of a constrained optimization problem with corners at a1 and a2, such as when yi is the proportion of person i’s pension assets invested in the stock market, so that a1 ¼ 0 and a2 ¼ 1. a. Find Pðy ¼ a1 j xÞ and Pðy ¼ a2 j xÞ in terms of the standard normal cdf, x, b, and s. For a1 < y < a2, ﬁnd Pðy a y j xÞ, and use this to ﬁnd the density of y given x for a1 < y < a2. b. If z @ Normalð0; 1Þ, it can be shown that Eðz j c1 < z < c2Þ ¼ ffðc1Þ \u0003 fðc2Þg= fFðc2Þ \u0003 Fðc1Þg for c1 < c2. Use this fact to ﬁnd Eðy j x; a1 < y < a2Þ and Eðy j xÞ. c. Consider the following method for estimating b. Using only the uncensored observations, that is, observations for which a1 < yi < a2, run the OLS regression of yi on xi. Explain why this does not generally produce a consistent estimator of b. d. Write down the log-likelihood function for observation i; it should consist of three parts. e. For a corner solution, how would you estimate Eðy j x; a1 < y < a2Þ and Eðy j xÞ? f. Show that qEðy j xÞ qxj ¼ fF½ða2 \u0003 xbÞ=s\u0004 \u0003 F½ða1 \u0003 xbÞ=s\u0004gbj Why is the scale factor multiplying bj necessarily between zero and one? g. For a corner solution outcome, suppose you obtain ^g from a standard OLS re- gression of yi on xi, using all observations. Would you compare ^gj to the Tobit esti- mate, ^bj? What would be a sensible comparison? h. For data censoring, how would the analysis change if a1 and a2 were replaced with ai1 and ai2, respectively, where ui is independent of ðxi; ai1; ai2Þ? 16.4. Use the data in JTRAIN1.RAW for this question. Corner Solution Outcomes and Censored Regression Models 545", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 555, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p556::c0", "text": "a. Using only the data for 1988, estimate a linear equation relating hrsemp to logðemployÞ, union, and grant. Compute the usual and heteroskedasticity-robust standard errors. Interpret the results. b. Out of the 127 ﬁrms with nonmissing data on all variables, how many have hrsemp ¼ 0? Estimate the model from part a by Tobit. Find the estimated e¤ect of grant on Eðhrsemp j employ; union; grant; hrsemp > 0Þ at the average employment for the 127 ﬁrms and union ¼ 1. What is the e¤ect on Eðhrsemp j employ; union; grantÞ? c. Are logðemployÞ and union jointly signiﬁcant in the Tobit model? d. In terms of goodness of ﬁt for the conditional mean, do you prefer the linear model or Tobit model for estimating Eðhrsemp j employ; union; grantÞ? 16.5. Use the data set FRINGE.RAW for this question. a. Estimate a linear model by OLS relating hrbens to exper, age, educ, tenure, mar- ried, male, white, nrtheast, nrthcen, south, and union. b. Estimate a Tobit model relating the same variables from part a. Why do you suppose the OLS and Tobit estimates are so similar? c. Add exper2 and tenure2 to the Tobit model from part b. Should these be included? d. Are there signiﬁcant di¤erences in hourly beneﬁts across industry, holding the other factors ﬁxed? 16.6. Consider a Tobit model with an endogenous binary explanatory variable: y1 ¼ maxð0; z1d1 þ a1y2 þ u1Þ y2 ¼ 1½zd2 þ v2 > 0\u0004 where ðu1; v2Þ is independent of z with a bivariate normal distribution with mean zero and Varðv2Þ ¼ 1. If u1 and v2 are correlated, y2 is endogenous. a. Find the density of the latent variable, y\u0001 1, given ðz; y2Þ. [Hint: As shown in Sec- tion 16.6.2, the density of y1 given ðz; v2Þ is normal with mean z1d1 þ a1 y2 þ r1v2 and variance s2 1 \u0003 r2 1, where r1 ¼ Covðu1; v2Þ. Integrate against the density of v2 given ðz; y2 ¼ 1Þ, as in equation (15.55), and similarly for y2 ¼ 0:\u0004 b. Write down the log-likelihood function for the parameters d1, a1, s2 1, d2, and r1 for observation i. 16.7. Suppose that y given x follows Cragg’s model from Section 16.7. a. Show that Eðy j x; y > 0Þ ¼ xb þ slðxb=sÞ, just as in the standard Tobit model. b. Use part a and equation (16.8) to ﬁnd Eðy j xÞ. Chapter 16 546", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 556, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p557::c0", "text": "c. Show that the elasticity of Eðy j xÞ with respect to, say, x1, is the sum of the elas- ticities of Pðy > 0 j xÞ and Eðy j x; y > 0Þ. 16.8. Consider three di¤erent approaches for modeling Eðy j xÞ when y b 0 is a corner solution outcome: (1) Eðy j xÞ ¼ xb; (2) Eðy j xÞ ¼ expðxbÞ; and (3) y given x follows a Tobit model. a. How would you estimate models 1 and 2? b. Obtain three goodness-of-ﬁt statistics that can be compared across models; each should measure how much sample variation in yi is explained by ^Eðyi j xiÞ. c. Suppose, in your sample, yi > 0 for all i. Show that the OLS and Tobit estimates of b are identical. Does the fact that they are identical mean that the linear model for Eðy j xÞ and the Tobit model produce the same estimates of Eðy j xÞ? Explain. d. If y > 0 in the population, does a Tobit model make sense? What is a simple alternative to the three approaches listed at the beginning of this problem? What assumptions are su‰cient for estimating Eðy j xÞ? 16.9. Let y be the percentage of annual income invested in a pension plan, and assume that a law caps this percentage at 10 percent. Thus, in a sample of data, we observe yi between zero and 10, with pileups at the end points. a. What model would you use for y? b. Explain the conceptual di¤erence between the outcomes y ¼ 0 and y ¼ 10. In particular, which limit can be viewed as a form of data censoring? c. Suppose you want to ask, What is the e¤ect on Eðy j xÞ if the cap were increased from 10 to 11? How would you estimate this? (Hint: Call the upper bound a2, and take a derivative.) d. If there are no observations at y ¼ 10, what does the estimated model reduce to? 16.10. Provide a careful derivation of equation (16.16). It will help to use the fact that dfðzÞ=dz ¼ \u0003zfðzÞ. 16.11. Let y be a corner solution response, and let Lðy j 1; xÞ ¼ g0 þ xg be the linear projection of y onto an intercept and x, where x is 1 \u0002 K. If we use a random sample on ðx; yÞ to estimate g0 and g by OLS, are the estimators inconsistent because of the corner solution nature of y? Explain. 16.12. Use the data in APPLE.RAW for this question. These are phone survey data, where each respondent was asked the amount of ‘‘ecolabeled’’ (or ‘‘ecologically friendly’’) apples he or she would purchase at given prices for both ecolabeled apples Corner Solution Outcomes and Censored Regression Models 547", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 557, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p558::c0", "text": "and regular apples. The prices are cents per pound, and ecolbs and reglbs are both in pounds. a. For what fraction of the sample is ecolbsi ¼ 0? Discuss generally whether ecolbs is a good candidate for a Tobit model. b. Estimate a linear regression model for ecolbs, with explanatory variables logðecoprcÞ, logðregprcÞ, logðfamincÞ, educ, hhsize, and num5_17. Are the signs of the coe‰cient for logðecoprcÞ and logðregprcÞ the expected ones? Interpret the estimated coe‰cient on logðecoprcÞ. c. Test the linear regression in part b for heteroskedasticity by running the regression ^u2 on 1, ec^olbs, ec^olbs2 and carrying out an F test. What do you conclude? d. Obtain the OLS ﬁtted values. How many are negative? e. Now estimate a Tobit model for ecolbs. Are the signs and statistical signiﬁcance of the explanatory variables the same as for the linear regression model? What do you make of the fact that the Tobit estimate on logðecoprcÞ is about twice the size of the OLS estimate in the linear model? f. Obtain the estimated partial e¤ect of logðecoprcÞ for the Tobit model using equa- tion (16.16), where the xj are evaluated at the mean values. What is the estimated price elasticity (again, at the mean values of the xj)? g. Reestimate the Tobit model dropping the variable logðregprcÞ. What happens to the coe‰cient on logðecoprcÞ? What kind of correlation does this result suggest be- tween logðecoprcÞ and logðregprcÞ? h. Reestimate the model from part e, but with ecoprc and regprc as the explanatory variables, rather than their natural logs. Which functional form do you prefer? (Hint: Compare log-likelihood functions.) 16.13. Suppose that, in the context of an unobserved e¤ects Tobit (or probit) panel data model, the mean of the unobserved e¤ect, ci, is related to the time average of detrended xit. Speciﬁcally, ci ¼ ð1=TÞ X T t¼1 ðxit \u0003 ptÞ \" # x þ ai where pt ¼ EðxitÞ, t ¼ 1; . . . ; T, and ai j xi @ Normalð0; s2 aÞ. How does this extension of equation (16.44) a¤ect estimation of the unobserved e¤ects Tobit (or probit) model? 16.14. Consider the random e¤ects Tobit model under assumptions (16.42), (16.43), and (16.48), but replace assumption (16.44) with Chapter 16 548", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 558, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p559::c0", "text": "ci j xi @ Normal½c þ xix; s2 a expðxilÞ\u0004 See Problem 15.18 for the probit case. a. What is the density of yit given ðxi; aiÞ, where ai ¼ ci \u0003 Eðci j xiÞ? b. Derive the log-likelihood function by ﬁrst ﬁnding the density of ðyi1; . . . ; yiTÞ given xi. c. Assuming you have estimated b, s2 u, c, x, s2 a, and l by CMLE, how would you estimate the average partial e¤ects? 16.15. Explain why the Smith and Blundell (1986) procedure (Procedure 16.1 in Section 16.6.2) extends immediately to the model y1 ¼ max½0; z1d1 þ gðy2Þa1 þ u1\u0004 where gðy2Þ is a row vector of functions of y2, under equation (16.27) and the assumption that ðu1; v2Þ is bivariate normal and independent of z. (See Problem 15.20 for the probit case.) Corner Solution Outcomes and Censored Regression Models 549", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 559, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p560::c0", "text": "17 Sample Selection, Attrition, and Stratiﬁed Sampling 17.1 Introduction Up to this point, with the exception of occasionally touching on cluster samples and independently pooled cross sections, we have assumed the availability of a random sample from the underlying population. This assumption is not always realistic: be- cause of the way some economic data sets are collected, and often because of the behavior of the units being sampled, random samples are not always available. A selected sample is a general term that describes a nonrandom sample. There are a variety of selection mechanisms that result in nonrandom samples. Some of these are due to sample design, while others are due to the behavior of the units being sam- pled, including nonresponse on survey questions and attrition from social programs. Before we launch into speciﬁcs, there is an important general point to remember: sample selection can only be an issue once the population of interest has been care- fully speciﬁed. If we are interested in a subset of a larger population, then the proper approach is to specify a model for that part of the population, obtain a random sample from that part of the population, and proceed with standard econometric methods. The following are some examples with nonrandomly selected samples. Example 17.1 (Saving Function): Suppose we wish to estimate a saving function for all families in a given country, and the population saving function is saving ¼ b0 þ b1income þ b2age þ b3married þ b4kids þ u ð17:1Þ where age is the age of the household head and the other variables are self-explanatory. However, we only have access to a survey that included families whose household head was 45 years of age or older. This limitation raises a sample selection issue be- cause we are interested in the saving function for all families, but we can obtain a random sample only for a subset of the population. Example 17.2 (Truncation Based on Wealth): We are interested in estimating the e¤ect of worker eligibility in a particular pension plan [for example, a 401(k) plan] on family wealth. Let the population model be wealth ¼ b0 þ b1plan þ b2educ þ b3age þ b4income þ u ð17:2Þ where plan is a binary indicator for eligibility in the pension plan. However, we can only sample people with a net wealth less than $200,000, so the sample is selected on the basis of wealth. As we will see, sampling based on a response variable is much more serious than sampling based on an exogenous explanatory variable.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 560, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p561::c0", "text": "In these two examples data were missing on all variables for a subset of the popu- lation as a result of survey design. In other cases, units are randomly drawn from the population, but data are missing on one or more variables for some units in the sample. Using a subset of a random sample because of missing data can lead to a sample selection problem. As we will see, if the reason the observations are missing is appropriately exogenous, using the subsample has no serious consequences. Our ﬁnal example illustrates a more subtle form of a missing data problem. Example 17.3 (Wage O¤er Function): Consider estimating a wage o¤er equation for people of working age. By deﬁnition, this equation is supposed to represent all people of working age, whether or not a person is actually working at the time of the survey. Because we can only observe the wage o¤er for working people, we e¤ectively select our sample on this basis. This example is not as straightforward as the previous two. We treat it as a sample selection problem because data on a key variable—the wage o¤er, wageo—are avail- able only for a clearly deﬁned subset of the population. This is sometimes called incidental truncation because wageo is missing as a result of the outcome of another variable, labor force participation. The incidental truncation in this example has a strong self-selection component: people self-select into employment, so whether or not we observe wageo depends on an individual’s labor supply decision. Whether we call examples like this sample selection or self-selection is largely irrelevant. The important point is that we must account for the nonrandom nature of the sample we have for estimating the wage o¤er equation. In the next several sections we cover a variety of sample selection issues, including tests and corrections. Section 17.7 treats sample selection and the related problem of attrition in panel data. Stratiﬁed sampling, which arises out of sampling design, is covered in Section 17.8. 17.2 When Can Sample Selection Be Ignored? In some cases, the fact that we have a nonrandom sample does not a¤ect the way we estimate population parameters; it is important to understand when this is the case. 17.2.1 Linear Models: OLS and 2SLS We begin by obtaining conditions under which estimation of the population model by 2SLS using the selected sample is consistent for the population parameters. These Chapter 17 552", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 561, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p562::c0", "text": "results are of interest in their own right, but we will also apply them to several speciﬁc models later in the chapter. We assume that there is a population represented by the random vector ðx; y; zÞ, where x is a 1 \u0001 K vector of explanatory variables, y is the scalar response variable, and z is a 1 \u0001 L vector of instruments. The population model is the standard single-equation linear model with possibly endogenous explanatory variables: y ¼ b1 þ b2x2 þ \u0002 \u0002 \u0002 þ bKxK þ u ð17:3Þ Eðu j zÞ ¼ 0 ð17:4Þ where we take x1 1 1 for notational simplicity. The sense in which the instruments z are exogenous, given in assumption (17.4), is stronger than we need for 2SLS to be consistent when using a random sample from the population. With random sam- pling, the zero correlation condition Eðz0uÞ ¼ 0 is su‰cient. If we could obtain a random sample from the population, equation (17.3) could be estimated by 2SLS under the condition rank½Eðz0xÞ\u0003 ¼ K. A leading special case is z ¼ x, so that the explanatory variables are exogenous and equation (17.3) is a model of the conditional expectation Eðy j xÞ: Eðy j xÞ ¼ b1 þ b2x2 þ \u0002 \u0002 \u0002 þ bKxK ð17:5Þ But our general treatment allows elements of x to be correlated with u. Rather than obtaining a random sample—that is, a sample representative of the population—we only use data points that satisfy certain conditions. Let s be a binary selection indicator representing a random draw from the population. By deﬁnition, s ¼ 1 if we use the draw in the estimation, and s ¼ 0 if we do not. Usually, we do not use observations when s ¼ 0 because data on at least some elements of ðx; y; zÞ are unobserved—because of survey design, nonresponse, or incidental truncation. The key assumption underlying the validity of 2SLS on selected sample is Eðu j z; sÞ ¼ 0 ð17:6Þ There are some important cases where assumption (17.6) necessarily follows from assumption (17.4). If s is a deterministic function of z, then Eðu j z; sÞ ¼ Eðu j zÞ. Such cases arise when selection is a ﬁxed rule involving only the exogenous variables z. Also, if selection is independent of ðz; uÞ—a su‰cient condition is that selection is independent of ðx; y; zÞ—then Eðu j z; sÞ ¼ Eðu j zÞ. In estimating equation (17.3), we apply 2SLS to the observations for which s ¼ 1. To study the properties of the 2SLS estimator on the selected sample, let Sample Selection, Attrition, and Stratiﬁed Sampling 553", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 562, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p563::c0", "text": "fðxi; yi; zi; siÞ: i ¼ 1; 2; . . . ; Ng denote a random sample from the population. We use observation i if si ¼ 1, but not if si ¼ 0. Therefore, we do not actually have N obser- vations to use in the estimation; in fact, we do not even need to know N. The 2SLS estimator using the selected sample can be expressed as ^b ¼ N\u00041 X N i¼1 siz0 ixi !0 N\u00041 X N i¼1 siz0 izi !\u00041 N\u00041 X N i¼1 siz0 ixi ! 2 4 3 5 \u00041 \u0001 N\u00041 X N i¼1 siz0 ixi !0 N\u00041 X N i¼1 siz0 izi !\u00041 N\u00041 X N i¼1 siz0 i yi ! Substituting yi ¼ xib þ ui gives ^b ¼ b þ N\u00041 X N i¼1 siz0 ixi !0 N\u00041 X N i¼1 siz0 izi !\u00041 N\u00041 X N i¼1 siz0 ixi ! 2 4 3 5 \u00041 \u0001 N\u00041 X N i¼1 siz0 ixi !0 N\u00041 X N i¼1 siz0 izi !\u00041 N\u00041 X N i¼1 siz0 iui ! ð17:7Þ By assumption, Eðui j zi; siÞ ¼ 0, and so Eðsiz0 iuiÞ ¼ 0 by iterated expectations. [In the case where s is a function of z, this result shows why assumption (17.4) cannot be replaced with Eðz0uÞ ¼ 0.] Now the law of large numbers applies to show that plim ^b ¼ b, at least under a modiﬁcation of the rank condition. We summarize with a theorem: theorem 17.1 (Consistency of 2SLS under Sample Selection): In model (17.3), assume that Eðu2Þ < y, Eðx2 j Þ < y, j ¼ 1; . . . ; K, and Eðz2 j Þ < y, j ¼ 1; . . . ; L. Maintain assumption (17.6) and, in addition, assume rank Eðz0z j s ¼ 1Þ ¼ L ð17:8Þ rank Eðz0x j s ¼ 1Þ ¼ K ð17:9Þ Then the 2SLS estimator using the selected sample is consistent for b and ﬃﬃﬃﬃ N p - asymptotically normal. Further, if Eðu2 j z; sÞ ¼ s2, then the usual asymptotic vari- ance of the 2SLS estimator is valid. Equation (17.7) essentially proves the consistency result. Showing that the usual 2SLS asymptotic variance matrix is valid requires two steps. First, under the homo- Chapter 17 554", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 563, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p564::c0", "text": "skedasticity assumption in the population, the usual iterated expectations argument gives Eðsu2z0zÞ ¼ s2Eðsz0zÞ. This equation can be used to show that Avar ﬃﬃﬃﬃ N p ð ^b \u0004 bÞ ¼ s2fEðsx0zÞ½Eðsz0zÞ\u0003\u00041Eðsz0xÞg\u00041. The second step is to show that the usual 2SLS estimator of s2 is consistent. This fact can be seen as follows. Under the homoskeda- sticity assumption, Eðsu2Þ ¼ EðsÞs2, where EðsÞ is just the fraction of the subpopu- lation in the overall population. The estimator of s2 (without degrees-of-freedom adjustment) is X N i¼1 si !\u00041X N i¼1 si^u2 i ð17:10Þ since PN i¼1 si is simply the number of observations in the selected sample. Removing the ‘‘^’’ from u2 i and applying the law of large numbers gives N\u00041 PN i¼1 si ! p EðsÞ and N\u00041 PN i¼1 siu2 i ! p Eðsu2Þ ¼ EðsÞs2. Since the N\u00041 terms cancel, expression (17.10) converges in probability to s2. If s is a function only of z, or s is independent of ðz; uÞ, and Eðu2 j zÞ ¼ s2— that is, if the homoskedasticity assumption holds in the original population—then Eðu2 j z; sÞ ¼ s2. Without the homoskedasticity assumption we would just use the heteroskedasticity-robust standard errors, just as if a random sample were available with heteroskedasticity present in the population model. When x is exogenous and we apply OLS on the selected sample, Theorem 17.1 implies that we can select the sample on the basis of the explanatory variables. Selection based on y or on endogenous elements of x is not allowed because then Eðu j z; sÞ 0 EðuÞ. Example 17.4 (Nonrandomly Missing IQ Scores): As an example of how Theorem 17.1 can be applied, consider the analysis in Griliches, Hall, and Hausman (1978) (GHH). The structural equation of interest is logðwageÞ ¼ z1d1 þ abil þ v; Eðv j z1; abil; IQÞ ¼ 0 and we assume that IQ is a valid proxy for abil in the sense that abil ¼ y1IQ þ e and Eðe j z1; IQÞ ¼ 0 (see Section 4.3.2). Write logðwageÞ ¼ z1d1 þ y1IQ þ u ð17:11Þ where u ¼ v þ e. Under the assumptions made, Eðu j z1; IQÞ ¼ 0. It follows imme- diately from Theorem 17.1 that, if we choose the sample excluding all people with IQs below a ﬁxed value, then OLS estimation of equation (17.11) will be consistent. This problem is not quite the one faced by GHH. Instead, GHH noticed that the Sample Selection, Attrition, and Stratiﬁed Sampling 555", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 564, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p565::c0", "text": "probability of IQ missing was higher at lower IQs (because people were reluctant to give permission to obtain IQ scores). A simple way to model this situation is s ¼ 1 if IQ þ r b 0, s ¼ 0 if IQ þ r < 0, where r is an unobserved random variable. If r is redundant in the structural equation and in the proxy variable equation for IQ, that is, if Eðv j z1; abil; IQ; rÞ ¼ 0 and Eðe j z1; IQ; rÞ ¼ 0, then Eðu j z1; IQ; rÞ ¼ 0. Since s is a function of IQ and r, it follows immediately that Eðu j z1; IQ; sÞ ¼ 0. Therefore, using OLS on the sample for which IQ is observed yields consistent estimators. If r is correlated with either v or e, Eðu j z1; IQ; sÞ 0 EðuÞ in general, and OLS es- timation of equation (17.11) using the selected sample would not consistently esti- mate d1 and y1. Therefore, even though IQ is exogenous in the population equation (17.11), the sample selection is not exogenous. In Section 17.4.2 we cover a method that can be used to correct for sample selection bias. Theorem 17.1 has other useful applications. Suppose that x is exogenous in equa- tion (17.3) and that s is a nonrandom function of ðx; vÞ, where v is a variable not appearing in equation (17.3). If ðu; vÞ is independent of x, then Eðu j x; vÞ ¼ Eðu j vÞ, and so Eðy j xÞ ¼ xb þ Eðu j x; vÞ ¼ xb þ Eðu j vÞ If we make an assumption about the functional form of Eðu j vÞ, for example, Eðu j vÞ ¼ gv, then we can write y ¼ xb þ gv þ e; Eðe j x; vÞ ¼ 0 ð17:12Þ where e ¼ u \u0004 Eðu j vÞ. Because s is just a function of ðx; vÞ, Eðe j x; v; sÞ ¼ 0, and so b and g can be estimated consistently by the OLS regression y on x, v, using the selected sample. E¤ectively, including v in the regression on the selected subsample eliminates the sample selection problem and allows us to consistently estimate b. [Incidentally, because v is independent of x, we would not have to include it in equation (17.3) to consistently estimate b if we had a random sample from the pop- ulation. However, including v would result in an asymptotically more e‰cient esti- mator of b when Varðy j x; vÞ is homoskedastic. See Problem 4.5.] In Section 17.5 we will see how equation (17.12) can be implemented. 17.2.2 Nonlinear Models Results similar to those in the previous section hold for nonlinear models as well. We will cover explicitly the case of nonlinear regression and maximum likelihood. See Problem 17.8 for the GMM case. Chapter 17 556", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 565, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p566::c0", "text": "In the nonlinear regression case, if Eðy j x; sÞ ¼ Eðy j xÞ—so that selection is igno- rable in the conditional mean sense—then NLS on the selected sample is consistent. Su‰cient is that s is a deterministic function of x. The consistency argument is sim- ple: NLS on the selected sample solves min b N\u00041 X N i¼1 si½yi \u0004 mðxi; bÞ\u00032 so it su‰ces to show that bo in Eðy j xÞ ¼ mðx; boÞ minimizes Efs½y \u0004 mðx; bÞ\u00032g over b. By iterated expectations, Efs½y \u0004 mðx; bÞ\u00032g ¼ EðsEf½y \u0004 mðx; bÞ\u00032 j x; sgÞ Next, write ½y\u0004mðx; bÞ\u00032 ¼ u2 þ2½mðx; boÞ\u0004mðx; bÞ\u0003uþ½mðx; boÞ\u0004mðx; bÞ\u00032, where u ¼ y \u0004 mðx; boÞ. By assumption, Eðu j x; sÞ ¼ 0. Therefore, Ef½y \u0004 mðx; bÞ\u00032 j x; sg ¼ Eðu2 j x; sÞ þ ½mðx; boÞ \u0004 mðx; bÞ\u00032 and the second term is clearly minimized at b ¼ bo. We do have to assume that bo is the unique value of b that makes Efs½mðx; bÞ \u0004 mðx; boÞ\u00032g zero. This is the identiﬁ- cation condition on the subpopulation. It can also be shown that, if Varðy j x; sÞ ¼ Varðy j xÞ and Varðy j xÞ ¼ s2 o, then the usual, nonrobust NLS statistics are valid. If heteroskedasticity exists either in the population or the subpopulation, standard heteroskedasticity-robust inference can be used. The arguments are very similar to those for 2SLS in the previous subsection. Another important case is the general conditional maximum likelihood setup. As- sume that the distribution of y given x and s is the same as the distribution of y given x: Dðy j x; sÞ ¼ Dðy j xÞ. This is a stronger form of ignorability of selection, but it always holds if s is a nonrandom function of x, or if s is independent of ðx; yÞ. In any case, Dðy j x; sÞ ¼ Dðy j xÞ ensures that the MLE on the selected sample is consistent and that the usual MLE statistics are valid. The analogy argument should be familiar by now. Conditional MLE on the selected sample solves max y N\u00041 X N i¼1 silðyi; xi; yÞ ð17:13Þ where lðyi; xi; yÞ is the log likelihood for observation i. Now for each x, yo maximizes E½lðy; x; yÞjx\u0003 over y. But E½slðy; x; yÞ\u0003 ¼ EfsE½lðy; x; yÞjx; s\u0003g ¼ EfsE½lðy; x; yÞjx\u0003g, since, by assumption, the conditional distribution of y given ðx; sÞ does not depend on s. Since E½lðy; x; yÞ j x\u0003 is maximized at yo, so is EfsE½lðy; x; yÞ j x\u0003g. We must make Sample Selection, Attrition, and Stratiﬁed Sampling 557", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 566, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p567::c0", "text": "the stronger assumption that yo is the unique maximum, just as in the previous cases: if the selected subset of the population is too small, we may not be able to identify yo. Inference can be carried out using the usual MLE statistics obtained from the selected subsample because the information equality now holds conditional on x and s under the assumption that Dðy j x; sÞ ¼ Dðy j xÞ. We omit the details. Problem 17.8 asks you to work through the case of GMM estimation of general nonlinear models based on conditional moment restrictions. 17.3 Selection on the Basis of the Response Variable: Truncated Regression Let ðxi; yiÞ denote a random draw from a population. In this section we explicitly treat the case where the sample is selected on the basis of yi. In applying the following methods it is important to remember that there is an underlying population of interest, often described by a linear conditional expectation: Eðyi j xiÞ ¼ xib. If we could observe a random sample from the population, then we would just use standard regression analysis. The problem comes about because the sample we can observe is chosen at least partly based on the value of yi. Unlike in the case where selection is based only on xi, selection based on yi causes problems for standard OLS analysis on the selected sample. A classic example of selection based on yi is Hausman and Wise’s (1977) study of the determinants of earnings. Hausman and Wise recognized that their sample from a negative income tax experiment was truncated because only families with income below 1.5 times the poverty level were allowed to participate in the program; no data were available on families with incomes above the threshold value. The truncation rule was known, and so the e¤ects of truncation could be accounted for. A similar example is Example 17.2. We do not observe data on families with wealth above $200,000. This case is di¤erent from the top coding example we dis- cussed in Chapter 16. Here, we observe nothing about families with high wealth: they are entirely excluded from the sample. In the top coding case, we have a random sample of families, and we always observe xi; the information on xi is useful even if wealth is top coded. We assume that yi is a continuous random variable and that the selection rule takes the form si ¼ 1½a1 < yi < a2\u0003 where a1 and a2 are known constants such that a1 < a2. A good way to think of the sample selection is that we draw ðxi; yiÞ randomly from the population. If yi falls in Chapter 17 558", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 567, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p568::c0", "text": "the interval ða1; a2Þ, then we observe both yi and xi. If yi is outside this interval, then we do not observe yi or xi. Thus all we know is that there is some subset of the population that does not enter our data set because of the selection rule. We know how to characterize the part of the population not being sampled because we know the constants a1 and a2. In most applications we are still interested in estimating Eðyi j xiÞ ¼ xib. However, because of sample selection based on yi, we must—at least in a parametric context— specify a full conditional distribution of yi given xi. Parameterize the conditional density of yi given xi by f ð\u0002 j xi; b; gÞ, where b are the conditional mean parameters and g is a G \u0001 1 vector of additional parameters. The cdf of yi given xi is Fð\u0002 j xi; b; gÞ. What we can use in estimation is the density of yi conditional on xi and the fact that we observe ðyi; xiÞ. In other words, we must condition on a1 < yi < a2 or, equivalently, si ¼ 1. The cdf of yi conditional on ðxi; si ¼ 1Þ is simply Pðyi a y j xi; si ¼ 1Þ ¼ Pðyi a y; si ¼ 1 j xiÞ Pðsi ¼ 1 j xiÞ Because yi is continuously distributed, Pðsi ¼1jxiÞ¼Pða1 < yi <a2jxiÞ¼Fða2jxi; b; gÞ \u0004 Fða1 j xi; b; gÞ > 0 for all possible values of xi. The case a2 ¼ y corresponds to truncation only from below, in which case Fða2 j xi; b; gÞ 1 1. If a1 ¼ \u0004y (truncation only from above), then Fða1 j xi; b; gÞ ¼ 0. To obtain the numerator when a1 < y < a2, we have Pðyi a y; si ¼ 1 j xiÞ ¼ Pða1 < yi a y j xiÞ ¼ Fðy j xi; b; gÞ \u0004 Fða1 j xi; b; gÞ When we put this equation over Pðsi ¼ 1 j xiÞ and take the derivative with respect to the dummy argument y, we obtain the density of yi given ðxi; si ¼ 1Þ: pðy j xi; si ¼ 1Þ ¼ f ðy j xi; b; gÞ Fða2 j xi; b; gÞ \u0004 Fða1 j xi; b; gÞ ð17:14Þ for a1 < y < a2. Given a model for f ðy j x; b; gÞ, the log-likelihood function for any ðxi; yiÞ in the sample can be obtained by plugging yi into equation (17.14) and taking the log. The CMLEs of b and g using the selected sample are e‰cient in the class of estimators that do not use information about the distribution of xi. Standard errors and test statistics can be computed using the general theory of conditional MLE. In most applications of truncated samples, the population conditional distribution is assumed to be Normalðxb; s2Þ, in which case we have the truncated Tobit model or truncated normal regression model. The truncated Tobit model is related to the cen- sored Tobit model for data-censoring applications (see Chapter 16), but there is a key Sample Selection, Attrition, and Stratiﬁed Sampling 559", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 568, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p569::c0", "text": "di¤erence: in censored regression, we observe the covariates x for all people, even those for whom the response is not known. If we drop observations entirely when the response is not observed, we obtain the truncated regression model. If in Example 16.1 we use the information in the top coded observations, we are in the censored regression case. If we drop all top coded observations, we are in the truncated re- gression case. (Given a choice, we should use a censored regression analysis, as it uses all of the information in the sample.) From our analysis of the censored regression model in Chapter 16, it is not sur- prising that heteroskedasticity or nonnormality in truncated regression results in in- consistent estimators of b. This outcome is unfortunate because, if not for the sample selection problem, we could consistently estimate b under Eðy j xÞ ¼ xb, without specifying Varðy j xÞ or the conditional distribution. Distribution-free methods for the truncated regression model have been suggested by Powell (1986) under the as- sumption of a symmetric error distribution; see Powell (1994) for a recent survey. Truncating a sample on the basis of y is related to choice-based sampling. Tradi- tional choice-based sampling applies when y is a discrete response taking on a ﬁnite number of values, where sampling frequencies di¤er depending on the outcome of y. [In the truncation case, the sampling frequency is one when y falls in the interval ða1; a2Þ and zero when y falls outside of the interval.] We do not cover choice-based sampling here; see Manksi and McFadden (1981), Imbens (1992), and Cosslett (1993). In Section 17.8 we cover some estimation methods for stratiﬁed sampling, which can be applied to some choice-based samples. 17.4 A Probit Selection Equation We now turn to sample selection corrections when selection is determined by a probit model. This setup applies to problems di¤erent from those in Section 17.3, where the problem was that a survey or program was designed to intentionally exclude part of the population. We are now interested in selection problems that are due to incidental truncation, attrition in the context of program evalution, and general nonresponse that leads to missing data on the response variable or the explanatory variables. 17.4.1 Exogenous Explanatory Variables The incidental truncation problem is motivated by Gronau’s (1974) model of the wage o¤er and labor force participation. Example 17.5 (Labor Force Participation and the Wage O¤er): Interest lies in esti- mating Eðwo i j xiÞ, where wo i is the hourly wage o¤er for a randomly drawn individual Chapter 17 560", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 569, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p570::c0", "text": "i. If wo i were observed for everyone in the (working age) population, we would pro- ceed in a standard regression framework. However, a potential sample selection problem arises because wo i is observed only for people who work. We can cast this problem as a weekly labor supply model: max h utiliðwo i h þ ai; hÞ subject to 0 a h a 168 ð17:15Þ where h is hours worked per week and ai is nonwage income of person i. Let siðhÞ 1 utiliðwo i h þ ai; hÞ, and assume that we can rule out the solution hi ¼ 168. Then the solution can be hi ¼ 0 or 0 < hi < 168. If dsi=dh a 0 at h ¼ 0, then the optimum is hi ¼ 0. Using this condition, straightforward algebra shows that hi ¼ 0 if and only if wo i a \u0004muh i ðai; 0Þ=muq i ðai; 0Þ ð17:16Þ where muh i ð\u0002 ; \u0002Þ is the marginal disutility of working and muq i ð\u0002 ; \u0002Þ is the marginal utility of income. Gronau (1974) called the right-hand side of equation (17.16) the reservation wage, wr i , which is assumed to be strictly positive. We now make the parametric assumptions wo i ¼ expðxi1b1 þ ui1Þ; wr i ¼ expðxi2b2 þ g2ai þ ui2Þ ð17:17Þ where ðui1; ui2Þ is independent of ðxi1; xi2; aiÞ. Here, xi1 contains productivity char- acteristics, and possibly demographic characteristics, of individual i, and xi2 contains variables that determine the marginal utility of leisure and income; these may overlap with xi1. From equation (17.17) we have the log wage equation log wo i ¼ xi1b1 þ ui1 ð17:18Þ But the wage o¤er wo i is observed only if the person works, that is, only if wo i b wr i , or log wo i \u0004 log wr i ¼ xi1b1 \u0004 xi2b2 \u0004 g2ai þ ui1 \u0004 ui2 1 xid2 þ vi2 > 0 This behavior introduces a potential sample selection problem if we use data only on working people to estimate equation (17.18). This example di¤ers in an important respect from top coding examples. With top coding, the censoring rule is known for each unit in the population. In Gronau’s ex- ample, we do not know wr i , so we cannot use wo i in a censored regression analysis. If wr i were observed and exogenous and xi1 were always observed, then we would be in the censored regression framework (see Problem 16.3). If wr i were observed and ex- ogenous but xi1 were observed only when wo i is, we would be in the truncated Tobit framework. But wr i is allowed to depend on unobservables, and so we need a new framework. Sample Selection, Attrition, and Stratiﬁed Sampling 561", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 570, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p571::c0", "text": "If we drop the i subscript, let y1 1 log wo, and let y2 be the binary labor force participation indicator, Gronau’s model can be written for a random draw from the population as y1 ¼ x1b1 þ u1 ð17:19Þ y2 ¼ 1½xd2 þ v2 > 0\u0003 ð17:20Þ We discuss estimation of this model under the following set of assumptions: assumption 17.1: (a) ðx; y2Þ are always observed, y1 is observed only when y2 ¼ 1; (b) ðu1; v2Þ is independent of x with zero mean; (c) v2 @ Normalð0; 1Þ; and (d) Eðu1 j v2Þ ¼ g1v2. Assumption 17.1a emphasizes the sample selection nature of the problem. Part b is a strong, but standard, form of exogeneity of x. We will see that Assumption 17.1c is needed to derive a conditional expectation given the selected sample. It is probably the most restrictive assumption because it is an explicit distributional assumption. Assuming Varðv2Þ ¼ 1 is without loss of generality because y2 is a binary variable. Assumption 17.1d requires linearity in the population regression of u1 on v2. It always holds if ðu1; v2Þ is bivariate normal—a standard assumption in these contexts—but Assumption 17.1d holds under weaker assumptions. In particular, we do not need to assume that u1 itself is normally distributed. Amemiya (1985) calls equations (17.19) and (17.20) the type II Tobit model. This name is ﬁne as a label, but we must understand that it is a model of sample selection, and it has nothing to do with y1 being a corner solution outcome. Unfortunately, in almost all treatments of this model, y1 is set to zero when y2 ¼ 0. Setting y1 to zero (or any value) when y2 ¼ 0 is misleading and can lead to inappropriate use of the model. For example, it makes no sense to set the wage o¤er to zero just because we do not observe it. As another example, it makes no sense to set the price per dollar of life insurance ðy1Þ to zero for someone who did not buy life insurance (so y2 ¼ 1 if and only if a person owns a life insurance policy). We also have some interest in the parameters of the selection equation (17.20); for example, in Gronau’s model it is a reduced-form labor force participation equation. In program evaluation with attrition, the selection equation explains the probability of dropping out of the program. We can allow a little more generality in the model by replacing x in equation (17.20) with x2; then, as will become clear, x1 would only need to be observed whenever y1 is, whereas x2 must always be observed. This extension is not especially useful for something like Gronau’s model because it implies that x1 contains elements Chapter 17 562", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 571, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p572::c0", "text": "that cannot also appear in x2. Because the selection equation is not typically a structural equation, it is undesirable to impose exclusion restrictions in equation (17.20). If a variable a¤ecting y1 is observed only along with y1, the instrumental variables method that we cover in Section 17.4.2 is more attractive. To derive an estimating equation, let ðy1; y2; x; u1; v2Þ denote a random draw from the population. Since y1 is observed only when y2 ¼ 1, what we can hope to estimate is Eðy1 j x; y2 ¼ 1Þ [along with Pðy2 ¼ 1 j xÞ]. How does Eðy1 j x; y2 ¼ 1Þ depend on the vector of interest, b1? First, under Assumption 17.1 and equation (17.19), Eðy1 j x; v2Þ ¼ x1b1 þ Eðu1 j x; v2Þ ¼ x1b1 þ Eðu1 j v2Þ ¼ x1b1 þ g1v2 ð17:21Þ where the second equality follows because ðu1; v2Þ is independent of x. Equation (17.21) is very useful. The ﬁrst thing to note is that, if g1 ¼ 0—which implies that u1 and v2 are uncorrelated—then Eðy1 j x; v2Þ ¼ Eðy1 j xÞ ¼ Eðy1 j x1Þ ¼ x1b1. Because y2 is a function of ðx; v2Þ, it follows immediately that Eðy1 j x; y2Þ ¼ Eðy1 j x1Þ. In other words, if g1 ¼ 0, then there is no sample selection problem, and b1 can be consistently estimated by OLS using the selected sample. What if g1 0 0? Using iterated expectations on equation (17.21), Eðy1 j x; y2Þ ¼ x1b1 þ g1Eðv2 j x; y2Þ ¼ x1b1 þ g1hðx; y2Þ where hðx; y2Þ ¼ Eðv2 j x; y2Þ. If we knew hðx; y2Þ, then, from Theorem 17.1, we could estimate b1 and g1 from the regression y1 on x1 and hðx; y2Þ, using only the selected sample. Because the selected sample has y2 ¼ 1, we need only ﬁnd hðx; 1Þ. But hðx; 1Þ ¼ Eðv2 j v2 > \u0004xd2Þ ¼ lðxd2Þ, where lð\u0002Þ 1 fð\u0002Þ=Fð\u0002Þ is the inverse Mills ratio, and so we can write Eðy1 j x; y2 ¼ 1Þ ¼ x1b1 þ g1lðxd2Þ ð17:22Þ Equation (17.22), which can be found in numerous places (see, for example, Heckman, 1979, and Amemiya, 1985) makes it clear that an OLS regression of y1 on x1 using the selected sample omits the term lðxd2Þ and generally leads to inconsistent estima- tion of b1. As pointed out by Heckman (1979), the presence of selection bias can be viewed as an omitted variable problem in the selected sample. An interesting point is that, even though only x1 appears in the population expectation, Eðy1 j xÞ, other ele- ments of x appear in the expectation on the subpopulation, Eðy1 j x; y2 ¼ 1Þ. Equation (17.22) also suggests a way to consistently estimate b1. Following Heckman (1979), we can consistently estimate b1 and g1 using the selected sample by regressing yi1 on xi1, lðxid2Þ. The problem is that d2 is unknown, so we cannot compute the additional regressor lðxid2Þ. Nevertheless, a consistent estimator of d2 is available from the ﬁrst-stage probit estimation of the selection equation. Sample Selection, Attrition, and Stratiﬁed Sampling 563", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 572, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p573::c0", "text": "Procedure 17.1: (a) Obtain the probit estimate ^d2 from the model Pðyi2 ¼ 1 j xiÞ ¼ Fðxid2Þ ð17:23Þ using all N observations. Then, obtain the estimated inverse Mills ratios ^li2 1 lðxi ^d2Þ (at least for i ¼ 1; . . . ; N1). (b) Obtain ^b1 and ^g1 from the OLS regression on the selected sample, yi1 on xi1; ^li2; i ¼ 1; 2; . . . ; N1 ð17:24Þ These estimators are consistent and ﬃﬃﬃﬃ N p -asymptotically normal. The procedure is sometimes called Heckit after Heckman (1976) and the tradition of putting ‘‘it’’ on the end of procedures related to probit (such as Tobit). A very simple test for selection bias is available from regression (17.24). Under the the null of no selection bias, H0: g1 ¼ 0, we have Varðy1 j x; y2 ¼ 1Þ ¼ Varðy1 j xÞ ¼ Varðu1Þ, and so homoskedasticity holds under H0. Further, from the results on gen- erated regressors in Chapter 6, the asymptotic variance of ^g1 (and ^b1) is not a¤ected by ^d2 when g1 ¼ 0. Thus, a standard t test on ^g1 is a valid test of the null hypothsesis of no selection bias. When g1 0 0, obtaining a consistent estimate for the asymptotic variance of ^b1 is complicated for two reasons. The ﬁrst is that, if g1 0 0, then Varðy1 j x; y2 ¼ 1Þ is not constant. As we know, heteroskedasticity itself is easy to correct for using the robust standard errors. However, we should also account for the fact that ^d2 is an estimator of d2. The adjustment to the variance of ð ^b1; ^g1Þ because of the two-step estimation is cumbersome—it is not enough to simply make the standard errors heteroskedasticity- robust. Some statistical packages now have this feature built in. As a technical point, we do not need x1 to be a strict subset of x for b1 to be indentiﬁed, and Procedure 17.1 does carry through when x1 ¼ x. However, if xi ^d2 does not have much variation in the sample, then ^li2 can be approximated well by a linear function of x. If x ¼ x1, this correlation can introduce severe collinearity among the regressors in regression (17.24), which can lead to large standard errors of the elements of ^b1. When x1 ¼ x, b1 is identiﬁed only due to the nonlinearity of the inverse Mills ratio. The situation is not quite as bad as in Section 9.5.1. There, identiﬁcation failed for certain values of the structural parameters. Here, we still have identiﬁcation for any value of b1 in equation (17.19), but it is unlikely we can estimate b1 with much pre- cision. Even if we can, we would have to wonder whether a statistically inverse Mills ratio term is due to sample selection or functional form misspeciﬁcation in the pop- ulation model (17.19). Chapter 17 564", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 573, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p574::c0", "text": "Example 17.6 (Wage O¤er Equation for Married Women): We use the data in MROZ.RAW to estimate a wage o¤er function for married women, accounting for potential selectivity bias into the workforce. Of the 753 women, we observe the wage o¤er for 428 working women. The labor force participation equation contains the variables in Table 15.1, including other income, age, number of young children, and number of older children—in addition to educ, exper, and exper2. The results of OLS on the selected sample and the Heckit method are given in Table 17.1. The di¤erences between the OLS and Heckit estimates are practically small, and the inverse Mills ratio term is statistically insigniﬁcant. The fact that the intercept estimates di¤er somewhat is usually unimportant. [The standard errors reported for Heckit are the unadjusted ones from regression (17.24). If ^l2 were statistically sig- niﬁcant, we should obtain the corrected standard errors.] The Heckit results in Table 17.1 use four exclusion restrictions in the structural equation, because nwifeinc, age, kidslt6, and kidsge6 are all excluded from the wage o¤er equation. If we allow all variables in the selection equation to also appear in the wage o¤er equation, the Heckit estimates become very imprecise. The coe‰cient on educ becomes .119 (se ¼ :034), compared with the OLS estimate .100 (se ¼ :015). The coe‰cient on kidslt6—which now appears in the wage o¤er equation—is \u0004:188 (se ¼ :232) in the Heckit estimation, and \u0004:056 (se ¼ :009) in the OLS estimation. The imprecision of the Heckit estimates is due to the severe collinearity that comes from adding ^l2 to the equation, because ^l2 is now a function only of the explanatory variables in the wage o¤er equation. In fact, using the selected sample, regressing ^l2 on Table 17.1 Wage O¤er Equation for Married Women Dependent Variable: logðwageÞ Independent Variable OLS Heckit educ .108 (.014) .109 (.016) exper .042 (.012) .044 (.016) exper2 \u0004.00081 (.00039) \u0004.00086 (.00044) constant \u0004.522 (.199) \u0004.578 (.307) ^l2 — .032 (.134) Sample size 428 428 R-squared .157 .157 Sample Selection, Attrition, and Stratiﬁed Sampling 565", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 574, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p575::c0", "text": "the seven explanatory variables gives R-squared ¼ :962. Unfortunately, comparing the OLS and Heckit results does not allow us to resolve some important issues. For example, the OLS results suggest that another young child reduces the wage o¤er by about 5.6 percent (t statistic A\u00046:2), other things being equal. Is this e¤ect real, or is it simply due to our inability to adequately correct for sample selection bias? Unless we have a variable that a¤ects labor force participation without a¤ecting the wage o¤er, we cannot answer this question. If we replace parts c and d in Assumption 17.1 with the stronger assumption that ðu1; v2Þ is bivariate normal with mean zero, Varðu1Þ ¼ s2 1, Covðu1; v2Þ ¼ s12, and Varðv2Þ ¼ 1, then partial maximum likelihood estimation can be used, as described generally in Problem 13.7. Partial MLE will be more e‰cient than the two-step pro- cedure under joint normality of u1 and v2, and it will produce standard errors and likelihood ratio statistics that can be used directly (this conclusion follows from Problem 13.7). The drawbacks are that it is less robust than the two-step procedure and that it is sometimes di‰cult to get the problem to converge. The reason we cannot perform full conditional MLE is that y1 is only observed when y2 ¼ 1. Thus, while we can use the full density of y2 given x, which is f ðy2 j xÞ ¼ ½Fðxd2Þ\u0003y2½1 \u0004 Fðxd2Þ\u00031\u0004y2, y2 ¼ 0, 1, we can only use the density f ðy1 j y2; xÞ when y2 ¼ 1. To ﬁnd f ðy1 j y2; xÞ at y2 ¼ 1, we can use Bayes’ rule to write f ðy1 j y2; xÞ ¼ f ðy2 j y1; xÞf ðy1 j xÞ= f ðy2 j xÞ. Therefore, f ðy1 j y2 ¼1; xÞ¼Pðy2¼1 j y1; xÞf ðy1 j xÞ= Pðy2 ¼1 j xÞ. But y1 j x @ Normalðx1b1; s2 1Þ. Further, y2 ¼1½xd2 þ s12s\u00042 1 ðy1 \u0004 x1b1Þ þ e2 > 0\u0003, where e2 is independent of ðx; y1Þ and e2 @Normalð0; 1 \u0004 s2 12s\u00042 1 Þ (this conclusion follows from standard conditional distribution results for joint normal random variables). Therefore, Pðy2 ¼ 1 j y1; xÞ ¼ Ff½xd2 þ s12s\u00042 1 ðy1 \u0004 x1b1Þ\u0003ð1 \u0004 s2 12s\u00042 1 Þ\u00041=2g Combining all of these pieces [and noting the cancellation of Pðy2 ¼ 1 j xÞ\u0003 we get liðyÞ ¼ ð1 \u0004 yi2Þ log½1 \u0004 Fðxid2Þ\u0003 þ yi2ðlog Ff½xd2 þ s12s\u00042 1 ðyi1 \u0004 xi1b1Þ\u0003 \u0001 ð1 \u0004 s2 12s\u00042 1 Þ\u00041=2g þ log f½ðyi1 \u0004 xi1b1Þ=s1\u0003 \u0004 logðs1ÞÞ The partial log likelihood is obtained by summing liðyÞ across all observations; yi2 ¼ 1 picks out when yi1 is observed and therefore contains information for esti- mating b1. Ahn and Powell (1993) show how to consistently estimate b1 without making any distributional assumptions; in particular, the selection equation need not have the probit form. Vella (1998) contains a recent survey. Chapter 17 566", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 575, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p576::c0", "text": "17.4.2 Endogenous Explanatory Variables We now study the sample selection model when one of the elements of x1 is thought to be correlated with u1. Or, all the elements of x1 are exogenous in the population model but data are missing on an element of x1, and the reason data are missing might be systematically related to u1. For simplicity, we focus on the case of a single endogenous explanatory variable. The model in the population is y1 ¼ z1d1 þ a1y2 þ u1 ð17:25Þ y2 ¼ zd2 þ v2 ð17:26Þ y3 ¼ 1ðzd3 þ v3 > 0Þ ð17:27Þ The ﬁrst equation is the structural equation of interest, the second equation is a linear projection for the potentially endogenous or missing variable y2, and the third equation is the selection equation. We allow arbitrary correlation among u1, v2, and v3. The setup in equations (17.25)–(17.27) encompasses at least three cases of interest. The ﬁrst occurs when y2 is always observed but is endogenous in equation (17.25). An example is seen when y1 is logðwageoÞ and y2 is years of schooling: years of schooling is generally available whether or not someone is in the workforce. The model also applies when y2 is observed only along with y1, as would happen if y1 ¼ logðwageoÞ and y2 is the ratio of the beneﬁts o¤er to wage o¤er. As a second exam- ple, let y1 be the percentage of voters supporting the incumbent in a congressional district, and let y2 be intended campaign expenditures. Then y3 ¼ 1 if the incumbent runs for reelection, and we only observe ðy1; y2Þ when y3 ¼ 1. A third application is to missing data only on y2, as in Example 17.4 where y2 is IQ score. In the last two cases, y2 might in fact be exogenous in equation (17.25), but endogenous sample selection e¤ectively makes y2 endogenous in the selected sample. If y1 and y2 were always observed along with z, we would just estimate equation (17.25) by 2SLS if y2 is endogenous. We can use the results from Section 17.2.1 to show that 2SLS with the inverse Mills ratio added to the regressors is consistent. Regardless of the data availability on y1 and y2, in the second step we use only observations for which both y1 and y2 are observed. assumption 17.2: (a) ðz; y3Þ is always observed, ðy1; y2Þ is observed when y3 ¼ 1; (b) ðu1; v3Þ is independent of z; (c) v3 @ Normalð0; 1Þ; (d) Eðu1 j v3Þ ¼ g1v3; and (e) Eðz0v2Þ ¼ 0 and, writing zd2 ¼ z1d21 þ z2d22, d22 0 0. Sample Selection, Attrition, and Stratiﬁed Sampling 567", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 576, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p577::c0", "text": "Parts b, c, and d are identical to the corresponding assumptions when all explana- tory variables are observed and exogenous. Assumption e is new, resulting from the endogeneity of y2 in equation (17.25). It is important to see that Assumption 17.2e is identical to the rank condition needed for identifying equation (17.25) in the absence of sample selection. As we will see, stating identiﬁcation in the population is not always su‰cient, but, from a practical point of view, the focus should be on As- sumption 17.2e. To derive an estimating equation, write (in the population) y1 ¼ z1d1 þ a1y2 þ gðz; y3Þ þ e1 ð17:28Þ where gðz; y3Þ 1 Eðu1 j z; y3Þ and e1 1 u1 \u0004 Eðu1 j z; y3Þ. By deﬁnition, Eðe1 j z; y3Þ ¼ 0. If we knew gðz; y3Þ then, from Theorem 17.1, we could just estimate equation (17.28) by 2SLS on the selected sample ðy3 ¼ 1Þ using instruments ½z; gðz; 1Þ\u0003. It turns out that we do know gðz; 1Þ up to some estimable parameters: Eðu1 j z; y3 ¼ 1Þ ¼ g1lðzd3Þ. Since d3 can be consistently estimated by probit of y3 on z (using the entire sample), we have the following: Procedure 17.2: (a) Obtain ^d3 from probit of y3 on z using all observations. Obtain the estimated inverse Mills ratios, ^li3 ¼ lðzi ^d3Þ. (b) Using the selected subsample (for which we observe both y1 and y2), estimate the equation yi1 ¼ zi1d1 þ a1yi2 þ g1^li3 þ errori ð17:29Þ by 2SLS, using instruments ðzi; ^li3Þ. The steps in this procedure show that identiﬁcation actually requires that z2 appear in the linear projection of y2 onto z1; z2, and lðzd3Þ in the selected subpopulation. It would be unusual if this statement were not true when the rank condition 17.2e holds in the population. The hypothesis-of-no-selection problem (allowing y2 to be endogenous or not), H0: g1 ¼ 0, is tested using the usual 2SLS t statistic for ^g1. When g1 0 0, standard errors and test statistics should be corrected for the generated regressors problem, as in Chapter 6. Example 17.7 (Education Endogenous and Sample Selection): In Example 17.6 we now allow educ to be endogenous in the wage o¤er equation, and we test for sample selection bias. Just as if we did not have a sample selection problem, we need IVs for educ that do not appear in the wage o¤er equation. As in Example 5.3, we use parents’ education (motheduc, fatheduc) and husband’s education as IVs. In addition, Chapter 17 568", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 577, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p578::c0", "text": "we need some variables that a¤ect labor force participation but not the wage o¤er; we use the same four variables as in Example 17.6. Therefore, all variables except educ (and, of course, the wage o¤er) are treated as exogenous. Unless we have very reliable prior information, all exogenous variables should appear in the selection equation, and all should be listed as instruments in estimating equation (17.29) by 2SLS. Dropping some exogenous variables in either the selection equation or in estimating equation (17.29) imposes exclusion restrictions on a reduced- form equation, something that can be dangerous and is unnecessary. Therefore, in the labor force participation equation we include exper, exper2, nwifeinc, kidslt6, kidsge6, motheduc, fatheduc, and huseduc (not educ). In estimating equation (17.29), the same set of variables, along with ^l3, are used as IVs. The 2SLS coe‰cient on ^l3 is .040 (se ¼ :133), and so, again, there is little evidence of sample selection bias. The coe‰cient on educ is .088 (se ¼ :021), which is similar to the 2SLS estimate obtained without the sample selection correction (see Example 5.3). Because there is little evi- dence of sample selection bias, the standard errors are not corrected for ﬁrst-stage estimation of d3. Importantly, Procedure 17.2 applies to any kind of endogenous variable y2, in- cluding binary and other discrete variables, without any additional assumptions. This statement is true because the reduced form for y2 is just a linear projection; we do not have to assume, for example, that v2 is normally distributed or even independent of z. As an example, we might wish to look at the e¤ects of participation in a job training program on the subsequent wage o¤er, accounting for the fact that not all who par- ticipated in the program will be employed in the following period (y2 is always observed in this case). If participation is voluntary, an instrument for it might be whether the person was randomly chosen as a potential participant. Even if y2 is exogenous in the population equation (17.25), when y2 is sometimes missing we generally need an instrument for y2 when selection is not ignorable [that is, Eðu1 j z1; y2; y3Þ 0 Eðu1Þ]. In Example 17.4 we could use family background vari- ables and another test score, such as KWW, as IVs for IQ, assuming these are always observed. We would generally include all such variables in the reduced-form selection equation. Procedure 17.2 works whether we assume IQ is a proxy variable for ability or an indicator of ability (see Chapters 4 and 5). As a practical matter, we should have at least two elements of z that are not also in z1; that is, we need at least two exclusion restrictions in the structural equation. Intuitively, for the procedure to be convincing, we should have at least one instru- ment for y2 and another exogenous variable that determines selection. Suppose that the scalar z2 is our only exogenous variable excluded from equation (17.25). Then, Sample Selection, Attrition, and Stratiﬁed Sampling 569", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 578, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p579::c0", "text": "under random sampling, the equation would be just identiﬁed. When we account for sample selection bias, the Mills ratio term in equation (17.29) is a function of z1 and z2. While the nonlinearity of the Mills ratio technically allows us to identify d1 and a1, it is unlikely to work very well in practice because of severe multicollinearity among the IVs. This situation is analogous to using the standard Heckit method when there are no exclusion restrictions in the structural equation (see Section 17.4.1). If we make stronger assumptions, it is possible to estimate model (17.25)–(17.27) by partial maximum likelihood of the kind discussed in Problem 13.7. One possibility is to assume that ðu1; v2; v3Þ is trivariate normal and independent of z. In addition to ruling out discrete y2, such a procedure would be computationally di‰cult. If y2 is binary, we can model it as y2 ¼ 1½zd2 þ v2 > 0\u0003, where v2 j z @ Normalð0; 1Þ. But maximum likelihood estimation that allows any correlation matrix for ðu1; v2; v3Þ is complicated and less robust than Procedure 17.2. 17.4.3 Binary Response Model with Sample Selection We can estimate binary response models with sample selection if we assume that the latent errors are bivariate normal and independent of the explanatory variables. Write the model as y1 ¼ 1½x1b1 þ u1 > 0\u0003 ð17:30Þ y2 ¼ 1½xd2 þ v2 > 0\u0003 ð17:31Þ where the second equation is the sample selection equation and y1 is observed only when y2 ¼ 1; we assume that x is always observed. For example, suppose y1 is an employment indicator and x1 contains a job training binary indicator (which we as- sume is exogenous), as well as other human capital and family background variables. We might lose track of some people who are eligible to participate in the program; this is an example of sample attrition. If attrition is systematically related to u1, esti- mating equation (17.30) on the sample at hand can result in an inconsistent estimator of b1. If we assume that ðu1; v2Þ is independent of x with a zero-mean normal distribution (and unit variances), we can apply partial maximum likelihood. What we need is the density of y1 conditional on x and y2 ¼ 1. We have essentially found this density in Chapter 15: in equation (15.55) set a1 ¼ 0, replace z with x, and replace d1 with b1. The parameter r1 is still the correlation between u1 and v2. A two-step procedure can be applied: ﬁrst, estimate d2 by probit of y2 on x. Then, estimate b1 and r1 in the second stage using equation (15.55) along with Pðy1 ¼ 0 j x; y2 ¼ 1Þ. Chapter 17 570", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 579, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p580::c0", "text": "A convincing analysis requires at least one variable in x—that is, something that determines selection—that is not also in x1. Otherwise, identiﬁcation is o¤ of the nonlinearities in the probit models. Allowing for endogenous explanatory variables in equation (17.30) along with sample selection is di‰cult, and it could be the focus of future research. 17.5 A Tobit Selection Equation We now study the case where more information is available on sample selection, primarily in the context of incidental truncation. In particular, we assume that selection is based on the outcome of a Tobit, rather than a probit, equation. The analysis of the models in this section comes from Wooldridge (1998). The model in Section 17.5.1 is a special case of the model studied by Vella (1992) in the context of testing for selectivity bias. 17.5.1 Exogenous Explanatory Variables We now consider the case where the selection equation is of the censored Tobit form. The population model is y1 ¼ x1b1 þ u1 ð17:32Þ y2 ¼ maxð0; xd2 þ v2Þ ð17:33Þ where ðx; y2Þ is always observed in the population but y1 is observed only when y2 > 0. A standard example occurs when y1 is the log of the hourly wage o¤er and y2 is weekly or annual hours of labor supply. assumption 17.3: (a) ðx; y2Þ is always observed in the population, but y1 is observed only when y2 > 0; (b) ðu1; v2Þ is independent of x; (c) v2 @ Normalð0; t2 2Þ; and (d) Eðu1 j v2Þ ¼ g1v2. These assumptions are very similar to the assumptions for a probit selection equa- tion. The only di¤erence is that v2 now has an unknown variance, since y2 is a cen- sored as opposed to binary variable. Amemiya (1985) calls equations (17.32) and (17.33) the type III Tobit model, but we emphasize that equation (17.32) is the structural population equation of interest and that equation (17.33) simply determines when y1 is observed. In the labor eco- nomics example, we are interested in the wage o¤er equation, and equation (17.33) is a reduced-form hours equation. It makes no sense to deﬁne y1 to be, say, zero, just because we do not observe y1. Sample Selection, Attrition, and Stratiﬁed Sampling 571", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 580, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p581::c0", "text": "The starting point is equation (17.21), just as in the probit selection case. Now deﬁne the selection indicator as s2 ¼ 1 if y2 > 0, and s2 ¼ 0 otherwise. Since s2 is a function of x and v2, it follows immediately that Eðy1 j x; v2; s2Þ ¼ x1b1 þ g1v2 ð17:34Þ This equation means that, if we could observe v2, then an OLS regression of y1 on x1, v2 using the selected subsample would consistently estimate ðb1; g1Þ, as we discussed in Section 17.2.1. While v2 cannot be observed when y2 ¼ 0 (because when y2 ¼ 0, we only know that v2 a \u0004xd2), for y2 > 0, v2 ¼ y2 \u0004 xd2. Thus, if we knew d2, we would know v2 whenever y2 > 0. It seems reasonable that, because d2 can be con- sistently estimated by Tobit on the whole sample, we can replace v2 with consistent estimates. Procedure 17.3: (a) Estimate equation (17.33) by standard Tobit using all N obser- vations. For yi2 > 0 (say i ¼ 1; 2; . . . ; N1), deﬁne ^vi2 ¼ yi2 \u0004 xi ^d2 ð17:35Þ (b) Using observations for which yi2 > 0, estimate b1, g1 by the OLS regression yi1 on xi1; ^vi2 i ¼ 1; 2; . . . ; N1 ð17:36Þ This regression produces consistent, ﬃﬃﬃﬃ N p -asymptotically normal estimators of b1 and g1 under Assumption 17.3. The statistic to test for selectivity bias is just the usual t statistic on ^vi2 in regression (17.36). This was suggested by Vella (1992). Wooldridge (1998) showed that this procedure also solves the selection problem when g1 0 0. It seems likely that there is an e‰ciency gain over Procedure 17.1. If v2 were known and we could use regression (17.36) for the entire population, there would deﬁnitely be an e‰ciency gain: the error variance is reduced by conditioning on v2 along with x, and there would be no heteroskedasticity in the population. See Prob- lem 4.5. Unlike in the probit selection case, x1 ¼ x causes no problems here: v2 always has separate variation from x1 because of variation in y2. We do not need to rely on the nonlinearity of the inverse Mills ratio. Example 17.8 (Wage O¤er Equation for Married Women): We now apply Proce- dure 17.3 to the wage o¤er equation for married women in Example 17.6. (We assume education is exogenous.) The only di¤erence is that the ﬁrst-step estimation is Tobit, rather than probit, and we include the Tobit residuals as the additional Chapter 17 572", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 581, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p582::c0", "text": "explanatory variables, not the inverse Mills ratio. In regression (17.36), the coe‰cient on ^v2 is \u0004:000053 (se ¼ :000041), which is somewhat more evidence of a sample selection problem, but we still do not reject the null hypothesis H0: g1 ¼ 0 at even the 15 percent level against a two-sided alternative. Further, the coe‰cient on educ is .103 (se ¼ :015), which is not much di¤erent from the OLS and Heckit estimates. (Again, we use the usual OLS standard error.) When we include all exogenous vari- ables in the wage o¤er equation, the estimates from Procedure 17.3 are much more stable than the Heckit estimates. For example, the coe‰cient on educ becomes .093 (se ¼ :016), which is comparable to the OLS estimates discussed in Example 17.6. For partial maximum likelihood estimation, we assume that ðu1; v2Þ is jointly normal, and we use the density for f ðy2 j xÞ for the entire sample and the con- ditional density f ðy1 j x; y2; s2 ¼ 1Þ ¼ f ðy1 j x; y2Þ for the selected sample. This ap- proach is fairly straightforward because, when y2 > 0, y1 j x; y2 @ Normal½x1b1 þ g1ðy2 \u0004 xd2Þ; h2 1\u0003, where h2 1 ¼ s2 1 \u0004 s2 12=t2 2, s2 1 ¼ Varðu1Þ, and s12 ¼ Covðu1; v2Þ. The log likelihood for observation i is liðyÞ ¼ si2 log f ðyi1 j xi; yi2; yÞ þ log f ðyi2 j xi; d2; t2 2Þ ð17:37Þ where f ðyi1 j xi; yi2; yÞ is the Normal½xi1b1 þ g1ðyi2 \u0004 xid2Þ; h2 1\u0003 distribution, eval- uated at yi1, and f ðyi2 j xi; d2; t2 2Þ is the standard censored Tobit density [see equation (16.19)]. As shown in Problem 13.7, the usual MLE theory can be used even though the log-likelihood function is not based on a true conditional density. It is possible to obtain sample selection corrections and tests for various other nonlinear models when the selection rule is of the Tobit form. For example, suppose that the binary variable y1 given z follows a probit model, but it is observed only when y2 > 0. A valid test for selection bias is to include the Tobit residuals, ^v2, in a probit of y1 on z, ^v2 using the selected sample; see Vella (1992). This procedure also produces consistent estimates (up to scale), as can be seen by applying the maximum likelihood results in Section 17.2.2 along with two-step estimation results. Honore´, Kyriazidou, and Udry (1997) show how to estimate the parameters of the type III Tobit model without making distributional assumptions. 17.5.2 Endogenous Explanatory Variables We explicitly consider the case of a single endogenous explanatory variable, as in Section 17.4.2. We use equations (17.25) and (17.26), and, in place of equation (17.27), we have a Tobit selection equation: y3 ¼ maxð0; zd3 þ v3Þ ð17:38Þ Sample Selection, Attrition, and Stratiﬁed Sampling 573", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 582, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p583::c0", "text": "assumption 17.4: (a) ðz; y3Þ is always observed, ðy1; y2Þ is observed when y3 > 0; (b) ðu1; v3Þ is independent of z; (c) v3 @ Normalð0; t2 3Þ; (d) Eðu1 j v3Þ ¼ g1v3; and (e) Eðz0v2Þ ¼ 0 and, writing zd2 ¼ z1d21 þ z2d22, d22 0 0. Again, these assumptions are very similar to those used with a probit selection mechanism. To derive an estimating equation, write y1 ¼ z1d1 þ a1y2 þ g1v3 þ e1 ð17:39Þ where e1 1 u1 \u0004 Eðu1 j v3Þ. Since ðe1; v3Þ is independent of z by Assumption 17.4b, Eðe1 j z; v3Þ ¼ 0. From Theorem 17.1, if v3 were observed, we could estimate equation (17.39) by 2SLS on the selected sample using instruments ðz; v3Þ. As before, we can estimate v3 when y3 > 0, since d3 can be consistently estimated by Tobit of y3 on z (using the entire sample). Procedure 17.4: (a) Obtain ^d3 from Tobit of y3 on z using all observations. Obtain the Tobit residuals ^vi3 ¼ yi3 \u0004 zi ^d3 for yi3 > 0. (b) Using the selected subsample, estimate the equation yi1 ¼ zi1d1 þ a1yi2 þ g1^vi3 þ errori ð17:40Þ by 2SLS, using instruments ðzi; ^vi3Þ. The estimators are ﬃﬃﬃﬃ N p -consistent and asymp- totically normal under Assumption 17.4. Comments similar to those after Procedure 17.2 hold here as well. Strictly speak- ing, identiﬁcation really requires that z2 appear in the linear projection of y2 onto z1, z2, and v3 in the selected subpopulation. The null of no selection bias is tested using the 2SLS t statistic (or maybe its heteroskedasticity-robust version) on ^vi3. When g1 0 0, standard errors should be corrected using two-step methods. As in the case with a probit selection equation, the endogenous variable y2 can be continuous, discrete, censored, and so on. Extending the method to multiple endog- enous explanatory variables is straightforward. The only restriction is the usual one for linear models: we need enough instruments to identify the structural equation. See Problem 17.6 for an application to the Mroz data. An interesting special case of model (17.25), (17.26), and (17.38) is when y2 ¼ y3. Actually, because we only use observations for which y3 > 0, y2 ¼ y\u0005 3 is also allowed, where y\u0005 3 ¼ zd3 þ v3. Either way, the variable that determines selection also appears in the structural equation. This special case could be useful when sample selection is caused by a corner solution outcome on y3 (in which case y2 ¼ y3 is Chapter 17 574", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 583, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p584::c0", "text": "natural) or because y\u0005 3 is subject to data censoring (in which case y2 ¼ y\u0005 3 is more realistic). An example of the former occurs when y3 is hours worked and we assume hours appears in the wage o¤er function. As a data-censoring example, suppose that y1 is a measure of growth in an infant’s weight starting from birth and that we observe y1 only if the infant is brought into a clinic within three months. Naturally, birth weight depends on age, and so y\u0005 3 —length of time between the ﬁrst and second mea- surements, which has quantitiative meaning—appears as an explanatory variable in the equation for y1. We have a data-censoring problem for y\u0005 3, which causes a sample selection problem for y1. In this case, we would estimate a censored regression model for y3 [or, possibly, logðy3Þ] to account for the data censoring. We would include the residuals ^vi3 ¼ yi3 \u0004 zi ^d3 in equation (17.40) for the noncensored observations. As our extra instrument we might use distance from the child’s home to the clinic. 17.6 Estimating Structural Tobit Equations with Sample Selection We brieﬂy show how a structural Tobit model can be estimated using the methods of the previous section. As an example, consider the structural labor supply model logðwoÞ ¼ z1b1 þ u1 ð17:41Þ h ¼ max½0; z2b2 þ a2 logðwoÞ þ u2\u0003 ð17:42Þ This system involves simultaneity and sample selection because we observe wo only if h > 0. The general form of the model is y1 ¼ z1b1 þ u1 ð17:43Þ y2 ¼ maxð0; z2b2 þ a2 y1 þ u2Þ ð17:44Þ assumption 17.5: (a) ðz; y2Þ is always observed; y1 is observed when y2 > 0; (b) ðu1; u2Þ is independent of z with a zero-mean bivariate normal distribution; and (c) z1 contains at least one element whose coe‰cient is di¤erent from zero that is not in z2. As always, it is important to see that equations (17.43) and (17.44) constitute a model describing a population. If y1 were always observed, then equation (17.43) could be estimated by OLS. If, in addition, u1 and u2 were uncorrelated, equation (17.44) could be estimated by censored Tobit. Correlation between u1 and u2 could be handled by the methods of Section 16.6.2. Now, we require new methods, whether or not u1 and u2 are uncorrelated, because y1 is not observed when y2 ¼ 0. Sample Selection, Attrition, and Stratiﬁed Sampling 575", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 584, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p585::c0", "text": "The restriction in Assumption 17.5c is needed to identify the structural parameters ðb2; a2Þ (b1 is always identiﬁed). To see that this condition is needed, and for ﬁnding the reduced form for y2, it is useful to introduce the latent variable y\u0005 2 1 z2b2 þ a2 y1 þ u2 ð17:45Þ so that y2 ¼ maxð0; y\u0005 2Þ. If equations (17.43) and (17.45) make up the system of interest—that is, if y1 and y\u0005 2 are always observed—then b1 is identiﬁed without further restrictions, but identiﬁcation of a2 and b2 requires exactly Assumption 17.5c. This turns out to be su‰cient even when y2 follows a Tobit model and we have nonrandom sample selection. The reduced form for y\u0005 2 is y\u0005 2 ¼ zd2 þ v2. Therefore, we can write the reduced form of equation (17.44) as y2 ¼ maxð0; zd2 þ v2Þ: ð17:46Þ But then equations (17.43) and (17.46) constitute the model we studied in Section 17.5.1. The vector d2 is consistently estimated by Tobit, and b1 is estimated as in Procedure 17.3. The only remaining issue is how to estimate the structural param- eters of equation (17.44), a2 and b2. In the labor supply case, these are the labor supply parameters. Assuming identiﬁcation, estimation of ða2; b2Þ is fairly straightforward after having estimated b1. To see this point, write the reduced form of y2 in terms of the structural parameters as y2 ¼ max½0; z2b2 þ a2ðz1b1Þ þ v2\u0003 ð17:47Þ Under joint normality of u1 and u2, v2 is normally distributed. Therefore, if b1 were known, b2 and a2 could be estimated by standard Tobit using z2 and z1b1 as regres- sors. Operationalizing this procedure requires replacing b1 with its consistent esti- mator. Thus, using all observations, b2 and a2 are estimated from the Tobit equation yi2 ¼ max½0; zi2b2 þ a2ðzi1 ^b1Þ þ errori\u0003 ð17:48Þ To summarize, we have the following: Procedure 17.5: (a) Use Procedure 17.3 to obtain ^b1. (b) Obtain ^b2 and ^a2 from the Tobit in equation (17.48). In applying this procedure, it is important to note that the explanatory variable in equation (17.48) is zi1 ^b1 for all i. These are not the ﬁtted values from regression (17.36), which depend on ^vi2. Also, it may be tempting to use yi1 in place of zi1 ^b1 for Chapter 17 576", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 585, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p586::c0", "text": "that part of the sample for which yi1 is observed. This approach is not a good idea: the estimators are inconsistent in this case. The estimation in equation (17.48) makes it clear that the procedure fails if z1 does not contain at least one variable not in z2. If z1 is a subset of z2, then zi1 ^b1 is a linear combination of zi2, and so perfect multicollinearity will exist in equation (17.48). Estimating Avarð^a2; ^b2Þ is even messier than estimating Avarð ^b1Þ, since ð^a2; ^b2Þ comes from a three-step procedure. Often just the usual Tobit standard errors and test statistics reported from equation (17.48) are used, even though these are not strictly valid. By setting the problem up as a large GMM problem, as illustrated in Chapter 14, correct standard errors and test statistics can be obtained. Under Assumption 17.5, a full maximum likelihood approach is possible. In fact, the log-likelihood function can be constructed from equations (17.43) and (17.47), and it has a form very similar to equation (17.37). The only di¤erence is that non- linear restrictions are imposed automatically on the structural parameters. In addi- tion to making it easy to obtain valid standard errors, MLE is desirable because it allows us to estimate s2 2 ¼ Varðu2Þ, which is needed to estimate average partial e¤ects in equation (17.44). In examples such as labor supply, it is not clear where the elements of z1 that are not in z2 might come from. One possibility is a union binary variable, if we believe that union membership increases wages (other factors accounted for) but has no e¤ect on labor supply once wage and other factors have been controlled for. This approach would require knowing union status for people whether or not they are working in the period covered by the survey. In some studies past experience is assumed to a¤ect wage—which it certainly does—and is assumed not to appear in the labor supply function, a tenuous assumption. 17.7 Sample Selection and Attrition in Linear Panel Data Models In our treatment of panel data models we have assumed that a balanced panel is available—each cross section unit has the same time periods available. Often, some time periods are missing for some units in the population of interest, and we are left with an unbalanced panel. Unbalanced panels can arise for several reasons. First, the survey design may simply rotate people or ﬁrms out of the sample based on pre- speciﬁed rules. For example, if a survey of individuals begins at time t ¼ 1, at time t ¼ 2 some of the original people may be dropped and new people added. At t ¼ 3 some additional people might be dropped and others added; and so on. This is an example of a rotating panel. Sample Selection, Attrition, and Stratiﬁed Sampling 577", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 586, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p587::c0", "text": "Provided the decision to rotate units out of a panel is made randomly, unbalanced panels are fairly easy to deal with, as we will see shortly. A more complicated prob- lem arises when attrition from a panel is due to units electing to drop out. If this de- cision is based on factors that are systematically related to the response variable, even after we condition on explanatory variables, a sample selection problem can result— just as in the cross section case. Nevertheless, a panel data set provides us with the means to handle, in a simple fashion, attrition that is based on a time-constant, unobserved e¤ect, provided we use ﬁrst-di¤erencing methods; we show this in Section 17.7.3. A di¤erent kind of sample selection problem occurs when people do not disappear from the panel but certain variables are unobserved for at least some time periods. This is the incidental truncation problem discussed in Section 17.4. A leading case is estimating a wage o¤er equation using a panel of individuals. Even if the population of interest is people who are employed in the initial year, some people will become unemployed in subsequent years. For those people we cannot observe a wage o¤er, just as in the cross-sectional case. This situation is di¤erent from the attrition prob- lem where people leave the sample entirely and, usually, do not reappear in later years. In the incidental truncation case we observe some variables on everyone in each time period. 17.7.1 Fixed E¤ects Estimation with Unbalanced Panels We begin by studying assumptions under which the usual ﬁxed e¤ects estimator on the unbalanced panel is consistent. The model is the usual linear, unobserved e¤ects model under random sampling in the cross section: for any i, yit ¼ xitb þ ci þ uit; t ¼ 1; . . . ; T ð17:49Þ where xit is 1 \u0001 K and b is the K \u0001 1 vector of interest. As before, we assume that N cross section observations are available and the asymptotic analysis is as N ! y. We explicitly cover the case where ci is allowed to be correlated with xit, so that all elements of xit are time varying. A random e¤ects analysis is also possible under stronger assumptions; see, for example, Verbeek and Nijman (1992, 1996). We covered the case where all T time periods are available in Chapters 10 and 11. Now we consider the case where some time periods might be missing for some of the cross section draws. Think of t ¼ 1 as the ﬁrst time period for which data on anyone in the population are available, and t ¼ T as the last possible time period. For a random draw i from the population, let si 1 ðsi1; . . . ; siTÞ0 denote the T \u0001 1 vector of selection indicators: sit ¼ 1 if ðxit; yitÞ is observed, and zero otherwise. Generally, we Chapter 17 578", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 587, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p588::c0", "text": "have an unbalanced panel. We can treat fðxi; yi; siÞ: i ¼ 1; 2; . . . ; Ng as a random sample from the population; the selection indicators tell us which time periods are missing for each i. We can easily ﬁnd assumptions under which the ﬁxed e¤ects estimator on the unbalanced panel is consistent by writing it as ^b ¼ N\u00041 X N i¼1 X T t¼1 sit€x0 it€xit !\u00041 N\u00041 X N i¼1 X T t¼1 sit€x0 it €yit ! ¼ b þ N\u00041 X N i¼1 X T t¼1 sit€x0 it€xit !\u00041 N\u00041 X N i¼1 X T t¼1 sit€x0 ituit ! ð17:50Þ where we deﬁne €xit 1 xit \u0004 T\u00041 i X T r¼1 sirxir; €yit 1 yit \u0004 T\u00041 i X T r¼1 sir yir; and Ti 1 X T t¼1 sit That is, Ti is the number of time periods observed for cross section i, and we apply the within transformation on the available time periods. If ﬁxed e¤ects on the unbalanced panel is to be consistent, we should have Eðsit€x0 ituitÞ ¼ 0 for all t. Now, since €xit depends on all of xi and si, a form of strict exogeneity is needed. assumption 17.6: (a) Eðuit j xi; si; ciÞ ¼ 0, t ¼ 1; 2; . . . ; T; (b) PT t¼1 Eðsit€x0 it€xitÞ is nonsingular; and (c) Eðuiu0 i j xi; si; ciÞ ¼ s2 uIT. Under Assumption 17.6a, Eðsit€x0 ituitÞ ¼ 0 from the law of iterated expectations [because sit€xit is a function of ðxi; siÞ]. The second assumption is the rank condition on the expected outer product matrix, after accounting for sample selection; natu- rally, it rules out time-constant elements in xit. These ﬁrst two assumptions ensure consistency of FE on the unbalanced panel. In the case of a randomly rotating panel, and in other cases where selection is en- tirely random, si is independent of ðui; xi; ciÞ, in which case Assumption 17.6a follows under the standard ﬁxed e¤ects assumption Eðuit j xi; ciÞ ¼ 0 for all t. In this case, the natural assumptions on the population model imply consistency and asympotic nor- mality on the unbalanced panel. Assumption 17.6a also holds under much weaker conditions. In particular, it does not assume anything about the relationship between si and ðxi; ciÞ. Therefore, if we think selection in all time periods is correlated with ci or xi, but that uit is mean independent of si given ðxi; ciÞ for all t, then FE on the Sample Selection, Attrition, and Stratiﬁed Sampling 579", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 588, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p589::c0", "text": "unbalanced panel is consistent and asymptotically normal. This conclusion may be a reasonable approximation, especially for short panels. What Assumption 17.6a rules out is selection that is partially correlated with the idiosyncratic errors, uit. A random e¤ects analysis on the unbalanced panel requires much stronger as- sumptions: it e¤ectively requires si and ci to be independent. Random e¤ects will be inconsistent if, say, in a wage o¤er equation, less able people are more likely to dis- appear from the sample. This conclusion is true even if Eðci j xiÞ ¼ 0 (Assumption RE.1 from Chapter 10) holds in the underlying population; see Wooldridge (1995a) for further discussion. When we add Assumption 17.6c, standard inference procedures based on FE are valid. In particular, under Assumptions 17.6a and 17.6c, Var X T t¼1 sit€x0 ituit ! ¼ s2 u X T t¼1 Eðsit€x0 it€xitÞ \" # Therefore, the asymptotic variance of the ﬁxed e¤ects estimator is estimated as ^s2 u X N i¼1 X T t¼1 sit€x0 it€xit !\u00041 ð17:51Þ The estimator ^s2 u can be derived from E X T t¼1 sit€u2 it ! ¼ E X T t¼1 sitEð€u2 it j siÞ \" # ¼ E Ti½s2 uð1 \u0004 1=TiÞ\u0003 \u0002 \u0003 ¼ s2 uE½ðTi \u0004 1Þ\u0003 Now, deﬁne the FE residuals as ^uit ¼ €yit \u0004 €xit ^b when sit ¼ 1. Then, because N\u00041 PN i¼1ðTi \u0004 1Þ ! p EðTi \u0004 1Þ, ^s2 u ¼ N\u00041 X N i¼1 ðTi \u0004 1Þ \" #\u00041 N\u00041 X N i¼1 X T t¼1 sit^u2 it ¼ X N i¼1 ðTi \u0004 1Þ \" #\u00041X N i¼1 X T t¼1 sit^u2 it is consistent for s2 u as N ! y. Standard software packages also make a degrees-of- freedom adjustment by subtracting K from PN i¼1ðTi \u0004 1Þ. It follows that all of the usual test statistics based on an unbalanced ﬁxed e¤ects analysis are valid. In partic- ular, the dummy variable regression discussed in Chapter 10 produces asymptotically valid statistics. Because the FE estimator uses time demeaning, any unit i for which Ti ¼ 1 drops out of the ﬁxed e¤ects estimator. To use these observations we would need to add more assumptions, such as the random e¤ects assumption Eðci j xi; siÞ ¼ 0. Chapter 17 580", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 589, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p590::c0", "text": "Relaxing Assumption 17.6c is easy: just apply the robust variance matrix estimator in equation (10.59) to the unbalanced panel. The only changes are that the rows of €Xi are sit€xit and the elements of ^ui are sit^uit, t ¼ 1; . . . ; T. Under Assumption 17.6, it is also valid to used a standard ﬁxed e¤ects analysis on any balanced subset of the unbalanced panel; in fact, we can condition on any out- comes of the sit. For example, if we use unit i only when observations are available in all time periods, we are conditioning on sit ¼ 1 for all t. Using similar arguments, it can be shown that any kind of di¤erencing method on any subset of the observed panel is consistent. For example, with T ¼ 3, we observe cross section units with data for one, two, or three time periods. Those units with Ti ¼ 1 drop out, but any other combinations of di¤erences can be used in a pooled OLS analysis. The analogues of Assumption 17.6 for ﬁrst di¤erencing—for example, Assumption 17.6c is replaced with EðDuiDu0 i j xi; si; ciÞ ¼ s2 e IT\u00041—ensure that the usual statistics from pooled OLS on the unbalanced ﬁrst di¤erences are asymptoti- cally valid. 17.7.2 Testing and Correcting for Sample Selection Bias The results in the previous subsection imply that sample selection in a ﬁxed e¤ects context is only a problem when selection is related to the idiosyncratic errors, uit. Therefore, any test for selection bias should test only this assumption. A simple test was suggested by Nijman and Verbeek (1992) in the context of random e¤ects esti- mation, but it works for ﬁxed e¤ects as well: add, say, the lagged selection indicator, si;t\u00041, to the equation, estimate the model by ﬁxed e¤ects (on the unbalanced panel), and do a t test (perhaps making it fully robust) for the signiﬁcance of si;t\u00041. (This method loses the ﬁrst time period for all observations.) Under the null hypothesis, uit is uncorrelated with sir for all r, and so selection in the previous time period should not be signiﬁcant in the equation at time t. (Incidentally, it never makes sense to put sit in the equation at time t because sit ¼ 1 for all i and t in the selected subsample.) Putting si;t\u00041 does not work if si;t\u00041 is unity whenever sit is unity because then there is no variation in si;t\u00041 in the selected sample. This is the case in attrition problems if (say) a person can only appear in period t if he or she appeared in t \u0004 1. An alterna- tive is to include a lead of the selection indicator, si;tþ1. For observations i that are in the sample every time period, si;tþ1 is always zero. But for attriters, si;tþ1 switches from zero to one in the period just before attrition. If we use ﬁxed e¤ects or ﬁrst dif- ferencing, we need T > 2 time periods to carry out the test. For incidental truncation problems it makes sense to extend Heckman’s (1976) test to the unobserved e¤ects panel data context. This is done in Wooldridge (1995a). Write the equation of interest as Sample Selection, Attrition, and Stratiﬁed Sampling 581", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 590, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p591::c0", "text": "yit1 ¼ xit1b1 þ ci1 þ uit1; t ¼ 1; . . . ; T ð17:52Þ Initially, suppose that yit1 is observed only if the binary selection indicator, sit2, is unity. Let xit denote the set of all exogenous variables at time t; we assume that these are observed in every time period, and xit1 is a subset of xit. Suppose that, for each t, sit2 is determined by the probit equation sit2 ¼ 1½xict2 þ vit2 > 0\u0003; vit2 j xi @ Normalð0; 1Þ ð17:53Þ where xi contains unity. This is best viewed as a reduced-form selection equation: we let the explanatory variables in all time periods appear in the selection equation at time t to allow for general selection models, including those with unobserved e¤ect and the Chamberlain (1980) device discussed in Section 15.8.2, as well as certain dynamic models of selection. A Mundlak (1978) approach would replace xi with ðxit; xiÞ at time t and assume that coe‰cients are constant across time. [See equation (15.68).] Then the parameters can be estimated by pooled probit, greatly conserving on degrees of freedom. Such conservation may be important for small N. For testing purposes, under the null hypothesis it does not matter whether equation (17.53) is the proper model of sample selection, but we will need to assume equation (17.53), or a Mundlak version of it, when correcting for sample selection. Under the null hypothesis in Assumption 17.6a (with the obvious notational changes), the inverse Mills ratio obtained from the sample selection probit should not be signiﬁcant in the equation estimated by ﬁxed e¤ects. Thus, let ^lit2 be the estimated Mills ratios from estimating equation (17.53) by pooled probit across i and t. Then a valid test of the null hypothesis is a t statistic on ^lit2 in the FE estimation on the unbalanced panel. Under Assumption 17.6c the usual t statistic is valid, but the ap- proach works whether or not the uit1 are homoskedastic and serially uncorrelated: just compute the robust standard error. Wooldridge (1995a) shows formally that the ﬁrst- stage estimation of c2 does not a¤ect the limiting distribution of the t statistic under H0. This conclusion also follows from the results in Chapter 12 on M-estimation. Correcting for sample selection requires much more care. Unfortunately, under any assumptions that actually allow for an unobserved e¤ect in the underlying selection equation, adding ^lit2 to equation (17.52) and using FE does not produce consistent estimators. To see why, suppose sit2 ¼ 1½xitd2 þ ci2 þ ait2 > 0\u0003; ait2 j ðxi; ci1; ci2Þ @ Normalð0; 1Þ ð17:54Þ Then, to get equation (17.53), vit2 depends on ait2 and, at least partially, on ci2. Now, suppose we make the strong assumption Eðuit1 j xi; ci1; ci2; vi2Þ ¼ gi1 þ r1vit2, which would hold under the assumption that the ðuit1; ait2Þ are independent across t condi- Chapter 17 582", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 591, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p592::c0", "text": "tional on ðxi; ci1; ci2Þ. Then we have yit1 ¼ xit1b1 þ r1Eðvit2 j xi; si2Þ þ ðci1 þ gi1Þ þ eit1 þ r1½vit2 \u0004 Eðvit2 j xi; si2Þ\u0003 The composite error, eit1 þ r1½vit2 \u0004 Eðvit2 j xi; si2Þ\u0003, is uncorrelated with any function of ðxi; si2Þ. The problem is that Eðvit2 j xi; si2Þ depends on all elements in si2, and this expectation is complicated for even small T. A method that does work is available using Chamberlain’s approach to panel data models, but we need some linearity assumptions on the expected values of uit1 and ci1 given xi and vit2. assumption 17.7: (a) The selection equation is given by equation (17.53); (b) Eðuit1 j xi; vit2Þ ¼ Eðuit1 j vit2Þ ¼ rt1vit2, t ¼ 1; . . . ; T; and (c) Eðci1 j xi; vit2Þ ¼ Lðci1 j 1; xi; vit2Þ The second assumption is standard and follows under joint normality of ðuit1; vit2Þ when this vector is independent of xi. Assumption 17.7c implies that Eðci1 j xi; vit2Þ ¼ xip1 þ ft1vit2 where, by equation (17.53) and iterated expectations, Eðci1 j xiÞ ¼ xip1 þ Eðvit2 j xitÞ ¼ xip1. These assumptions place no restrictions on the serial dependence in ðuit1; vit2Þ. They do imply that Eðyit1 j xi; vit2Þ ¼ xit1b1 þ xip1 þ gt1vit2 ð17:55Þ Conditioning on sit2 ¼ 1 gives Eðyit1 j xi; sit2 ¼ 1Þ ¼ xit1b1 þ xip1 þ gt1lðxict2Þ Therefore, we can consistently estimate b1 by ﬁrst estimating a probit of sit2 on xi for each t and then saving the inverse Mills ratio, ^lit2, all i and t. Next, run the pooled OLS regression using the selected sample: yit1 on xit1; xi; ^lit2; d2t^lit2; . . . ; dTt^lit2 for all sit ¼ 1 ð17:56Þ where d2t through dTt are time dummies. If gt1 in equation (17.55) is constant across t, simply include ^lit2 by itself in equation (17.56). The asymptotic variance of ^b1 needs to be corrected for general heteroskedas- ticity and serial correlation, as well as ﬁrst-stage estimation of the ct2. These correc- tions can be made using the formulas for two-step M-estimation from Chapter 12; Wooldridge (1995a) contains the formulas. If the selection equation is of the Tobit form, we have somewhat more ﬂexibility. Write the selection equation now as Sample Selection, Attrition, and Stratiﬁed Sampling 583", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 592, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p593::c0", "text": "yit2 ¼ maxð0; xict2 þ vit2Þ; vit2 j xi @ Normalð0; s2 t2Þ ð17:57Þ where yit1 is observed if yit2 > 0. Then, under Assumption 17.6, with the Tobit selection equation in place of equation (17.53), consistent estimation follows from the pooled regression (17.56) where ^lit2 is replaced by the Tobit residuals, ^vit2 when yit2 > 0 ðsit2 ¼ 1Þ. The Tobit residuals are obtained from the T cross section Tobits in equation (17.57); alternatively, especially with small N, we can use a Mundlak-type approach and use pooled Tobit with xict2 replaced with xitd2 þ xip2; see equation (16.52). It is easy to see that we can add a1yit2 to the structural equation (17.52), provided we make an explicit exclusion restriction in Assumption 17.7. In particular, we must assume that Eðci1 j xi; vit2Þ ¼ xi1p1 þ ft1vit2, and that xit1 is a strict subset of xit. Then, because yit2 is a function of ðxi; vit2Þ, we can write Eðyit1 j xi; vit2Þ ¼ xit1b1 þ a1yit2 þ xi1p1 þ gt1vit2. We obtain the Tobit residuals, ^vit2 for each t, and then run the regres- sion yit1 on xit1; yit2; xi1, and ^vit2 (possibly interacted with time dummies) for the selected sample. If we do not have an exclusion restriction, this regression su¤ers from perfect multicollinearity. As an example, we can easily include hours worked in a wage o¤er function for panel data, provided we have a variable a¤ecting labor supply (such as the number of young children) but not the wage o¤er. A pure ﬁxed e¤ects approach is more fruitful when the selection equation is of the Tobit form. The following assumption comes from Wooldridge (1995a): assumption 17.8: (a) The selection equation is equation (17.57). (b) For some unobserved e¤ect gi1, Eðuit1 j xi; ci1; gi1; vi2Þ ¼ Eðuit1 j gi1; vit2Þ ¼ gi1 þ r1vit2. Under part b of this assumption, Eðyit1 j xi; vi2; ci1; gi1Þ ¼ xit1b1 þ r1vit2 þ fi1 ð17:58Þ where fi1 ¼ ci1 þ gi1. The same expectation holds when we also condition on si2 (since si2 is a function of xi, vi2). Therefore, estimating equation (17.58) by ﬁxed e¤ects on the unbalanced panel would consistently estimate b1 and r1. As usual, we replace vit2 with the Tobit residuals ^vit2 whenever yit2 > 0. A t test of H0: r1 ¼ 0 is valid very generally as a test of the null hypothesis of no sample selection. If the fuit1g satisfy the standard homoskedasticity and serial uncorrelatedness assumptions, then the usual t statistic is valid. A fully robust test may be warranted. (Again, with an exclusion restriction, we can add yit2 as an additional explanatory variable.) Wooldridge (1995a) discusses an important case where Assumption 17.8b holds: in the Tobit version of equation (17.54) with ðui1; ai2Þ independent of ðxi; ci1; ci2Þ and Eðuit1 j ai2Þ ¼ Eðuit1 j ait2Þ ¼ r1ait2. The second-to-last equality holds under the com- mon assumption that fðuit1; ait2Þ: t ¼ 1; . . . ; Tg is serially independent. Chapter 17 584", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 593, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p594::c0", "text": "The preceding methods assume normality of the errors in the selection equation and, implicitly, the unobserved heterogeneity. Kyriazidou (1997) and Honore´ and Kyriazidou (2000b) have proposed methods that do not require distributional assumptions. Dustmann and Rochina-Barrachina (2000) apply Wooldridge’s (1995a) and Kyriazidou’s (1997) methods to the problem of estimating a wage o¤er equation with selection into the work force. 17.7.3 Attrition We now turn speciﬁcally to testing and correcting for attrition in a linear, unobserved e¤ects panel data model. General attrition, where units may reenter the sample after leaving, is complicated. We analyze a common special case. At t ¼ 1 a random sample is obtained from the relevant population—people, for concreteness. In t ¼ 2 and beyond, some people drop out of the sample for reasons that may not be entirely random. We assume that, once a person drops out, he or she is out forever: attrition is an absorbing state. Any panel data set with attrition can be set up in this way by ignoring any subsequent observations on units after they initially leave the sample. In Section 17.7.2 we discussed one way to test for attrition bias when we assume that attrition is an absorbing state: include si;tþ1 as an additional explanatory variable in a ﬁxed e¤ects analysis. One method for correcting for attrition bias is closely related to the corrections for incidental truncation covered in the previous subsection. Write the model for a ran- dom draw from the population as in equation (17.49), where we assume that ðxit; yitÞ is observed for all i when t ¼ 1. Let sit denote the selection indicator for each time period, where sit ¼ 1 if ðxit; yitÞ are observed. Because we ignore units once they ini- tially leave the sample, sit ¼ 1 implies sir ¼ 1 for r < t. The sequential nature of attrition makes ﬁrst di¤erencing a natural choice to re- move the unobserved e¤ect: Dyit ¼ Dxitb þ Duit; t ¼ 2; . . . ; T Conditional on si;t\u00041 ¼ 1, write a (reduced-form) selection equation for t b 2 as sit ¼ 1½witdt þ vit > 0\u0003; vit j fwit; si;t\u00041 ¼ 1g @ Normalð0; 1Þ ð17:59Þ where wit must contain variables observed at time t for all units with si;t\u00041 ¼ 1. Good candidates for wit include the variables in xi;t\u00041 and any variables in xit that are observed at time t when si;t\u00041 ¼ 1 (for example, if xit contains lags of variables or a variable such as age). In general, the dimension of wit can grow with t. For example, if equation (17.49) is dynamically complete, then yi;t\u00042 is orthogonal to Duit, and so it can be an element of wit. Since yi;t\u00041 is correlated with ui;t\u00041, it should not be included in wit. Sample Selection, Attrition, and Stratiﬁed Sampling 585", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 594, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p595::c0", "text": "If the xit are strictly exogenous and selection does not depend on Dxit once wit has been controlled for, a reasonable assumption (say, under joint normality of Duit and vit) is EðDuit j Dxit; wit; vit; si;t\u00041 ¼ 1Þ ¼ EðDuit j vitÞ ¼ rtvit ð17:60Þ Then EðDyit j Dxit; wit; sit ¼ 1Þ ¼ Dxitb þ rtlðwitdtÞ; t ¼ 2; . . . ; T ð17:61Þ Notice how, because si;t\u00041 ¼ 1 when sit ¼ 1, we do not have to condition on si;t\u00041 in equation (17.61). It now follows from equation (17.61) that pooled OLS of Dyit on Dxit; d2t^lit; . . . ; dTt^lit, t ¼ 2; . . . ; T, where the ^lit are from the T \u0004 1 cross section probits in equation (17.59), is consistent for b1 and the rt. A joint test of H0: rt ¼ 0, t ¼ 2; . . . ; T, is a fairly simple test for attrition bias, although nothing guarantees serial independence of the errors. There are two potential problems with this approach. For one, the ﬁrst equality in equation (17.60) is restrictive because it means that xit does not a¤ect attrition once the elements in wit have been controlled for. Second, we have assumed strict exoge- neity of xit. Both these restrictions can be relaxed by using an IV procedure. Let zit be a vector of variables such that zit is redundant in the selection equation (possibly because wit contains zit) and that zit is exogenous in the sense that equation (17.58) holds with zit in place of Dxit; for example, zit should contain xir for r < t. Now, using an argument similar to the cross section case in Section 17.4.2, we can estimate the equation Dyit ¼ Dxitb þ r2 d2t^lit þ \u0002 \u0002 \u0002 þ rT dTt^lit þ errorit ð17:62Þ by instrumental variables with instruments ðzit; d2t^lit; . . . ; dTt^litÞ, using the selected sample. For example, the pooled 2SLS estimator on the selected sample is consistent and asymptotically normal, and attrition bias can be tested by a joint test of H0: rt ¼ 0, t ¼ 2; . . . ; T. Under H0, only serial correlation and heteroskedasticity adjust- ments are possibly needed. If H0 fails we have the usual generated regressors problem for estimating the asymptotic variance. Other IV procedures, such as GMM, can also be used, but they too must account for the generated regressors problem. Example 17.9 (Dynamic Model with Attrition): Consider the model yit ¼ gitg þ h1 yi;t\u00041 þ ci þ uit; t ¼ 1; . . . ; T ð17:63Þ where we assume that ðyi0; gi1; yi1Þ are all observed for a random sample from the population. Assume that Eðuit j gi; yi;t\u00041; . . . ; yi0; ciÞ ¼ 0, so that git is strictly exoge- Chapter 17 586", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 595, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p596::c0", "text": "nous. Then the explanatory variables in the probit at time t, wit, can include gi;t\u00041, yi;t\u00042, and further lags of these. After estimating the selection probit for each t, and di¤erencing, we can estimate Dyit ¼ Dgitb þ h1Dyi;t\u00041 þ r3 d3t^lit þ \u0002 \u0002 \u0002 þ rT dTt^lit þ errorit by pooled 2SLS on the selected sample starting at t ¼ 3, using instruments ðgi;t\u00041; gi;t\u00042; yi;t\u00042; yi;t\u00043Þ. As usual, there are other possibilities for the instruments. Although the focus in this section has been on pure attrition, where units disappear entirely from the sample, the methods can also be used in the context of incidental truncation without strictly exogenous explanatory variables. For example, suppose we are interested in the population of men who are employed at t ¼ 0 and t ¼ 1, and we would like to estimate a dynamic wage equation with an unobserved e¤ect. Problems arise if men become unemployed in future periods. Such events can be treated as an attrition problem if all subsequent time periods are dropped once a man ﬁrst becomes unemployed. This approach loses information but makes the econo- metrics relatively straightforward, especially because, in the preceding general model, xit will always be observed at time t and so can be included in the labor force par- ticipation probit (assuming that men do not leave the sample entirely). Things be- come much more complicated if we are interested in the wage o¤er for all working age men at t ¼ 1 because we have to deal with the sample selection problem into employment at t ¼ 0 and t ¼ 1. The methods for attrition and selection just described apply only to linear models, and it is di‰cult to extend them to general nonlinear models. An alternative ap- proach is based on inverse probability weighting (IPW), which can be applied to general M-estimation, at least under certain assumptions. Mo‰tt, Fitzgerald, and Gottschalk (1999) (MFG) propose inverse probability weighting to estimate linear panel data models under possibly nonrandom attrition. [MFG propose a di¤erent set of weights, analogous to those studied by Horowitz and Manski (1998), to solve missing data problems. The weights we use require estima- tion of only one attrition model, rather than two as in MFG.] IPW must be used with care to solve the attrition problem. As before, we assume that we have a random sample from the population at t ¼ 1. We are interested in some feature, such as the conditional mean, or maybe the entire conditional distribution, of yit given xit. Ide- ally, at each t we would observe ðyit; xitÞ for any unit that was in the random sample at t ¼ 1. Instead, we observe ðyit; xitÞ only if sit ¼ 1. We can easily solve the attrition problem if we assume that, conditional on observables in the ﬁrst time period, say, zi1, ðyit; xitÞ is independent of sit: Sample Selection, Attrition, and Stratiﬁed Sampling 587", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 596, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p597::c0", "text": "Pðsit ¼ 1 j yit; xit; zi1Þ ¼ Pðsit ¼ 1 j zi1Þ; t ¼ 2; . . . ; T ð17:64Þ Assumption (17.64) has been called selection on observables because we assume that zi1 is a strong enough predictor of selection in each time period so that the dis- tribution of sit given ½zi1; ðyit; xitÞ\u0003 does not depend on ðyit; xitÞ. In the statistics liter- ature, selection on observables is also called ignorability of selection conditional on zi1. [The more standard approach, where selection is given by equation (17.59) and Duit is correlated with vit, is sometimes called selection on unobservables. These cate- gorizations are not strictly correct, as selection in both cases depends on observables and unobservables, but they serve as useful shorthand.] Inverse probability weighting involves two steps. First, for each t, we estimate a probit or logit of sit on zi1. (A crucial point is that the same cross section units— namely, all units appearing in the ﬁrst time period—are used in the probit or logit for each time period.) Let ^pit be the ﬁtted probabilities, t ¼ 2; . . . ; T, i ¼ 1; . . . ; N. In the second step, the objective function for ði; tÞ is weighted by 1=^pit. For general M- estimation, the objective function is X N i¼1 X T t¼1 ðsit=^pitÞqtðwit; yÞ ð17:65Þ where wit 1 ðyit; xitÞ and qtðwit; yÞ is the objective function in each time period. As usual, the selection indicator sit chooses the observations where we actually observe data. (For t ¼ 1, sit ¼ ^pit ¼ 1 for all i.) For least squares, qtðwit; yÞ is simply the squared residual function; for partial MLE, qtðwit; yÞ is the log-likelihood function. The argument for why IPW works is rather simple. Let yo denote the value of y that solves the population problem miny A Y PT t¼1 E½qtðwit; yÞ\u0003. Let do t denote the true values of the selection response parameters in each time period, so that Pðsit ¼ 1 j zi1Þ ¼ ptðzi1; do t Þ 1 po it. Now, under standard regularity conditions, we can replace po it with ^pit 1 ptðzi1; ^dtÞ without a¤ecting the consistency argument. So, apart from reg- ularity conditions, it is su‰cient to show that yo minimizes PT t¼1 E½ðsit=po itÞqtðwit; yÞ\u0003 over Y. But, from iterated expectations, E½ðsit=po itÞqtðwit; yÞ\u0003 ¼ EfE½ðsit=po itÞqtðwit; yÞ j wit; zi1\u0003g ¼ Ef½Eðsit j wit; zi1Þ=po it\u0003qtðwit; yÞg ¼ E½qtðwit; yÞ\u0003 because Eðsit j wit; zi1Þ ¼ Pðsit ¼ 1 j zi1Þ by assumption (17.64). Therefore, the proba- bility limit of the weighted objective function is identical to that of the unweighted function if we had no attrition problem. Using this simple analogy argument, Wool- dridge (2000d) shows that the inverse probability weighting produces a consistent, Chapter 17 588", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 597, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p598::c0", "text": "ﬃﬃﬃﬃ N p -asymptotically normal estimator. The methods for adjusting the asymptotic variance matrix of two step M-estimators—described in Subsection 12.5.2—can be applied to the IPW M-estimator from (17.65). For reasons we will see, a sequential method of estimating attrition probabilities can be more attractive. MFG propose an IPW scheme where the conditioning variables in the attri- tion probits change across time. In particular, at time t an attrition probit is estimated restricting attention to those units still in the sample at time t \u0004 1. (Out of this group, some are lost to attrition at time t, and some are not.) If we assume that attrition is an absorbing state, we can include in the conditioning variables, zit, all values of y and x dated at time t \u0004 1 and earlier (as well as other variables observed for all units in the sample at t \u0004 1). This approach is appealing because the ignorability assumption is much more plausible if we can condition on both recent responses and covariates. [That is, Pðsit ¼ 1 j wit; wi;t\u00041; . . . ; wi1; si;t\u00041 ¼ 1Þ ¼ Pðsit ¼ 1 j wi;t\u00041; . . . ; wi1; si;t\u00041 ¼ 1Þ is more likely than assumption (17.64).] Unfortunately, obtaining the ﬁtted probabilities in this way and using them in an IPW procedure does not gener- ally produce consistent estimators. The problem is that the selection models at each time period are not representative of the population that was originally sampled at t ¼ 1. Letting po it ¼ Pðsit ¼ 1 j wi;t\u00041; . . . ; wi1; si;t\u00041 ¼ 1Þ, we can no longer use the iterated expectations argument to conclude that E½ðsit=po itÞqtðwit; yÞ\u0003 ¼ E½qtðwit; yÞ\u0003. Only if E½qtðwit; yÞ\u0003 ¼ E½qtðwit; yÞ j si;t\u00041 ¼ 1\u0003 for all y does the argument work, but this assumption essentially requires that wit be independent of si;t\u00041. It is possible to allow the covariates in the selection probabilities to increase in richness over time, but the MFG procedure must be modiﬁed. For the case where attrition is an absorbing state, Wooldridge (2000d), building on work for regression models by Robins, Rotnitzky, and Zhao (1995) (RRZ), shows that the following probabilities can be used in the IPW procedure: pitðdo t Þ 1 pi2ðgo 2Þpi3ðgo 3Þ \u0002 \u0002 \u0002 pitðgo t Þ; t ¼ 2; . . . ; T ð17:66Þ where pitðgo t Þ 1 Pðsit ¼ 1 j zit; si;t\u00041 ¼ 1Þ ð17:67Þ In other words, as in the MFG procedure, we estimate probit models at each time t, restricted to units that are in the sample at t \u0004 1. The covariates in the probit are essentially everything we can observe for units in the sample at time t \u0004 1 that might a¤ect attrition. For t ¼ 2; . . . ; T, let ^pit denote the ﬁtted selection probabilities. Then we construct the probability weights as the product ^pit 1 ^pi2^pi3 \u0002 \u0002 \u0002 ^pit and use the objective function (17.65). Naturally, this method only works under certain assump- tions. The key ignorability condition can be stated as Sample Selection, Attrition, and Stratiﬁed Sampling 589", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 598, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p599::c0", "text": "Pðsit ¼ 1 j vi1; . . . ; viT; si;t\u00041 ¼ 1Þ ¼ Pðsit ¼ 1 j zit; si;t\u00041 ¼ 1Þ ð17:68Þ where vit 1 ðwit; zitÞ. Now, we must include future values of wit and zit in the con- ditioning set on the left-hand side. Assumption (17.68) is fairly strong, but it does allow for attrition to be strongly related to past outcomes on y and x (which can be included in zit). A convenient feature of the sequential method described above is that ignoring the ﬁrst-stage estimation of the probabilities actually leads to conservative inference con- cerning y0: the (correct) asymptotic variance that adjusts for the ﬁrst-stage estimation is actually smaller than the one that does not. See Wooldridge (2000d) for the general case and RRZ (1995) for the nonlinear regression case. See Wooldridge (2000d) for more on the pros and cons of using inverse probability weighting to reduce attrition bias. 17.8 Stratiﬁed Sampling Nonrandom samples also come in the form of stratiﬁed samples, where di¤erent subsets of the population are sampled with di¤erent frequencies. For example, certain surveys are designed to learn primarily about a particular subset of the population, in which case that group is usually overrepresented in the sample. Stratiﬁcation can be based on exogenous variables or endogenous variables (which are known once a model and assumptions have been speciﬁed), or some combination of these. As in the case of sample selection problems, it is important to know which is the case. As mentioned in Section 17.3, choice-based sampling occurs when the stratiﬁcation is based entirely on a discrete response variable. Various methods have been proposed for estimating discrete response models from choice-based samples under di¤erent assumptions; most of these are variations of maximum likelihood. Manski and McFadden (1981) and Cosslett (1993) contain general treatments, with the latter being a very useful survey. For a class of discrete response models, Cosslett (1981) proposed an e‰cient estimator, and Imbens (1992) obtained a computationally simple method of moments estimator that also achieves the e‰ciency bound. Imbens and Lancaster (1996) allow for general response variables in a maximum likelihood setting. Here, we focus on a simple, albeit often ine‰cient, method for estimating models in the context of two kinds of stratiﬁed sampling. 17.8.1 Standard Stratiﬁed Sampling and Variable Probability Sampling The two most common kinds of stratiﬁcation used in obtaining data sets in the social sciences are standard stratiﬁed sampling (SS sampling) and variable probability sam- Chapter 17 590", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 599, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p600::c0", "text": "pling (VP sampling). In SS sampling, the population is ﬁrst partitioned into J groups, W1; W2; . . . ; WJ, which we assume are nonoverlapping and exhaustive. We let w denote the random variable representing the population of interest. standard stratified sampling: For j ¼ 1; . . . ; J, draw a random sample of size Nj from stratum j. For each j, denote this random sample by fwij: i ¼ 1; 2; . . . ; Njg. The strata sample sizes Nj are nonrandom. Therefore, the total sample size, N ¼ N1 þ \u0002 \u0002 \u0002 þ NJ, is also nonrandom. A randomly drawn observation from stratum j; wij, has distribution Dðw j w A WjÞ. Therefore, while observations within a stratum are identically distributed, observations across strata are not. A scheme that is similar in nature to SS sampling is called multinomial sampling, where a stratum is ﬁrst picked at random and then an observation is randomly drawn from the stratum. This does result in i.i.d. observations, but it does not correspond to how stratiﬁed samples are obtained in practice. It also leads to the same estimators as under SS sampling, so we do not discuss it further; see Cosslett (1993) or Wooldridge (1999b) for further discussion. Variable probability samples are obtained using a di¤erent scheme. First, an obser- vation is drawn at random from the population. If the observation falls into stratum j, it is kept with probability pj. Thus, random draws from the population are discarded with varying frequencies depending on which stratum they fall into. This kind of sampling is appropriate when information on the variable or variables that determine the strata is relatively easy to obtain compared with the rest of the information. Survey data sets, including initial interviews to collect panel or longitudinal data, are good examples. Suppose we want to oversample individuals from, say, lower income classes. We can ﬁrst ask an individual her or his income. If the response is in income class j, this person is kept in the sample with probability pj, and then the remaining information, such as education, work history, family background, and so on can be collected; otherwise, the person is dropped without further interviewing. A key feature of VP sampling is that observations within a stratum are discarded randomly. As discussed by Wooldridge (1999b), VP sampling is equivalent to the following: variable probability sampling: Repeat the following steps N times: 1. Draw an observation wi at random from the population. 2. If wi is in stratum j, toss a (biased) coin with probability pj of turning up heads. Let hij ¼ 1 if the coin turns up heads and zero otherwise. 3. Keep observation i if hij ¼ 1; otherwise, omit it from the sample. Sample Selection, Attrition, and Stratiﬁed Sampling 591", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 600, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p601::c0", "text": "The number of observations falling into stratum j is denoted Nj, and the number of data points we actually have for estimation is N0 ¼ N1 þ N2 þ \u0002 \u0002 \u0002 þ NJ. Notice that if N—the number of times the population is sampled—is ﬁxed, then N0 is a random variable: we do not know what each Nj will be prior to sampling. Also, we will not use information on the number of discarded observations in each stratum, so that N is not required to be known. The assumption that the probability of the coin turning up heads in step 2 depends only on the stratum ensures that sampling is random within each stratum. This roughly reﬂects how samples are obtained for certain large cross-sectional and panel data sets used in economics, including the panel study of income dynamics and the national longitudinal survey. To see that a VP sample can be analyzed as a random sample, we construct a population that incorporates the stratiﬁcation. The VP sampling scheme is equivalent to ﬁrst tossing all J coins before actually observing which stratum wi falls into; this gives ðhi1; . . . ; hiJÞ. Next, wi is observed to fall into one of the strata. Finally, the outcome is kept or not depending on the coin ﬂip for that stratum. The result is that the vector ðwi; hiÞ, where hi is the J-vector of binary indicators hij, is a random sam- ple from a new population with sample space W \u0001 H, where W is the original sample space and H denotes the sample space associated with outcomes from ﬂip- ping J coins. Under this alternative way of viewing the sampling scheme, hi is inde- pendent of wi. Treating ðwi; hiÞ as a random draw from the new population is not at odds with the fact that our estimators are based on a nonrandom sample from the original population: we simply use the vector hi to determine which observations are kept in the estimation procedure. 17.8.2 Weighted Estimators to Account for Stratiﬁcation With variable probability sampling, it is easy to construct weighted objective func- tions that produce consistent and asymptotically normal estimators of the population parameters. It is useful to deﬁne a set of binary variables that indicate whether a random draw wi is kept in the sample and, if so, which stratum it falls into: rij ¼ hijsij ð17:69Þ By deﬁnition, rij ¼ 1 for at most one j. If hij ¼ 1 then rij ¼ sij. If rij ¼ 0 for all j ¼ 1; 2; . . . ; J, then the random draw wi does not appear in the sample (and we do not know which stratum it belonged to). With these deﬁnitions, we can deﬁne the weighted M-estimator, ^yw, as the solution to min y A Y X N i¼1 X J j¼1 p\u00041 j rijqðwi; yÞ ð17:70Þ Chapter 17 592", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 601, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p602::c0", "text": "where qðw; yÞ is the objective function that is chosen to identify the population parameters yo. Note how the outer summation is over all potential observations, that is, the observations that would appear in a random sample. The indicators rij simply pick out the observations that actually appear in the available sample, and these indicators also attach each observed data point to its stratum. The objective function (17.70) weights each observed data point in the sample by the inverse of the sampling probability. For implementation it is useful to write the objective function as min y A Y X N0 i¼1 p\u00041 ji qðwi; yÞ ð17:71Þ where, without loss of generality, the data points actually observed are ordered i ¼ 1; . . . ; N0. Since ji is the stratum for observation i, p\u00041 ji is the weight attached to ob- servation i in the estimation. In practice, the p\u00041 ji are the sampling weights reported with other variables in stratiﬁed samples. The objective function qðw; yÞ contains all of the M-estimator examples we have covered so far in the book, including least squares (linear and nonlinear), conditional maximum likelihood, and partial maximum likelihood. In panel data applications, the probability weights are from sampling in an initial year. Weights for later years are intended to reﬂect both stratiﬁcation (if any) and possible attrition, as discussed in Section 17.7.3 and in Wooldridge (2000d). Wooldridge (1999b) shows that, under the same assumptions as Theorem 12.2 and the assumption that each sampling probability is strictly positive, the weighted M- estimator consistently estimates yo, which is assumed to uniquely minimize E½qðw; yÞ\u0003. To see that the weighted objective function identiﬁes yo, we use the fact that hj is in- dependent of w [and therefore of ðw; sjÞ for each j], and so E X J j¼1 p\u00041 j hjsjqðw; yÞ \" # ¼ X J j¼1 p\u00041 j EðhjÞE½sjqðw; yÞ\u0003 ¼ X J j¼1 p\u00041 j pjE½sjqðw; yÞ\u0003 ¼ E X J j¼1 sj ! qðw; yÞ \" # ¼ E½qðw; yÞ\u0003 ð17:72Þ where the ﬁnal equality follows because the sj sum to unity. Therefore, the expected value of the weighted objective function [over the distribution of ðw; hÞ] equals the expected value of qðw; yÞ (over the distribution of w). Consistency of the weighted M- estimator follows under the regularity conditions in Theorem 12.2. Asymptotic normality also follows under the same regularity conditions as in Chapter 12. Wooldridge (1999b) shows that a valid estimator of the asymptotic Sample Selection, Attrition, and Stratiﬁed Sampling 593", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 602, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p603::c0", "text": "variance of ^yw is X N0 i¼1 p\u00041 ji ‘2 y qið^ywÞ \" #\u00041 X N0 i¼1 p\u00042 ji ‘yqið^ywÞ0‘yqið^ywÞ \" # X N0 i¼1 p\u00041 ji ‘2 y qið^ywÞ \" #\u00041 ð17:73Þ which looks like the standard formula for a robust variance matrix estimator except for the presence of the sampling probabilities pji. When w partitions as ðx; yÞ, an alternative estimator replaces the Hessian ‘2 y qið^ywÞ in expression (17.73) with Aðxi; ^ywÞ, where Aðxi; yoÞ 1 E½‘2 y qðwi; yoÞ j xi\u0003, as in Chapter 12. Asymptotic standard errors and Wald statistics can be obtained using either estimate of the asymptotic variance. Example 17.10 (Linear Model under Stratiﬁed Sampling): In estimating the linear model y ¼ xbo þ u; Eðx0uÞ ¼ 0 ð17:74Þ by weighted least squares, the asymptotic variance matrix estimator is X N0 i¼1 p\u00041 ji x0 ixi !\u00041 X N0 i¼1 p\u00042 ji ^u2 i x0 ixi ! X N0 i¼1 p\u00041 ji x0 ixi !\u00041 ð17:75Þ where ^ui ¼ yi \u0004 xi ^bw is the residual after WLS estimation. Interestingly, this is simply the White (1980b) heteroskedasticity-consistent covariance matrix estimator applied to the stratiﬁed sample, where all variables for observation i are weighted by p\u00041=2 ji before performing the regression. This estimator has been suggested by, among others, Hausman and Wise (1981). Hausman and Wise use maximum likelihood to obtain more e‰cient estimators in the context of the normal linear regression model, that is, u j x @ Normalðxbo; s2 oÞ. Because of stratiﬁcation, MLE is not generally robust to failure of the homoskedastic normality assumption. It is important to remember that the form of expression (17.75) in this example is not due to potential heteroskedasticity in the underlying population model. Even if Eðu2 j xÞ ¼ s2 o, the estimator (17.75) is generally needed because of the stratiﬁed sampling. This estimator also works in the presence of heteroskedasticity of arbitrary and unknown form in the population, and it is routinely computed by many regres- sion packages. Example 17.11 (Conditional MLE under Stratiﬁed Sampling): When f ðy j x; yÞ is a correctly speciﬁed model for the density of yi given xi in the population, the inverse- probability-weighted MLE is obtained with qiðyÞ 1 \u0004log½ f ðyi j xi; yÞ\u0003. This estimator Chapter 17 594", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 603, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p604::c0", "text": "is consistent and asymptotically normal, with asymptotic variance estimator given by expression (17.73) [or, preferably, the form that uses Aðxi; ^ywÞ\u0003. A weighting scheme is also available in the standard stratiﬁed sampling case, but the weights are di¤erent from the VP sampling case. To derive them, let Qj ¼ Pðw A WjÞ denote the population frequency for stratum j; we assume that the Qj are known. By the law of iterated expectations, E½qðw; yÞ\u0003 ¼ Q1E½qðw; yÞ j w A W1\u0003 þ \u0002 \u0002 \u0002 þ QJE½qðw; yÞ j w A WJ\u0003 ð17:76Þ for any y. For each j, E½qðw; yÞ j w A Wj\u0003 can be consistently estimated using a ran- dom sample obtained from stratum j. This scheme leads to the sample objective function Q1 N\u00041 1 X N1 i¼1 qðwi1; yÞ \" # þ \u0002 \u0002 \u0002 þ QJ N\u00041 J X NJ i¼1 qðwiJ; yÞ \" # where wij denotes a random draw i from stratum j and Nj is the nonrandom sample size for stratum j. We can apply the uniform law of large numbers to each term, so that the sum converges uniformly to equation (17.76) under the regularity conditions in Chapter 12. By multiplying and dividing each term by the total number of obser- vations N ¼ N1 þ \u0002 \u0002 \u0002 þ NJ, we can write the sample objective function more simply as N\u00041 X N i¼1 ðQji=HjiÞqðwi; yÞ ð17:77Þ where ji denotes the stratum for observation i and Hj 1 Nj=N denotes the fraction of observations in stratum j. Because we have the stratum indicator ji, we can drop the j subscript on wi. When we omit the division by N, equation (17.77) has the same form as equation (17.71), but the weights are ðQji=HjiÞ rather than p\u00041 ji (and the arguments for why each weighting works are very di¤erent). Also, in general, the formula for the asymptotic variance is di¤erent in the SS sampling case. In addition to the minor notational change of replacing N0 with N, the middle matrix in equation (17.73) becomes X J j¼1 ðQ2 j =H 2 j Þ X Nj i¼1 ð‘y^qij \u0004 ‘yqjÞ0ð‘y^qij \u0004 ‘yqjÞ \" # where ‘y^qij 1 ‘yqðwij; ^ywÞ and ‘yqj 1 N\u00041 j PNj i¼1 ‘y^qij (the within-stratum sample average). This approach requires us to explicitly partition observations into their respective strata. See Wooldridge (2001) for a detailed derivation. [If in the VP Sample Selection, Attrition, and Stratiﬁed Sampling 595", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 604, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p605::c0", "text": "sampling case the population frequencies Qj are known, it is better to use as weights Qj=ðNj=N0Þ rather than p\u00041 j , which makes the analysis look just like the SS sampling case. See Wooldridge (1999b) for details.] If in Example 17.11 we have standard stratiﬁed sampling rather than VP sam- pling, the weighted MLE is typically called the weighted exogenous sample MLE (WESMLE); this estimator was suggested by Manski and Lerman (1977) in the context of choice-based sampling in discrete response models. [Actually, Manski and Lerman (1977) use multinomial sampling where Hj is the probability of picking stratum j. But Cosslett (1981) showed that a more e‰cient estimator is obtained by using Nj=N, as one always does in the case of SS sampling; see Wooldridge (1999b) for an extension of Cosslett’s result to the M-estimator case.] Provided that the sampling weights Qji=Hji or p\u00041 ji are given (along with the stra- tum), analysis with the weighted M-estimator under SS or VP sampling is fairly straightforward, but it is not likely to be e‰cient. In the conditional maximum like- lihood case it is certainly possible to do better. See Imbens and Lancaster (1996) for a careful treatment. 17.8.3 Stratiﬁcation Based on Exogenous Variables When w partitions as ðx; yÞ, where x is exogenous in a sense to be made precise, and stratiﬁcation is based entirely on x, the standard unweighted estimator on the strati- ﬁed sample is consistent and asymptotically normal. The sense in which x must be exogenous is that yo solves min y A Y E½qðw; yÞ j x\u0003 ð17:78Þ for each possible outcome x. This assumption holds in a variety of contexts with conditioning variables and correctly speciﬁed models. For example, as we discussed in Chapter 12, this holds for nonlinear regression when the conditional mean is cor- rectly speciﬁed and yo is the vector of conditional mean parameters; in Chapter 13 we showed that this holds for conditional maximum likelihood when the density of y given x is correct. It also holds in other cases, including quasi-maximum likelihood, which we cover in Chapter 19. One interesting observation is that, in the linear re- gression model (17.74), the exogeneity of x must be strengthened to Eðu j xÞ ¼ 0. In the case of VP sampling, selection on the basis of x means that each selection indicator sj is a deterministic function of x. The unweighted M-estimator on the stratiﬁed sample, ^yu, minimizes X N i¼1 X J j¼1 hijsijqðwi; yÞ ¼ X N0 i¼1 qðwi; yÞ Chapter 17 596", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 605, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p606::c0", "text": "Consistency follows from standard M-estimation results if we can show that yo uniquely solves min y A y X J j¼1 E½hjsjqðw; yÞ\u0003 ð17:79Þ Since sj is a function of x and hj is independent of w (and therefore x), E½hjsjqðw; yÞ j x\u0003 ¼ Eðhj j xÞsjE½qðw; yÞ j x\u0003 ¼ pjsjE½qðw; yÞ j x\u0003 for each j. By assump- tion, E½qðw; yÞ j x\u0003 is minimized at yo for all x, and therefore so is pjsjE½qðw; yÞ j x\u0003 (but probably not uniquely). By iterated expectations it follows that yo is a solution to equation (17.79). Unlike in the case of the weighted estimator, it no longer su‰ces to assume that yo uniquely minimizes E½qðw; yÞ\u0003; we must directly assume yo is the unique solution to problem (17.79). This assumption could fail if, for example, pj ¼ 0 for some j—so that we do not observe part of the population at all. (Unlike in the case of the weighted estimator, pj ¼ 0 for at least some j is allowed for the unweighted estimator, subject to identiﬁcation holding.) For example, in the context of linear wage regression, we could not identify the return to education if we only sample those with exactly a high school education. Wooldridge (1999b) shows that the usual asymptotic variance estimators (see Sec- tion 12.5) are valid when stratiﬁcation is based on x and we ignore the stratiﬁcation problem. For example, the usual conditional maximum likelihood analysis holds. In the case of regression, we can use the usual heteroskedasticity-robust variance matrix estimator. Or, if we assume homoskedasticity in the population, the nonrobust form [see equation (12.58)] is valid with the usual estimator of the error variance. When a generalized conditional information matrix equality holds, and stratiﬁca- tion is based on x, Wooldridge (1999b) shows that the unweighted estimator is more e‰cient than the weighted estimator. The key assumption is E½‘yqðw; yoÞ0‘yqðw; yoÞ j x\u0003 ¼ s2 oE½‘2 y qðw; yoÞ j x\u0003 ð17:80Þ for some s2 o > 0. When assumption (17.80) holds and yo solves equation (17.79), the asymptotic variance of the unweighted M-estimator is smaller than that for the weighted M-estimator. This generalization includes conditional maximum likelihood (with s2 o ¼ 1) and nonlinear regression under homoskedasticity. Very similar conclusions hold for standard stratiﬁed sampling. One useful fact is that, when stratiﬁcation is based on x, the estimator (17.73) is valid with pj ¼ Hj=Qj (and N0 ¼ N); therefore, we need not compute within-strata variation in the esti- mated score. The unweighted estimator is consistent when stratiﬁcation is based on x and the usual asymptotic variance matrix estimators are valid. The unweighted Sample Selection, Attrition, and Stratiﬁed Sampling 597", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 606, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p607::c0", "text": "estimator is also more e‰cient when assumption (17.80) holds. See Wooldridge (2001) for statements of assumptions and proofs of theorems. Problems 17.1. a. Suppose you are hired to explain ﬁre damage to buildings in terms of building and neighborhood characteristics. If you use cross section data on reported ﬁres, is there a sample selection problem due to the fact that most buildings do not catch ﬁre during the year? b. If you want to estimate the relationship between contributions to a 401(k) plan and the match rate of the plan—the rate at which the employer matches employee contributions—is there a sample selection problem if you only use a sample of workers already enrolled in a 401(k) plan? 17.2. In Example 17.4, suppose that IQ is an indicator of abil, and KWW is another indicator (see Section 5.3.2). Find assumptions under which IV on the selected sam- ple is valid. 17.3. Let f ð\u0002 j xi; yÞ denote the density of yi given xi for a random draw from the population. Find the conditional density of yi given ðxi; si ¼ 1Þ when the selection rule is si ¼ 1½a1ðxiÞ < yi < a2ðxiÞ\u0003, where a1ðxÞ and a2ðxÞ are known functions of x. In the Hausman and Wise (1977) example, a2ðxÞ was a function of family size be- cause the poverty income level depends on family size. 17.4. Suppose in Section 17.4.1 we replace Assumption 17.1d with Eðu1 j v2Þ ¼ g1v2 þ g2ðv2 2 \u0004 1Þ (We subtract unity from v2 2 to ensure that the second term has zero expectation.) a. Using the fact that Varðv2 j v2 > \u0004aÞ ¼ 1 \u0004 lðaÞ½lðaÞ þ a\u0003, show that Eðy1 j x; y2 ¼ 1Þ ¼ x1b1 þ g1lðxd2Þ \u0004 g2lðxd2Þxd2 [Hint: Take a ¼ xd2 and use the fact that Eðv2 2 j v2 > \u0004aÞ ¼ Varðv2 j v2 > \u0004aÞ þ ½Eðv2 j v2 > \u0004aÞ\u00032.] b. Explain how to correct for sample selection in this case. c. How would you test for the presence of sample selection bias? 17.5. Consider the following alternative to Procedure 17.2. First, run the OLS re- gression of y2 on z and obtain the ﬁtted values, ^y2. Next, get the inverse Mills ratio, Chapter 17 598", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 607, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p608::c0", "text": "^l3, from the probit of y3 on z. Finally, run the OLS regression y1 on z1; ^y2; ^l3 using the selected sample. a. Find a set of su‰cient conditions that imply consistency of the proposed proce- dure. (Do not worry about regularity conditions.) b. Show that the assumptions from part a are more restrictive than those in Proce- dure 17.2, and give some examples that are covered by Procedure 17.2 but not by the alternative procedure. 17.6. Apply Procedure 17.4 to the data in MROZ.RAW. Use a constant, exper, and exper2 as elements of z1; take y2 ¼ educ. The other elements of z should include age, kidslt6, kidsge6, nwifeinc, motheduc, fatheduc, and huseduc. 17.7. Consider the model y1 ¼ zd1 þ v1 y2 ¼ zd2 þ v2 y3 ¼ maxð0; a31y1 þ a32y2 þ z3d3 þ u3Þ where ðz; y2; y3Þ are always observed and y1 is observed when y3 > 0. The ﬁrst two equations are reduced-form equations, and the third equation is of primary interest. For example, take y1 ¼ logðwageoÞ, y2 ¼ educ, and y3 ¼ hours, and then education and logðwageoÞ are possibly endogenous in the labor supply function. Assume that ðv1; v2; u3Þ are jointly zero-mean normal and independent of z. a. Find a simple way to consistently estimate the parameters in the third equation allowing for arbitrary correlations among ðv1; v2; u3Þ. Be sure to state any identiﬁca- tion assumptions needed. b. Now suppose that y2 is observed only when y3 > 0; for example, y1 ¼ logðwageoÞ, y2 ¼ logðbenefitsoÞ, y3 ¼ hours. Now derive a multistep procedure for estimating the third equation under the same assumptions as in part a. c. How can we estimate the average partial e¤ects? 17.8. Consider the following conditional moment restrictions problem with a selected sample. In the population, E½rðw; yoÞ j x\u0003 ¼ 0. Let s be the selection indicator, and assume that E½rðw; yoÞ j x; s\u0003 ¼ 0 Su‰cient is that s ¼ f ðxÞ for a nonrandom function f . a. Let Zi be a G \u0001 L matrix of functions of xi. Show that yo satisﬁes Sample Selection, Attrition, and Stratiﬁed Sampling 599", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 608, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p609::c0", "text": "E½siZ0 irðwi; yoÞ\u0003 ¼ 0 b. Write down the objective function for the system nonlinear 2SLS estimator based on the selected sample. Argue that, under the appropriate rank condition, the esti- mator is consistent and ﬃﬃﬃﬃ N p -asymptotically normal. c. Write down the objective function for a minimum chi-square estimator using the selected sample. Use the estimates from part b to estimate the weighting matrix. Argue that the estimator is consistent and ﬃﬃﬃﬃ N p -asymptotically normal. 17.9. Consider the problem of standard stratiﬁed sampling. Argue that when yo solves equation (17.78) for each x, yo is identiﬁed in the population, stratiﬁcation is based on x, and Hj > 0 for j ¼ 1; . . . ; J, the unweighted estimator is consistent. fHint: Write the objective function for the unweighted estimator as X J j¼1 Hj N\u00041 j X Nj i¼1 qðwij; yÞ \" # ð17:81Þ and assume that Hj ! Hj > 0 as N ! y. If the strata are X1; X2; . . . ; XJ, argue that equation (17.81) converges uniformly to H1E½qðw; yÞ j x A X1\u0003 þ \u0002 \u0002 \u0002 þ HJE½qðw; yÞ j x A XJ\u0003 ð17:82Þ Why does yo necessarily minimize expression (17.82)? Identiﬁcation follows when you show that E½qðw; yÞ j x A Xj\u0003 is uniquely minimized at yo for at least one j.g 17.10. Consider model (17.25), where selection is ignorable in the sense that Eðu1 j z; u3Þ ¼ 0. However, data are missing on y2 when y3 ¼ 0, and Eðy2 j z; y3Þ 0 Eðy2 j zÞ. a. Find Eðy1 j z; y3Þ. b. If, in addition to Assumption 17.2, ðv2; v3Þ is independent of z and Eðv2 j v3Þ ¼ g2v3, ﬁnd Eðy1 j z; y3 ¼ 1Þ. c. Suggest a two-step method for consistently estimating d1 and a1. d. Does this method generally work if Eðu1 j z; y3Þ 0 0? e. Would you bother with the method from part c if Eðu1 j z; y2; y3Þ ¼ 0? Explain. 17.11. In Section 16.7 we discussed two-part models for a corner solution out- come, say, y. These models have sometimes been studied in the context of incidental truncation. Chapter 17 600", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 609, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p610::c0", "text": "a. Suppose you have a parametric model for the distribution of y conditional on x and y > 0. (Cragg’s model and the lognormal model from Section 16.7 are exam- ples.) If you estimate the parameters of this model by conditional MLE, using only the observations for which yi > 0, do the parameter estimates su¤er from sample selection bias? Explain. b. If instead you specify only Eðy j x; y > 0Þ ¼ expðxbÞ and estimate b by nonlinear least squares using observations for which yi > 0, do the estimates su¤er from sample selection bias? c. In addition to the speciﬁcation from part b, suppose that Pðy ¼ 0 j xÞ ¼ 1 \u0004 FðxgÞ. How would you estimate g? d. Given the assumptions in parts b and c, how would you estimate Eðy j xÞ? e. Given your answers to the ﬁrst four parts, do you think viewing estimation of two- part models as an incidental truncation problem is appropriate? 17.12. Consider Theorem 17.1. Suppose that we relax assumption (17.6) to Eðu j z; sÞ ¼ Eðu j sÞ ¼ ð1 \u0004 sÞa0 þ sa1. The ﬁrst equality is the assumption; the second is unrestrictive, as it simply allows the mean of u to di¤er in the selected and un- selected subpopulations. a. Show that 2SLS estimation using the selected subsample consistently estimates the slope parameters, b2; . . . ; bK. What is the plim of the intercept estimator? [Hint: Re- place u with ð1 \u0004 sÞa0 þ sa1 þ e, where Eðe j z; sÞ ¼ 0.] b. Show that Eðu j z; sÞ ¼ Eðu j sÞ if ðu; sÞ is independent of z. Does independence of s and z seem reasonable? 17.13. Suppose that y given x follows a standard censored Tobit, where y is a cor- ner solution response. However, there is at least one element of x that we can observe only when y > 0. (An example is seen when y is quantity demanded of a good or service, and one element of x is price, derived as total expenditure on the good div- ided by y whenever y > 0.) a. Explain why we cannot use standard censored Tobit maximum likelihood esti- mation to estimate b and s2. What method can we use instead? b. How is it that we can still estimate Eðy j xÞ, even though we do not observe some elements of x when y ¼ 0? Sample Selection, Attrition, and Stratiﬁed Sampling 601", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 610, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p611::c0", "text": "18 Estimating Average Treatment E¤ects 18.1 Introduction In this chapter we explicitly study the problem of estimating an average treatment e¤ect (ATE). An average treatment e¤ect is a special case of an average partial e¤ect: an ATE is an average partial e¤ect for a binary explanatory variable. Estimating ATEs has become important in the program evaluation literature, such as in the evaluation of job training programs. Originally, the binary indicators repre- sented medical treatment or program participation, but the methods are applicable when the explanatory variable of interest is any binary variable. We begin by introducing a counterfactual framework pioneered by Rubin (1974) and since adopted by many in both statistics and econometrics, including Rosen- baum and Rubin (1983), Heckman (1992, 1997), Imbens and Angrist (1994), Angrist, Imbens, and Rubin (1996), Manski (1996), Heckman, Ichimura, and Todd (1997), and Angrist (1998). The counterfactual framework allows us to deﬁne various treatment e¤ects that may be of interest. Once we deﬁne the di¤erent treatment e¤ects, we can study ways to consistently estimate these e¤ects. We will not provide a comprehensive treatment of this rapidly growing literature, but we will show that, under certain as- sumptions, estimators that we are already familiar with consistently estimate average treatment e¤ects. We will also study some extensions that consistently estimate ATEs under weaker assumptions. Broadly, most estimators of ATEs ﬁt into one of two categories. The ﬁrst set exploits assumptions concerning ignorability of the treatment conditional on a set of covariates. As we will see in Section 18.3, this approach is analogous to the proxy variable solution to the omitted variables problem that we discussed in Chapter 4, and in some cases reduces exactly to an OLS regression with many controls. A sec- ond set of estimators relies on the availability of one or more instrumental variables that are redundant in the response equations but help determine participation. Dif- ferent IV estimators are available depending on functional form assumptions con- cerning how unobserved heterogeneity a¤ects the responses. We study IV estimators in Section 18.4. In Section 18.5 we brieﬂy discuss some further topics, including special consid- erations for binary and corner solution responses, using panel data to estimate treat- ment e¤ects, and nonbinary treatments. 18.2 A Counterfactual Setting and the Self-Selection Problem The modern literature on treatment e¤ects begins with a counterfactual, where each individual (or other agent) has an outcome with and without treatment (where", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 611, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p612::c0", "text": "‘‘treatment’’ is interpreted very broadly). This section draws heavily on Heckman (1992, 1997), Imbens and Angrist (1994), and Angrist, Imbens, and Rubin (1996) (hereafter AIR). Let y1 denote the outcome with treatment and y0 the outcome with- out treatment. Because an individual cannot be in both states, we cannot observe both y0 and y1; in e¤ect, the problem we face is one of missing data. It is important to see that we have made no assumptions about the distributions of y0 and y1. In many cases these may be roughly continuously distributed (such as salary), but often y0 and y1 are binary outcomes (such as a welfare participation in- dicator), or even corner solution outcomes (such as married women’s labor supply). However, some of the assumptions we make will be less plausible for discontinuous random variables, something we discuss after introducing the assumptions. The following discussion assumes that we have an independent, identically distri- buted sample from the population. This assumption rules out cases where the treat- ment of one unit a¤ects another’s outcome (possibly through general equilibrium e¤ects, as in Heckman, Lochner, and Taber, 1998). The assumption that treatment of unit i a¤ects only the outcome of unit i is called the stable unit treatment value assumption (SUTVA) in the treatment literature (see, for example, AIR). We are making a stronger assumption because random sampling implies SUTVA. Let the variable w be a binary treatment indicator, where w ¼ 1 denotes treatment and w ¼ 0 otherwise. The triple ðy0; y1; wÞ represents a random vector from the underlying population of interest. For a random draw i from the population, we write ðyi0; yi1; wiÞ. However, as we have throughout, we state assumptions in terms of the population. To measure the e¤ect of treatment, we are interested in the di¤erence in the out- comes with and without treatment, y1 \u0001 y0. Because this is a random variable (that is, it is individual speciﬁc), we must be clear about what feature of its distribution we want to estimate. Several possibilities have been suggested in the literature. In Rosenbaum and Rubin (1983), the quantity of interest is the average treatment e¤ect (ATE), ATE 1 Eðy1 \u0001 y0Þ ð18:1Þ ATE is the expected e¤ect of treatment on a randomly drawn person from the pop- ulation. Some have criticized this measure as not being especially relevant for policy purposes: because it averages across the entire population, it includes in the average units who would never be eligible for treatment. Heckman (1997) gives the example of a job training program, where we would not want to include millionaires in com- puting the average e¤ect of a job training program. This criticism is somewhat mis- leading, as we can—and would—exclude people from the population who would never be eligible. For example, in evaluating a job training program, we might re- Chapter 18 604", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 612, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p613::c0", "text": "strict attention to people whose pretraining income is below a certain threshold; wealthy people would be excluded precisely because we have no interest in how job training a¤ects the wealthy. In evaluating the beneﬁts of a program such as Head Start, we could restrict the population to those who are actually eligible for the pro- gram or are likely to be eligible in the future. In evaluating the e¤ectiveness of en- terprise zones, we could restrict our analysis to block groups whose unemployment rates are above a certain threshold or whose per capita incomes are below a certain level. A second quantity of interest, and one that has received much recent attention, is the average treatment e¤ect on the treated, which we denote ATE1: ATE1 1 Eðy1 \u0001 y0 j w ¼ 1Þ ð18:2Þ That is, ATE1 is the mean e¤ect for those who actually participated in the program. As we will see, in some special cases equations (18.1) and (18.2) are equivalent, but generally they di¤er. Imbens and Angrist (1994) deﬁne another treatment e¤ect, which they call a local average treatment e¤ect (LATE). LATE has the advantage of being estimable using instrumental variables under very weak conditions. It has two potential drawbacks: (1) it measures the e¤ect of treatment on a generally unidentiﬁable subpopulation; and (2) the deﬁnition of LATE depends on the particular instrumental variable that we have available. We will discuss LATE in the simplest setting in Section 18.4.2. We can expand the deﬁnition of both treatment e¤ects by conditioning on covari- ates. If x is an observed covariate, the ATE conditional on x is simply Eðy1 \u0001 y0 j xÞ; similarly, equation (18.2) becomes Eðy1 \u0001 y0 j x; w ¼ 1Þ. By choosing x appropri- ately, we can deﬁne ATEs for various subsets of the population. For example, x can be pretraining income or a binary variable indicating poverty status, race, or gender. For the most part, we will focus on ATE and ATE1 without conditioning on covariates. As noted previously, the di‰culty in estimating equation (18.1) or (18.2) is that we observe only y0 or y1, not both, for each person. More precisely, along with w, the observed outcome is y ¼ ð1 \u0001 wÞy0 þ wy1 ¼ y0 þ wðy1 \u0001 y0Þ ð18:3Þ Therefore, the question is, How can we estimate equation (18.1) or (18.2) with a random sample on y and w (and usually some observed covariates)? First, suppose that the treatment indicator w is statistically independent of ðy0; y1Þ, as would occur when treatment is randomized across agents. One implication of independence between treatment status and the potential outcomes is that ATE and ATE1 are identical: Eðy1 \u0001 y0 j w ¼ 1Þ ¼ Eðy1 \u0001 y0Þ. Furthermore, estimation of Estimating Average Treatment E¤ects 605", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 613, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p614::c0", "text": "ATE is simple. Using equation (18.3), we have Eðy j w ¼ 1Þ ¼ Eðy1 j w ¼ 1Þ ¼ Eðy1Þ where the last equality follows because y1 and w are independent. Similarly, Eðy j w ¼ 0Þ ¼ Eðy0 j w ¼ 0Þ ¼ Eðy0Þ It follows that ATE ¼ ATE1 ¼ Eðy j w ¼ 1Þ \u0001 Eðy j w ¼ 0Þ ð18:4Þ The right-hand side is easily estimated by a di¤erence in sample means: the sample average of y for the treated units minus the sample average of y for the untreated units. Thus, randomized treatment guarantees that the di¤erence-in-means estimator from basic statistics is unbiased, consistent, and asymptotically normal. In fact, these properties are preserved under the weaker assumption of mean independence: Eðy0 j wÞ ¼ Eðy0Þ and Eðy1 j wÞ ¼ Eðy1Þ. Randomization of treatment is often infeasible in program evaluation (although randomization of eligibility often is feasible; more on this topic later). In most cases, individuals at least partly determine whether they receive treatment, and their deci- sions may be related to the beneﬁts of treatment, y1 \u0001 y0. In other words, there is self-selection into treatment. It turns out that ATE1 can be consistently estimated as a di¤erence in means under the weaker assumption that w is independent of y0, without placing any restriction on the relationship between w and y1. To see this point, note that we can always write Eðy j w ¼ 1Þ \u0001 Eðy j w ¼ 0Þ ¼ Eðy0 j w ¼ 1Þ \u0001 Eðy0 j w ¼ 0Þ þ Eðy1 \u0001 y0 j w ¼ 1Þ ¼ ½Eðy0 j w ¼ 1Þ \u0001 Eðy0 j w ¼ 0Þ\u0002 þ ATE1 ð18:5Þ If y0 is mean independent of w, that is, Eðy0 j wÞ ¼ Eðy0Þ ð18:6Þ then the ﬁrst term in equation (18.5) disappears, and so the di¤erence in means esti- mator is an unbiased estimator of ATE1. Unfortunately, condition (18.6) is a strong assumption. For example, suppose that people are randomly made eligible for a voluntary job training program. Condition (18.6) e¤ectively implies that the partici- pation decision is unrelated to what people would earn in the absence of the program. A useful expression relating ATE1 and ATE is obtained by writing y0 ¼ m0 þ v0 and y1 ¼ m1 þ v1, where mg ¼ EðygÞ, g ¼ 0; 1. Then y1 \u0001 y0 ¼ ðm1 \u0001 m0Þ þ ðv1 \u0001 v0Þ ¼ ATE þ ðv1 \u0001 v0Þ Chapter 18 606", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 614, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p615::c0", "text": "Taking the expectation of this equation conditional on w ¼ 1 gives ATE1 ¼ ATE þ Eðv1 \u0001 v0 j w ¼ 1Þ We can think of v1 \u0001 v0 as the person-speciﬁc gain from participation, and so ATE1 di¤ers from ATE by the expected person-speciﬁc gain for those who participated. If y1 \u0001 y0 is not mean independent of w, ATE1 and ATE generally di¤er. Fortunately, we can estimate ATE and ATE1 under assumptions less restrictive than independence of ðy0; y1Þ and w. In most cases, we can collect data on individ- ual characteristics and relevant pretreatment outcomes—sometimes a substantial amount of data. If, in an appropriate sense, treatment depends on the observables and not on the unobservables determining ðy0; y1Þ, then we can estimate average treatment e¤ects quite generally, as we show in the next section. 18.3 Methods Assuming Ignorability of Treatment We adopt the framework of the previous section, and, in addition, we let x denote a vector of observed covariates. Therefore, the population is described by ðy0; y1; w; xÞ, and we observe y, w, and x, where y is given by equation (18.3). When w and ðy0; y1Þ are allowed to be correlated, we need an assumption in order to identify treatment e¤ects. Rosenbaum and Rubin (1983) introduced the following assumption, which they called ignorability of treatment (given observed covariates x): assumption ATE.1: Conditional on x, w and ðy0; y1Þ are independent. For many purposes, it su‰ces to assume ignorability in a conditional mean indepen- dence sense: assumption ATE.10: (a) Eðy0 j x; wÞ ¼ Eðy0 j xÞ; and (b) Eðy1 j x; wÞ ¼ Eðy1 j xÞ. Naturally, Assumption ATE.1 implies Assumption ATE.10. In practice, Assumption ATE.10 might not a¤ord much generality, although it does allow Varðy0 j x; wÞ and Varðy1 j x; wÞ to depend on w. The idea underlying Assumption ATE.10 is this: if we can observe enough information (contained in x) that determines treatment, then ðy0; y1Þ might be mean independent of w, conditional on x. Loosely, even though ðy0; y1Þ and w might be correlated, they are uncorrelated once we partial out x. Assumption ATE.1 certainly holds if w is a deterministic function of x, which has prompted some authors in econometrics to call assumptions like ATE.1 selection on observables; see, for example, Barnow, Cain, and Goldberger (1980, 1981), Heckman and Robb (1985), and Mo‰tt (1996). (We discussed a similar assumption in Section Estimating Average Treatment E¤ects 607", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 615, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p616::c0", "text": "17.7.3 in the context of attrition in panel data.) The name is ﬁne as a label, but we must realize that Assumption ATE.1 does allow w to depend on unobservables, albeit in a restricted fashion. If w ¼ gðx; aÞ, where a is an unobservable random variable independent of ðx; y0; y1Þ, then Assumption ATE.1 holds. But a cannot be arbitrarily correlated with y0 and y1. An important fact is that, under Assumption ATE.10, the average treatment e¤ect conditional on x and the average treatment e¤ect of the treated, conditional on x, are identical: ATE1ðxÞ 1 Eðy1 \u0001 y0 j x; w ¼ 1Þ ¼ Eðy1 \u0001 y0 j xÞ ¼ ATEðxÞ because Eðyg j x; wÞ ¼ Eðyg j xÞ, g ¼ 0; 1. However, the unconditional versions of the treatment e¤ects are not generally equal. For clarity, deﬁne rðxÞ ¼ Eðy1 \u0001 y0 j xÞ ¼ ATEðxÞ. Then ATE is the expected value of rðxÞ across the entire population, whereas ATE1 is the expected value of rðxÞ in the treated subpopulation. Mathe- matically, ATE ¼ E½rðxÞ\u0002 and ATE1 ¼ E½rðxÞ j w ¼ 1\u0002 If we can estimate rð\u0003Þ, then ATE can be estimated by averaging across the entire random sample from the population, whereas ATE1 would be estimated by averaging across the part of the sample with wi ¼ 1. We will discuss speciﬁc estimation strat- egies in the next subsection. An interesting feature of Assumptions ATE.1 and ATE.10—and one that is per- haps foreign to economists—is that they are stated without imposing any kind of model on joint or conditional distributions. It turns out that no more structure is needed in order to identify either of the treatment e¤ects. We ﬁrst show how the ignorability assumption relates to standard regression analysis. 18.3.1 Regression Methods We can use equation (18.3), along with Assumption ATE.10, to obtain estimators of ATEðxÞ, which can then be used to estimate ATE and ATE1. First, Eðy j x; wÞ ¼ Eðy0 j x; wÞ þ w½Eðy1 j x; wÞ \u0001 Eðy0 j x; wÞ\u0002 ¼ Eðy0 j xÞ þ w½Eðy1 j xÞ \u0001 Eðy0 j xÞ\u0002 where the ﬁrst equality follows from equation (18.3) and the second follows from Assumption ATE.10. Therefore, under Assumption ATE.10, Eðy j x; w ¼ 1Þ \u0001 Eðy j x; w ¼ 0Þ ¼ Eðy1 j xÞ \u0001 Eðy0 j xÞ ¼ ATEðxÞ ð18:7Þ Chapter 18 608", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 616, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p617::c0", "text": "Because we have a random sample on ðy; w; xÞ from the relevant population, r1ðxÞ 1 Eðy j x; w ¼ 1Þ and r0ðxÞ 1 Eðy j x; w ¼ 0Þ are nonparametrically identiﬁed. That is, these are conditional expectations that depend entirely on observables, and so they can be consistently estimated quite generally. (See Ha¨rdle and Linton, 1994, for as- sumptions and methods.) For the purposes of identiﬁcation, we can just assume r1ðxÞ and r0ðxÞ are known, and the fact that they are known means that ATEðxÞ is identi- ﬁed. If ^r1ðxÞ and ^r0ðxÞ are consistent estimators (in an appropriate sense), using the random sample of size N, a consistent estimator of ATE under fairly weak assump- tions is A ^TE ¼ N\u00011 X N i¼1 ½^r1ðxiÞ \u0001 ^r0ðxiÞ\u0002 while a consistent estimator of ATE1 is A ^TE1 ¼ X N i¼1 wi !\u00011 X N i¼1 wi½^r1ðxiÞ \u0001 ^r0ðxiÞ\u0002 ( ) The formula for A ^TE1 simply averages ½^r1ðxiÞ \u0001 ^r0ðxiÞ\u0002 over the subsample with wi ¼ 1. There are several implementation issues that arise in computing and using A ^TE and A ^TE1. The most obvious of these is obtaining ^r1ð\u0003Þ and ^r0ð\u0003Þ. To be as ﬂexible as possible, we could use nonparametric estimators, such as a kernel estimator (see Ha¨rdle and Linton, 1994). Obtaining reliable standard errors when we use non- parametric estimates can be di‰cult. An alternative is to use ﬂexible parametric models, such as low-order polynomials that include interaction terms. [Presumably, we would also account for the nature of y in estimating Eðy j x; w ¼ 1Þ and Eðy j x; w ¼ 0Þ. For example, if y is binary, we would use a ﬂexible logit or probit; if y is a corner solution, we might use a ﬂexible Tobit or a ﬂexible exponential regression function.] With plenty of data, a third possibility is to list all possible values that x can take, say, c1; c2; . . . ; cM, and to estimate Eðy j x ¼ cm; w ¼ 1Þ by averaging the yi over all i with xi ¼ cm and wi ¼ 1; Eðy j x ¼ cm; w ¼ 0Þ is estimated similarly. For each m and w ¼ 0 or 1, this method is just estimation of a mean using a sample average. Typi- cally, M is large because x takes on many values, and many of the cells may have only a small number of observations. Regardless of how ^r1ð\u0003Þ and ^r0ð\u0003Þ are obtained, to use the estimated treatment e¤ects we need to obtain asymptotically valid standard errors. Generally, this task can be very di‰cult, especially if nonparametric methods are used in estimation. Estimating Average Treatment E¤ects 609", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 617, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p618::c0", "text": "Nevertheless, we will show how a linear regression model involving level e¤ects and interactions can be used to obtain good estimates of the treatment e¤ects as well as reliable standard errors. Before we turn to standard regression models, we need to discuss a problem that can arise in the evaluation of programs, especially when ﬂexible estimation of Eðy j x; w ¼ 1Þ and Eðy j x; w ¼ 0Þ is desirable. To illustrate the problem, suppose there is only one binary covariate, x, and Assumption ATE.10 holds; for concrete- ness, x could be an indicator for whether pretraining earnings are below a certain threshold. Suppose that everyone in the relevant population with x ¼ 1 participates in the program. Then, while we can estimate Eðy j x ¼ 1; w ¼ 1Þ with a random sample from the population, we cannot estimate Eðy j x ¼ 1; w ¼ 0Þ because we have no data on the subpopulation with x ¼ 1 and w ¼ 0. Intuitively, we only observe the coun- terfactual y1 when x ¼ 1; we never observe y0 for any members of the population with x ¼ 1. Therefore, ATEðxÞ is not identiﬁed at x ¼ 1. If some people with x ¼ 0 participate while others do not, we can estimate Eðy j x ¼ 0; w ¼ 1Þ \u0001 Eðy j x ¼ 0; w ¼ 0Þ using a simple di¤erence in averages over the group with x ¼ 0, and so ATEðxÞ is identiﬁed at x ¼ 0. But if we cannot estimate ATEð1Þ, we cannot estimate the unconditional ATE because ATE ¼ Pðx ¼ 0Þ \u0003 ATEð0Þ þ Pðx ¼ 1Þ \u0003 ATEð1Þ. In e¤ect, we can only estimate the ATE over the sub- population with x ¼ 0, which means that we must redeﬁne the population of interest. This limitation is unfortunate: presumably we would be very interested in the pro- gram’s e¤ects on the group that always participates. A similar conclusion holds if the group with x ¼ 0 never participates in the pro- gram. Then ATEð0Þ is not estimable because Eðy j x ¼ 0; w ¼ 1Þ is not estimable. If some people with x ¼ 1 participated while others did not, ATEð1Þ would be identi- ﬁed, and then we would view the population of interest as the subgroup with x ¼ 1. There is one important di¤erence between this situation and the one where the x ¼ 1 group always receives treatment: it seems perfectly natural to exclude from the pop- ulation people who have no chance of treatment based on observed covariates. This observation is related to the issue we discussed in Section 18.2 concerning the rele- vant population for deﬁning ATE. If, for example, people with very high preprogram earnings ðx ¼ 0Þ have no chance of participating in a job training program, then we would not want to average together ATEð0Þ and ATEð1Þ; ATEð1Þ by itself is much more interesting. Although the previous example is extreme, its consequences can arise in more plausible settings. Suppose that x is a vector of binary indicators for pretraining in- come intervals. For most of the intervals, the probability of participating is strictly between zero and one. If the participation probability is zero at the highest income Chapter 18 610", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 618, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p619::c0", "text": "level, we simply exclude the high-income group from the relevant population. Unfortunately, if participation is certain at low income levels, we must exclude low- income groups as well. As a practical matter, we often determine whether the probability of participation is one or zero by looking at the random sample. If we list the possible values of the explanatory variables, c1; . . . ; cM, as described earlier, the problem arises when there is a value, say cm, where all units with xi ¼ cm participate in the program. Because we cannot estimate Eðy j x ¼ cm; w ¼ 0Þ, the subpopulation with x ¼ cm must be ex- cluded from the analysis. We now turn to standard parametric regression methods for estimating ATE, and then brieﬂy discuss estimating ATE1. It is useful to decompose the counterfactual outcomes into their means and a stochastic part with zero mean, as we did at the end of Section 18.2: y0 ¼ m0 þ v0; Eðv0Þ ¼ 0 ð18:8Þ y1 ¼ m1 þ v1; Eðv1Þ ¼ 0 ð18:9Þ Plugging these into equation (18.3) gives y ¼ m0 þ ðm1 \u0001 m0Þw þ v0 þ wðv1 \u0001 v0Þ ð18:10Þ This is a simple example of a switching regression model, where the outcome equa- tions depend on the regime (treatment status in this case). If we assume that v1 \u0001 v0 has zero mean conditional on x, we obtain a standard regression model under Assumption ATE.10. proposition 18.1: Under Assumption ATE.10, assume, in addition, that Eðv1 j xÞ ¼ Eðv0 j xÞ ð18:11Þ Then ATE1 ¼ ATE, and Eðy j w; xÞ ¼ m0 þ aw þ g0ðxÞ ð18:12Þ where a 1 ATE and g0ðxÞ ¼ Eðv0 j xÞ. If, in addition, Eðv0 j xÞ ¼ h0 þ h0ðxÞb0 for some vector function h0ðxÞ, then Eðy j w; xÞ ¼ g0 þ aw þ h0ðxÞb0 ð18:13Þ where g0 ¼ m0 þ h0. Proof: Under Assumption ATE.10, Eðy1 j w; xÞ ¼ m1 þ Eðv1 j xÞ and Eðy0 j w; xÞ ¼ m0 þ Eðv0 j xÞ. Under assumption (18.11), Eðy1 j w; xÞ \u0001 Eðy0 j w; xÞ ¼ m1 \u0001 m0. Estimating Average Treatment E¤ects 611", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 619, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p620::c0", "text": "Therefore, by iterated expectations, Eðy1 j wÞ \u0001 Eðy0 j wÞ ¼ m1 \u0001 m0, which implies that ATE1 ¼ ATE. The proof of equation (18.12) follows by taking the expectation of equation (18.10) given w, x and using Assumption ATE.10 and assumption (18.11). This proposition shows that when the predicted person-speciﬁc gain given x is zero—that is, when Eðv1 \u0001 v0 j xÞ ¼ 0—Eðy j w; xÞ is additive in w and a function of x, and the coe‰cient on w is the average treatment e¤ect. It follows that standard regression methods can be used to estimate ATE. While nonlinear regression methods can be used if Eðv0 j xÞ is assumed to be nonlinear in parameters, typically we would use an assumption such as equation (18.13). Then, regressing y on an intercept, w, and h0ðxÞ consistently estimates the ATE. By putting enough controls in x, we have arranged it so that w and unobservables a¤ecting ðy0; y1Þ are appropriately unrelated. In e¤ect, x proxies for the unobservables. Using ﬂexible functional forms for the elements of h0ðxÞ should provide a good approximation to Eðv0 j xÞ. The function h0ðxÞb0 in equation (18.13) is an example of a control function: when added to the regression of y on 1, w, it controls for possible self-selection bias. Of course, this statement is only true under the assumptions in Proposition 18.1. Given Assumption ATE.10, the additively separable form of equation (18.12) hinges crucially on assumption (18.11). Though assumption (18.11) might be rea- sonable in some cases, it need not generally hold. [A su‰cient, but not necessary, condition for assumption (18.11) is v1 ¼ v0 or y1 ¼ a þ y0, which means the e¤ect of treatment is the same for everyone in the population.] If we relax assumption (18.11), then we no longer have equality of ATE and ATE1. Nevertheless, a regression for- mulation can be used to estimate ATE: proposition 18.2: Under Assumption ATE.10, Eðy j w; xÞ ¼ m0 þ aw þ g0ðxÞ þ w½g1ðxÞ \u0001 g0ðxÞ\u0002 ð18:14Þ where a ¼ ATE, g0ðxÞ 1 Eðv0 j xÞ, and g1ðxÞ 1 Eðv1 j xÞ. The proof of Proposition 18.2 is immediate by taking the expectation of equation (18.10) given ðw; xÞ. Equation (18.14) is interesting because it shows that, under As- sumption ATE.10 only, Eðy j w; xÞ is additive in w, a function of x, and an interaction between w and another function of x. The coe‰cient on w is the average treatment e¤ect (but not generally ATE1). To operationalize equation (18.14) in a parametric framework, we would replace g0ð\u0003Þ and g1ð\u0003Þ with parametric functions of x; typi- cally, these would be linear in parameters, say h0 þ h0ðxÞb0 and h1 þ h1ðxÞb1. For notational simplicity, assume that these are both linear in x. Then we can write Chapter 18 612", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 620, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p621::c0", "text": "Eðy j w; xÞ ¼ g þ aw þ xb0 þ w \u0003 ðx \u0001 cÞd ð18:15Þ where b0 and d are vectors of unknown parameters and c 1 EðxÞ. Subtracting the mean from x ensures that ATE is the coe‰cient on w. In practice, either we would subtract o¤ the known population mean from each element of x, or, more likely, we would demean each element of x using the sample average. Therefore, under equa- tion (18.15), we would estimate a as the coe‰cient on w in the regression yi on 1; wi; xi; wiðxi \u0001 xÞ; i ¼ 1; 2; . . . ; N ð18:16Þ where x is the vector of sample averages. (Subtracting the sample averages rather than population averages introduces a generated regressor problem. However, as argued in Problem 6.10, the adjustments to the standard errors typically have minor e¤ects.) The control functions in this case involve not just the xi, but also interactions of the covariates with the treatment variable. If desired, we can be selective about which elements of ðxi \u0001 xÞ we interact with wi. Adding functions of x, such as squares or logarithms, as both level terms and interactions, is simple, provided we demean any functions before constructing the interactions. Because regression (18.16) consistently estimates d, we can also study how the ATE given x, that is, ATEðxÞ ¼ Eðy1 \u0001 y0 j xÞ, changes with elements of x. In particular, for any x in the valid range, A ^TEðxÞ ¼ ^a þ ðx \u0001 xÞ^d We can then average this equation over interesting values of x to obtain the ATE for a subset of the population. For example, if x contains pretraining earnings or indi- cators for earnings groups, we can estimate how the ATE changes for various levels of pretraining earnings. If the functions of x appearing in the regression are very ﬂexible, problems with estimating ATEðxÞ at certain values of x can arise. In the extreme case, we deﬁne dummy variables for each possible outcome on x and use these in place of x. This approach results in what is known as a saturated model. We will not be able to in- clude dummy variables for groups that are always treated or never treated, with the result that our estimator of ATE is for the population that excludes these groups. To estimate ATE1, write ATE1 ¼ a þ ½Eðx j w ¼ 1Þ \u0001 c\u0002d, and so a consistent es- timator is A ^TE1 ¼ ^a þ X N i¼1 wi !\u00011 X N i¼1 wiðxi \u0001 xÞ^d \" # Estimating Average Treatment E¤ects 613", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 621, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p622::c0", "text": "Obtaining a standard error for this estimator is somewhat complicated, but it can be done using the delta method or bootstrapping. Example 18.1 (E¤ects of Enterprise Zones on Economic Development): Consider evaluating the e¤ects of enterprise zone (EZ) designation on employment growth, for block groups in a particular state. Suppose that we have 1980 and 1990 census data, and that the EZ designation originated in the early 1980s. To account for the fact that zone designation is likely to depend on prior economic performance, and per- haps other block characteristics, we can estimate a model such as the following: gemp ¼ m0 þ aez þ b1 logðemp80Þ þ b2 logðpop80Þ þ b3 percmanf 80 þ b4 logðhousval80Þ þ b5ez \u0003 ½logðemp80Þ \u0001 m1\u0002 þ b6ez \u0003 ½logðpop80Þ \u0001 m2\u0002 þ b7ez \u0003 ½ percmanf 80 \u0001 m3\u0002 þ b8ez \u0003 ½logðhousval80Þ \u0001 m4\u0002 þ error where the right-hand-side variables are a dummy variable for EZ designation, em- ployment, population, percent of employment in manufacturing, and median housing value, all in 1980, and where the mj are the sample averages. The regression estimator (18.16), especially with ﬂexible functions of the covari- ates, applies directly to what are called regression discontinuity designs. In this case, treatment is determined as a nonstochastic function of a covariate, say w ¼ f ðsÞ, where s is an element of x that has su‰cient variation. The key is that f is a discon- tinuous function of s, typically a step function, w ¼ 1½s a s0\u0002, where s0 is a known threshold. The idea is that once s, which could be income level or class size, reaches a certain threshold, a policy automatically kicks in. (See, for example, Angrist and Lavy, 1999.) Because s is a nonrandom function of x, the conditional independence assumption in Assumption ATE.1 must hold. The key is obtaining ﬂexible functional forms for g0ð\u0003Þ and g1ð\u0003Þ. Generally, we can identify a only if we are willing to assume that g0ð\u0003Þ and g1ð\u0003Þ are smooth functions of x (which is almost always the case when we estimate parametric or nonparametric regression functions). If we allow g0ð\u0003Þ to be discontinuous in s—that is, with jumps—we could never distinguish between changes in y due to a change in s or a change in treatment status. 18.3.2 Methods Based on the Propensity Score Rosenbaum and Rubin (1983) use the ignorability-of-treatment assumption di¤er- ently in estimating ATE. Regression (18.16) makes functional form assumptions about Eðv0 j xÞ and Eðv1 j xÞ, where v0 and v1 are unobserved. Alternatively, it turns out that ATE and ATE1 can both be estimated by modeling Chapter 18 614", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 622, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p623::c0", "text": "pðxÞ 1 Pðw ¼ 1 j xÞ ð18:17Þ which is the probability of treatment given the covariates. The function pðxÞ, which is simply the response probability for treatment, is called the propensity score in the evaluation literature. Interestingly, ATE and ATE1 can be written in terms of the propensity score. proposition 18.3: Under Assumption ATE.10, assume in addition that 0 < pðxÞ < 1; all x ð18:18Þ Then ATE ¼ Eð½w \u0001 pðxÞ\u0002y=fpðxÞ½1 \u0001 pðxÞ\u0002gÞ ð18:19Þ and ATE1 ¼ Ef½w \u0001 pðxÞ\u0002y=½1 \u0001 pðxÞ\u0002g=Pðw ¼ 1Þ ð18:20Þ Proof: Plugging equation (18.3) into the numerator inside the expectation in equa- tion (18.19) gives ½w \u0001 pðxÞ\u0002y ¼ ½w \u0001 pðxÞ\u0002½ð1 \u0001 wÞy0 þ wy1\u0002 ¼ wy1 \u0001 pðxÞð1 \u0001 wÞy0 \u0001 pðxÞwy1 Taking the expectation of this equation conditional on ðw; xÞ and using Assumption ATE.10 gives wm1ðxÞ \u0001 pðxÞð1 \u0001 wÞm0ðxÞ \u0001 pðxÞwm1ðxÞ where mjðxÞ 1 Eðyj j xÞ, j ¼ 0; 1. Taking the expectation conditional on x gives pðxÞm1ðxÞ \u0001 pðxÞ½1 \u0001 pðxÞ\u0002m0ðxÞ \u0001 ½ pðxÞ\u00022m1ðxÞ ¼ pðxÞ½1 \u0001 pðxÞ\u0002½m1ðxÞ \u0001 m0ðxÞ\u0002 because pðxÞ ¼ Eðw j xÞ. Therefore, the expected value of the term in equation (18.19) conditional on x is simply ½m1ðxÞ \u0001 m0ðxÞ\u0002; iterated expectations implies that the right-hand side of equation (18.19) is m1 \u0001 m0. Very similar reasoning shows that Ef½w \u0001 pðxÞ\u0002y=½1 \u0001 pðxÞ\u0002 j xg ¼ pðxÞ½m1ðxÞ \u0001 m0ðxÞ\u0002 Next, by iterated expectations, EfpðxÞ½m1ðxÞ \u0001 m0ðxÞ\u0002g ¼ Efw½m1ðxÞ \u0001 m0ðxÞ\u0002g ¼ E½wðy1 \u0001 y0Þ\u0002 where the last equality follows from Assumption ATE.10. But Estimating Average Treatment E¤ects 615", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 623, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p624::c0", "text": "E½wðy1 \u0001 y0Þ\u0002 ¼ Pðw ¼ 1ÞE½wðy1 \u0001 y0Þ j w ¼ 1\u0002 þ Pðw ¼ 0ÞE½wðy1 \u0001 y0Þ j w ¼ 0\u0002 ¼ Pðw ¼ 1ÞEðy1 \u0001 y0 j w ¼ 1Þ Therefore, the right-hand side of equation (18.20) is fPðw ¼ 1ÞEðy1 \u0001 y0 j w ¼ 1Þg=Pðw ¼ 1Þ ¼ ATE1: Rosenbaum and Rubin (1983) call Assumption ATE.1 plus condition (18.18) strong ignorability of treatment (given covariates x). Proposition 18.3 shows, in a di¤erent way from Section 18.3.1, that ATE and ATE1 are nonparametrically identiﬁed under strong ignorability of treatment: the response probability, Pðy ¼ 1 j xÞ, can be assumed known for the purposes of identiﬁcation analysis. Wooldridge (1999c) obtained equation (18.19) in the more general setting of a random coe‰cient model (see Sec- tion 18.5.3), while equation (18.20) is essentially due to Dehejia and Wahba (1999, Proposition 4), who make the stronger assumption ATE.1. Condition (18.18) is precisely the restriction on the response probability that arose in Section 18.3.1 for identifying ATE. Equation (18.20) shows that ATE1 is still identiﬁed if pðxÞ ¼ 0 for some x, but this ﬁnding has little practical value because we probably want to exclude units that have no chance of being treated, anyway. Im- portantly, in estimating ATE or ATE1, we rule out pðxÞ ¼ 1: we cannot estimate ATE or ATE1 by including in the population units that are treated with certainty, conditional on x. Of course, to estimate ATE and ATE1, we need an estimator of pð\u0003Þ. Rosenbaum and Rubin (1983) suggest using a ﬂexible logit model, where x and various functions of x—for example, quadratics and interactions—are included. [In this case there is no danger of ^pðxÞ ¼ 0 or 1 because logit ﬁtted values are strictly in the unit interval, but this functional form restriction might simply mask the problem in the popula- tion.] The propensity score can also be estimated using fully nonparametric methods— see, for example, Powell (1994) and Heckman, Ichimura, and Todd (1997). Here, we focus on ﬂexible parametric methods. If ^pðxÞ 1 Fðx; ^gÞ is such an estimator, where ^g is obtained in a ﬁrst-stage binary response estimation of w on x, then a consistent estimator of ATE is A ^TE ¼ N\u00011 X N i¼1 ½wi \u0001 ^pðxiÞ\u0002yi=f^pðxiÞ½1 \u0001 ^pðxiÞ\u0002g ð18:21Þ Interestingly, after simple algebra this estimator can be shown to be identical to an estimator due to Horvitz and Thompson (1952) for handling nonrandom sampling. Consistency under standard regularity conditions follows from Lemma 12.1. Simi- Chapter 18 616", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 624, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p625::c0", "text": "larly, a consistent estimator of ATE1 is A ^TE1 ¼ N\u00011 X N i¼1 wi !\u00011 N\u00011 X N i¼1 ½wi \u0001 ^piðxiÞ\u0002yi=½1 \u0001 ^pðxiÞ\u0002 ( ) ð18:22Þ Notice that N\u00011 PN i¼1 wi is a consistent estimator of Pðw ¼ 1Þ. Obtaining valid asymptotic standard errors using the delta method is somewhat complicated, as we need a ﬁrst-order representation for ﬃﬃﬃﬃ N p ð^g \u0001 gÞ—see Section 12.5.2. Notice that only the predicted probabilities appear in equations (18.21) and (18.22). Therefore, di¤er- ent methods of estimating pðxÞ that lead to similar predicted values ^pðxiÞ will tend to produce similar treatment e¤ect estimates. It turns out that the estimators in equations (18.21) and (18.22) not only are con- venient, but also can be made to have the smallest asymptotic variances among esti- mators that are based only on Assumption ATE.1 and condition (18.18) (as well as several regularity conditions). Hirano, Imbens, and Ridder (2000) (HIR) have recently shown that (18.21) and (18.22) achieve the semiparametric e‰ciency bound obtained by Hahn (1998). In order to achieve the bound, HIR assume that ^pð\u0003Þ is a series estimator, so that the conditions in Newey (1994) can be veriﬁed. As a practical matter, series estimation is not ideal, because, for a binary response, it is identical to a linear probability model in functions of x. Plus, it is di‰cult to estimate the asymp- totic variance of the resulting estimators, A ^TE and A ^TE1. Probably little is lost by using a ﬂexible logit or probit and then obtaining the standard errors by the usual delta method. A simple, popular estimator in program evaluation is obtained from an OLS re- gression that simply includes the estimated propensity score, ^pðxÞ, as an additional regressor: yi on 1; wi; ^pðxiÞ; i ¼ 1; 2; . . . ; N ð18:23Þ where the coe‰cient on wi is the estimate of the treatment e¤ect. In other words, the estimated propensity score plays the role of the control function. The idea is that the estimated propensity score should contain all the information in the covariates that is relevant for estimating the treatment e¤ect. The question is, When does regression (18.23) consistently estimate the average treatment e¤ect? The following is a special case of Wooldridge (1999c, Proposition 3.2): proposition 18.4: In addition to Assumption ATE.10, assume that Eðy1 \u0001 y0 j xÞ ¼ m1ðxÞ \u0001 m0ðxÞ is uncorrelated with Varðw j xÞ ¼ pðxÞ½1 \u0001 pðxÞ\u0002. If the parametric es- timator ^pð\u0003Þ is consistent and ﬃﬃﬃﬃ N p -asymptotically normal, then the OLS coe‰cient on Estimating Average Treatment E¤ects 617", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 625, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p626::c0", "text": "w from regression (18.23) is consistent and ﬃﬃﬃﬃ N p -asymptotically normal for the aver- age treatment e¤ect, ATE. The assumption that m1ðxÞ \u0001 m0ðxÞ is uncorrelated with Varðw j xÞ may appear un- likely, as both are functions of x. However, remember that correlation is a linear measure of dependence. The conditional variance Varðw j xÞ is a nonmonotonic qua- dratic in pðxÞ, while m1ðxÞ \u0001 m0ðxÞ is likely to be monotonic in many elements of x; zero correlation might hold approximately. (This observation is analogous to the fact that if z is a standard normal random variable, then z and z2 are uncorrelated.) Using di¤erent auxiliary assumptions, Rosenbaum and Rubin (1983, Corollary 4.3) suggest a more general version of regression (18.23) for estimating ATE: yi on 1; wi; ^pi; wið^pi \u0001 ^mpÞ; i ¼ 1; 2; . . . ; N ð18:24Þ where ^mp is the sample average of ^pi, i ¼ 1; 2; . . . ; N. proposition 18.5: Under Assumption ATE.1, assume in addition that E½y0 j pðxÞ\u0002 and E½y1 j pðxÞ\u0002 are linear in pðxÞ. Then the coe‰cient on wi in regression (18.24) consistently estimates ATE. Proof: Rosenbaum and Rubin (1983, Theorem 3) show that, under Assumption ATE.1, ðy0; y1Þ and w are independent conditional on pðxÞ. For completeness, we present the argument. It su‰ces to show P½w ¼ 1 j y0; y1; pðxÞ\u0002 ¼ P½w ¼ 1 j pðxÞ\u0002 or E½w j y0; y1; pðxÞ\u0002 ¼ E½w j pðxÞ\u0002. But, under Assumption ATE.1, Eðw j y0; y1; xÞ ¼ Eðw j xÞ ¼ pðxÞ. By iterated expectations, E½w j y0; y1; pðxÞ\u0002 ¼ E½Eðw j y0; y1; xÞ j y0; y1; pðxÞ\u0002 ¼ E½ pðxÞ j y0; y1; pðxÞ\u0002 ¼ pðxÞ We can now use this equation to obtain E½y j w; pðxÞ\u0002. Write y ¼ y0 þ ðm1 \u0001 m0Þw þ wðv1 \u0001 v0Þ. We just showed that ðy0; y1Þ and w are independent given pðxÞ, and so E½y j w; pðxÞ\u0002 ¼ E½y0 j pðxÞ\u0002 þ ðm1 \u0001 m0Þw þ wfE½v1 j pðxÞ\u0002 \u0001 E½v0 j pðxÞ\u0002g ¼ d0 þ d1pðxÞ þ ðm1 \u0001 m0Þw þ d2w½ pðxÞ \u0001 mp\u0002 under the linearity assumptions, where mp 1 E½ pðxÞ\u0002. fRemember, as v1 and v0 have zero means, the linear function of pðxÞ must have a zero mean, too; we can always write it as d2½ pðxÞ \u0001 mp\u0002.g This step completes the proof, as replacing mp with its sample average in the regression does not a¤ect consistency (or asymptotic normality). The linearity assumptions for E½y0 j pðxÞ\u0002 and E½y1 j pðxÞ\u0002 are probably too re- strictive in many applications. As pðxÞ is bounded between zero and one, E½y0 j pðxÞ\u0002 and E½y1 j pðxÞ\u0002 are necessarily bounded under linearity, which might be a poor as- Chapter 18 618", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 626, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p627::c0", "text": "sumption if the yg have a wide support. If y is binary, linearity of these expected values is also questionable, but it could be a reasonable approximation. Of course, it is a simple matter to replace ^pi with a low-order polynomial in ^pi, being sure to de- mean any term before constructing its interaction with wi. Example 18.2 (E¤ects of Job Training on Earnings): The data in JTRAIN2.RAW are from a job training experiment in the 1970s. The response variable is real earnings in 1978, measured in thousands of dollars. Real earnings are zero for men who did not work during the year. Training began up to two years prior to 1978. We use regressions (18.23) and (18.24) to estimate the average treatment e¤ect. The elements of x are real earnings in 1974 and 1975, age (in quadratic form), a binary high school degree indicator (nodegree), marital status, and binary variables for black and His- panic. In the ﬁrst-stage probit of train on x, only nodegree is statistically signiﬁcant at the 5 percent level. Once we have the ﬁtted propensity scores, we can run regression (18.23). This gives ^a ¼ 1:626 (se ¼ :644), where the standard error is not adjusted for the probit ﬁrst-stage estimation. Job training is estimated to increase earnings by about $1,626. Interestingly, this estimate is very close to the regression re78 on 1, train, x: ^a ¼ 1:625 (se ¼ :640); both are somewhat smaller than the simple comparison-of- means estimate, which is 1.794 (se ¼ :633). Adding the interaction term in regression (18.24), with ^mp ¼ :416, lowers the esti- mate somewhat: ^a ¼ 1:560 (se ¼ :642). The interaction term (again, based on the usual OLS standard error) is marginally signiﬁcant. Regressions (18.23) and (18.24) are attractive because they account for possibly nonrandom assignment of treatment by including a single function of the covariates, the estimated propensity score. Compared with the regressions that include the full set of covariates, in ﬂexible ways, possibly interacted with the treatment [as in equation (18.16)], the propensity score approach seems much more parsimonious. However, this parsimony is somewhat illusory. Remember, the propensity score is estimated by a ﬁrst-stage probit or logit, where the treatment is the dependent variable and ﬂexible functions of the elements of x are the explanatory variables. It is not obvious that estimating a ﬂexible binary response model in the ﬁrst stage is somehow better than the kitchen sink regression (18.16). In fact, if the propensity score were estimated using a linear probability model, regression (18.23) and regression (18.16) without the interaction terms would produce identical estimates of a. Also, using regres- sion (18.23) or (18.24) makes it tempting to ignore the ﬁrst-stage estimation of the propensity score in obtaining the standard error of the treatment e¤ect (as we did in Example 18.2). At least in regression (18.16) we know that the standard error of ^a is reliable; at worst, we must make the standard error robust to heteroskedasticity. In Estimating Average Treatment E¤ects 619", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 627, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p628::c0", "text": "Example 18.2, we have no way of knowing how much the sampling variation in the ﬁrst-stage probit estimates would a¤ect a properly computed standard error short of actually doing the calculations. Because the propensity score approach and the standard regression approach require di¤erent assumptions for consistency, neither generally dominates the other. [If anything, the linearity assumptions on E½y0 j pðxÞ\u0002 and E½y1 j pðxÞ\u0002 are less palatable than the linearity assumptions underlying equation (18.15).] If we use the propensity score as in equation (18.21) then we need not make aux- iliary assumptions as required by regressions (18.23) and (18.24). But we should still adjust the standard error of A ^TE to account for ﬁrst-stage estimation of the propensity score. Apparently, not much work has been done comparing regression methods that use the propensity score with standard kitchen sink–type regressions, let alone com- paring these procedures with the estimator from equation (18.21). All the previous estimates of ATEs that use the estimated propensity score involve either regressions or formulas that appear similar to regressions in the sense that the propensity score is included in a sample average [see equations (18.21) and (18.22)]. Estimates of the propensity score are also used in a very di¤erent way in the treat- ment e¤ect literature. Various matching estimators have been proposed, and asymp- totic distributions are available in many cases. The matching approach suggested by Rosenbaum and Rubin (1983) is motivated by the following thought experiment. Suppose we choose a propensity score, pðxÞ, at random from the population. Then, we select two agents from the population sharing the chosen propensity score, where one agent receives treatment and the other does not. Under Assumption ATE.1, the expected di¤erence in the observed outcomes for these agents is E½y j w ¼ 1; pðxÞ\u0002 \u0001 E½y j w ¼ 0; pðxÞ\u0002 ¼ E½y1 \u0001 y0 j pðxÞ\u0002 which is the ATE conditional on pðxÞ. By iterated expectations, averaging across the distribution of propensity scores gives ATE ¼ Eðy1 \u0001 y0Þ. An estimation strategy requires estimating the propensity scores, estimating the response di¤erences for pairs matched on the basis of the estimated propensity scores, and then averaging over all such pairs. Because getting identical predicted proba- bilities is often unlikely, grouping into cells or local averaging is used instead. E¤ec- tively, agents with similar propensity scores are considered a match. Heckman, Ichimura, and Todd (1997) (HIT), Angrist (1998), and Dehejia and Wahba (1999) provide recent treatments of matching methods. As with the regression methods discussed in Section 18.3.1, a practical problem with matching on the propensity score is that it can be hard to ﬁnd treated and untreated agents with similar estimated propensity scores. HIT discuss trimming Chapter 18 620", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 628, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p629::c0", "text": "strategies in a nonparametric context and derive asymptotically valid standard errors. Similarly, the practice of grouping on the basis of the estimated propensity scores, and then ignoring the sampling variation in both the estimated propensity scores and the grouping when constructing standard errors and conﬁdence intervals, may be misleading. HIT show how to obtain valid inference. 18.4 Instrumental Variables Methods We now turn to instrumental variables estimation of average treatment e¤ects when we suspect failure of the ignorability-of-treatment assumption (ATE.1 or ATE.10). IV methods for estimating ATEs can be very e¤ective if a good instrument for treatment is available. We need the instrument to predict treatment (after partialing out any controls). As we discussed in Section 5.3.1, the instrument should be redundant in a certain conditional expectation and unrelated to unobserved heterogeneity; we give precise assumptions in the following subsections. Our primary focus in this section is on the average treatment e¤ect deﬁned in equation (18.1), although we touch on estimating ATE1. In Section 18.4.2, we brieﬂy discuss estimating the local average treatment e¤ect. 18.4.1 Estimating the ATE Using IV In studying IV procedures, it is useful to write the observed outcome y as in equation (18.10): y ¼ m0 þ ðm1 \u0001 m0Þw þ v0 þ wðv1 \u0001 v0Þ ð18:25Þ However, unlike in Section 18.3, we do not assume that v0 and v1 are mean inde- pendent of w, given x. Instead, we assume the availability of instruments, which we collect in the vector z. (Here we separate the extra instruments from the covariates, so that x and z do not overlap. In many cases z is a scalar, but the analysis is no easier in that case.) If we assume that the stochastic parts of y1 and y0 are the same, that is, v1 ¼ v0, then the interaction term disappears (and ATE ¼ ATE1). Without the interaction term we can use standard IV methods under weak assumptions. assumption ATE.2: (a) In equation (18.25), v1 ¼ v0; (b) Lðv0 j x; zÞ ¼ Lðv0 j xÞ; and (c) Lðw j x; zÞ 0 Lðw j xÞ. All linear projections in this chapter contain unity, which we suppress for notational simplicity. Estimating Average Treatment E¤ects 621", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 629, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p630::c0", "text": "Under parts a and b of Assumption ATE.2, we can write y ¼ d0 þ aw þ xb0 þ u0 ð18:26Þ where a ¼ ATE and u0 1 v0 \u0001 Lðv0 j x; zÞ. By deﬁnition, u0 has zero mean and is uncorrelated with ðx; zÞ, but w and u0 are generally correlated, which makes OLS estimation of equation (18.26) inconsistent. The redundancy of z in the linear pro- jection Lðv0 j x; zÞ means that z is appropriately excluded from equation (18.26); this is the part of identiﬁcation that we cannot test (except indirectly using the over- identiﬁcation test from Chapter 6). Part c means that z has predictive power in the linear projection of treatment on ðx; zÞ; this is the standard rank condition for identiﬁcation from Chapter 5, and we can test it using a ﬁrst-stage regression and heteroskedasticity-robust tests of exclusion restrictions. Under Assumption ATE.2, a [and the other parameters in equation (18.26)] are identiﬁed, and they can be con- sistently estimated by 2SLS. Because the only endogenous explanatory variable in equation (18.26) is binary, equation (18.25) is called a dummy endogenous variable model (Heckman, 1978). As we discussed in Chapter 5, there are no special consid- erations in estimating equation (18.26) by 2SLS when the endogenous explanatory variable is binary. Assumption ATE.2b holds if the instruments z are independent of ðy0; xÞ. For ex- ample, suppose z is a scalar determining eligibility in a job training program or some other social program. Actual participation, w, might be correlated with v0, which could contain unobserved ability. If eligibility is randomly assigned, it is often rea- sonable to assume that z is independent of ðy0; xÞ. Eligibility would positively inﬂu- ence participation, and so Assumption ATE.2c should hold. Random assignment of eligibility is no guarantee that eligibility is a valid instru- ment for participation. The outcome of z could a¤ect other behavior, which could feed back into u0 in equation (18.26). For example, consider Angrist’s (1990) draft lottery application, where draft lottery number is used as an instrument for enlisting. Lottery number clearly a¤ected enlistment, so Assumption ATE.2c is satisﬁed. As- sumption ATE.2b is also satisﬁed if men did not change behavior in unobserved ways that a¤ect wage, based on their lottery number. One concern is that men with low lottery numbers may get more education as a way of avoiding service through a de- ferment. Including years of education in x e¤ectively solves this problem. But what if men with high draft lottery numbers received more job training because employers did not fear losing them? If a measure of job training status cannot be included in x, lottery number would generally be correlated with u0. See AIR and Heckman (1997) for additional discussion. Chapter 18 622", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 630, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p631::c0", "text": "As the previous discussion implies, the redundancy condition in Assumption ATE.2b allows the instruments z to be correlated with elements of x. For example, in the population of high school graduates, if w is a college degree indicator and the instrument z is distance to the nearest college while attending high school, then z is allowed to be correlated with other controls in the wage equation, such as geographic indicators. Under v1 ¼ v0 and the key assumptions on the instruments, 2SLS on equation (18.26) is consistent and asymptotically normal. But if we make stronger assump- tions, we can ﬁnd a more e‰cient IV estimator. assumption ATE.20: (a) In equation (18.25), v1 ¼ v0; (b) Eðv0 j x; zÞ ¼ Lðv0 j xÞ; (c) Pðw ¼ 1 j x; zÞ 0 Pðw ¼ 1 j xÞ and Pðw ¼ 1 j x; zÞ ¼ Gðx; z; gÞ is a known parametric form (usually probit or logit); and (d) Varðv0 j x; zÞ ¼ s2 0. Part b assumes that Eðv0 j xÞ is linear in x, and so it is more restrictive than Assumption ATE.2b. It does not usually hold for discrete response variables y, although it may be a reasonable approximation in some cases. Under parts a and b, the error u0 in equation (18.26) has a zero conditional mean: Eðu0 j x; zÞ ¼ 0 ð18:27Þ Part d implies that Varðu0 j x; zÞ is constant. From the results on e‰cient choice of instruments in Section 14.5.3, the optimal IV for w is Eðw j x; zÞ ¼ Gðx; z; gÞ. There- fore, we can use a two-step IV method: Procedure 18.1 (Under Assumption ATE.20): (a) Estimate the binary response model Pðw ¼ 1 j x; zÞ ¼ Gðx; z; gÞ by maximum likelihood. Obtain the ﬁtted probabilities, ^Gi. The leading case occurs when Pðw ¼ 1 j x; zÞ follows a probit model. (b) Estimate equation (18.26) by IV using instruments 1, ^Gi, and xi. There are several nice features of this IV estimator. First, it can be shown that the conditions su‰cient to ignore the estimation of g in the ﬁrst stage hold; see Section 6.1.2. Therefore, the usual 2SLS standard errors and test statistics are asymptotically valid. Second, under Assumption ATE.20, the IV estimator from step b is asymp- totically e‰cient in the class of estimators where the IVs are functions of ðxi; ziÞ; see Problem 8.11. If Assumption ATE.2d does not hold, all statistics should be made robust to heteroskedasticity, and we no longer have the e‰cient IV estimator. Procedure 18.1 has an important robustness property. Because we are using ^Gi as an instrument for wi, the model for Pðw ¼ 1 j x; zÞ does not have to be correctly speciﬁed. For example, if we specify a probit model for Pðw ¼ 1 j x; zÞ, we do not need the probit Estimating Average Treatment E¤ects 623", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 631, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p632::c0", "text": "model to be correct. Generally, what we need is that the linear projection of w onto ½x; Gðx; z; g\u0004Þ\u0002 actually depends on Gðx; z; g\u0004Þ, where we use g\u0004 to denote the plim of the maximum likelihood estimator when the model is misspeciﬁed (see White, 1982a). These requirements are fairly weak when z is partially correlated with w. Technically, a and b are identiﬁed even if we do not have extra exogenous varia- bles excluded from x. But we can rarely justify the estimator in this case. For con- creteness, suppose that w given x follows a probit model [and we have no z, or z does not appear in Pðw ¼ 1 j x; zÞ]. Because Gðx; gÞ 1 Fðg0 þ xg1Þ is a nonlinear function of x, it is not perfectly correlated with x, so it can be used as an IV for w. This situ- ation is very similar to the one discussed in Section 17.4.1: while identiﬁcation holds for all values of a and b if g1 0 0, we are achieving identiﬁcation o¤ of the non- linearity of Pðw ¼ 1 j xÞ. Further, Fðg0 þ xg1Þ and x are typically highly correlated. As we discussed in Section 5.2.6, severe multicollinearity among the IVs can result in very imprecise IV estimators. In fact, if Pðw ¼ 1 j xÞ followed a linear probability model, a would not be identiﬁed. See Problem 18.5 for an illustration. Example 18.3 (Estimating the E¤ects of Education on Fertility): We use the data in FERTIL2.RAW to estimate the e¤ect of attaining at least seven years of education on fertility. The data are for women of childbearing age in Botswana. Seven years of education is, by far, the modal amount of positive education. (About 21 percent of women report zero years of education. For the subsample with positive education, about 33 percent report seven years of education.) Let y ¼ children, the number of living children, and let w ¼ educ7 be a binary indicator for at least seven years of education. The elements of x are age, age2, evermarr (ever married), urban (lives in an urban area), electric (has electricity), and tv (has a television). The OLS estimate of ATE is \u0001:394 (se ¼ :050). We also use the variable frsthalf, a binary variable equal to one if the woman was born in the ﬁrst half of the year, as an IV for educ7. It is easily shown that educ7 and frsthalf are signiﬁcantly negatively related. The usual IV estimate is much larger in magnitude than the OLS estimate, but only marginally signiﬁcant: \u00011:131 (se ¼ :619). The estimate from Procedure 18.1 is even bigger in magnitude, and very signiﬁcant: \u00011:975 (se ¼ :332). The stan- dard error that is robust to arbitrary heteroskedasticity is even smaller. Therefore, using the probit ﬁtted values as an IV, rather than the usual linear projection, pro- duces a more precise estimate (and one notably larger in magnitude). The IV estimate of education e¤ect seems very large. One possible problem is that, because children is a nonnegative integer that piles up at zero, the assumptions underlying Procedure 18.1—namely, Assumptions ATE.20a and ATE.20b—might not be met. In Chapter 19 we will discuss other methods for handling integer responses. Chapter 18 624", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 632, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p633::c0", "text": "In principle, it is important to recognize that Procedure 18.1 is not the same as using ^G as a regressor in place of w. That is, IV estimation of equation (18.26) is not the same as the OLS estimator from yi on 1; ^Gi; xi ð18:28Þ Consistency of the OLS estimators from regression (18.28) relies on having the model for Pðw ¼ 1 j x; zÞ correctly speciﬁed. If the ﬁrst three parts of Assumption ATE.20 hold, then Eðy j x; zÞ ¼ d0 þ aGðx; z; gÞ þ xb and, from the results on generated regressors in Chapter 6, the estimators from re- gression (18.28) are generally consistent. Procedure 18.1 is more robust because it does not require Assumption ATE.20c for consistency. Another problem with regression (18.28) is that the usual OLS standard errors and test statistics are not valid, for two reasons. First, if Varðu0 j x; zÞ is constant, Varðy j x; zÞ cannot be constant because Varðw j x; zÞ is not constant. By itself this is a minor nuisance because heteroskedasticity-robust standard errors and test statistics are easy to obtain. [However, it does call into question the e‰ciency of the estimator from regression (18.28).] A more serious problem is that the asymptotic variance of the estimator from regression (18.28) depends on the asymptotic variance of ^g unless a ¼ 0, and the heteroskedasticity-robust standard errors do not correct for this. In summary, using ﬁtted probabilities from a ﬁrst-stage binary response model, such as probit or logit, as an instrument for w is a nice way to exploit the binary nature of the endogenous explanatory variable. In addition, the asymptotic inference is always standard. Using ^Gi as an instrument does require the assumption that Eðv0 j x; zÞ depends only on x and is linear in x, which can be more restrictive than Assumption ATE.2b. Allowing for the interaction wðv1 \u0001 v0Þ in equation (18.25) is notably harder. In general, when v1 0 v0, the IV estimator (using z or ^G as IVs for w) does not con- sistently estimate ATE (or ATE1). Nevertheless, it is useful to ﬁnd assumptions under which IV estimation does consistently estimate ATE. This problem has been studied by Angrist (1991), Heckman (1997), and Wooldridge (1997b), and we synthesize results from these papers. Under the conditional mean redundancy assumptions Eðv0 j x; zÞ ¼ Eðv0 j xÞ and Eðv1 j x; zÞ ¼ Eðv1 j xÞ ð18:29Þ we can always write equation (18.25) as y ¼ m0 þ aw þ g0ðxÞ þ w½g1ðxÞ \u0001 g0ðxÞ\u0002 þ e0 þ wðe1 \u0001 e0Þ ð18:30Þ Estimating Average Treatment E¤ects 625", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 633, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p634::c0", "text": "where a is the ATE and v0 ¼ g0ðxÞ þ e0; Eðe0 j x; zÞ ¼ 0 ð18:31Þ v1 ¼ g1ðxÞ þ e1; Eðe1 j x; zÞ ¼ 0 ð18:32Þ Given functional form assumptions for g0 and g1—which would typically be linear in parameters—we can estimate equation (18.30) by IV, where the error term is e0 þ wðe1 \u0001 e0Þ. For concreteness, suppose that g0ðxÞ ¼ h0 þ xb0; g1ðxÞ \u0001 g0ðxÞ ¼ ðx \u0001 cÞd ð18:33Þ where c ¼ EðxÞ. If we plug these equations into equation (18.30), we need instru- ments for w and wðx \u0001 cÞ (note that x does not contain a constant here). If q 1 qðx; zÞ is the instrument for w (such as the response probability in Procedure 18.1), the natural instrument for w \u0003 x is q \u0003 x. (And, if q is the e‰cient IV for w, q \u0003 x is the e‰cient instrument for w \u0003 x.) When will applying IV to y ¼ g þ aw þ xb0 þ wðx \u0001 cÞd þ e0 þ wðe1 \u0001 e0Þ ð18:34Þ be consistent? If the last term disappears, and, in particular, if e1 ¼ e0 ð18:35Þ then the error e0 has zero mean given ðx; zÞ; this result means that IV estimation of equation (18.34) produces consistent, asymptotically normal estimators. assumption ATE.3: With y expressed as in equation (18.25), conditions (18.29), (18.33), and (18.35) hold. In addition, Assumption ATE.20c holds. We have the following extension of Procedure 18.1: Procedure 18.2 (Under Assumption ATE.3): (a) Same as Procedure 18.1. (b) Estimate the equation yi ¼ g þ awi þ xib0 þ ½wiðxi \u0001 xÞ\u0002d þ errori ð18:36Þ by IV, using instruments 1, ^Gi, xi, and ^Giðxi \u0001 xÞ. If we add Assumption ATE.20d, Procedure 18.2 produces the e‰cient IV estimator [when we ignore estimation of EðxÞ]. As with Procedure 18.1, we do not actually need the binary response model to be correctly speciﬁed for identiﬁcation. As an alterna- tive, we can use zi and interactions between zi and xi as instruments, which generally results in testable overidentifying restrictions. Technically, the fact that x is an estimator of EðxÞ should be accounted for in computing the standard errors of the IV estimators. But, as shown in Problem 6.10, Chapter 18 626", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 634, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p635::c0", "text": "the adjustments for estimating EðxÞ can be expected to have a trivial e¤ect on the standard errors; in practice, we can just use the usual or heteroskedasticity-robust standard errors. Example 18.4 (An IV Approach to Evaluating Job Training): To evaluate the e¤ects of a job training program on subsequent wages, suppose that x includes education, experience, and the square of experience. If z indicates eligibility in the program, we would estimate the equation logðwageÞ ¼ m0 þ a jobtrain þ b01educ þ b02exper þ b03exper2 þ d1 jobtrain \u0003 ðeduc \u0001 educÞ þ d2 jobtrain \u0003 ðexper \u0001 experÞ þ d3 jobtrain \u0003 ðexper2 \u0001 exper2Þ þ error by IV, using instruments 1, z, educ, exper, exper2, and interactions of z with all demeaned covariates. Notice that for the last interaction, we subtract o¤ the average of exper2. Alternatively, we could use in place of z the ﬁtted values from a probit of jobtrain on ðx; zÞ. Procedure 18.2 is easy to carry out, but its consistency generally hinges on condi- tion (18.35), not to mention the functional form assumptions in equation (18.33). We can relax condition (18.35) to E½wðe1 \u0001 e0Þ j x; z\u0002 ¼ E½wðe1 \u0001 e0Þ\u0002 ð18:37Þ We do not need wðe1 \u0001 e0Þ to have zero mean, as a nonzero mean only a¤ects the intercept. It is important to see that correlation between w and ðe1 \u0001 e0Þ does not in- validate the IV estimator of a from Procedure 18.2. However, we must assume that the covariance conditional on ðx; zÞ is constant. Even if this assumption is not exactly true, it might be approximately true. It is easy to see why, along with conditions (18.29) and (18.33), condition (18.37) implies consistency of the IV estimator. We can write equation (18.34) as y ¼ x þ aw þ xb0 þ wðx \u0001 cÞd þ e0 þ r ð18:38Þ where r ¼ wðe1 \u0001 e0Þ \u0001 E½wðe1 \u0001 e0Þ\u0002 and x ¼ g þ E½wðe1 \u0001 e0Þ\u0002. Under condition (18.37), Eðr j x; zÞ ¼ 0, and so the composite error e0 þ r has zero mean conditional on ðx; zÞ. Therefore, any function of ðx; zÞ can be used as instruments in equation (18.38). Under the following modiﬁcation of Assumption ATE.3, Procedure 18.2 is still consistent: assumption ATE.30: With y expressed as in equation (18.25), conditions (18.29), (18.33), and (18.37) hold. In addition, Assumption ATE.20c holds. Estimating Average Treatment E¤ects 627", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 635, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p636::c0", "text": "Even if Assumption ATE.20d holds in addition to Assumption ATE.20c, the IV esti- mator is generally not e‰cient because Varðr j x; zÞ would typically be heteroskedastic. Angrist (1991) provided primitive conditions for assumption (18.37) in the case where z is independent of ðy0; y1; xÞ. Then, the covariates can be dropped entirely from the analysis (leading to IV estimation of the simple regression equation y ¼ x þ aw þ error). We can extend those conditions here to allow z and x to be corre- lated. Assume that Eðw j x; z; e1 \u0001 e0Þ ¼ hðx; zÞ þ kðe1 \u0001 e0Þ ð18:39Þ for some functions hð\u0003Þ and kð\u0003Þ and that e1 \u0001 e0 is independent of ðx; zÞ ð18:40Þ Under these two assumptions, E½wðe1 \u0001 e0Þ j x; z\u0002 ¼ hðx; zÞEðe1 \u0001 e0 j x; zÞ þ E½ðe1 \u0001 e0Þkðe1 \u0001 e0Þ j x; z\u0002 ¼ hðx; zÞ \u0003 0 þ E½ðe1 \u0001 e0Þkðe1 \u0001 e0Þ\u0002 ¼ E½ðe1 \u0001 e0Þkðe1 \u0001 e0Þ\u0002 ð18:41Þ which is just an unconditional moment in the distribution of e1 \u0001 e0. We have used the fact that Eðe1 \u0001 e0 j x; zÞ ¼ 0 and that any function of e1 \u0001 e0 is independent of ðx; zÞ under assumption (18.40). If we assume that kð\u0003Þ is the identity function (as in Wooldridge, 1997b), then equation (18.41) is Varðe1 \u0001 e0Þ. Assumption (18.40) is reasonable for continuously distributed responses, but it would not generally be reasonable when y is a discrete response or corner solution outcome. Further, even if assumption (18.40) holds, assumption (18.39) is violated when w given x, z, and ðe1 \u0001 e0Þ follows a standard binary response model. For ex- ample, a probit model would have Pðw ¼ 1 j x; z; e1 \u0001 e0Þ ¼ F½p0 þ xp1 þ zp2 þ rðe1 \u0001 e0Þ\u0002 ð18:42Þ which is not separable in ðx; zÞ and ðe1 \u0001 e0Þ. Nevertheless, assumption (18.39) might be a reasonable approximation in some cases. Without covariates, Angrist (1991) presents simulation evidence that suggests the simple IV estimator does quite well for estimating the ATE even when assumption (18.39) is violated. Rather than assuming (18.39), di¤erent approaches are available, but they require di¤erent assumptions. We ﬁrst consider a solution that involves adding a nonlinear function of ðx; zÞ to equation (18.38) and estimating the resulting equation by 2SLS. We add to assumptions (18.40) and (18.42) a normality assumption, Chapter 18 628", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 636, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p637::c0", "text": "e1 \u0001 e0 @ Normalð0; t2Þ ð18:43Þ Under assumptions (18.40), (18.42), and (18.43) we can derive an estimating equation to show that ATE is usually identiﬁed. To derive an estimating equation, note that conditions (18.40), (18.42), and (18.43) imply that Pðw ¼ 1 j x; zÞ ¼ Fðy0 þ xy1 þ zy2Þ ð18:44Þ where each theta is the corresponding pi multiplied by ½1 þ r2t2\u0002\u00011=2. If we let a denote the latent error underlying equation (18.44) (with a standard normal distri- bution), and deﬁne c 1 e1 \u0001 e0, then conditions (18.40), (18.42), and (18.43) imply that ða; cÞ has a zero-mean bivariate normal distribution that is independent of ðx; zÞ. Therefore, Eðc j a; x; zÞ ¼ Eðc j aÞ ¼ xa for some parameter x, and Eðwc j x; zÞ ¼ E½wEðc j a; x; zÞ j x; z\u0002 ¼ xEðwa j x; zÞ: Using the fact that a @ Normalð0; 1Þ and is independent of ðx; zÞ, we have Eðwa j x; zÞ ¼ ðy \u0001y 1½y0 þ xy1 þ zy2 þ a b 0\u0002afðaÞ da ¼ fð\u0001fy0 þ xy1 þ zy2gÞ ¼ fðy0 þ xy1 þ zy2Þ ð18:45Þ where fð\u0003Þ is the standard normal density. Therefore, we can now write y ¼ g þ aw þ xb þ wðx \u0001 cÞd þ xfðy0 þ xy1 þ zy2Þ þ e0 þ r ð18:46Þ where r ¼ wc \u0001 Eðwc j x; zÞ. The composite error in (18.46) has zero mean conditional on ðx; zÞ, and so we can estimate the parameters using IV methods. One catch is the nonlinear function fðy0 þ xy1 þ zy2Þ. We could use nonlinear two stage least squares, as described in Chapter 14. But a two-step approach is easier. First, we gather together the assumptions: assumption ATE.4: With y written as in equation (18.25), maintain assumptions (18.29), (18.33), (18.40), (18.42) (with p2 0 0), and (18.43). Procedure 18.3 (Under Assumption ATE.4): (a) Estimate y0, y1, and y2 from a probit of w on ð1; x; zÞ. Form the predicted probabilities, ^Fi, along with ^fi ¼ fð^y0 þ xi ^y1 þ zi ^y2Þ, i ¼ 1; 2; . . . ; N. (b) Estimate the equation yi ¼ g þ awi þ xib0 þ wiðxi \u0001 xÞd þ x ^fi þ errori ð18:47Þ by IV, using instruments ½1; ^Fi; xi; ^Fiðxi \u0001 xÞ; ^fi\u0002. Estimating Average Treatment E¤ects 629", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 637, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p638::c0", "text": "The term ^fi 1 fð^y0 þ xi ^y1 þ zi ^y2Þ in equation (18.47) is another example of a control function, although, unlike in Section 18.3, it is obtained from instrumental variables assumptions, rather than ignorability of treatment assumptions. Even if x 0 0, the e¤ect of adding ^fi to the estimate of a can be small. Consider the version of (18.46) without covariates x and with a scalar instrument, z: y ¼ g þ aw þ xfðy0 þ y1zÞ þ u; Eðu j zÞ ¼ 0 ð18:48Þ This equation holds, for example, if the instrument z is independent of ðx; v0; v1Þ. The simple IV estimator of a is obtained by omitting fðy0 þ y1zÞ. If we use z as an IV for w, the simple IV estimator is consistent provided z and fðy0 þ y1zÞ are uncorrelated. (Remember, having an omitted variable that is uncorrelated with the IV does not cause inconsistency of the IV estimator.) Even though fðy0 þ y1zÞ is a function of z, these two variables might have small correlation because z is monotic while fðy0 þ y1zÞ is symmetric about \u0001ðy0=y1Þ. This discussion shows that condition (18.37) is not necessary for IV to consistently estimate the ATE: It could be that while E½wðe1 \u0001 e0Þ j x; z\u0002 is not constant, it is roughly uncorrelated with x (or the functions of x) that appear in (18.38), as well as with the functions of z used as instruments. Equation (18.48) illustrates another important point: If x 0 0 and the single instrument z is binary, a is not identiﬁed. Lack of identiﬁcation occurs because fðy0 þ y1zÞ takes on only two values, which means it is perfectly linearly related to z. So long as z takes on more than two values, a is generally identiﬁed, although the identiﬁcation is due to the fact that fð\u0003Þ is a di¤erent nonlinear function than Fð\u0003Þ. With x in the model ^fi and ^Fi might be collinear, resulting in imprecise IV estimates. Because r in (18.46) is heteroskedastic, the instruments below (18.47) are not opti- mal, and so we might simply use zi along with interactions of zi with ðxi \u0001 xÞ and ^fi as IVs. If zi has dimension greater than one, then we can test the overidentifying restrictions as a partial test of instrument selection and the normality assumptions. Of course, we could use the results of Chapter 14 to characterize and estimate the opti- mal instruments, but this is fairly involved [see, for example, Newey and McFadden (1994)]. A di¤erent approach to estimating the ATE when assumption (18.39) fails is to compute the expected value of y given the endogenous treatment and all exogenous variables: Eðy j w; x; zÞ. Finding this expectation requires somewhat more by way of assumptions, but it also has some advantages, which we discuss later. For complete- ness, we list a set of assumptions: assumption ATE.40: With y written as in equation (18.25), maintain assumptions (18.29) and (18.33). Furthermore, the treatment can be written as w ¼ 1½y0 þ xy1 þ Chapter 18 630", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 638, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p639::c0", "text": "zy2 þ a b 0\u0002, where ða; e0; e1Þ is independent of ðx; zÞ with a trivariate normal distri- bution; in particular, a @ Normalð0; 1Þ. Under Assumption ATE.40, we can use calculations very similar to those used in Section 17.4.1 to obtain Eðy j w; x; zÞ. In particular, Eðy jw; x; zÞ ¼ g þ aw þ xb0 þ wðx \u0001 cÞd þ r1w½fðqyÞ=FðqyÞ\u0002 þ r2ð1 \u0001 wÞffðqyÞ=½1 \u0001 FðqyÞ\u0002g ð18:49Þ where qy 1 y0 þ xy1 þ zy2 and r1 and r2 are additional parameters. Heckman (1978) used this expectation to obtain two-step estimators of the switching regression model. [See Vella and Verbeek (1999) for a recent discussion of the switching regression model in the context of treatment e¤ects.] Not surprisingly, (18.49) suggests a simple two-step procedure, where the ﬁrst step is identical to that in Procedure 18.3: Procedure 18.4 (Under Assumption ATE.40): (a) Estimate y0, y1, and y2 from a probit of w on ð1; x; zÞ. Form the predicted probabilities, ^Fi, along with ^fi ¼ fð^y0 þ xi ^y1 þ zi ^y2Þ, i ¼ 1; 2; . . . ; N. (b) Run the OLS regression yi on 1; wi; xi; wiðxi \u0001 xÞ; wið ^fi=^FiÞ; ð1 \u0001 wiÞ½ ^fi=ð1 \u0001 ^FiÞ\u0002 ð18:50Þ using all of the observations. The coe‰cient on wi is a consistent estimator of a, the ATE. When we restrict attention to the wi ¼ 1 subsample, thereby dropping wi and wiðxi \u0001 xÞ, we obtain the sample selection correction from Section 17.4.1; see equa- tion (17.24). (The treatment wi becomes the sample selection indicator.) But the goal of sample selection corrections is very di¤erent from estimating an average treatment e¤ect. For the sample selection problem, the goal is to estimate b0, which indexes Eðy j xÞ in the population. By contrast, in estimating an ATE we are interested in the causal e¤ect that w has on y. It makes sense to check for joint signiﬁcance of the last two regressors in regression (18.50) as a test of endogeneity of w. Because the coe‰cients r1 and r2 are zero under H0, we can use the results from Chapter 6 to justify the usual Wald test (perhaps made robust to heteroskedasticity). If these terms are jointly insigniﬁcant at a su‰- ciently high level, we can justify the usual OLS regression without unobserved heter- ogeneity. If we reject H0, we must deal with the generated regressors problem in obtaining a valid standard error for ^a. Technically, Procedure 18.3 is more robust than Procedure 18.4 because the former does not require a trivariate normality assumption. Linear conditional expectations, Estimating Average Treatment E¤ects 631", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 639, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p640::c0", "text": "along with the assumption that w given ðx; zÞ follows a probit, su‰ce. In addition, Procedure 18.3 allows us to separate the issues of endogeneity of w and nonconstant treatment e¤ect: if we ignore the estimation error involved with demeaning xi in the interaction term—which generally seems reasonable—then a standard t-test (perhaps made robut to heteroskedasticity) for H0: x ¼ 0 is valid for testing the presence of wðe1 \u0001 e0Þ, even when w is endogenous. Practically, the extra assumption in Procedure 18.4 is that e0 is independent of ðx; zÞ with a normal distribution. We may be willing to make this assumption, espe- cially if the estimates from Procedure 18.3 are too imprecise to be useful. The e‰ciency issue is a di‰cult one because of the two-step estimation involved, but, intuitively, Procedure 18.4 is likely to be more e‰cient because it is based on Eðy j w; x; zÞ. Pro- cedure 18.3 involves replacing the unobserved composite error with its expectation conditional only on ðx; zÞ. In at least one case, Procedure 18.4 gives results when Procedure 18.3 cannot: when x is not in the equation and there is a single binary instrument. Under a variant of Assumption ATE.30, we can consistently estimate ATE1 by IV. As before, we express y as in equation (18.25). First, we show how to consistently estimate ATE1ðxÞ, which can be written as ATE1ðxÞ ¼ Eðy1 \u0001 y0 j x; w ¼ 1Þ ¼ ðm1 \u0001 m0Þ þ Eðv1 \u0001 v0 j x; w ¼ 1Þ The following assumption identiﬁes ATE1ðxÞ: assumption ATE.300: (a) With y expressed as in equation (18.25), the ﬁrst part of assumption (18.29) holds, that is, Eðv0 j x; zÞ ¼ Eðv0 j xÞ; (b) Eðv1 \u0001 v0 j x; z; w ¼ 1Þ ¼ Eðv1 \u0001 v0 j x; w ¼ 1Þ; and (c) Assumption ATE.20c holds. We discussed part a of this assumption earlier, as it also appears in Assumption ATE.30. It can be violated if agents change their behavior based on z. Part b deserves some discussion. Recall that v1 \u0001 v0 is the person-speciﬁc gain from participation or treatment. Assumption ATE.300 requires that for those in the treatment group, the gain is not predictable given z, once x is controlled for. Heckman (1997) discusses Angrist’s (1990) draft lottery example, where z (a scalar) is draft lottery number. Men who had a large z were virtually certain to escape the draft. But some men with large draft numbers chose to serve anyway. Even with good controls in x, it seems plausi- ble that, for those who chose to serve, a higher z is associated with a higher gain to military service. In other words, for those who chose to serve, v1 \u0001 v0 and z are pos- itively correlated, even after controlling for x. This argument directly applies to esti- mation of ATE1; the e¤ect on estimation of ATE is less clear. Chapter 18 632", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 640, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p641::c0", "text": "Assumption ATE.300b is plausible when z is a binary indicator for eligibility in a program, which is randomly determined and does not induce changes in behavior other than whether or not to participate. To see how Assumption ATE.300 identiﬁes ATE1ðxÞ, rewrite equation (18.25) as y ¼ m0 þ g0ðxÞ þ w½ðm1 \u0001 m0Þ þ Eðv1 \u0001 v0 j x; w ¼ 1Þ\u0002 þ w½ðv1 \u0001 v0Þ \u0001 Eðv1 \u0001 v0 j x; w ¼ 1Þ\u0002 þ e0 ¼ m0 þ g0ðxÞ þ w \u0003 ATE1ðxÞ þ a þ e0 ð18:51Þ where a 1 w½ðv1 \u0001 v0Þ \u0001 Eðv1 \u0001 v0 j x; w ¼ 1Þ\u0002 and e0 is deﬁned in equation (18.31). Under Assumption ATE.300a, Eðe0 j x; zÞ ¼ 0. The hard part is dealing with the term a. When w ¼ 0, a ¼ 0. Therefore, to show that Eða j x; zÞ ¼ 0, it su‰ces to show that Eða j x; z; w ¼ 1Þ ¼ 0. [Remember, Eða j x; zÞ ¼ Pðw ¼ 0Þ \u0003 Eða j x; z; w ¼ 0Þ þ Pðw ¼ 1Þ \u0003 Eða j x; z; w ¼ 1Þ.] But this result follows under Assumption ATE.300b: Eða j x; z; w ¼ 1Þ ¼ Eðv1 \u0001 v0 j x; z; w ¼ 1Þ \u0001 Eðv1 \u0001 v0 j x; w ¼ 1Þ ¼ 0 Now, letting r 1 a þ e0 and assuming that g0ðxÞ ¼ h0 þ hðxÞb0 and ATE1ðxÞ ¼ t þ fðxÞd for some row vector of functions hðxÞ and fðxÞ, we can write y ¼ g0 þ h0ðxÞb0 þ tw þ ½w \u0003 fðxÞ\u0002d þ r; Eðr j x; zÞ ¼ 0 All the parameters of this equation can be consistently estimated by IV, using any functions of ðx; zÞ as IVs. [These would include include 1, h0ðxÞ, Gðx; z; ^gÞ—the ﬁtted treatment probabilities—and Gðx; z; ^gÞ \u0003 fðxÞ.] The average treatment e¤ect on the treated for any x is estimated as ^t þ fðxÞ^d. Averaging over the observations with wi ¼ 1 gives a consistent estimator of ATE1. 18.4.2 Estimating the Local Average Treatment E¤ect by IV We now discuss estimation of an evaluation parameter introduced by Imbens and Angrist (1994), the local average treatment e¤ect (LATE), in the simplest possible setting. This requires a slightly more complicated notation. (More general cases re- quire even more complicated notation, as in AIR.) As before, we let w be the observed treatment indicator (taking on zero or one), and let the counterfactual out- comes be y1 with treatment and y0 without treatment. The observed outcome y can be written as in equation (18.3). To deﬁne LATE, we need to have an instrumental variable, z. In the simplest case z is a binary variable, and we focus attention on that case here. For each unit i in a random draw from the population, zi is zero or one. Associated with the two possible outcomes on z are counterfactual treatments, w0 and w1. These are the treatment Estimating Average Treatment E¤ects 633", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 641, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p642::c0", "text": "statuses we would observe if z ¼ 0 and z ¼ 1, respectively. For each unit, we observe only one of these. For example, z can denote whether a person is eligible for a par- ticular program, while w denotes actual participation in the program. Write the observed treatment status as w ¼ ð1 \u0001 zÞw0 þ zw1 ¼ w0 þ zðw1 \u0001 w0Þ ð18:52Þ When we plug this equation into y ¼ y0 þ wðy1 \u0001 y0Þ we get y ¼ y0 þ w0ðy1 \u0001 y0Þ þ zðw1 \u0001 w0Þðy1 \u0001 y0Þ A key assumption is z is independent of ðy0; y1; w0; w1Þ ð18:53Þ Under assumption (18.53), all expectations involving functions of ðy0; y1; w0; w1Þ, conditional on z, do not depend on z. Therefore, Eðy j z ¼ 1Þ ¼ Eðy0Þ þ E½w0ðy1 \u0001 y0Þ\u0002 þ E½ðw1 \u0001 w0Þðy1 \u0001 y0Þ\u0002 and Eðy j z ¼ 0Þ ¼ Eðy0Þ þ E½w0ðy1 \u0001 y0Þ\u0002 Subtracting the second equation from the ﬁrst gives Eðy j z ¼ 1Þ \u0001 Eðy j z ¼ 0Þ ¼ E½ðw1 \u0001 w0Þðy1 \u0001 y0Þ\u0002 ð18:54Þ which can be written [see equation (2.49)] as 1 \u0003 Eðy1 \u0001 y0 j w1 \u0001 w0 ¼ 1ÞPðw1 \u0001 w0 ¼ 1Þ þ ð\u00011ÞEðy1 \u0001 y0 j w1 \u0001 w0 ¼ \u00011ÞPðw1 \u0001 w0 ¼ \u00011Þ þ 0 \u0003 Eðy1 \u0001 y0 j w1 \u0001 w0 ¼ 0ÞPðw1 \u0001 w0 ¼ 0Þ ¼ Eðy1 \u0001 y0 j w1 \u0001 w0 ¼ 1ÞPðw1 \u0001 w0 ¼ 1Þ \u0001 Eðy1 \u0001 y0 j w1 \u0001 w0 ¼ \u00011ÞPðw1 \u0001 w0 ¼ \u00011Þ To get further, we introduce another important assumption, called monotonicity by Imbens and Angrist: w1 b w0 ð18:55Þ In other words, we are ruling out w1 ¼ 0 and w0 ¼ 1. This assumption has a simple interpretation when z is a dummy variable representing assignment to the treatment group: anyone in the population who would be in the treatment group in the absence Chapter 18 634", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 642, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p643::c0", "text": "of assignment (or eligibility) would be in the treatment group if assigned to the treatment group. Units of the population who do not satisfy monotonicity are called deﬁers. In many applications, this assumption seems very reasonable. For example, if z denotes randomly assigned eligibility in a job training program, assumption (18.55) simply requires that people who would participate without being eligible would also participate if eligible. Under assumption (18.55), Pðw1 \u0001 w0 ¼ \u00011Þ ¼ 0, so assumptions (18.53) and (18.55) imply Eðy j z ¼ 1Þ \u0001 Eðy j z ¼ 0Þ ¼ Eðy1 \u0001 y0 j w1 \u0001 w0 ¼ 1ÞPðw1 \u0001 w0 ¼ 1Þ ð18:56Þ In this setup, Imbens and Angrist (1994) deﬁne LATE to be LATE ¼ Eðy1 \u0001 y0 j w1 \u0001 w0 ¼ 1Þ ð18:57Þ Because w1 \u0001 w0 ¼ 1 is equivalent to w1 ¼ 1, w0 ¼ 0, LATE has the following inter- pretation: it is the average treatment e¤ect for those who would be induced to par- ticipate by changing z from zero to one. There are two things about LATE that make it di¤erent from the other treatment parameters. First, it depends on the instrument, z. If we use a di¤erent instrument, then LATE generally changes. The parameters ATE and ATE1 are deﬁned without reference to an IV, but only with reference to a population. Second, because we cannot observe both w1 and w0, we cannot identify the subpopulation with w1 \u0001 w0 ¼ 1. By contrast, ATE averages over the entire population, while ATE1 is the average for those who are actually treated. Example 18.5 (LATE for Attending a Catholic High School): Suppose that y is a standardized test score, w is an indicator for attending a Catholic high school, and z is an indicator for whether the student is Catholic. Then, generally, LATE is the mean e¤ect on test scores for those individuals who choose a Catholic high school because they are Catholic. Evans and Schwab (1995) use a high school graduation indicator for y, and they estimate a probit model with an endogenous binary ex- planatory variable, as described in Section 15.7.3. Under the probit assumptions, it is possible to estimate ATE, whereas the simple IV estimator identiﬁes LATE under weaker assumptions. Because Eðy j z ¼ 1Þ and Eðy j z ¼ 0Þ are easily estimated using a random sample, LATE is identiﬁed if Pðw1 \u0001 w0 ¼ 1Þ is estimable and nonzero. Importantly, from the monotonicity assumption, w1 \u0001 w0 is a binary variable because Pðw1 \u0001 w0 ¼ \u00011Þ ¼ 0. Therefore, Pðw1 \u0001 w0 ¼ 1Þ ¼ Eðw1 \u0001 w0Þ ¼ Eðw1Þ \u0001 Eðw0Þ ¼ Eðw j z ¼ 1Þ \u0001 Eðw j z ¼ 0Þ ¼ Pðw ¼ 1 j z ¼ 1Þ \u0001 Pðw ¼ 1 j z ¼ 0Þ Estimating Average Treatment E¤ects 635", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 643, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p644::c0", "text": "where the second-to-last equality follows from equations (18.52) and (18.53). Each conditional probability can be consistently estimated given a random sample on ðw; zÞ. Therefore, the ﬁnal assumption is Pðw ¼ 1 j z ¼ 1Þ 0 Pðw ¼ 1 j z ¼ 0Þ ð18:58Þ To summarize, under assumptions (18.53), (18.55), and (18.58), LATE ¼ ½Eðy j z ¼ 1Þ \u0001 Eðy j z ¼ 0Þ\u0002=½Pðw ¼ 1 j z ¼ 1Þ \u0001 Pðw ¼ 1 j z ¼ 0Þ\u0002 ð18:59Þ Therefore, a consistent estimator is L ^ ATE ¼ ðy1 \u0001 y0Þ=ðw1 \u0001 w0Þ, where y1 is the sample average of yi over that part of the sample where zi ¼ 1 and y0 is the sample average over zi ¼ 0, and similarly for w1 and w0 (which are sample proportions). From Problem 5.13b, we know that L ^ ATE is identical to the IV estimator of a in the simple equation y ¼ d0 þ aw þ error, where z is the IV for w. Our conclusion is that, in the simple case of a binary instrument for the binary treatment, the usual IV estimator consistently estimates LATE under weak assump- tions. See Angrist, Imbens, and Rubin (1996) and the discussants’ comments for much more. 18.5 Further Issues As we have seen in Sections 18.3 and 18.4, under certain assumptions, OLS or IV can be used to estimate average treatment e¤ects. Therefore, at least in some cases, problems such as attrition or other forms of sample selection can be easily handled using the methods in Chapter 17. For example, in equation (18.34) under assumption (18.35), it is reasonable to assume that the assumptions of Procedure 17.2 hold, with the straightforward extension that the interaction between wi (which plays the role of yi2) and ðxi \u0001 xÞ is added, along with the appropriate IVs. If the problem is attrition, we need some exogenous elements that a¤ect attrition but do not appear in x or z. Other situations may require special attention, and we now brieﬂy discuss some of these. 18.5.1 Special Considerations for Binary and Corner Solution Responses The deﬁnitions of ATE and ATE1, as well as LATE, are valid for any kind of re- sponse variable. ATE is simply Eðy1Þ \u0001 Eðy0Þ, and for this to be well deﬁned we only need to assume that the expected values exist. If y0 and y1 are binary—such as em- ployment indicators—the expected values are probabilities of success. If y0 and y1 are corner solution outcomes—such as labor supply—ATE and ATE1 estimate the e¤ect of treatment on the so-called unconditional expectation rather than, say, Chapter 18 636", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 644, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p645::c0", "text": "Eðy1 \u0001 y0 j y0 > 0Þ. Still, ATE and ATE1 are often of interest for corner solution outcomes. If the average treatment e¤ects as we deﬁned them in Section 18.2 are still of interest, why do we need to consider alternative methods for estimating treatment e¤ects? The answer is that some of the assumptions we have discussed are unrealistic for discrete or corner solution outcomes. For example, to arrive at equation (18.15), we assumed that Eðyg j xÞ is linear in some functions of x. Though these assumptions can be relaxed, the computation of valid standard errors is no longer straightforward because a linear regression no longer generally estimates ATE. [Expression (18.7) is general and does not impose any functional forms, and so it can be used as the basis for estimating ATE. We would simply estimate Eðy j x; w ¼ 1Þ and Eðy j x; w ¼ 0Þ in a way that is consistent with the features of y.] Under ignorability of treatment, the propensity score approach is attractive be- cause it requires no modeling of expectations involving y0 or y1. Only the propensity score needs to be modeled, and this is always a binary response probability. When we cannot assume ignorability of treatment and must resort to IV methods, allowing for discrete and corner solution responses is theoretically harder. As we discussed in Section 18.4.1, conditions such as equation (18.33) cannot be literally true for binary and Tobit-like responses, and this condition appears in all of the assumptions for IV estimation. It is not easy to relax this assumption because if, say, yg is a corner solution outcome, a reasonable model for Eðv1 \u0001 v0 j xÞ is not obvious. Of course, it could be that, even if the assumptions in Section 18.4 cannot be ex- actly true, the IV methods may nevertheless produce reasonable estimates of ATE and ATE1. Angrist’s (1991) simulation evidence is compelling for binary responses, but he only studies the case without covariates. As an alternative to the various treatment e¤ect estimators covered in this chapter, we can use probit and Tobit models with a binary endogenous explanatory variable. The maximum likelihood estimator described in Section 15.7.3 requires a strong set of assumptions, but it delivers estimates of the exact average treatment e¤ect, condi- tional on the exogenous variables, if the probit assumptions hold. Similarly, a Tobit model with a binary endogenous variable can be estimated by maximum likelihood (see Problem 16.6); again, estimates of ATE can be obtained directly. 18.5.2 Panel Data The availability of panel data allows us to consistently estimate treatment e¤ects without assuming ignorability of treatment and without an instrumental variable, provided the treatment varies over time and is uncorrelated with time-varying unob- servables that a¤ect the response. Estimating Average Treatment E¤ects 637", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 645, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p646::c0", "text": "If the treatment is assumed to have the same e¤ect for each unit and if the e¤ect is constant over time, ﬁxed e¤ects or ﬁrst-di¤erencing methods can be used, as de- scribed in Chapter 10. This approach works well when the treatment and control groups are designated based on time-constant variables and when treatment status is not constant across time. Of course, we must observe the responses and other con- trols for each cross section unit in at least two di¤erent time periods. A more com- plicated model allows the treatment e¤ect to interact with observable variables and unobserved heterogeneity. For example, consider the model yit ¼ xitb þ a1wit þ ci þ withi þ uit where wit is a binary treatment indicator of some training program, yit is the response variable, and ci and hi are unobserved heterogeneity. This is a special case of the model studied in Section 11.2.2. The average treatment e¤ect is a1 þ EðhiÞ, and we can use the methods of Section 11.2.2 to estimate a1 and EðhiÞ. The problem of attrition can be handled as in Section 17.7, provided the treatment e¤ect has an additive form. If attrition is determined solely by whether the partici- pant was not selected for the program, then no adjustments are needed if wit is orthogonal to the idiosyncratic error, uit: this is just attrition on the basis of exoge- nous explanatory variables. 18.5.3 Nonbinary Treatments So far, we have restricted attention to the case where w is a binary variable. But we can also estimate average treatment e¤ects when w takes on more than two values. The deﬁnitions of ATE, ATE1, and LATE are more complicated in this case because the counterfactual is more complicated; see Angrist and Imbens (1995) and Heckman (1997). Here, we focus on a random coe‰cient model for the observed outcome, as in Garen (1984), Heckman and Vytlacil (1998), and Wooldridge (1997b, 1999c). The average treatment e¤ect is easy to deﬁne in this context, as it is just an average partial e¤ect. As in the case of binary treatment, two approaches can be used to identify ATE: we can assume ignorability of treatment, conditional on a set of covariates, or we can use an instrumental variables approach. In either case, the model is the same: Eðy j w; cÞ ¼ a þ bw ð18:60Þ where c ¼ ða; bÞ and a and b may both depend on observable covariates as well as unobserved heterogeneity. A more traditional approach would introduce observables and unobservables into the equation separately in a parametric fashion—usually, Chapter 18 638", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 646, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p647::c0", "text": "linear in a set of parameters—but this step is unnecessary when we are interested in estimating b 1 EðbÞ, which is the average partial e¤ect of w on Eðy j w; cÞ. It is important to see that, unlike in the binary treatment case, equation (18.60) imposes a functional form assumption. This is not as restrictive as it might seem, because a and b are allowed to depend on individual-speciﬁc observables and unob- servables. Nevertheless, as we know, linear models can have drawbacks for binary and corner solution responses (unless w is binary). When w is binary, equation (18.60) encompasses the counterfactual setup in Sec- tion 18.2, which we analyzed in Sections 18.3 and 18.4. Equation (18.3) shows this result immediately, where we take a ¼ y0 and b ¼ y1 \u0001 y0. We now establish identiﬁcation of b under ignorability conditions. The assump- tions are collected together as follows: assumption ATE.5: (a) Equation (18.60) holds. For a set of covariates x, the fol- lowing redundancy assumptions hold: (b) Eðy j w; c; xÞ ¼ Eðy j w; cÞ; and (c) Condi- tional on x, c is redundant in the ﬁrst two conditional moments of w: Eðw j x; cÞ ¼ Eðw j xÞ and Varðw j x; cÞ ¼ Varðw j xÞ. Given the functional form assumption (18.60), Assumption ATE.5b is not very con- troversial because a and b can depend in an arbitrary way on x. In e¤ect, a and b already capture any dependence of Eðy j w; cÞ on x. Assumption ATE.5c is much more restrictive, but it is the analogue of the ignorability-of-treatment Assumption ATE.10. In fact, when w is binary, Assumption ATE.10 implies Assumption ATE.5c. For general w, Assumption ATE.5c is slightly less restrictive than assuming ðw; cÞ are independent given x. The following is from Wooldridge (1999c, Proposition 3.1): proposition 18.6: Under Assumption ATE.5, assume, in addition, that Varðw j xÞ > 0 for all x in the relevant population. Then b ¼ E½Covðw; y j xÞ=Varðw j xÞ\u0002 ð18:61Þ Because Varðw j xÞ and Covðw; y j xÞ can be estimated generally, equation (18.61) shows that b is identiﬁed. If ^mð\u0003Þ and ^hð\u0003Þ are consistent estimators of Eðw j xÞ and Varðw j xÞ, respectively, a consistent estimator of b, under fairly weak assumptions, is N\u00011 PN i¼1½wi \u0001 ^mðxiÞ\u0002yi=^hðxiÞ; this is the extension of equation (18.21) to the case of nonbinary treatments. Estimating mð\u0003Þ and hð\u0003Þ is easily done using ﬂexible parametric models that should reﬂect the nature of w. When w is binary, we simply estimate the propensity score. When w is a roughly continuous variable over a broad range, Eðw j xÞ linear in Estimating Average Treatment E¤ects 639", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 647, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p648::c0", "text": "functions of x and Varðw j xÞ constant might be reasonable, in which case, as shown in Wooldridge (1999c), a ‘‘kitchen sink’’ regression of y on w and functions of x can consistently estimate b. Wooldridge (1999c) discusses additional examples, including when w is a count variable or a fractional variable (both of which we discuss in Chapter 19), and contains an example. As a computational device, it is useful to see that consistent estimators of b can be computed using an instrumental variables approach. As shown in Wooldridge (1999c), under Assumption ATE.5 we can write y ¼ bw þ gðxÞy þ v ð18:62Þ where gð\u0003Þ is any vector function of x and E½gðxÞ0v\u0002 ¼ 0. Typically, we would include levels, squares, and cross products, or logarithms, as elements of gð\u0003Þ. Adding gðxÞ is intended to e¤ectively reduce the error variance. Further, if we deﬁne r ¼ ½w \u0001 mðxÞ\u0002=hðxÞ, it can be shown that EðrvÞ ¼ 0. Because r and w are highly corre- lated, we can use ½r; gðxÞ\u0002 as IVs in equation (18.62). In practice, we replace each unknown ri with ^ri ¼ ½wi \u0001 ^mðxiÞ\u0002=^hðxiÞ, and use ð^ri; giÞ as the IVs for ðwi; giÞ. This estimator is consistent and ﬃﬃﬃﬃ N p -asymptotically normal. Unfortunately, the su‰cient conditions for ignoring estimation of the IVs in the ﬁrst stage—see Section 6.1.2— are not always met in this application. An alternative approach assumes that w is ignorable in Eða j x; wÞ and Eðb j x; wÞ. Under additional linearity assumptions, this leads directly to equation (18.15), re- gardless of the nature of w. The previous methods assume some kind of ignorability of treatment. The IV approach also begins with equation (18.60). As in the binary treatment case, we separate the covariates (x) and the IVs (z). We assume both are redundant in equa- tion (18.60): Eðy j w; c; x; zÞ ¼ Eðy j w; cÞ ð18:63Þ Again, this assumption is noncontroversial once we specify the functional form in equation (18.60). Assumption (18.63) holds trivially in the counterfactual framework with binary treatment. The di¤erence between x and z is that a and b may have conditional means that depend on x, but not on z. For example, if w is a measure of class attendance, x might contain measures of student ability and motivation. By contrast, we assume z is redundant for explaining a and b, given x. In the class attendance example, z might be indicators for di¤erent living situations or distances from residence to lecture halls. In e¤ect, the distinction between x and z is the kind of distinction we make in struc- Chapter 18 640", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 648, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p649::c0", "text": "tural models, where some ‘‘exogenous’’ variables (x) are allowed to appear in the structural equation and others (z) are not. Mathematically, we have Eða j x; zÞ ¼ Eða j xÞ; Eðb j x; zÞ ¼ Eðb j xÞ: ð18:64Þ Assumption (18.64) is completely analogous to assumption (18.29). For simplicity, we also assume the expectations are linear in x: Eða j xÞ ¼ g0 þ xg; Eðb j xÞ ¼ d0 þ xd ð18:65Þ Then we can write y ¼ h0 þ xg þ bw þ wðx \u0001 cÞd þ u þ w \u0003 v þ e ð18:66Þ where c ¼ EðxÞ, u ¼ a \u0001 Eða j x; zÞ, v ¼ b \u0001 Eðb j x; zÞ, e is the error implied by equation (18.60), and so Eðe j w; c; x; zÞ ¼ 0. This equation is basically the same as equation (18.34), except that now w need not be binary. To apply IV to equation (18.66), it su‰ces that the composite error, u þ w \u0003 v þ e, has a constant mean given ðx; zÞ. But Eðu þ e j x; zÞ ¼ 0, and so it su‰ces to assume Eðw \u0003 v j x; zÞ ¼ Eðw \u0003 vÞ ð18:67Þ which is the same as Covðw; v j x; zÞ ¼ Covðw; vÞ because Eðv j x; zÞ ¼ 0. When as- sumption (18.67) holds along with conditions (18.60), (18.63), (18.64), and (18.65) and an appropriate rank condition—essentially, w is partially correlated with z— 2SLS estimation of equation (18.66) consistently estimates all parameters except the intercept. The IVs would be ð1; x; z; z1x; . . . ; zLxÞ, or we could use ^Eðw j x; zÞ and ^Eðw j x; zÞ \u0003 x as IVs for ½w; wðx \u0001 cÞ\u0002, where ^Eðw j x; zÞ is an estimate of Eðw j x; zÞ. As before, in practice c would be replaced with x. The 2SLS estimator is ﬃﬃﬃﬃ N p - consistent and asymptotically normal. Generally, the error in equation (18.66) is heteroskedastic. Condition (18.67) is the same one we used in the binary treatment case to justify the usual IV estimator. As we discussed in Section 18.4.1, condition (18.67) does not hold when Pðw ¼ 1 j x; zÞ satisﬁes logit or probit binary response models. If w is a continuous treatment, condition (18.67) is more reasonable. For example, if Eðw j x; z; vÞ is additive in v, then condition (18.67) holds when Varðv j x; zÞ is con- stant, in which case Covðw; v j x; zÞ ¼ s2 v . See Wooldridge (1997b, 2000f ) for further discussion. Wooldridge (2000f ) also covers more general cases when condition (18.67) is not true and the treatment is not binary. Heckman and Vytlacil (1998) use similar assumptions in a general random coe‰- cient model to arrive at a related estimation method. In their simplest approach, Estimating Average Treatment E¤ects 641", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 649, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p650::c0", "text": "Heckman and Vytlacil suggest a two-step estimator, where Eðw j x; zÞ is estimated using a linear model in the ﬁrst stage and the ﬁtted values are used in a second-stage regression. The preceding analysis shows that a linear functional form for Eðw j x; zÞ is not needed for the IV estimator to be consistent, although condition (18.67) gen- erally is. 18.5.4 Multiple Treatments Sometimes the treatment variable is not simply a scalar. For example, for the popu- lation of working high school graduates, w1 could be credit hours at two-year colleges and w2 credit hours at four-year colleges. If we make ignorability assumptions of the kind in Section 18.3.1, equation (18.15) extends in a natural way: each treatment variable appears by itself and interacted with the (demeaned) covariates. This approach does not put any restrictions on the nature of the treatments. Alternatively, as in Wooldridge (1999c), Assumption 18.5 extends to a vector w, which leads to an extension of condition (18.60) for multiple treatments. Wooldridge (2000f ) shows how the IV methods in Section 18.4.1 extend easily to multiple treatments, binary or otherwise. For multiple binary treatments, a reduced- form probit is estimated for each treatment, and then terms wijðxi \u0001 xÞ and ^fij for each treatment j are added to equation (18.47). See Wooldridge (2000f ) for further discussion. An approach based on ﬁnding Eðy j w1; . . . ; wM; x; zÞ, for M treatments, is di‰cult but perhaps tractable in some cases. Problems 18.1. Consider the di¤erence-in-means estimator, d ¼ y1 \u0001 y0, where yg is the sam- ple average of the yi with wi ¼ g, g ¼ 0; 1. a. Show that, as an estimator of ATE1, the bias in y1 \u0001 y0 is Eðy0 j w ¼ 1Þ \u0001 Eðy0 j w ¼ 0Þ. b. Let y0 be the earnings someone would earn in the absence of job training, and let w ¼ 1 denote the job training indicator. Explain the meaning of Eðy0 j w ¼ 1Þ < Eðy0 j w ¼ 0Þ. Intuitively, does it make sense that Eðd Þ < ATE1? 18.2. Show that ATE1ðxÞ is identiﬁed under Assumption ATE.10a; Assumption ATE.10b is not needed. 18.3. Using the data in JTRAIN2.RAW, repeat the analysis in Example 18.2, using unem78 as the response variable. For comparison, use the same x as in Example 18.2. Chapter 18 642", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 650, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p651::c0", "text": "Compare the estimates from regressions (18.23) and (18.24), along with the estimate of ATE from linear regression unem78 on 1, train, x. 18.4. Carefully derive equation (18.45). 18.5. Use the data in JTRAIN2.RAW for this question. a. As in Example 18.2, run a probit of train on 1, x, where x contains the covariates from Example 18.2. Obtain the probit ﬁtted values, say ^Fi. b. Estimate the equation re78i ¼ g0 þ a traini þ xig þ ui by IV, using instruments ð1; ^Fi; xiÞ. Comment on the estimate of a and its standard error. c. Regress ^Fi on xi to obtain the R-squared. What do you make of this result? d. Does the nonlinearity of the probit model for train allow us to estimate a when we do not have an additional instrument? Explain. 18.6. In Procedure 18.2, explain why it is better to estimate equation (18.36) by IV rather than to run the OLS regression yi on 1, ^Gi, xi, ^Giðxi \u0001 xÞ, i ¼ 1; . . . ; N. 18.7. Use the data in JTRAIN2.RAW for this question. a. In the ignorability setup of Section 18.5.3, let w ¼ mostrn, the number of months spent in job training. Assume that Eðw j xÞ ¼ expðg0 þ xgÞ, where x contains the same covariates as in Example 18.2. Estimate the parameters by nonlinear least squares, and let ^mi be the ﬁtted values. Which elements of x are signiﬁcant? (You may use the usual NLS standard errors.) b. Suppose that Varðw j xÞ ¼ hðxÞ ¼ d0 þ d1Eðw j xÞ þ d2½Eðw j xÞ\u00022. Use the estimates from part a to estimate the dj. (Hint: Regress the squared NLS residuals on a qua- dratic in the NLS ﬁtted values.) Are any of ^hi—the estimated variances—negative? c. Form ^ri ¼ ðwi \u0001 ^miÞ=^hi. Estimate equation (18.62) using ^ri as an IV for wi, where gðxÞ ¼ ð1; xÞ and y ¼ re78. Compare ^b with the OLS estimate of b. 18.8. In the IV setup of Section 18.5.3, suppose that b ¼ b, and therefore we can write y ¼ a þ bw þ e; Eðe j a; x; zÞ ¼ 0 Assume that conditions (18.64) and (18.65) hold for a. a. Suppose w is a corner solution outcome, such as hours spent in a job training program. If z is used as IVs for w in y ¼ g0 þ bw þ xg þ r, what is the identiﬁcation condition? Estimating Average Treatment E¤ects 643", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 651, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p652::c0", "text": "b. If w given ðx; zÞ follows a standard Tobit model, propose an IV estimator that uses the Tobit ﬁtted values for w. c. If Varðe j a; x; zÞ ¼ s2 e and Varða j x; zÞ ¼ s2 a , argue that the IV estimator from part b is asymptotically e‰cient. d. What is an alternative to IV estimation that would use the Tobit ﬁtted values for w? Which method do you prefer? e. If b 0 b, but assumptions (18.64) and (18.65) hold, how would you estimate b? 18.9. Consider the IV approach in Section 18.5.3, under assumptions (18.60), (18.63), (18.64), and (18.65). In place of assumption (18.67), assume that Eðw j x; z; vÞ ¼ expðp0 þ xp1 þ zp2 þ p3vÞ, where v is independent of ðx; zÞ with E½expðp3vÞ\u0002 ¼ 1. (Therefore, w is some nonnegative treatment.) a. Show that we can write y ¼ h0 þ xg þ bw þ w \u0003 ðx \u0001 cÞd þ xEðw j x; zÞ þ r where Eðw j x; zÞ ¼ expðh0 þ xp1 þ zp2Þ for some p0, and Eðr j x; zÞ ¼ 0. b. Use part a to show that b is not identiﬁed. fHint: Let q 1 Eðw j x; zÞ, and let h be any other function of ðx; zÞ. Does the linear projection of w on ½1; x; h; h \u0003 ðx \u0001 cÞ; q\u0002 depend on h?g c. For w > 0 (strictly positive treatment), add the assumption that Eðu j v; x; zÞ ¼ rv. Find Eðy jw; x; zÞ ¼ Eðy j v; x; zÞ and propose a two-step estimator of b. Chapter 18 644", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 652, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p653::c0", "text": "19 Count Data and Related Models 19.1 Why Count Data Models? A count variable is a variable that takes on nonnegative integer values. Many vari- ables that we would like to explain in terms of covariates come as counts. A few examples include the number of times someone is arrested during a given year, number of emergency room drug episodes during a given week, number of cigarettes smoked per day, and number of patents applied for by a ﬁrm during a year. These examples have two important characteristics in common: there is no natural a priori upper bound, and the outcome will be zero for at least some members of the popu- lation. Other count variables do have an upper bound. For example, for the number of children in a family who are high school graduates, the upper bound is number of children in the family. If y is the count variable and x is a vector of explanatory variables, we are often interested in the population regression, Eðy j xÞ. Throughout this book we have dis- cussed various models for conditional expectations, and we have discussed di¤erent methods of estimation. The most straightforward approach is a linear model, Eðy j xÞ ¼ xb, estimated by OLS. For count data, linear models have shortcomings very similar to those for binary responses or corner solution responses: because y b 0, we know that Eðy j xÞ should be nonnegative for all x. If ^b is the OLS estimator, there usually will be values of x such that x^b < 0—so that the predicted value of y is negative. For strictly positive variables, we often use the natural log transformation, logðyÞ, and use a linear model. This approach is not possible in interesting count data applications, where y takes on the value zero for a nontrivial fraction of the popula- tion. Transformations could be applied that are deﬁned for all y b 0—for example, logð1 þ yÞ—but logð1 þ yÞ itself is nonnegative, and it is not obvious how to recover Eðy j xÞ from a linear model for E½logð1 þ yÞ j x\u0001. With count data, it is better to model Eðy j xÞ directly and to choose functional forms that ensure positivity for any value of x and any parameter values. When y has no upper bound, the most popular of these is the exponential function, Eðy j xÞ ¼ expðxbÞ. In Chapter 12 we discussed nonlinear least squares (NLS) as a general method for estimating nonlinear models of conditional means. NLS can certainly be applied to count data models, but it is not ideal: NLS is relatively ine‰cient unless Varðy j xÞ is constant (see Chapter 12), and all of the standard distributions for count data imply heteroskedasticity. In Section 19.2 we discuss the most popular model for count data, the Poisson re- gression model. As we will see, the Poisson regression model has some nice features. First, if y given x has a Poisson distribution—which used to be the maintained", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 653, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p654::c0", "text": "assumption in count data contexts—then the conditional maximum likelihood esti- mators are fully e‰cient. Second, the Poisson assumption turns out to be unneces- sary for consistent estimation of the conditional mean parameters. As we will see in Section 19.2, the Poisson quasi–maximum likelihood estimator is fully robust to dis- tributional misspeciﬁcation. It also maintains certain e‰ciency properties even when the distribution is not Poisson. In Section 19.3 we discuss other count data models, and in Section 19.4 we cover quasi-MLEs for other nonnegative response variables. In Section 19.5 we cover mul- tiplicative panel data models, which are motivated by unobserved e¤ects count data models but can also be used for other nonnegative responses. 19.2 Poisson Regression Models with Cross Section Data In Chapter 13 we used the basic Poisson regression model to illustrate maximum likelihood estimation. Here, we study Poisson regression in much more detail, em- phasizing the properties of the estimator when the Poisson distributional assumption is incorrect. 19.2.1 Assumptions Used for Poisson Regression The basic Poisson regression model assumes that y given x 1 ðx1; . . . ; xKÞ has a Poisson distribution, as in El Sayyad (1973) and Maddala (1983, Section 2.15). The density of y given x under the Poisson assumption is completely determined by the conditional mean mðxÞ 1 Eðy j xÞ: f ðy j xÞ ¼ exp½\u0002mðxÞ\u0001½mðxÞ\u0001y=y!; y ¼ 0; 1; . . . ð19:1Þ where y! is y factorial. Given a parametric model for mðxÞ [such as mðxÞ ¼ expðxbÞ] and a random sample fðxi; yiÞ: i ¼ 1; 2; . . . ; Ng on ðx; yÞ, it is fairly straightforward to obtain the conditional MLEs of the parameters. The statistical properties then follow from our treatment of CMLE in Chapter 13. It has long been recognized that the Poisson distributional assumption imposes restrictions on the conditional moments of y that are often violated in applications. The most important of these is equality of the conditional variance and mean: Varðy j xÞ ¼ Eðy j xÞ ð19:2Þ The variance-mean equality has been rejected in numerous applications, and later we show that assumption (19.2) is violated for fairly simple departures from the Poisson Chapter 19 646", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 654, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p655::c0", "text": "model. Importantly, whether or not assumption (19.2) holds has implications for how we carry out statistical inference. In fact, as we will see, it is assumption (19.2), not the Poisson assumption per se, that is important for large-sample inference; this point will become clear in Section 19.2.2. In what follows we refer to assumption (19.2) as the Poisson variance assumption. A weaker assumption allows the variance-mean ratio to be any positive constant: Varðy j xÞ ¼ s2Eðy j xÞ ð19:3Þ where s2 > 0 is the variance-mean ratio. This assumption is used in the generalized linear models (GLM) literature, and so we will refer to assumption (19.3) as the Poisson GLM variance assumption. The GLM literature is concerned with quasi- maximum likelihood estimation of a class of nonlinear models that contains Poisson regression as a special case. We do not need to introduce the full GLM apparatus and terminology to analyze Poisson regression. See McCullagh and Nelder (1989). The case s2 > 1 is empirically relevant because it implies that the variance is greater than the mean; this situation is called overdispersion (relative to the Poisson case). One distribution for y given x where assumption (19.3) holds with over- dispersion is what Cameron and Trivedi (1986) call NegBin I—a particular param- eterization of the negative binomial distribution. When s2 < 1 we say there is underdispersion. Underdispersion is less common than overdispersion, but under- dispersion has been found in some applications. There are plenty of count distributions for which assumption (19.3) does not hold—for example, the NegBin II model in Cameron and Trivedi (1986). Therefore, we are often interested in estimating the conditional mean parameters without speci- fying the conditional variance. As we will see, Poisson regression turns out to be well suited for this purpose. Given a parametric model mðx; bÞ for mðxÞ, where b is a P \u0003 1 vector of parame- ters, the log likelihood for observation i is liðbÞ ¼ yi log½mðxi; bÞ\u0001 \u0002 mðxi; bÞ ð19:4Þ where we drop the term logðyi!Þ because it does not depend on the parameters b (for computational reasons dropping this term is a good idea in practice, too, as yi! gets very large for even moderate yi). We let B H RP denote the parameter space, which is needed for the theoretical development but is practically unimportant in most cases. The most common mean function in applications is the exponential: mðx; bÞ ¼ expðxbÞ ð19:5Þ Count Data and Related Models 647", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 655, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p656::c0", "text": "where x is 1 \u0003 K and contains unity as its ﬁrst element, and b is K \u0003 1. Under as- sumption (19.5) the log likelihood is liðbÞ ¼ yixib \u0002 expðxibÞ. The parameters in model (19.5) are easy to interpret. If xj is continuous, then qEðy j xÞ qxj ¼ expðxbÞbj and so bj ¼ qEðy j xÞ qxj \u0004 1 Eðy j xÞ ¼ q log½Eðy j xÞ\u0001 qxj Therefore, 100bj is the semielasticity of Eðy j xÞ with respect to xj: for small changes Dxj, the percentage change in Eðy j xÞ is roughly ð100bjÞDxj. If we replace xj with logðxjÞ, bj is the elasticity of Eðy j xÞ with respect to xj. Using assumption (19.5) as the model for Eðy j xÞ is analogous to using logðyÞ as the dependent variable in linear regression analysis. Quadratic terms can be added with no additional e¤ort, except in interpreting the parameters. In what follows, we will write the exponential function as in assumption (19.5), leaving transformations of x—such as logs, quadratics, interaction terms, and so on—implicit. See Wooldridge (1997c) for a discussion of other functional forms. 19.2.2 Consistency of the Poisson QMLE Once we have speciﬁed a conditional mean function, we are interested in cases where, other than the conditional mean, the Poisson distribution can be arbitrarily mis- speciﬁed (subject to regularity conditions). When yi given xi does not have a Poisson distribution, we call the estimator ^b that solves max b A B X N i¼1 liðbÞ ð19:6Þ the Poisson quasi–maximum likelihood estimator (QMLE). A careful discussion of the consistency of the Poisson QMLE requires introduction of the true value of the parameter, as in Chapters 12 and 13. That is, we assume that for some value bo in the parameter space B, Eðy j xÞ ¼ mðx; boÞ ð19:7Þ To prove consistency of the Poisson QMLE under assumption (19.5), the key is to show that bo is the unique solution to max b A B E½liðbÞ\u0001 ð19:8Þ Chapter 19 648", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 656, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p657::c0", "text": "Then, under the regularity conditions listed in Theorem 12.2, it follows from this theorem that the solution to equation (19.6) is weakly consistent for bo. Wooldridge (1997c) provides a simple proof that bo is a solution to equation (19.8) when assumption (19.7) holds (see also Problem 19.1). It also follows from the gen- eral results on quasi-MLE in the linear exponential family (LEF) by Gourieroux, Monfort, and Trognon (1984a) (hereafter, GMT, 1984a). Uniqueness of bo must be assumed separately, as it depends on the distribution of xi. That is, in addition to assumption (19.7), identiﬁcation of bo requires some restrictions on the distribution of explanatory variables, and these depend on the nature of the regression function m. In the linear regression case, we require full rank of Eðx0 ixiÞ. For Poisson QMLE with an exponential regression function expðxbÞ, it can be shown that multiple solu- tions to equation (19.8) exist whenever there is perfect multicollinearity in xi, just as in the linear regression case. If we rule out perfect multicollinearity, we can usually conclude that bo is identiﬁed under assumption (19.7). It is important to remember that consistency of the Poisson QMLE does not re- quire any additional assumptions concerning the distribution of yi given xi. In par- ticular, Varðyi j xiÞ can be virtually anything (subject to regularity conditions needed to apply the results of Chapter 12). 19.2.3 Asymptotic Normality of the Poisson QMLE If the Poission QMLE is consistent for bo without any assumptions beyond (19.7), why did we introduce assumptions (19.2) and (19.3)? It turns out that whether these assumptions hold determines which asymptotic variance matrix estimators and in- ference procedures are valid, as we now show. The asymptotic normality of the Poisson QMLE follows from Theorem 12.3. The result is ﬃﬃﬃﬃ N p ð ^b \u0002 boÞ ! d Normalð0; A\u00021 o BoA\u00021 o Þ ð19:9Þ where Ao 1 E½\u0002HiðboÞ\u0001 ð19:10Þ and Bo 1 E½siðboÞsiðboÞ0\u0001 ¼ Var½siðboÞ\u0001 ð19:11Þ where we deﬁne Ao in terms of minus the Hessian because the Poisson QMLE solves a maximization rather than a minimization problem. Taking the gradient of equation (19.4) and transposing gives the score for observation i as siðbÞ ¼ ‘bmðxi; bÞ0½ yi \u0002 mðxi; bÞ\u0001=mðxi; bÞ ð19:12Þ Count Data and Related Models 649", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 657, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p658::c0", "text": "It is easily seen that, under assumption (19.7), siðboÞ has a zero mean conditional on xi. The Hessian is more complicated but, under assumption (19.7), it can be shown that \u0002E½HiðboÞ j xi\u0001 ¼ ‘bmðxi; boÞ0‘bmðxi; boÞ=mðxi; boÞ ð19:13Þ Then Ao is the expected value of this expression (over the distribution of xi). A fully robust asymptotic variance matrix estimator for ^b follows from equation (12.49): X N i¼1 ^Ai !\u00021 X N i¼1 ^si^s0 i ! X N i¼1 ^Ai !\u00021 ð19:14Þ where ^si is obtained from equation (19.12) with ^b in place of b, and ^Ai is the right- hand side of equation (19.13) with ^b in place of bo. This is the fully robust variance matrix estimator in the sense that it requires only assumption (19.7) and the regularity conditions from Chapter 12. The asymptotic variance of ^b simpliﬁes under the GLM assumption (19.3). Main- taining assumption (19.3) (where s2 o now denotes the true value of s2) and deﬁning ui 1 yi \u0002 mðxi; boÞ, the law of iterated expectations implies that Bo ¼ E½u2 i ‘bmiðboÞ0‘bmiðboÞ=fmiðboÞg2\u0001 ¼ E½Eðu2 i j xiÞ‘bmiðboÞ0‘bmiðboÞ=fmiðboÞg2\u0001 ¼ s2 oAo since Eðu2 i j xiÞ ¼ s2 omiðboÞ under assumptions (19.3) and (19.7). Therefore, A\u00021 o BoA\u00021 o ¼ s2 oA\u00021 o , so we only need to estimate s2 o in addition to obtaining ^A. A consistent es- timator of s2 o is obtained from s2 o ¼ E½u2 i =miðboÞ\u0001, which follows from assumption (19.3) and iterated expectations. The usual analogy principle argument gives the estimator ^s2 ¼ N\u00021 X N i¼1 ^u2 i = ^mi ¼ N\u00021 X N i¼1 ð^ui= ﬃﬃﬃﬃﬃ ^mi p Þ2 ð19:15Þ The last representation shows that ^s2 is simply the average sum of squared weighted residuals, where the weights are the inverse of the estimated nominal standard devi- ations. (In the GLM literature, the weighted residuals ~ui 1 ^ui= ﬃﬃﬃﬃﬃ ^mi p are sometimes called the Pearson residuals. In earlier chapters we also called them standardized residuals.) In the GLM literature, a degrees-of-freedom adjustment is usually made by replacing N\u00021 with ðN \u0002 PÞ\u00021 in equation (19.15). Given ^s2 and ^A, it is straightforward to obtain an estimate of Avarð ^bÞ under as- sumption (19.3). In fact, we can write Chapter 19 650", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 658, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p659::c0", "text": "Av^arð ^bÞ ¼ ^s2 ^A\u00021=N ¼ ^s2 X N i¼1 ‘b ^m0 i‘b ^mi= ^mi !\u00021 ð19:16Þ Note that the matrix is always positive deﬁnite when the inverse exists, so it produces well-deﬁned standard errors (given, as usual, by the square roots of the diagonal ele- ments). We call these the GLM standard errors. If the Poisson variance assumption (19.2) holds, things are even easier because s2 is known to be unity; the estimated asymptotic variance of ^b is given in equation (19.16) but with ^s2 1 1. The same estimator can be derived from the MLE theory in Chapter 13 as the inverse of the estimated information matrix (conditional on the xi); see Section 13.5.2. Under assumption (19.3) in the case of overdispersion ðs2 > 1Þ, standard errors of the ^bj obtained from equation (19.16) with ^s2 ¼ 1 will systematically underestimate the asymptotic standard deviations, sometimes by a large factor. For example, if s2 ¼ 2, the correct GLM standard errors are, in the limit, 41 percent larger than the incorrect, nominal Poisson standard errors. It is common to see very signiﬁcant coe‰cients reported for Poisson regressions—a recent example is Model (1993)—but we must interpret the standard errors with caution when they are obtained under as- sumption (19.2). The GLM standard errors are easily obtained by multiplying the Poisson standard errors by ^s 1 ﬃﬃﬃﬃﬃ ^s2 p . The most robust standard errors are obtained from expression (19.14), as these are valid under any conditional variance assump- tion. In practice, it is a good idea to report the fully robust standard errors along with the GLM standard errors and ^s. If y given x has a Poisson distribution, it follows from the general e‰ciency of the conditional MLE—see Section 14.5.2—that the Poisson QMLE is fully e‰cient in the class of estimators that ignores information on the marginal distribution of x. A nice property of the Poisson QMLE is that it retains some e‰ciency for certain departures from the Poisson assumption. The e‰ciency results of GMT (1984a) can be applied here: if the GLM assumption (19.3) holds for some s2 > 0, the Poisson QMLE is e‰cient in the class of all QMLEs in the linear exponential family of dis- tributions. In particular, the Poisson QMLE is more e‰cient than the nonlinear least squares estimator, as well as many other QMLEs in the LEF, some of which we cover in Sections 19.3 and 19.4. Wooldridge (1997c) gives an example of Poisson regression to an economic model of crime, where the response variable is number of arrests of a young man living in California during 1986. Wooldridge ﬁnds overdispersion: ^s is either 1.228 or 1.172, depending on the functional form for the conditional mean. The following example shows that underdispersion is possible. Count Data and Related Models 651", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 659, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p660::c0", "text": "Example 19.1 (E¤ects of Education on Fertility): We use the data in FERTIL2. RAW to estimate the e¤ects of education on women’s fertility in Botswana. The re- sponse variable, children, is number of living children. We use a standard exponential regression function, and the explanatory variables are years of schooling (educ), a quadratic in age, and binary indicators for ever married, living in an urban area, having electricity, and owning a television. The results are given in Table 19.1. A linear regression model is also included, with the usual OLS standard errors. For Poisson regression, the standard errors are the GLM standard errors. A total of 4,358 observations are used. As expected, the signs of the coe‰cients agree in the linear and exponential mod- els, but their interpretations di¤er. For Poisson regression, the coe‰cient on educ implies that another year of education reduces expected number of children by about 2.2 percent, and the e¤ect is very statistically signiﬁcant. The linear model estimate implies that another year of education reduces expected number of children by about .064. (So, if 100 women get another year of education, we estimate they will have about six fewer children.) Table 19.1 OLS and Poisson Estimates of a Fertility Equation Dependent Variable: children Independent Variable Linear (OLS) Exponential (Poisson QMLE) educ \u0002.0644 (.0063) \u0002.0217 (.0025) age .272 (.017) .337 (.009) age2 \u0002.0019 (.0003) \u0002.0041 (.0001) evermarr .682 (.052) .315 (.021) urban \u0002.228 (.046) \u0002.086 (.019) electric \u0002.262 (.076) \u0002.121 (.034) tv \u0002.250 (.090) \u0002.145 (.041) constant \u00023.394 (.245) \u00025.375 (.141) Log-likelihood value — \u00026,497.060 R-squared .590 .598 ^s 1.424 .867 Chapter 19 652", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 660, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p661::c0", "text": "The estimate of s in the Poisson regression implies underdispersion: the variance is less than the mean. (Incidentally, the ^s’s for the linear and Poisson models are not comparable.) One implication is that the GLM standard errors are actually less than the corresponding Poisson MLE standard errors. For the linear model, the R-squared is the usual one. For the exponential model, the R-squared is computed as the squared correlation coe‰cient between childreni and chilˆdreni ¼ expðxi ^bÞ. The exponential regression function ﬁts slightly better. 19.2.4 Hypothesis Testing Classical hypothesis testing is fairly straightforward in a QMLE setting. Testing hypotheses about individual parameters is easily carried out using asymptotic t sta- tistics after computing the appropriate standard error, as we discussed in Section 19.2.3. Multiple hypotheses tests can be carried out using the Wald, quasi–likelihood ratio, or score test. We covered these generally in Sections 12.6 and 13.6, and they apply immediately to the Poisson QMLE. The Wald statistic for testing nonlinear hypotheses is computed as in equation (12.63), where ^V is chosen appropriately depending on the degree of robustness desired, with expression (19.14) being the most robust. The Wald statistic is conve- nient for testing multiple exclusion restrictions in a robust fashion. When the GLM assumption (19.3) holds, the quasi–likelihood ratio statistic can be used. Let \u0001b be the restricted estimator, where Q restrictions of the form cð \u0001bÞ ¼ 0 have been imposed. Let ^b be the unrestricted QMLE. Let LðbÞ be the quasi–log likeli- hood for the sample of size N, given in expression (19.6). Let ^s2 be given in equation (19.15) (with or without the degrees-of-freedom adjustment), where the ^ui are the residuals from the unconstrained maximization. The QLR statistic, QLR 1 2½Lð ^bÞ \u0002 Lð \u0001bÞ\u0001=^s2 ð19:17Þ converges in distribution to w2 Q under H0, under the conditions laid out in Section 12.6.3. The division of the usual likelihood ratio statistic by ^s2 provides for some degree of robustness. If we set ^s2 ¼ 1, we obtain the usual LR statistic, which is valid only under assumption (19.2). There is no usable quasi-LR statistic when the GLM assumption (19.3) does not hold. The score test can also be used to test multiple hypotheses. In this case we estimate only the restricted model. Partition b as ða0; g0Þ0, where a is P1 \u0003 1 and g is P2 \u0003 1, and assume that the null hypothesis is H0: go ¼ g ð19:18Þ where g is a P2 \u0003 1 vector of speciﬁed constants (often, g ¼ 0). Let \u0001b be the estimator of b obtained under the restriction g ¼ g [so \u0001b 1 ð\u0001a0; g0Þ0\u0001, and deﬁne quantities under Count Data and Related Models 653", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 661, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p662::c0", "text": "the restricted estimation as \u0001mi 1 mðxi; \u0001bÞ, \u0001ui 1 yi \u0002 \u0001mi, and ‘b \u0001mi 1 ð‘a \u0001mi; ‘g \u0001miÞ 1 ‘bmðxi; \u0001bÞ. Now weight the residuals and gradient by the inverse of nominal Poisson standard deviation, estimated under the null, 1= ﬃﬃﬃﬃﬃ \u0001mi p : ~ui 1 \u0001ui= ﬃﬃﬃﬃﬃ \u0001mi p ; ‘b ~mi 1 ‘b \u0001mi= ﬃﬃﬃﬃﬃ \u0001mi p ð19:19Þ so that the ~ui here are the Pearson residuals obtained under the null. A form of the score statistic that is valid under the GLM assumption (19.3) [and therefore under assumption (19.2)] is NR2 u from the regression ~ui on ‘b ~mi; i ¼ 1; 2; . . . ; N ð19:20Þ where R2 u denotes the uncentered R-squared. Under H0 and assumption (19.3), NR2 u @ a w2 P2. This is identical to the score statistic in equation (12.68) but where we use ~B ¼ ~s2 ~A, where the notation is self-explanatory. For more, see Wooldridge (1991a, 1997c). Following our development for nonlinear regression in Section 12.6.2, it is easy to obtain a test that is completely robust to variance misspeciﬁcation. Let ~ri denote the 1 \u0003 P2 residuals from the regression ‘g ~mi on ‘a ~mi ð19:21Þ In other words, regress each element of the weighted gradient with respect to the restricted parameters on the weighted gradient with respect to the unrestricted parameters. The residuals are put into the 1 \u0003 P2 vector ~ri. The robust score statistic is obtained as N \u0002 SSR from the regression 1 on ~ui~ri; i ¼ 1; 2; . . . ; N ð19:22Þ where ~ui~ri ¼ ð~ui~ri1; ~ui~ri2; . . . ; ~ui~riP2Þ is a 1 \u0003 P2 vector. As an example, consider testing H0: g ¼ 0 in the exponential model Eðy j xÞ ¼ expðxbÞ ¼ expðx1a þ x2gÞ. Then ‘bmðx; bÞ ¼ expðxbÞx. Let \u0001a be the Poisson QMLE obtained under g ¼ 0, and deﬁne \u0001mi 1 expðxi1\u0001aÞ, with \u0001ui the residuals. Now ‘a \u0001mi ¼ expðxi1\u0001aÞxi1, ‘g \u0001mi ¼ expðxi1\u0001aÞxi2, and ‘b ~mi ¼ \u0001mixi= ﬃﬃﬃﬃﬃ \u0001mi p ¼ ﬃﬃﬃﬃﬃ \u0001mi p xi. Therefore, the test that is valid under the GLM variance assumption is NR2 u from the OLS regres- sion ~ui on ﬃﬃﬃﬃﬃ \u0001mi p xi, where the ~ui are the weighted residuals. For the robust test, ﬁrst obtain the 1 \u0003 P2 residuals ~ri from the regression ﬃﬃﬃﬃﬃ \u0001mi p xi2 on ﬃﬃﬃﬃﬃ \u0001mi p xi1; then obtain the statistic from regression (19.22). 19.2.5 Speciﬁcation Testing Various speciﬁcation tests have been proposed in the context of Poisson regression. The two most important kinds are conditional mean speciﬁcation tests and condi- Chapter 19 654", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 662, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p663::c0", "text": "tional variance speciﬁcation tests. For conditional mean tests, we usually begin with a fairly simple model whose parameters are easy to interpret—such as mðx; bÞ ¼ expðxbÞ—and then test this against other alternatives. Once the set of conditioning variables x has been speciﬁed, all such tests are functional form tests. A useful class of functional form tests can be obtained using the score principle, where the null model mðx; bÞ is nested in a more general model. Fully robust tests and less robust tests are obtained exactly as in the previous section. Wooldridge (1997c, Section 3.5) contains details and some examples, including an extension of RESET to exponential regression models. Conditional variance tests are more di‰cult to compute, especially if we want to maintain only that the ﬁrst two moments are correctly speciﬁed under H0. For ex- ample, it is very natural to test the GLM assumption (19.3) as a way of determining whether the Poisson QMLE is e‰cient in the class of estimators using only assump- tion (19.7). Cameron and Trivedi (1986) propose tests of the stronger assumption (19.2) and, in fact, take the null to be that the Poisson distribution is correct in its entirety. These tests are useful if we are interested in whether y given x truly has a Poisson distribution. However, assumption (19.2) is not necessary for consistency or relative e‰ciency of the Poisson QMLE. Wooldridge (1991b) proposes fully robust tests of conditional variances in the context of the linear exponential family, which contains Poisson regression as a spe- cial case. To test assumption (19.3), write ui ¼ yi \u0002 mðxi; boÞ and note that, under assumptions (19.3) and (19.7), u2 i \u0002 s2 omðxi; boÞ is uncorrelated with any function of xi. Let hðxi; bÞ be a 1 \u0003 Q vector of functions of xi and b, and consider the alterna- tive model Eðu2 i j xiÞ ¼ s2 omðxi; boÞ þ hðxi; boÞdo ð19:23Þ For example, the elements of hðxi; bÞ can be powers of mðxi; bÞ. Popular choices are unity and fmðxi; bÞg2. A test of H0: do ¼ 0 is then a test of the GLM assumption. While there are several moment conditions that can be used, a fruitful one is to use the weighted residuals, as we did with the conditional mean tests. We base the test on N\u00021 X N i¼1 ð^hi= ^miÞ0fð^u2 i \u0002 ^s2 ^miÞ= ^mig ¼ N\u00021 X N i¼1 ~h0 ið~u2 i \u0002 ^s2Þ ð19:24Þ where ~hi ¼ ^hi= ^mi and ~ui ¼ ^ui= ﬃﬃﬃﬃﬃ ^mi p . (Note that ^hi is weighted by 1= ^mi, not 1= ﬃﬃﬃﬃﬃ ^mi p .) To turn this equation into a test statistic, we must confront the fact that its stan- dardized limiting distribution depends on the limiting distributions of ﬃﬃﬃﬃ N p ð ^b \u0002 boÞ and ﬃﬃﬃﬃ N p ð^s2 \u0002 s2 oÞ. To handle this problem, we use a trick suggested by Wooldridge Count Data and Related Models 655", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 663, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p664::c0", "text": "(1991b) that removes the dependence of the limiting distribution of the test statistic on that of ﬃﬃﬃﬃ N p ð^s2 \u0002 s2 oÞ: replace ~hi in equation (19.24) with its demeaned counter- part, ~ri 1 ~hi \u0002 h, where h is just the 1 \u0003 Q vector of sample averages of each element of ~hi. There is an additional purging that then leads to a simple regression-based statistic. Let ‘b ^mi be the unweighted gradient of the conditional mean function, evaluated at the Poisson QMLE ^b, and deﬁne ‘b ^mi 1 ‘b ^mi= ﬃﬃﬃﬃﬃ ^mi p , as before. The fol- lowing steps come from Wooldridge (1991b, Procedure 4.1): 1. Obtain ^s2 as in equation (19.15) and ^A as in equation (19.16), and deﬁne the P \u0003 Q matrix ^J ¼ ^s2ðN\u00021 PN i¼1 ‘b ^m0 i~ri= ^miÞ. 2. For each i, deﬁne the 1 \u0003 Q vector ^zi 1 ð~u2 i \u0002 ^s2Þ~ri \u0002 ^s0 i ^A\u00021^J ð19:25Þ where ^si 1 ‘b ~m0 i ~ui is the Poisson score for observation i. 3. Run the regression 1 on ^zi; i ¼ 1; 2; . . . ; N ð19:26Þ Under assumptions (19.3) and (19.7), N \u0002 SSR from this regression is distributed asymptotically as w2 Q. The leading case occurs when ^mi ¼ expðxi ^bÞ and ‘b ^mi ¼ expðxi ^bÞxi ¼ ^mixi. The subtraction of ^s0 i ^A\u00021 ^J in equation (19.25) is a simple way of handling the fact that the limiting distribution of ﬃﬃﬃﬃ N p ð ^b \u0002 boÞ a¤ects the limiting distribution of the unadjusted statistic in equation (19.24). This particular adjustment ensures that the tests are just as e‰cient as any maximum-likelihood-based statistic if s2 o ¼ 1 and the Poisson as- sumption is correct. But this procedure is fully robust in the sense that only assump- tions (19.3) and (19.7) are maintained under H0. For further discussion the reader is referred to Wooldridge (1991b). In practice, it is probably su‰cient to choose the number of elements in Q to be small. Setting ^hi ¼ ð1; ^m2 i Þ, so that ~hi ¼ ð1= ^mi; ^miÞ, is likely to produce a fairly power- ful two-degrees-of-freedom test against a fairly broad class of alternatives. The procedure is easily modiﬁed to test the more restrictive assumption (19.2). First, replace ^s2 everywhere with unity. Second, there is no need to demean the auxiliary regressors ~hi (so that now ~hi can contain a constant); thus, wherever ~ri appears, sim- ply use ~hi. Everything else is the same. For the reasons discussed earlier, when the focus is on Eðy j xÞ, we are more interested in testing assumption (19.3) than as- sumption (19.2). Chapter 19 656", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 664, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p665::c0", "text": "19.3 Other Count Data Regression Models 19.3.1 Negative Binomial Regression Models The Poisson regression model nominally maintains assumption (19.2) but retains some asymptotic e‰ciency under assumption (19.3). A popular alternative to the Poisson QMLE is full maximum likelihood analysis of the NegBin I model of Cameron and Trivedi (1986). NegBin I is a particular parameterization of the nega- tive binomial distribution. An important restriction in the NegBin I model is that it implies assumption (19.3) with s2 > 1, so that there cannot be underdispersion. (We drop the ‘‘o’’ subscript in this section for notational simplicity.) Typically, NegBin I is parameterized through the mean parameters b and an additional parameter, h2 > 0, where s2 ¼ 1 þ h2. On the one hand, when b and h2 are estimated jointly, the maximum likelihood estimators are generally inconsistent if the NegBin I assumption fails. On the other hand, if the NegBin I distribution holds, then the NegBin I MLE is more e‰cient than the Poisson QMLE (this conclusion follows from Section 14.5.2). Still, under assumption (19.3), the Poisson QMLE is more e‰cient than an estimator that requires only the conditional mean to be correctly speciﬁed for consistency. On balance, because of its robustness, the Poisson QMLE has the edge over NegBin I for estimating the parameters of the conditional mean. If conditional probabilities need to be estimated, then a more ﬂexible model is probably warranted. Other count data distributions imply a conditional variance other than assumption (19.3). A leading example is the NegBin II model of Cameron and Trivedi (1986). The NegBin II model can be derived from a model of unobserved heterogeneity in a Poisson model. Speciﬁcally, let ci > 0 be unobserved heterogeneity, and assume that yi j xi; ci @ Poisson½cimðxi; bÞ\u0001 If we further assume that ci is independent of xi and has a gamma distribution with unit mean and VarðciÞ ¼ h2, then the distribution of yi given xi can be shown to be negative binomial, with conditional mean and variance Eðyi j xiÞ ¼ mðxi; bÞ; ð19:27Þ Varðyi j xiÞ ¼ E½Varðyi j xi; ciÞ j xi\u0001 þ Var½Eðyi j xi; ciÞ j xi\u0001 ¼ mðxi; bÞ þ h2½mðxi; bÞ\u00012 ð19:28Þ so that the conditional variance of yi given xi is a quadratic in the conditional mean. Because we can write equation (19.28) as Eðyi j xiÞ½1 þ h2Eðyi j xiÞ\u0001, NegBin II also Count Data and Related Models 657", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 665, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p666::c0", "text": "implies overdispersion, but where the amount of overdispersion increases with Eðyi j xiÞ. The log-likelihood function for observation i is liðb; h2Þ ¼ h\u00022 log h\u00022 h\u00022 þ mðxi; bÞ \u0002 \u0003 þ yi log mðxi; bÞ h\u00022 þ mðxi; bÞ \u0002 \u0003 þ log½Gðyi þ h\u00022Þ=Gðh\u00022Þ\u0001 ð19:29Þ where Gð\u0004Þ is the gamma function deﬁned for r > 0 by GðrÞ ¼ Ð y 0 zr\u00021 expð\u0002zÞ dz. You are referred to Cameron and Trivedi (1986) for details. The parameters b and h2 can be jointly estimated using standard maximum likelihood methods. It turns out that, for ﬁxed h2, the log likelihood in equation (19.29) is in the linear exponential family; see GMT (1984a). Therefore, if we ﬁx h2 at any positive value, say h2, and estimate b by maximizing PN i¼1 liðb; h2Þ with respect to b, then the resulting QMLE is consistent under the conditional mean assumption (19.27) only: for ﬁxed h2, the negative binomial QMLE has the same robustness properties as the Poisson QMLE. (Notice that when h2 is ﬁxed, the term involving the gamma func- tion in equation (19.29) does not a¤ect the QMLE.) The structure of the asymptotic variance estimators and test statistics is very simi- lar to the Poisson regression case. Let ^vi ¼ ^mi þ h2 ^m2 i ð19:30Þ be the estimated nominal variance for the given value h2. We simply weight the residuals ^ui and gradient ‘b ^mi by 1= ﬃﬃﬃ^vi p : ~ui ¼ ^ui= ﬃﬃﬃ ^vi p ; ‘b ~mi ¼ ‘b ^mi= ﬃﬃﬃ ^vi p ð19:31Þ For example, under conditions (19.27) and (19.28), a valid estimator of Avarð ^bÞ is X N i¼1 ‘b ^m0 i‘b ^mi=^vi !\u00021 If we drop condition (19.28), the estimator in expression (19.14) should be used but with the standardized residuals and gradients given by equation (19.31). Score sta- tistics are modiﬁed in the same way. When h2 is set to unity, we obtain the geometric QMLE. A better approach is to replace h2 by a ﬁrst-stage estimate, say ^h2, and then estimate b by two-step QMLE. As we discussed in Chapters 12 and 13, sometimes the asymptotic distribution of the ﬁrst-stage estimator needs to be taken into account. A nice feature of the two-step Chapter 19 658", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 666, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p667::c0", "text": "QMLE in this context is that the key condition, assumption (12.37), can be shown to hold under assumption (19.27). Therefore, we can ignore the ﬁrst-stage estimation of h2. Under assumption (19.28), a consistent estimator of h2 is easy to obtain, given an initial estimator of b (such as the Poisson QMLE or the geometric QMLE). Given ^^b^b, form ^^m^mi and ^^u^ui as the usual ﬁtted values and residuals. One consistent estimator of h2 is the coe‰cient on ^^m^m2 i in the regression (through the origin) of ^^u^u2 i \u0002 ^^m^mi on ^^m^m2 i ; this is the estimator suggested by Gourieroux, Monfort, and Trognon (1984b) and Cameron and Trivedi (1986). An alternative estimator of h2, which is closely related to the GLM estimator of s2 suggested in equation (19.15), is a weighted least squares estimate, which can be obtained from the OLS regression ~~u~u2 i \u0002 1 on ^^m^mi, where the ~~u~ui are residuals ^^u^ui weighted by ^^m^m\u00021=2 i . The resulting two-step estimator of b is consistent under assumption (19.7) only, so it is just as robust as the Poisson QMLE. It makes sense to use fully robust standard errors and test statistics. If assumption (19.3) holds, the Poisson QMLE is asymptotically more e‰cient; if assumption (19.28) holds, the two-step negative binomial estimator is more e‰cient. Notice that neither variance assumption contains the other as a special case for all parameter values; see Wool- dridge (1997c) for additional discussion. The variance speciﬁcation tests discussed in Section 19.2.5 can be extended to the negative binomial QMLE; see Wooldridge (1991b). 19.3.2 Binomial Regression Models Sometimes we wish to analyze count data conditional on a known upper bound. For example, Thomas, Strauss, and Henriques (1990) study child mortality within families conditional on number of children ever born. Another example takes the dependent variable, yi, to be the number of adult children in family i who are high school gradu- ates; the known upper bound, ni, is the number of children in family i. By conditioning on ni we are, presumably, treating it as exogenous. Let xi be a set of exogenous variables. A natural starting point is to assume that yi given ðni; xiÞ has a binomial distribution, denoted Binomial ½ni; pðxi; bÞ\u0001, where pðxi; bÞ is a function bounded between zero and one. Usually, yi is viewed as the sum of ni independent Bernoulli (zero-one) random variables, and pðxi; bÞ is the (condi- tional) probability of success on each trial. The binomial assumption is too restrictive for all applications. The presence of an unobserved e¤ect would invalidate the binomial assumption (after the e¤ect is inte- grated out). For example, when yi is the number of children in a family graduating from high school, unobserved family e¤ects may play an important role. Count Data and Related Models 659", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 667, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p668::c0", "text": "As in the case of unbounded support, we assume that the conditional mean is correctly speciﬁed: Eðyi j xi; niÞ ¼ ni pðxi; bÞ 1 miðbÞ ð19:32Þ This formulation ensures that Eðyi j xi; niÞ is between zero and ni. Typically, pðxi; bÞ ¼ GðxibÞ, where Gð\u0004Þ is a cumulative distribution function, such as the standard normal or logistic function. Given a parametric model pðx; bÞ, the binomial quasi–log likelihood for observa- tion i is liðbÞ ¼ yi log½ pðxi; bÞ\u0001 þ ðni \u0002 yiÞ log½1 \u0002 pðxi; bÞ\u0001 ð19:33Þ and the binomial QMLE is obtained by maximizing the sum of liðbÞ over all N observations. From the results of GMT (1984a), the conditional mean parameters are consistently estimated under assumption (19.32) only. This conclusion follows from the general M-estimation results after showing that the true value of b maximizes the expected value of equation (19.33) under assumption (19.32) only. The binomial GLM variance assumption is Varðyi j xi; niÞ ¼ s2ni pðxi; bÞ½1 \u0002 pðxi; bÞ\u0001 ¼ s2viðbÞ ð19:34Þ which generalizes the nominal binomial assumption with s2 ¼ 1. [McCullagh and Nelder (1989, Section 4.5) discuss a model that leads to assumption (19.34) with s2 > 1. But underdispersion is also possible.] Even the GLM assumption can fail if the binary outcomes comprising yi are not independent conditional on ðxi; niÞ. Therefore, it makes sense to use the fully robust asymptotic variance estimator for the binomial QMLE. Owing to the structure of LEF densities, and given our earlier analysis of the Poisson and negative binomial cases, it is straightforward to describe the econometric analysis for the binomial QMLE: simply take ^mi 1 ni pðxi; ^bÞ, ^ui 1 yi \u0002 ^mi, ‘b ^mi 1 ni‘b^pi, and ^vi 1 ni^pið1 \u0002 ^piÞ in equations (19.31). An estimator of s2 under assump- tion (19.34) is also easily obtained: replace ^mi in equation (19.15) with ^vi. The struc- ture of asymptotic variances and score tests is identical. 19.4 Other QMLES in the Linear Exponential Family Sometimes we want to use a quasi-MLE analysis for other kinds of response vari- ables. We will consider two here. The exponential regression model is well suited to strictly positive, roughly continuous responses. Fractional logit regression can be used when the response variable takes on values in the unit interval. Chapter 19 660", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 668, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p669::c0", "text": "19.4.1 Exponential Regression Models Just as in the Poisson regression model, in an exponential regression model we specify a conditional mean function, mðx; bÞ. However, we now use the exponential quasi– log likelihood function, liðbÞ ¼ \u0002yi=mðxi; bÞ \u0002 log½mðxi; bÞ\u0001. [The ‘‘exponential’’ in ‘‘exponential regression model’’ refers to the quasi-likelihood used, and not to the mean function mðx; bÞ.] The most popular choice of mðx; bÞ happens to be expðxbÞ. The results of GMT (1984a) imply that, provided the conditional mean is correctly speciﬁed, the exponential QMLE consistently estimates the conditional mean param- eters. Thus the exponential QMLE enjoys the same robustness properties as the Poisson QMLE. The GLM variance assumption for exponential regression is Varðy j xÞ ¼ s2½Eðy j xÞ\u00012 ð19:35Þ When s2 ¼ 1, assumption (19.35) gives the variance-mean relationship for the expo- nential distribution. Under assumption (19.35), s is the coe‰cient of variation: it is the ratio of the conditional standard deviation of y to its conditional mean. Whether or not assumption (19.35) holds, an asymptotic variance matrix can be estimated. The fully robust form is expression (19.14), but, in deﬁning the score and expected Hessian, the residuals and gradients are weighted by 1= ^mi rather than ^m\u00021=2 i . Under assumption (19.35), a valid estimator is ^s2 X N i¼1 ‘b ^m0 i‘b ^mi=^vi !\u00021 where ^s2 ¼ N\u00021 PN i¼1 ^u2 i = ^m2 i and ^vi ¼ ^m2 i . Score tests and quasi–likelihood ratio tests can be computed just as in the Poisson case. Most statistical packages implement exponential regression with an exponential mean function; it is sometimes called the gamma regression model because the exponential distribution is a special case of the gamma distribution. 19.4.2 Fractional Logit Regression Quasi-likelihood methods are also available when y is a variable restricted to the unit interval, ½0; 1\u0001. fBy rescaling, we can cover the case where y is restricted to the in- terval ½a; b\u0001 for known constants a < b. The transformation is ðy \u0002 aÞ=ðb \u0002 aÞ.g Examples include fraction of income contributed to charity, fraction of weekly hours spent working, proportion of a ﬁrm’s total capitalization accounted for by debt cap- ital, and high school graduation rates. In some cases, each yi might be obtained by dividing a count variable by an upper bound, ni. Count Data and Related Models 661", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 669, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p670::c0", "text": "Given explanatory variables x, a linear model for Eðy j xÞ has the same strengths and weaknesses as the linear probability model for binary y. When y is strictly be- tween zero and one, a popular alternative is to assume that the log-odds transforma- tion, log½y=ð1 \u0002 yÞ\u0001, has a conditional expectation of the form xb. The motivation for using log½y=ð1 \u0002 yÞ\u0001 as a dependent variable in a linear model is that log½y=ð1 \u0002 yÞ\u0001 ranges over all real values as y ranges between zero and one. This approach leads to estimation of b by OLS. Unfortunately, using the log-odds transformation has two drawbacks. First, it cannot be used directly if y takes on the boundary values, zero and one. While we can always use adjustments for the boundary values, such adjustments are necessarily arbitrary. Second, even if y is strictly inside the unit in- terval, b is di‰cult to interpret: without further assumptions, it is not possible to re- cover an estimate of Eðy j xÞ, and with further assumptions, it is still nontrivial to estimate Eðy j xÞ. See Papke and Wooldridge (1996) and Problem 19.8 for further discussion. An approach that avoids both these problems is to model Eðy j xÞ as a logistic function: Eðy j xÞ ¼ expðxbÞ=½1 þ expðxbÞ\u0001 ð19:36Þ This model ensures that predicted values for y are in ð0; 1Þ and that the e¤ect of any xj on Eðy j xÞ diminishes as xb ! y. Just as in the binary logit model, qEðy j xÞ=qxj ¼ bjgðxbÞ, where gðzÞ ¼ expðzÞ=½1 þ expðzÞ\u00012. In applications, the partial e¤ects should be evaluated at the ^bj and interesting values of x. Plugging in the sample averages, x, makes the partial e¤ects from equation (19.36) roughly comparable to the coe‰cients from a linear regression for Eðy j xÞ: ^gj A ^bjgðx^bÞ, where the ^gj are the OLS estimates from the linear regression of y on x. Given equation (19.36), one approach to estimating b is nonlinear least squares, as we discussed in Chapter 12. However, the assumption that implies relative e‰ciency of NLS—namely, Varðy j xÞ ¼ s2—is unlikely to hold for fractional y. A method that is just as robust [in the sense that it consistently estimates b under assumption (19.36) only] is quasi-MLE, where the quasi-likelihood function is the binary choice log likelihood. Therefore, quasi–log likelihood for observation i is exactly as in equation (15.17) [with Gð\u0004Þ the logistic function], although yi can be any value in ½0; 1\u0001. The mechanics of obtaining ^b are identical to the binary response case. Inference is complicated by the fact that the binary response density cannot be the actual density of y given x. Generally, a fully robust variance matrix estimator and test statistics should be obtained. These are gotten by applying the formulas for the binomial case with ni 1 1 and pðx; bÞ 1 expðxbÞ=½1 þ expðxbÞ\u0001. The GLM assump- tion for fractional logit regression is given in assumption (19.34) with ni ¼ 1. See Chapter 19 662", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 670, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p671::c0", "text": "Papke and Wooldridge (1996) for more details, as well as suggestions for speciﬁca- tion tests and for an application to participation rates in 401(k) pension plans. 19.5 Endogeneity and Sample Selection with an Exponential Regression Function With all of the previous models, standard econometric problems can arise. In this section, we will study two of the problems when the regression function for y has an exponential form: endogeneity of an explanatory variable and incidental truncation. We follow the methods in Wooldridge (1997c), which are closely related to those suggested by Terza (1998). Gurmu and Trivedi (1994) and the references therein dis- cuss the problems of data censoring, truncation, and two-tier or hurdle models. 19.5.1 Endogeneity We approach the problem of endogenous explanatory variables from an omitted variables perspective. Let y1 be the nonnegative, in principle unbounded variable to be explained, and let z and y2 be observable explanatory variables (of dimension 1 \u0003 L and 1 \u0003 G1, respectively). Let c1 be an unobserved latent variable (or unob- served heterogeneity). We assume that the (structural) model of interest is an omitted variables model of exponential form, written in the population as Eðy1 j z; y2; c1Þ ¼ expðz1d1 þ y2g1 þ c1Þ ð19:37Þ where z1 is a 1 \u0003 L1 subset of z containing unity; thus, the model (19.37) incorporates some exclusion restrictions. On the one hand, the elements in z are assumed to be exogenous in the sense that they are independent of c1. On the other hand, y2 and c1 are allowed to be correlated, so that y2 is potentially endogenous. To use a quasi-likelihood approach, we assume that y2 has a linear reduced form satisfying certain assumptions. Write y2 ¼ zP2 þ v2 ð19:38Þ where P2 is an L \u0003 G1 matrix of reduced form parameters and v2 is a 1 \u0003 G1 vector of reduced form errors. We assume that the rank condition for identiﬁcation holds, which requires the order condition L \u0002 L1 b G1. In addition, we assume that ðc1; v2Þ is independent of z, and that c1 ¼ v2r1 þ e1 ð19:39Þ where e1 is independent of v2 (and necessarily of z). (We could relax the independence assumptions to some degree, but we cannot just assume that v2 is uncorrelated with z Count Data and Related Models 663", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 671, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p672::c0", "text": "and that e1 is uncorrelated with v2.) It is natural to assume that v2 has zero mean, but it is convenient to assume that E½expðe1Þ\u0001 ¼ 1 rather than Eðe1Þ ¼ 0. This assumption is without loss of generality whenever a constant appears in z1, which should almost always be the case. If ðc1; v2Þ has a multivariate normal distribution, then the representation in equa- tion (19.39) under the stated assumptions always holds. We could also extend equa- tion (19.39) by putting other functions of v2 on the right-hand side, such as squares and cross products, but we do not show these explicitly. Note that y2 is exogenous if and only if r1 ¼ 0. Under the maintained assumptions, we have Eðy1 j z; y2; v2Þ ¼ expðz1d1 þ y2g1 þ v2r1Þ ð19:40Þ and this equation suggests a strategy for consistently estimating d1, g1, and r1. If v2 were observed, we could simply use this regression function in one of the QMLE earlier methods (for example, Poisson, two-step negative binomial, or exponential). Because these methods consistently estimate correctly speciﬁed conditional means, we can immediately conclude that the QMLEs would be consistent. [If y1 conditional on ðz; y2; c1Þ has a Poisson distribution with mean in equation (19.37), then the dis- tribution of y1 given ðz; y2; v2Þ has overdispersion of the type (19.28), so the two-step negative binomial estimator might be preferred in this context.] To operationalize this procedure, the unknown quantities v2 must be replaced with estimates. Let ^P2 be the L \u0003 G1 matrix of OLS estimates from the ﬁrst-stage esti- mation of equation (19.38); these are consistent estimates of P2. Deﬁne ^v2 ¼ y2 \u0002 z ^P2 (where the observation subscript is suppressed). Then estimate the exponential regression model using regressors ðz1; y2;^v2Þ by one of the QMLEs. The estimates ð^d1; ^g1; ^r1Þ from this procedure are consistent using standard arguments from two- step estimation in Chapter 12. This method is similar in spirit to the methods we saw for binary response (Chapter 15) and censored regression models (Chapter 16). There is one di¤erence: here, we do not need to make distributional assumptions about y1 or y2. However, we do assume that the reduced-form errors v2 are independent of z. In addition, we assume that c1 and v2 are linearly related with e1 in equation (19.39) independent of v2. Later we will show how to relax these assumptions using a method of moments approach. Because ^v2 depends on ^P2, the variance matrix estimators for ^d1, ^g1, and ^r1 should generally be adjusted to account for this dependence, as described in Sections 12.5.2 and 14.1. Using the results from Section 12.5.2, it can be shown that estimation of P2 does not a¤ect the asymptotic variance of the QMLEs when r1 ¼ 0, just as we saw Chapter 19 664", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 672, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p673::c0", "text": "when testing for endogeneity in probit and Tobit models. Therefore, testing for endogeneity of y2 is relatively straightforward: simply test H0: r1 ¼ 0 using a Wald or LM statistic. When G1 ¼ 1, the most convenient statistic is probably the t statistic on ^v2, with the fully robust form being the most preferred (but the GLM form is also useful). The LM test for omitted variables is convenient when G1 > 1 because it can be computed after estimating the null model ðr1 ¼ 0Þ and then doing a variable ad- dition test for ^v2. The test has G1 degrees of freedom in the chi-square distribution. There is a ﬁnal comment worth making about this test. The null hypothesis is the same as Eðy1 j z; y2Þ ¼ expðz1d1 þ y2g1Þ. The test for endogeneity of y2 simply looks for whether a particular linear combination of y2 and z appears in this conditional expectation. For the purposes of getting a limiting chi-square distribution, it does not matter where the linear combination ^v2 comes from. In other words, under the null hypothesis none of the assumptions we made about ðc1; v2Þ need to hold: v2 need not be independent of z, and e1 in equation (19.39) need not be independent of v2. Therefore, as a test, this procedure is very robust, and it can be applied when y2 contains binary, count, or other discrete variables. Unfortunately, if y2 is endoge- nous, the correction does not work without something like the assumptions made previously. Example 19.2 (Is Education Endogenous in the Fertility Equation?): We test for endogeneity of educ in Example 19.1. The IV for educ is a binary indicator for whether the woman was born in the ﬁrst half of the year ( frsthalf ), which we assume is ex- ogenous in the fertility equation. In the reduced-form equation for educ, the coe‰- cient on frsthalf is \u0002:636 (se ¼ :104), and so there is a signiﬁcant negative partial relationship between years of schooling and being born in the ﬁrst half of the year. When we add the ﬁrst-stage residuals, ^v2, to the Poisson regression, its coe‰cient is .025, and its GLM standard error is .028. Therefore, there is little evidence against the null hypothesis that educ is exogenous. The coe‰cient on educ actually becomes larger in magnitude ð\u0002:046Þ, but it is much less precisely estimated. Mullahy (1997) has shown how to estimate exponential models when some ex- planatory variables are endogenous without making assumptions about the reduced form of y2. This approach is especially attractive for dummy endogenous and other discrete explanatory variables, where the linearity in equation (19.39) coupled with independence of z and v2 is unrealistic. To sketch Mullahy’s approach, write x1 ¼ ðz1; y2Þ and b1 ¼ ðd0 1; g0 1Þ0. Then, under the model (19.37), we can write y1 expð\u0002x1b1Þ ¼ expðc1Þa1; Eða1 j z; y2; c1Þ ¼ 1 ð19:41Þ Count Data and Related Models 665", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 673, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p674::c0", "text": "If we assume that c1 is independent of z—a standard assumption concerning unobserved heterogeneity and exogenous variables—and use the normalization E½expðc1Þ\u0001 ¼ 1, we have the conditional moment restriction E½ y1 expð\u0002x1b1Þ j z\u0001 ¼ 1 ð19:42Þ Because y1, x1, and z are all observable, condition (19.42) can be used as the basis for generalized method of moments estimation. The function gðy1; y2; z1; b1Þ 1 y1 expð\u0002x1b1Þ \u0002 1, which depends on observable data and the parameters, is uncor- related with any function of z (at the true value of b1). GMM estimation can be used as in Section 14.2 once a vector of instrumental variables has been chosen. An important feature of Mullahy’s approach is that no assumptions, other than the standard rank condition for identiﬁcation in nonlinear models, are made about the distribution of y2 given z: we need not assume the existence of a linear reduced form for y2 with errors independent of z. Mullahy’s procedure is computationally more di‰cult, and testing for endogeneity in his framework is harder than in the QMLE approach. Therefore, we might ﬁrst use the two-step quasi-likelihood method pro- posed earlier for testing, and if endogeneity seems to be important, Mullahy’s GMM estimator can be implemented. See Mullahy (1997) for details and an empirical example. 19.5.2 Sample Selection It is also possible to test and correct for sample selection in exponential regression models. The case where selection is determined by the dependent variable being above or below a known threshold requires full maximum likelihood methods using a truncated count distribution; you are referred to the book by Cameron and Trivedi (1998). Here, we assume that sample selection is related to an unobservable in the population model Eðy1 j x; c1Þ ¼ expðx1b1 þ c1Þ ð19:43Þ where x1 is a 1 \u0003 K1 vector of exogenous variables containing a constant, and c1 is an unobserved random variable. The full set of exogenous variables is x, and c1 is inde- pendent of x. Therefore, if a random sample on ðx1; y1Þ were available, b1 could be consistently estimated by a Poisson regression of y1 on x1 (or by some other QMLE) under the normalization E½expðc1Þ\u0001 ¼ 1. A sample selection problem arises when a random sample on ðx1; y1Þ from the relevant population is not available. Let y2 denote a binary selection indicator, which is unity if ðx1; y1Þ is observed and zero otherwise. We assume that y2 is determined by y2 ¼ 1½x2d2 þ v2 > 0\u0001, where 1½\u0004\u0001 is the indicator function, x2 is a subset of x (typi- Chapter 19 666", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 674, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p675::c0", "text": "cally, x2 ¼ x), and v2 is unobserved. This is a standard sample selection mechanism, where y2 and x2 must be observable for all units in the population. In this setting, sample selection bias arises when v2 is correlated with c1. In partic- ular, if we write equation (19.43) with a multiplicative error, y1 ¼ expðx1b1 þ c1Þa1, with Eða1 j x; c1Þ ¼ 1 by deﬁnition, we also assume that Eða1 j x; c1; v2Þ ¼ Eða1Þ ¼ 1. In other words, selection may be correlated with c1 but not a1. This model is similar to the linear model with sample selection in Section 17.4.1 where the error in the regression equation can be decomposed into two parts, one that is correlated with v2 ðc1Þ and one that is not ða1Þ. To derive a simple correction, assume that ðc1; v2Þ is independent of x and bivari- ate normal with zero mean; v2 also has a unit variance, so that y2 given x follows a probit model. These assumptions imply that E½expðc1Þ j x; v2\u0001 ¼ E½expðc1Þ j v2\u0001 ¼ expðr0 þ r1v2Þ for parameters r0 and r1. Provided x1 contains a constant, we can use the normalization expðr0Þ ¼ 1, and we do so in what follows. Then Eðy1 j x; v2Þ ¼ expðx1b1 þ r1v2Þ, and so by iterated expectations, Eðy1 j x; y2 ¼ 1Þ ¼ expðx1b1Þgðx2d2; r1Þ ð19:44Þ where gðx2d2; r1Þ 1 E½expðr1v2Þ j v2 > \u0002x2d2\u0001. By integrating the function expðr1v2Þ against the truncated standard normal density conditional on v2 > \u0002x2d2, it can be shown that gðx2d2; r1Þ ¼ expðr2 1ÞFðr1 þ x2d2Þ=Fðx2d2Þ, where Fð\u0004Þ is the standard normal cdf. Given equation (19.44), we can apply a two-step method similar to Heckman’s (1976) method for linear models that we covered in Chapter 17. First, run a probit of y2 on x2 using the entire sample. Let ^d2 be the probit estimator of d2. Next, on the selected subsample for which ðy1; x1Þ is observed, use a QMLE analysis with condi- tional mean function expðx1b1Þgðx2 ^d2; r1Þ to estimate b1 and r1. If r1 0 0, then, as usual, the asymptotic variance of ^b1 and ^r1 should be adjusted for estimation of d2. Testing r1 ¼ 0 is simple if we use the robust score test. This requires the derivative of the mean function with respect to r1, evaluated at r1 ¼ 0. But qgðx2d2; 0Þ=qr1 ¼ lðx2d2Þ, where lð\u0004Þ is the usual inverse Mills ratio that appears in linear sample selection contexts. Thus the derivative of the mean function with respect to r1, eval- uated at all estimates under the null, is simply expðx1 ^b1Þlðx2 ^d2Þ. This result gives the following procedure to test for sample selection: (1) let ^b1 be a QMLE (for example, the Poisson) using the selected sample, and deﬁne ^yi1 1 expðxi1 ^b1Þ, ^ui1 1 yi1 \u0002 ^yi1, and ~ui1 1 ^ui1= ﬃﬃﬃﬃﬃ ^yi1 p for all i in the selected sample; (2) obtain ^d2 from the probit of y2 onto x2, using the entire sample; denote the estimated inverse Mills ratio for each observation i by ^li2; and (3) regress ~ui1 onto ﬃﬃﬃﬃﬃ ^yi1 p xi1, ﬃﬃﬃﬃﬃ ^yi1 p ^li2 using the selected sample, and use N1R2 u as asymptotically w2 1, where N1 is the number of observations Count Data and Related Models 667", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 675, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p676::c0", "text": "in the selected sample. This approach assumes that the GLM assumption holds under H0. For the fully robust test, ﬁrst regress ﬃﬃﬃﬃﬃ ^yi1 p ^li2 onto ﬃﬃﬃﬃﬃ ^yi1 p xi1 using the selected sample and save the residuals, ~ri1; then regress 1 on ~ui1~ri1, i ¼ 1; 2; . . . ; N1, and use N1 \u0002 SSR as asymptotically w2 1. 19.6 Panel Data Methods In this ﬁnal section, we discuss estimation of panel data models, primarily focusing on count data. Our main interest is in models that contain unobserved e¤ects, but we initially cover pooled estimation when the model does not explicitly contain an unobserved e¤ect. The pioneering work in unobserved e¤ects count data models was done by Haus- man, Hall, and Griliches (1984) (HHG), who were interested in explaining patent applications by ﬁrms in terms of spending on research and development. HHG devel- oped random and ﬁxed e¤ects models under full distributional assumptions. Wool- dridge (1999a) has shown that one of the approaches suggested by HHG, which is typically called the ﬁxed e¤ects Poisson model, has some nice robustness properties. We will study those here. Other count panel data applications include (with response variable in parentheses) Rose (1990) (number of airline accidents), Papke (1991) (number of ﬁrm births in an industry), Downes and Greenstein (1996) (number of private schools in a public school district), and Page (1995) (number of housing units shown to individuals). The time series dimension in each of these studies allows us to control for unobserved het- erogeneity in the cross section units, and to estimate certain dynamic relationships. As with the rest of the book, we explicitly consider the case with N large relative to T, as the asymptotics hold with T ﬁxed and N ! y. 19.6.1 Pooled QMLE As with the linear case, we begin by discussing pooled estimation after specifying a model for a conditional mean. Let fðxt; ytÞ: t ¼ 1; 2; . . . ; Tg denote the time series observations for a random draw from the cross section population. We assume that, for some bo A B, Eðyt j xtÞ ¼ mðxt; boÞ; t ¼ 1; 2; . . . ; T ð19:45Þ This assumption simply means that we have a correctly speciﬁed parametric model for Eðyt j xtÞ. For notational convenience only, we assume that the function m itself does not change over time. Relaxing this assumption just requires a notational Chapter 19 668", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 676, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p677::c0", "text": "change, or we can include time dummies in xt. For yt b 0 and unbounded from above, the most common conditional mean is expðxtbÞ. There is no restriction on the time dependence of the observations under assumption (19.45), and xt can contain any observed variables. For example, a static model has xt ¼ zt, where zt is dated contemporaneously with yt. A ﬁnite distributed lag has xt containing lags of zt. Strict exogeneity of ðx1; . . . ; xTÞ, that is, Eðyt j x1; . . . ; xTÞ ¼ Eðyt j xtÞ, is not assumed. In particular, xt can contain lagged dependent variables, although how these might ap- pear in nonlinear models is not obvious (see Wooldridge, 1997c, for some possibil- ities). A limitation of model (19.45) is that it does not explicitly incorporate an unobserved e¤ect. For each i ¼ 1; 2; . . . ; N; fðxit; yitÞ: t ¼ 1; 2; . . . ; Tg denotes the time series obser- vations for cross section unit i. We assume random sampling from the cross section. One approach to estimating bo is pooled nonlinear least squares, which was intro- duced in Problem 12.6. When y is a count variable, a Poisson QMLE can be used. This approach is completely analogous to pooled probit and pooled Tobit estimation with panel data. Note, however, that we are not assuming that the Poisson distribu- tion is true. For each i the quasi–log likelihood for pooled Poisson estimation is (up to additive constants) liðbÞ ¼ X T t¼1 fyit log½mðxit; bÞ\u0001 \u0002 mðxit; bÞg 1 X T t¼1 litðbÞ ð19:46Þ The pooled Poisson QMLE then maximizes the sum of liðbÞ across i ¼ 1; . . . ; N. Consistency and asymptotic normality of this estimator follows from the Chapter 12 results, once we use the fact that bo maximizes E½liðbÞ\u0001; this follows from GMT (1984a). Thus pooled Poisson estimation is robust in the sense that it consistently estimates bo under assumption (19.45) only. Without further assumptions we must be careful in estimating the asymptotic variance of ^b. Let siðbÞ be the P \u0003 1 score of liðbÞ, which can be written as siðbÞ ¼ PT t¼1 sitðbÞ, where sitðbÞ is the score of litðbÞ; each sitðbÞ has the form (19.12) but with ðxit; yitÞ in place of ðxi; yiÞ. The asymptotic variance of ﬃﬃﬃﬃ N p ð ^b \u0002 boÞ has the usual form A\u00021 o BoA\u00021 o , where Ao 1 PT t¼1 E½‘bmitðboÞ0‘bmitðboÞ=mitðboÞ\u0001 and Bo 1 E½siðboÞsiðboÞ0\u0001. Consistent estimators are ^A ¼ N\u00021 X N i¼1 X T t¼1 ‘b ^m0 it‘b ^mit= ^mit ð19:47Þ Count Data and Related Models 669", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 677, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p678::c0", "text": "^B ¼ N\u00021 X N i¼1 sið ^bÞsið ^bÞ0 ð19:48Þ and we can use ^A\u00021^B^A\u00021=N for Av^arð ^bÞ. This procedure is fully robust to the pres- ence of serial correlation in the score and arbitrary conditional variances. It should be used in the construction of standard errors and Wald statistics. The quasi-LR statistic is not usually valid in this setup because of neglected time dependence and possible violations of the Poisson variance assumption. If the conditional mean is dynamically complete in the sense that Eðyt j xt; yt\u00021; xt\u00021; . . . ; y1; x1Þ ¼ Eðyt j xtÞ ð19:49Þ then fsitðboÞ: t ¼ 1; 2; . . . ; Tg is serially uncorrelated. Consequently, under assump- tion (19.49), a consistent estimator of B is ^B ¼ N\u00021 X N i¼1 X T t¼1 sitð ^bÞsitð ^bÞ0 ð19:50Þ Using this equation along with ^A produces the asymptotic variance that results from treating the observations as one long cross section, but without the Poisson or GLM variance assumptions. Thus, equation (19.50) a¤ords a certain amount of robustness, but it requires the dynamic completeness assumption (19.49). There are many other possibilities. If we impose the GLM assumption Varðyit j xitÞ ¼ s2 omðxit; boÞ; t ¼ 1; 2; . . . ; T ð19:51Þ along with dynamic completeness, then Avarð ^bÞ can be estimated by ^s2 X N i¼1 X T t¼1 ‘b ^m0 it‘b ^mit= ^mit !\u00021 ð19:52Þ where ^s2 ¼ ðNT \u0002 PÞ\u00021 PN i¼1 PT t¼1 ~u2 it, ~uit ¼ ^uit= ﬃﬃﬃﬃﬃﬃ ^mit p , and ^uit ¼ yit \u0002 mitð ^bÞ. This estimator results in a standard GLM analysis on the pooled data. 19.6.2 Specifying Models of Conditional Expectations with Unobserved E¤ects We now turn to models that explicitly contain an unobserved e¤ect. The issues that arise here are similar to those that arose in linear panel data models. First, we must know whether the explanatory variables are strictly exogenous conditional on an unobserved e¤ect. Second, we must decide how the unobserved e¤ect should appear in the conditional mean. Chapter 19 670", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 678, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p679::c0", "text": "Given conditioning variables xt, strict exogeneity conditional on the unobserved e¤ect c is deﬁned just as in the linear case: Eðyt j x1; . . . ; xT; cÞ ¼ Eðyt j xt; cÞ ð19:53Þ As always, this deﬁnition rules out lagged values of y in xt, and it can rule out feed- back from yt to future explanatory variables. In static models, where xt ¼ zt for variables zt dated contemporaneously with yt, assumption (19.53) implies that neither past nor future values of z a¤ect the expected value of yt, once zt and c have been controlled for. This can be too restrictive, but it is often the starting point for ana- lyzing static models. A ﬁnite distributed lag relationship assumes that Eðyt j zt; zt\u00021; . . . ; z1; cÞ ¼ Eðyt j zt; zt\u00021; . . . ; zt\u0002Q; cÞ; t > Q ð19:54Þ where Q is the length of the distributed lag. Under assumption (19.54), the strict exogeneity assumption conditional on c becomes Eðyt j z1; z2; . . . ; zT; cÞ ¼ Eðyt j z1; . . . ; zt; cÞ ð19:55Þ which is less restrictive than in the purely static model because lags of zt explicitly appear in the model; it still rules out general feedback from yt to ðztþ1; . . . ; zTÞ. With count variables, a multiplicative unobserved e¤ect is an attractive functional form: Eðyt j xt; cÞ ¼ c \u0004 mðxt; boÞ ð19:56Þ where mðxt; bÞ is a parametric function known up to the P \u0003 1 vector of parameters bo. Equation (19.56) implies that the partial e¤ect of xtj on log Eðyt j xt; cÞ does not depend on the unobserved e¤ect c. Thus quantities such as elasticities and semi- elasticities depend only on xt and bo. The most popular special case is the exponential model Eðyt j xt; aÞ ¼ expða þ xtbÞ, which is obtained by taking c ¼ expðaÞ. 19.6.3 Random E¤ects Methods A multiplicative random e¤ects model maintains, at a minimum, two assumptions for a random draw i from the population: Eðyit j xi1; . . . ; xiT; ciÞ ¼ cimðxit; boÞ; t ¼ 1; 2; . . . ; T ð19:57Þ Eðci j xi1; . . . ; xiTÞ ¼ EðciÞ ¼ 1 ð19:58Þ where ci is the unobserved, time-constant e¤ect, and the observed explanatory vari- ables, xit, may be time constant or time varying. Assumption (19.57) is the strict Count Data and Related Models 671", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 679, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p680::c0", "text": "exogeneity assumption of the xit conditional on ci, combined with a regression func- tion multiplicative in ci. When yit b 0, such as with a count variable, the most pop- ular choice of the parametric regression function is mðxt; bÞ ¼ expðxtbÞ, in which case xit would typically contain a full set of time dummies. Assumption (19.58) says that the unobserved e¤ect, ci, is mean independent of xi; we normalize the mean to be one, a step which is without loss of generality for common choices of m, including the exponential function with unity in xt. Under assumptions (19.57) and (19.58), we can ‘‘integrate out’’ ci by using the law of iterated expectations: Eðyit j xiÞ ¼ Eðyit j xitÞ ¼ mðxit; boÞ; t ¼ 1; 2; . . . ; T ð19:59Þ Equation (19.59) shows that bo can be consistently estimated by the pooled Poisson method discussed in Section 19.6.1. The robust variance matrix estimator that allows for an arbitrary conditional variance and serial correlation produces valid inference. Just as in a linear random e¤ects model, the presence of the unobserved heterogeneity causes the yit to be correlated over time, conditional on xi. When we introduce an unobserved e¤ect explicitly, a random e¤ects analysis typi- cally accounts for the overdispersion and serial dependence implied by assumptions (19.57) and (19.58). For count data, the Poisson random e¤ects model is given by yit j xi; ci @ Poisson½cimðxit; boÞ\u0001 ð19:60Þ yit; yir are independent conditional on xi; ci; t 0 r ð19:61Þ ci is independent of xi and distributed as Gammaðdo; doÞ ð19:62Þ where we parameterize the gamma distribution so that EðciÞ ¼ 1 and VarðciÞ ¼ 1=do 1 h2 o. While Varðyit j xi; ciÞ ¼ Eðyit j xi; ciÞ under assumption (19.60), by equa- tion (19.28), Varðyit j xiÞ ¼ Eðyit j xiÞ½1 þ h2 oEðyit j xiÞ\u0001, and so assumptions (19.60) and (19.62) imply overdispersion in Varðyit j xiÞ. Although other distributional assumptions for ci can be used, the gamma distribution leads to a tractable density for ðyi1; . . . ; yiTÞ given xi, which is obtained after ci has been integrated out. (See HHG, p. 917, and Problem 19.11.) Maximum likelihood analysis (conditional on xi) is relatively straightforward and is implemented by some econometrics packages. If assumptions (19.60), (19.61), and (19.62) all hold, the conditional MLE is e‰- cient among all estimators that do not use information on the distribution of xi; see Section 14.5.2. The main drawback with the random e¤ects Poisson model is that it is sensitive to violations of the maintained assumptions, any of which could be false. (Problem 19.5 covers some ways to allow ci and xi to be correlated, but they still rely on stronger assumptions than the ﬁxed e¤ects Poisson estimator that we cover in Section 19.6.4.) Chapter 19 672", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 680, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p681::c0", "text": "A quasi-MLE random e¤ects analysis keeps some of the key features of assump- tions (19.60)–(19.62) but produces consistent estimators under just the conditional mean assumptions (19.57) and (19.58). Nominally, we maintain assumptions (19.60)– (19.62). Deﬁne uit 1 yit \u0002 Eðyit j xitÞ ¼ yit \u0002 mðxit; bÞ. Then we can write uit ¼ cimitðboÞ þ eit \u0002 mitðboÞ ¼ eit þ mitðboÞðci \u0002 1Þ, where eit 1 yit \u0002 Eðyit j xit; ciÞ. As we showed in Section 19.3.1, Eðu2 it j xiÞ ¼ mitðboÞ þ h2 om2 itðboÞ ð19:63Þ Further, for t 0 r, Eðuituir j xiÞ ¼ E½ðci \u0002 1Þ2\u0001mitðboÞmirðboÞ ¼ h2 omitðboÞmirðboÞ ð19:64Þ where h2 o ¼ VarðciÞ. The serial correlation in equation (19.64) is reminiscent of the serial correlation that arises in linear random e¤ects models under standard assump- tions. This shows explicitly that we must correct for serial dependence in computing the asymptotic variance of the pooled Poisson QMLE in Section 19.6.1. The over- dispersion in equation (19.63) is analogous to the variance of the composite error in a linear model. A QMLE random e¤ects analysis exploits these nominal variance and covariance expressions but does not rely on either of them for consistency. If we use equation (19.63) while ignoring equation (19.64), we are led to a pooled negative binomial analysis, which is very similar to the pooled Poisson analysis except that the quasi–log likelihood for each time period is the negative binomial discussed in Sec- tion 19.3.1. See Wooldridge (1997c) for details. If assumption (19.64) holds, it is more e‰cient—possibly much more e‰cient—to use this information. Multivariate weighted nonlinear least squares (MWNLS)—see Problem 12.10—can be used for these purposes. (See GMT, 1984b, for an applica- tion to a related model.) The MWNLS estimator in this context is essentially the same as the generalized estimating equation (GEE) approach of Zeger, Liang, and Albert (1988). In the GEE literature, the estimated model is called the population- averaged model (see Section 15.8.2 for the binary response case). For multiplicative models where ci and xi are independent, the distinction is unimportant. The MWNLS estimator solves the problem min b X N i¼1 ½yi \u0002 miðbÞ\u00010 ^W\u00021 i ½yi \u0002 miðbÞ\u0001 where ^Wi is the T \u0003 T matrix with the elements from equation (19.63) down its di- agonal and the elements from equation (19.64) as its o¤-diagonals; bo is replaced with Count Data and Related Models 673", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 681, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p682::c0", "text": "the pooled Poisson estimate, and h2 o is replaced by the estimate from the pooled regression ~u2 it \u0002 1 on ^mit, t ¼ 1; . . . ; T, i ¼ 1; . . . ; N, where ~uit ¼ ðyit \u0002 ^mitÞ= ﬃﬃﬃﬃﬃﬃ ^mit p is the standardized residual. Under assumptions (19.63) and (19.64), the MWNLS esti- mator is relatively e‰cient among estimators that only require a correct conditional mean for consistency, and its asymptotic variance can be estimated as Av^arð ^bÞ ¼ X N i¼1 ‘b ^m0 i ^W\u00021 i ‘b ^mi !\u00021 As with the other QMLEs, the WNLS estimator is consistent under assumptions (19.57) and (19.58) only, but if assumption (19.63) or (19.64) is violated, the variance matrix needs to be made robust. Letting ^ui 1 yi \u0002 mið ^bÞ (a T \u0003 1 vector), the robust estimator is X N i¼1 ‘b ^m0 i ^W\u00021 i ‘b ^mi !\u00021 X N i¼1 ‘b ^m0 i ^W\u00021 i ^ui^u0 i ^W\u00021 i ‘b ^mi ! X N i¼1 ‘b ^m0 i ^W\u00021 i ‘b ^mi !\u00021 This expression gives a way to obtain fully robust inference while having a relatively e‰cient estimator under the random e¤ects assumptions (19.63) and (19.64). GMT (1984b) cover a model that suggests an alternative form of ^Wi. The matrix ^Wi can be modiﬁed for other nominal distributional assumptions, such as the expo- nential (which would be natural to apply to continuous, nonnegative yit.) Any WNLS method is more robust than a fully parametric maximum likelihood analysis, such as that in assumptions (19.60)–(19.62). We must be aware that none of these methods produces consistent estimators if either the strict exogeneity assumption fails or Eðci j xiÞ depends on xi. 19.6.4 Fixed E¤ects Poisson Estimation HHG ﬁrst showed how to do a ﬁxed-e¤ect-type analysis of count panel data models, which allows for arbitrary dependence between ci and xi. Their ﬁxed e¤ects Poisson assumptions are (19.60) and (19.61), with the conditional mean given still by as- sumption (19.57). The key is that neither assumption (19.62) nor assumption (19.58) is maintained; in other words, arbitrary dependence between ci and xi is allowed. HHG take mðxit; bÞ ¼ expðxitbÞ, which is by far the leading case. HHG use Andersen’s (1970) conditional ML methodology to estimate b. Let ni 1 PT t¼1 yit denote the sum across time of the counts across t. Using standard results on obtaining a joint distribution conditional on the sum of its components, HHG show that Chapter 19 674", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 682, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p683::c0", "text": "yi j ni; xi; ci @ Multinomialfni; p1ðxi; boÞ; . . . ; pTðxi; boÞg ð19:65Þ where ptðxi; bÞ 1 mðxit; bÞ= X T r¼1 mðxir; bÞ \" # ð19:66Þ Because this distribution does not depend on ci, equation (19.65) is also the distribu- tion of yi conditional on ni and xi. Therefore, bo can be estimated by standard con- ditional MLE techniques using the multinomial log likelihood. The conditional log likelihood for observation i, apart from terms not depending on b, is liðbÞ ¼ X T t¼1 yit log½ ptðxi; bÞ\u0001 ð19:67Þ The estimator ^b that maximizes PN i¼1 liðbÞ will be called the ﬁxed e¤ects Poisson (FEP) estimator. (Note that when yit ¼ 0 for all t, the cross section observation i does not contribute to the estimation.) Obtaining the FEP estimator is computationally fairly easy, especially when mðxit; bÞ ¼ expðxitbÞ. But the assumptions used to derive the conditional log likeli- hood in equation (19.67) can be restrictive in practice. Fortunately, the FEP estimator has very strong robustness properties for estimating the parameters in the conditional mean. As shown in Wooldridge (1999a), the FEP estimator is consistent for bo under the conditional mean assumption (19.57) only. Except for the conditional mean, the distribution of yit given ðxi; ciÞ is entirely unrestricted; in particular, there can be overdispersion or underdispersion in the latent variable model. Also, there is no re- striction on the dependence between yit and yir, t 0 r. This is another case where the QMLE derived under fairly strong nominal assumptions turns out to have very de- sirable robustness properties. The argument that the FEP estimator is consistent under assumption (19.57) hinges on showing that bo maximizes the expected value of equation (19.67) under assumption (19.57) only. This result is shown in Wooldridge (1999a). Uniqueness holds under general identiﬁcation assumptions, but certain kinds of explanatory variables are ruled out. For example, when the conditional mean has an exponential form, it is easy to see that the coe‰cients on time-constant explanatory variables drop out of equation (19.66), just as in the linear case. Interactions between time- constant and time-varying explanatory variables are allowed. Consistent estimation of the asymptotic variance of ^b follows from the results on M-estimation in Chapter 12. The score for observation i can be written as Count Data and Related Models 675", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 683, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p684::c0", "text": "siðbÞ 1 ‘bliðbÞ ¼ X T t¼1 yit½‘bptðxi; bÞ0=ptðxi; bÞ\u0001 1 ‘bpðxi; bÞ0Wðxi; bÞfyi \u0002 pðxi; bÞnig ð19:68Þ where Wðxi; bÞ 1 ½diagfp1ðxi; bÞ; . . . ; pTðxi; bÞg\u0001\u00021, uiðbÞ 1 yi \u0002 pðxi; bÞni, pðxi; bÞ 1 ½ p1ðxi; bÞ; . . . ; pTðxi; bÞ\u00010, and ptðxi; bÞ is given by equation (19.66). The expected Hessian for observation i can be shown to be Ao 1 E½ni‘bpðxi; boÞ0Wðxi; boÞ‘bpðxi; boÞ\u0001 The asymptotic variance of ^b is A\u00021 o BoA\u00021 o =N, where Bo 1 E½siðboÞsiðboÞ0\u0001. A con- sistent estimate of A is ^A ¼ N\u00021 X N i¼1 ni‘bpðxi; ^bÞ0Wðxi; ^bÞ‘bpðxi; ^bÞ ð19:69Þ and B is estimated as ^B ¼ N\u00021 X N i¼1 sið ^bÞsið ^bÞ0 ð19:70Þ The robust variance matrix estimator, ^A\u00021^B^A\u00021=N, is valid under assumption (19.57); in particular, it allows for any deviations from the Poisson distribution and arbitrary time dependence. The usual maximum likelihood estimate, ^A\u00021=N, is valid under assumptions (19.60) and (19.61). For more details, including methods for speciﬁcation testing, see Wooldridge (1999a). Applications of the ﬁxed e¤ects Poisson estimator, which compute the robust variance matrix and some speciﬁcation test statistics, are given in Papke (1991), Page (1995), and Gordy (1999). We must emphasize that, while the leading applica- tion is to count data, the ﬁxed e¤ects Poisson estimator works whenever assumption (19.57) holds. Therefore, yit could be a nonnegative continuous variable, or even a binary response if we believe the unobserved e¤ect is multiplicative (in contrast to the models in Sections 15.8.2 and 15.8.3). 19.6.5 Relaxing the Strict Exogeneity Assumption We end this chapter with a brief discussion about how to relax the strict exogeneity assumption in a multiplicative unobserved e¤ects panel data model. In place of as- sumption (19.57) we assume Chapter 19 676", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 684, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p685::c0", "text": "Eðyit j xi1; . . . ; xit; ciÞ ¼ cimðxit; boÞ; t ¼ 1; 2; . . . ; T ð19:71Þ These are sequential moment restrictions of the kind we discussed in Chapter 11. The model (19.71) is applicable to static and distributed lag models with possible feed- back, as well as to models with lagged dependent variables. Again, yit need not be a count variable here. Chamberlain (1992b) and Wooldridge (1997a) have suggested residual functions that lead to conditional moment restrictions. Assuming that mðxit; bÞ > 0, deﬁne ritðbÞ 1 yit \u0002 yi;tþ1½mðxit; bÞ=mðxi;tþ1; bÞ\u0001; t ¼ 1; . . . ; T \u0002 1 ð19:72Þ Under assumption (19.72), we can use iterated expectations to show that E½ritðboÞ j xi1; . . . ; xit\u0001 ¼ 0. This expression means that any function of xi1; . . . ; xit is uncorrelated with ritðboÞ and is the basis for generalized method of moments esti- mation. One can easily test the strict exogeneity assumption in a GMM framework. For further discussion and details on implementation, as well as an alternative re- sidual function, see Wooldridge (1997a). Blundell, Gri‰th, and Windmeijer (1998) consider variants of moment conditions in a linear feedback model, where the mean function contains a lagged dependent variable, which enters additively, in addition to an exponential regression function in other conditioning variables with a multiplicated unobserved e¤ect. They apply their model to the patents and R&D relationship. A di¤erent approach is conditional maximum likelihood, as we discussed in Sections 15.8.4 and 16.8.3—see Section 13.9 for a general discussion. For example, if we want to estimate a model for yit given ðzit; yi;t\u00021; ciÞ, where zit contains con- temporaneous variables, we can model it as a Poisson variable with exponential mean ci expðzitbo þ royi;t\u00021Þ. Then, assuming that Dðyit j zi; yi;t\u00021; . . . ; yi0; ciÞ ¼ Dðyit j zit; yi;t\u00021; ciÞ, we can obtain the density of ðyi1; . . . ; yiTÞ given ðyi0; zi; ciÞ by multiplication; see equation (13.60). Given a density speciﬁcation for Dðci j yi0; ziÞ, we can obtain the conditional log likelihood for each i as in equation (13.62). A very convenient speciﬁcation is ci ¼ expðao þ xoyi0 þ zigoÞai, where ai is independent of ðyi0; ziÞ and distributed as Gammaðdo; doÞ. Then, for each t, yit given ðyi;t\u00021; . . . ; yi0; zi; aiÞ has a Poisson distribution with mean ai expðao þ zitbo þ royi;t\u00021 þ xoyi0 þ zigoÞ (As always, we would probably want aggregate time dummies included in this equa- tion.) It is easy to see that the distribution of ðyi1; . . . ; yiTÞ given ðyi0; ziÞ has the random e¤ects Poisson form with gamma heterogeneity; therefore, standard random Count Data and Related Models 677", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 685, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p686::c0", "text": "e¤ects Poisson software can be used to estimate ao; bo; ro; xo; go, and do. The usual conditional MLE standard errors, t statistics, Wald statistics, and likelihood ratio statistics are asymptotically valid for large N. See Wooldridge (2000e) for further details. Problems 19.1. a. For estimating the mean of a nonnegative random variable y, the Poisson quasi–log likelihood for a random draw is liðmÞ ¼ yi logðmÞ \u0002 m; m > 0 (where terms not depending on m have been dropped). Letting mo 1 EðyiÞ, we have E½liðmÞ\u0001 ¼ mo logðmÞ \u0002 m. Show that this function is uniquely maximized at m ¼ mo. This simple result is the basis for the consistency of the Poisson QMLE in the general case. b. The exponential quasi–log likelihood is liðmÞ ¼ \u0002yi=m \u0002 logðmÞ; m > 0 Show that E½liðmÞ\u0001 is uniquely maximized at m ¼ mo. 19.2. Carefully write out the robust variance matrix estimator (19.14) when mðx; bÞ ¼ expðxbÞ. 19.3. Use the data in SMOKE.RAW to answer this question. a. Use a linear regression model to explain cigs, the number of cigarettes smoked per day. Use as explanatory variables logðcigpricÞ, logðincomeÞ, restaurn, white, educ, age, and age2. Are the price and income variables signiﬁcant? Does using hetero- skedasticity-robust standard errors change your conclusions? b. Now estimate a Poisson regression model for cigs, with an exponential conditional mean and the same explanatory variables as in part a. Using the usual MLE standard errors, are the price and income variables each signiﬁcant at the 5 percent level? In- terpret their coe‰cients. c. Find ^s. Is there evidence of overdispersion? Using the GLM standard errors, dis- cuss the signiﬁcance of logðcigpricÞ and logðincomeÞ. d. Compare the usual MLE likelihood ratio statistic for joint signiﬁcance of logðcigpricÞ and logðincomeÞ with the QLR statistic in equation (19.17). Chapter 19 678", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 686, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p687::c0", "text": "e. Compute the fully robust standard errors, and compare these with the GLM standard errors. f. In the model estimated from part b, at what point does the e¤ect of age on expected cigarette consumption become negative? g. Do you think a two-part, or double-hurdle, model for count variables is a better way to model cigs? 19.4. Show that, under the conditional moment restriction Eðy j xÞ ¼ mðx; boÞ, the Poisson QMLE achieves the e‰ciency bound in equation (14.66) when the GLM variance assumption holds. 19.5. Consider an unobserved e¤ects model for count data with exponential re- gression function Eðyit j xi1; . . . ; xiT; ciÞ ¼ ci expðxitbÞ a. If Eðci j xi1; . . . ; xiTÞ ¼ expða þ xigÞ, ﬁnd Eðyit j xi1; . . . ; xiTÞ. b. Use part a to derive a test of mean independence between ci and xi. Assume under H0 that Varðyit j xi; ciÞ ¼ Eðyit j xi; ciÞ, that yit and yir are uncorrelated conditional on ðxi; ciÞ, and that ci and xi are independent. (Hint: You should devise a test in the context of multivariate weighted nonlinear least squares.) c. Suppose now that assumptions (19.60) and (19.61) hold, with mðxit; bÞ ¼ expðxitbÞ, but assumption (19.62) is replaced by ci ¼ ai expða þ xigÞ, where ai j x @ Gammaðd; dÞ. Now how would you estimate b, a, and g, and how would you test H0: g ¼ 0? 19.6. A model with an additive unobserved e¤ect, strictly exogenous regressors, and a nonlinear regression function is Eðyit j xi; ciÞ ¼ ci þ mðxit; boÞ; t ¼ 1; . . . ; T a. For each i and t deﬁne the time-demeaned variables €yit 1 yit \u0002 yi and, for each b, €mitðbÞ ¼ mðxit; bÞ \u0002 ð1=TÞ PT r¼1 mðxir; b). Argue that, under standard regularity conditions, the pooled nonlinear least squares estimator of bo that solves min b X N i¼1 X T t¼1 ½ €yit \u0002 €mitðbÞ\u00012 ð19:73Þ is generally consistent and ﬃﬃﬃﬃ N p -asymptotically normal (with T ﬁxed). [Hint: Show that Eð€yit j xiÞ ¼ €mitðboÞ for all t.] Count Data and Related Models 679", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 687, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p688::c0", "text": "b. If Varðyi j xi; ciÞ ¼ s2 oIT, how would you estimate the asymptotic variance of the pooled NLS estimator? c. If the variance assumption in part b does not hold, how would you estimate the asymptotic variance? d. Show that the NLS estimator based on time demeaning from part a is in fact identical to the pooled NLS estimator that estimates fc1; c2; . . . ; cNg along with bo: min fc1;c2;...;cN;bg X N i¼1 X T t¼1 ½ yit \u0002 ci \u0002 mðxit; bÞ\u00012 ð19:74Þ Thus, this is another case where treating the unobserved e¤ects as parameters to es- timate does not result in an inconsistent estimator of bo. [Hint: It is easiest to con- centrate out the ci from the sum of square residuals; see Section 12.7.4. In the current context, for given b, ﬁnd ^ci as functions of yi; xi, and b. Then plug these back into equation (19.74) and show that the concentrated sum of squared residuals function is identical to equation (19.73).] 19.7. Assume that the standard Poisson ﬁxed e¤ects assumptions hold, so that, conditional on ðxi; ciÞ; yi1; . . . ; yiT are independent Poisson random variables with means cimðxit; boÞ. a. Show that, if we treat the ci as parameters to estimate along with bo, then the con- ditional log likelihood for observation i (apart from terms not depending on ci or b) is liðci; bÞ 1 log½ f ðyi1; . . . ; yiT j xi; ci; bÞ\u0001 ¼ X T t¼1 f\u0002cimðxit; bÞ þ yit½logðciÞ\u0001 þ log½mðxit; bÞ\u0001g where we now group ci with b as a parameter to estimate. (Note that ci > 0 is a needed restriction.) b. Let ni ¼ yi1 þ \u0004 \u0004 \u0004 þ yiT, and assume that ni > 0. For given b, maximize liðci; bÞ only with respect to ci. Find the solution, ciðbÞ > 0. c. Plug the solution from part b into li½ciðbÞ; b\u0001, and show that li½ciðbÞ; b\u0001 ¼ X T t¼1 yit log½ ptðxi; bÞ\u0001 þ ðni \u0002 1Þ logðniÞ d. Conclude from part c that the log-likelihood function for all N cross section observations, with ðc1; . . . ; cNÞ concentrated out, is Chapter 19 680", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 688, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p689::c0", "text": "X N i¼1 X T t¼1 yit log½ ptðxi; bÞ\u0001 þ X N i¼1 ðni \u0002 1Þ logðniÞ What does this mean about the conditional MLE from Section 19.6.4 and the esti- mator that treats the ci as parameters to estimate along with bo? 19.8. Let y be a fractional response, so that 0 a y a 1. a. Suppose that 0 < y < 1, so that w 1 log½y=ð1 \u0002 yÞ\u0001 is well deﬁned. If we assume the linear model Eðw j xÞ ¼ xa, does Eðy j xÞ have any simple relationship to xa? What would we need to know to obtain Eðy j xÞ? Let ^a be the OLS estimator from the regression wi on xi, i ¼ 1; . . . ; N. b. If we estimate the fractional logit model for Eðy j xÞ from Section 19.4.2, should we expect the estimated parameters, ^b, to be close to ^a from part a? Explain. c. Now suppose that y takes on the values zero and one with positive probability. To model this population feature we use a latent variable model: y\u0005 j x @ Normalðxg; s2Þ y ¼ 0; y\u0005 a 0 ¼ y\u0005; 0 < y\u0005 < 1 ¼ 1; y\u0005 b 1 How should we estimate g and s2? (Hint: See Problem 16.3.) d. Given the estimate ^g from part c, does it make sense to compare the magnitude of ^gj to the corresponding ^aj from part a or the ^bj from part b? Explain. e. How might we choose between the models estimated in parts b and c? (Hint: Think about goodness of ﬁt for the conditional mean.) f. Now suppose that 0 a y < 1. Suppose we apply fractional logit, as in part b, and fractional logit to the subsample with 0 < yi < 1. Should we necessarily get similar answers? g. With 0 a y < 1 suppose that Eðyi j xi; yi > 0Þ ¼ expðxidÞ=½1 þ expðxidÞ\u0001. If we estimate d using the QMLE from Section 19.4.2, using only observations with 0 < yi < 1, is there a sample selection bias? Explain. h. To the assumptions from part g add Pðyi ¼ 0 j xiÞ ¼ 1 \u0002 GðxihÞ, where Gð\u0004Þ is a di¤erentiable, strictly increasing cumulative distribution function. How should we estimate Eðyi j xiÞ? Count Data and Related Models 681", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 689, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p690::c0", "text": "19.9. Use the data in ATTEND.RAW to answer this question. a. Estimate a linear regression relating atndrte to ACT, priGPA, frosh, and soph; compute the usual OLS standard errors. Interpret the coe‰cients on ACT and priGPA. Are any ﬁtted values outside the unit interval? b. Model Eðatndrte j xÞ as a logistic function, as in Section 19.4.2. Use the QMLE for the Bernoulli log likelihood, and compute the GLM standard errors. What is ^s, and how does it a¤ect the standard errors? c. For priGPA ¼ 3:0 and frosh ¼ soph ¼ 0, estimate the e¤ect of increasing ACT from 25 to 30 using the estimated equation from part b. How does the estimate compare with that from the linear model? d. Does a linear model or a logistic model provide a better ﬁt to Eðatndrte j xÞ? 19.10. Use the data in PATENT.RAW for this exercise. a. Estimate a pooled Poisson regression model relating patents to lsales ¼ logðsalesÞ and current and four lags of lrnd ¼ logð1 þ rndÞ, where we add one before taking the log to account for the fact that rnd is zero for some ﬁrms in some years. Use an ex- ponential mean function and include a full set of year dummies. Which lags of lrnd are signiﬁcant using the usual Poisson MLE standard errors? b. Give two reasons why the usual Poisson MLE standard errors from part a might be invalid. c. Obtain ^s for the pooled Poisson estimation. Using the GLM standard errors (but without an adjustment for possible serial dependence), which lags of lrnd are signiﬁcant? d. Obtain the QLR statistic for joint signiﬁcance of lags one through four of lrnd. (Be careful here; you must use the same set of years in estimating the restricted version of the model.) How does it compare to the usual LR statistic? e. Compute the standard errors that are robust to an arbitrary conditional variance and serial dependence. How do they compare with the standard errors from parts a and c? f. What is the estimated long-run elasticity of expected patents with respect to R&D spending? (Ignore the fact that one has been added to the R&D numbers before taking the log.) Obtain a fully robust standard error for the long-run elasticity. g. Now use the ﬁxed e¤ects Poisson estimator, and compare the estimated lag coef- ﬁcients to those from the pooled Poisson analysis. Estimate the long-run elasticity, and obtain its standard error. (Assume that the full set of FEP assumptions hold.) Chapter 19 682", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 690, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p691::c0", "text": "19.11. a. For a random draw i from the cross section, assume that (1) for each time period t, yit j xi; ci @ PoissonðcimitÞ, where ci > 0 is unobserved heterogeneity and mit > 0 is typically a function of only xit; and (2) ðyi1; . . . ; yiTÞ are independent con- ditional on ðxi; ciÞ. Derive the density of ðyi1; . . . ; yiTÞ conditional on ðxi; ciÞ. b. To the assumptions from part a, add the assumption that ci j xi @ Gammaðd; dÞ, so that EðciÞ ¼ 1 and VarðciÞ ¼ 1=d. fThe density of ci is hðcÞ ¼ ½dd= GðdÞ\u0001cd\u00021 expð\u0002dcÞ, where GðdÞ is the gamma function.g Let s ¼ y1 þ \u0004 \u0004 \u0004 þ yT and Mi ¼ mi1 þ \u0004 \u0004 \u0004 þ miT. Show that the density of ðyi1; . . . ; yiTÞ given xi is Y T t¼1 myt it =yt! ! ½dd=GðdÞ\u0001½GðMi þ sÞ=ðMi þ dÞðsþdÞ\u0001 [Hint: The easiest way to show this result is to turn the integral into one involving a Gammaðs þ d; Mi þ dÞ density and a multiplicative term. Naturally, the density must integrate to unity, and so what is left over is the density we seek.] 19.12. For a random draw i from the cross section, assume that (1) for each t, yit j xi; ci @ Gammaðmit; 1=ciÞ, where ci > 0 is unobserved heterogeneity and mit > 0; and (2) ðyi1; . . . ; yiTÞ are independent conditional on ðxi; ciÞ. The gamma distribution is parameterized so that Eðyit j xi; ciÞ ¼ cimit and Varðyit j xi; ciÞ ¼ c2 i mit. a. Let si ¼ yi1 þ \u0004 \u0004 \u0004 þ yiT. Show that the density of ðyi1; yi2; . . . ; yiTÞ conditional on ðsi; xi; ciÞ is f ðy1; . . . ; yT j si; xi; ciÞ ¼ Gðmi1 þ \u0004 \u0004 \u0004 þ miTÞ= Y T t¼1 GðmitÞ \" # \u0003 Y T t¼1 ymit\u00021 t ! =sfðmi1þ\u0004\u0004\u0004þmiTÞ\u00021g i \" # where Gð\u0004Þ is the gamma function. Note that the density does not depend on ci. fHint: If Y1; . . . ; YT are independent random variables and S ¼ Y1 þ \u0004 \u0004 \u0004 þ YT, the joint density of Y1; . . . ; YT given S ¼ s is f1ðy1Þ \u0004 \u0004 \u0004 fT\u00021ðyT\u00021ÞfTðs \u0002 y1 \u0002 \u0004 \u0004 \u0004 \u0002 yT\u00021Þ= gðsÞ, where gðsÞ is the density of S. When Yt has a Gammaðat; lÞ distribution for each t, so that ftðytÞ ¼ ½lat=GðatÞ\u0001yðat\u00021Þ t expð\u0002lytÞ, S @ Gammaða1 þ \u0004 \u0004 \u0004 þ aT; lÞ.g b. Let mtðxi; bÞ be a parametric function for mit—for example, expðxitbÞ. Write down the log-likelihood function for observation i. The conditional MLE in this case is called the ﬁxed e¤ects gamma estimator. Count Data and Related Models 683", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 691, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p692::c0", "text": "20 Duration Analysis 20.1 Introduction Some response variables in economics come in the form of a duration, which is the time elapsed until a certain event occurs. A few examples include weeks unemployed, months spent on welfare, days until arrest after incarceration, and quarters until an Internet ﬁrm ﬁles for bankruptcy. The recent literature on duration analysis is quite rich. In this chapter we focus on the developments that have been used most often in applied work. In addition to providing a rigorous introduction to modern duration analysis, this chapter should prepare you for more advanced treatments, such as Lancaster’s (1990) monograph. Duration analysis has its origins in what is typically called survival analysis, where the duration of interest is survival time of a subject. In survival analysis we are interested in how various treatments or demographic characteristics a¤ect survival times. In the social sciences, we are interested in any situation where an individual— or family, or ﬁrm, and so on—begins in an initial state and is either observed to exit the state or is censored. (We will discuss the exact nature of censoring in Sections 20.3 and 20.4.) The calendar dates on which units enter the initial state do not have to be the same. (When we introduce covariates in Section 20.2.2, we note how dummy variables for di¤erent calendar dates can be included in the covariates, if necessary, to allow for systematic di¤erences in durations by starting date.) Traditional duration analysis begins by specifying a population distribution for the duration, usually conditional on some explanatory variables (covariates) observed at the beginning of the duration. For example, for the population of people who became unemployed during a particular period, we might observe education levels, experi- ence, marital status—all measured when the person becomes unemployed—wage on prior job, and a measure of unemployment beneﬁts. Then we specify a distribution for the unemployment duration conditional on the covariates. Any reasonable dis- tribution reﬂects the fact that an unemployment duration is nonnegative. Once a complete conditional distribution has been speciﬁed, the same maximum likelihood methods that we studied in Chapter 16 for censored regression models can be used. In this framework, we are typically interested in estimating the e¤ects of the covariates on the expected duration. Recent treatments of duration analysis tend to focus on the hazard function. The hazard function allows us to approximate the probability of exiting the initial state within a short interval, conditional on having survived up to the starting time of the interval. In econometric applications, hazard functions are usually conditional on some covariates. An important feature for policy analysis is allowing the hazard function to depend on covariates that change over time.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 692, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p693::c0", "text": "In Section 20.2 we deﬁne and discuss hazard functions, and we settle certain issues involved with introducing covariates into hazard functions. In Section 20.3 we show how censored regression models apply to standard duration models with single-cycle ﬂow data, when all covariates are time constant. We also discuss the most common way of introducing unobserved heterogeneity into traditional duration analysis. Given parametric assumptions, we can test for duration dependence—which means that the probability of exiting the initial state depends on the length of time in the state—as well as for the presence of unobserved heterogeneity. In Section 20.4 we study methods that allow ﬂexible estimation of a hazard func- tion, both with time-constant and time-varying covariates. We assume that we have grouped data; this term means that durations are observed to fall into ﬁxed intervals (often weekly or monthly intervals) and that any time-varying covariates are assumed to be constant within an interval. We focus attention on the case with two states, with everyone in the population starting in the initial state, and single-cycle data, where each person either exits the initial state or is censored before exiting. We also show how heterogeneity can be included when the covariates are strictly exogenous. We touch on some additional issues in Section 20.5. 20.2 Hazard Functions The hazard function plays a central role in modern duration analysis. In this section, we discuss various features of the hazard function, both with and without covariates, and provide some examples. 20.2.1 Hazard Functions without Covariates Often in this chapter it is convenient to distinguish random variables from particular outcomes of random variables. Let T b 0 denote the duration, which has some dis- tribution in the population; t denotes a particular value of T. (As with any econo- metric analysis, it is important to be very clear about the relevant population, a topic we consider in Section 20.3.) In survival analysis, T is the length of time a subject lives. Much of the current terminology in duration analysis comes from survival applications. For us, T is the time at which a person (or family, ﬁrm, and so on) leaves the initial state. For example, if the initial state is unemployment, T would be the time, measured in, say, weeks, until a person becomes employed. The cumulative distribution function (cdf ) of T is deﬁned as FðtÞ ¼ PðT a tÞ; t b 0 ð20:1Þ Chapter 20 686", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 693, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p694::c0", "text": "The survivor function is deﬁned as SðtÞ 1 1 \u0001 FðtÞ ¼ PðT > tÞ, and this is the prob- ability of ‘‘surviving’’ past time t. We assume in the rest of this section that T is continuous—and, in fact, has a di¤erentiable cdf—because this assumption simpliﬁes statements of certain probabilities. Discreteness in observed durations can be viewed as a consequence of the sampling scheme, as we discuss in Section 20.4. Denote the density of T by f ðtÞ ¼ dF dt ðtÞ. For h > 0, Pðt a T < t þ h j T b tÞ ð20:2Þ is the probabilty of leaving the initial state in the interval ½t; t þ hÞ given survival up until time t. The hazard function for T is deﬁned as lðtÞ ¼ lim h#0 Pðt a T < t þ h j T b tÞ h ð20:3Þ For each t, lðtÞ is the instantaneous rate of leaving per unit of time. From equation (20.3) it follows that, for ‘‘small’’ h, Pðt a T < t þ h j T b tÞAlðtÞh ð20:4Þ Thus the hazard function can be used to approximate a conditional probability in much the same way that the height of the density of T can be used to approximate an unconditional probability. Example 20.1 (Unemployment Duration): If T is length of time unemployed, mea- sured in weeks, then lð20Þ is (approximately) the probability of becoming employed between weeks 20 and 21. The phrase ‘‘becoming employed’’ reﬂects the fact that the person was unemployed up through week 20. That is, lð20Þ is roughly the probability of becoming employed between weeks 20 and 21, conditional on having been unem- ployed through week 20. Example 20.2 (Recidivism Duration): Suppose T is the number of months before a former prisoner is arrested for a crime. Then lð12Þ is roughly the probability of being arrested during the 13th month, conditional on not having been arrested during the ﬁrst year. We can express the hazard function in terms of the density and cdf very simply. First, write Pðt a T < t þ h j T b tÞ ¼ Pðt a T < t þ hÞ=PðT b tÞ ¼ Fðt þ hÞ \u0001 FðtÞ 1 \u0001 FðtÞ Duration Analysis 687", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 694, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p695::c0", "text": "When the cdf is di¤erentiable, we can take the limit of the right-hand side, divided by h, as h approaches zero from above: lðtÞ ¼ lim h#0 Fðt þ hÞ \u0001 FðtÞ h \u0002 1 1 \u0001 FðtÞ ¼ f ðtÞ 1 \u0001 FðtÞ ¼ f ðtÞ SðtÞ ð20:5Þ Because the derivative of SðtÞ is \u0001f ðtÞ, we have lðtÞ ¼ \u0001 d log SðtÞ dt ð20:6Þ and, using Fð0Þ ¼ 0, we can integrate to get FðtÞ ¼ 1 \u0001 exp \u0001 ð t 0 lðsÞ ds \u0001 \u0002 ; t b 0 ð20:7Þ Straightforward di¤erentiation of equation (20.7) gives the density of T as f ðtÞ ¼ lðtÞ exp \u0001 ð t 0 lðsÞ ds \u0001 \u0002 ð20:8Þ Therefore, all probabilities can be computed using the hazard function. For example, for points a1 < a2, PðT b a2 j T b a1Þ ¼ 1 \u0001 Fða2Þ 1 \u0001 Fða1Þ ¼ exp \u0001 ð a2 a1 lðsÞ ds \u0001 \u0002 and Pða1 a T < a2 j T b a1Þ ¼ 1 \u0001 exp \u0001 ð a2 a1 lðsÞ ds \u0001 \u0002 ð20:9Þ This last expression is especially useful for constructing the log-likelihood functions needed in Section 20.4. The shape of the hazard function is of primary interest in many empirical appli- cations. In the simplest case, the hazard function is constant: lðtÞ ¼ l; all t b 0 ð20:10Þ This function means that the process driving T is memoryless: the probability of exit in the next interval does not depend on how much time has been spent in the initial state. From equation (20.7), a constant hazard implies FðtÞ ¼ 1 \u0001 expð\u0001ltÞ ð20:11Þ Chapter 20 688", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 695, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p696::c0", "text": "which is the cdf of the exponential distribution. Conversely, if T has an exponential distribution, it has a constant hazard. When the hazard function is not constant, we say that the process exhibits duration dependence. Assuming that lð\u0002Þ is di¤erentiable, there is positive duration dependence at time t if dlðtÞ=dt > 0; if dlðtÞ=dt > 0 for all t > 0, then the process exhibits posi- tive duration dependence. With positive duration dependence, the probability of exiting the initial state increases the longer one is in the initial state. If the derivative is negative, then there is negative duration dependence. Example 20.3 (Weibull Distribution): If T has a Weibull distribution, its cdf is given by FðtÞ ¼ 1 \u0001 expð\u0001gtaÞ, where g and a are nonnegative parameters. The density is f ðtÞ ¼ gata\u00011 expð\u0001gtaÞ. By equation (20.5), the hazard function is lðtÞ ¼ f ðtÞ=SðtÞ ¼ gata\u00011 ð20:12Þ When a ¼ 1, the Weibull distribution reduces to the exponential with l ¼ g. If a > 1, the hazard is monotonically increasing, so the hazard everywhere exhibits positive duration dependence; for a < 1, the hazard is monotonically decreasing. Provided we think the hazard is monotonically increasing or decreasing, the Weibull distribution is a relatively simple way to capture duration dependence. We often want to specify the hazard directly, in which case we can use equation (20.7) to determine the duration distribution. Example 20.4 (Log-Logistic Hazard Function): The log-logistic hazard function is speciﬁed as lðtÞ ¼ gata\u00011 1 þ gta ð20:13Þ where g and a are positive parameters. When a ¼ 1, the hazard is monotonically decreasing from g at t ¼ 0 to zero as t ! y; when a < 1, the hazard is also monot- onically decreasing to zero as t ! y, but the hazard is unbounded as t approaches zero. When a > 1, the hazard is increasing until t ¼ ½ða \u0001 1Þ=g\u00031\u0001a, and then it decreases to zero. Straightforward integration gives ð t 0 lðsÞ ds ¼ logð1 þ gtaÞ ¼ \u0001log½ð1 þ gtaÞ\u00011\u0003 so that, by equation (20.7), Duration Analysis 689", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 696, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p697::c0", "text": "FðtÞ ¼ 1 \u0001 ð1 þ gtaÞ\u00011; t b 0 ð20:14Þ Di¤erentiating with respect to t gives f ðtÞ ¼ gata\u00011ð1 þ gtaÞ\u00012 Using this density, it can be shown that Y 1 logðTÞ has density gðyÞ ¼ a exp½aðy \u0001 mÞ\u0003=f1 þ exp½aðy \u0001 mÞ\u0003g2, where m ¼ \u0001a\u00011 logðgÞ is the mean of Y. In other words, logðTÞ has a logistic distribution with mean m and variance p2=ð3a2Þ (hence the name ‘‘log-logistic’’). 20.2.2 Hazard Functions Conditional on Time-Invariant Covariates Usually in economics we are interested in hazard functions conditional on a set of covariates or regressors. When these do not change over time—as is often the case given the way many duration data sets are collected—then we simply deﬁne the hazard (and all other features of T ) conditional on the covariates. Thus, the condi- tional hazard is lðt; xÞ ¼ lim h#0 Pðt a T < t þ h j T b t; xÞ h where x is a vector of explanatory variables. All of the formulas from the previous subsection continue to hold provided the cdf and density are deﬁned conditional on x. For example, if the conditional cdf Fð\u0002 j xÞ is di¤erentiable, we have lðt; xÞ ¼ f ðt j xÞ 1 \u0001 Fðt j xÞ ð20:15Þ where f ð\u0002 j xÞ is the density of T given x. Often we are interested in the partial e¤ects of the xj on lðt; xÞ, which are deﬁned as partial derivatives for continuous xj and as di¤erences for discrete xj. If the durations start at di¤erent calendar dates—which is usually the case—we can include indicators for di¤erent starting dates in the covariates. These allow us to control for seasonal di¤erences in duration distributions. An especially important class of models with time-invariant regressors consists of proportional hazard models. A proportional hazard can be written as lðt; xÞ ¼ kðxÞl0ðtÞ ð20:16Þ where kð\u0002Þ > 0 is a nonnegative function of x and l0ðtÞ > 0 is called the baseline hazard. The baseline hazard is common to all units in the population; individual haz- ard functions di¤er proportionately based on a function kðxÞ of observed covariates. Chapter 20 690", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 697, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p698::c0", "text": "Typically, kð\u0002Þ is parameterized as kðxÞ ¼ expðxbÞ, where b is a vector of param- eters. Then log lðt; xÞ ¼ xb þ log l0ðtÞ ð20:17Þ and bj measures the semielasticity of the hazard with respect to xj. [If xj is the log of an underlying variable, say xj ¼ logðzjÞ, bj is the elasticity of the hazard with respect to zj.] Occasionally we are interested only in how the covariates shift the hazard function, in which case estimation of l0 is not necessary. Cox (1972) obtained a partial maxi- mum likelihood estimator for b that does not require estimating l0ð\u0002Þ. We discuss Cox’s approach brieﬂy in Section 20.5. In economics, much of the time we are inter- ested in the shape of the baseline hazard. We discuss estimation of proportional hazard models with a ﬂexible baseline hazard in Section 20.4. If in the Weibull hazard function (20.12) we replace g with expðxbÞ, where the ﬁrst element of x is unity, we obtain a proportional hazard model with l0ðtÞ 1 ata\u00011. However, if we replace g in equation (20.13) with expðxbÞ—which is the most com- mon way of introducing covariates into the log-logistic model—we do not obtain a hazard with the proportional hazard form. Example 20.1 (continued): If T is an unemployment duration, x might contain education, labor market experience, marital status, race, and number of children, all measured at the beginning of the unemployment spell. Policy variables in x might reﬂect the rules governing unemployment beneﬁts, where these are known before each person’s unemployment duration. Example 20.2 (continued): To explain the length of time before arrest after release from prison, the covariates might include participation in a work program while in prison, years of education, marital status, race, time served, and past number of convictions. 20.2.3 Hazard Functions Conditional on Time-Varying Covariates Studying hazard functions is more complicated when we wish to model the e¤ects of time-varying covariates on the hazard function. For one thing, it makes no sense to specify the distribution of the duration T conditional on the covariates at only one time period. Nevertheless, we can still deﬁne the appropriate conditional probabilities that lead to a conditional hazard function. Let xðtÞ denote the vector of regressors at time t; again, this is the random vector describing the population. For t b 0, let XðtÞ, t b 0, denote the covariate path up Duration Analysis 691", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 698, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p699::c0", "text": "through time t: XðtÞ 1 fxðsÞ: 0 a s a tg. Following Lancaster (1990, Chapter 2), we deﬁne the conditional hazard function at time t by l½t; XðtÞ\u0003 ¼ lim h#0 P½t a T < t þ h j T b t; Xðt þ hÞ\u0003 h ð20:18Þ assuming that this limit exists. A discussion of assumptions that ensure existence of equation (20.18) is well beyond the scope of this book; see Lancaster (1990, Chapter 2). One case where this limit exists very generally occurs when T is continuous and, for each t, xðt þ hÞ is constant for all h A ½0; hðtÞ\u0003 for some function hðtÞ > 0. Then we can replace Xðt þ hÞ with XðtÞ in equation (20.18) [because Xðt þ hÞ ¼ XðtÞ for h su‰ciently small]. For reasons we will see in Section 20.4, we must assume that time- varying covariates are constant over the interval of observation (such as a week or a month), anyway, in which case there is no problem in deﬁning equation (20.18). For certain purposes, it is important to know whether time-varying covariates are strictly exogenous. With the hazard deﬁned as in equation (20.18), Lancaster (1990, Deﬁnition 2.1) provides a deﬁnition that rules out feedback from the duration to future values of the covariates. Speciﬁcally, if Xðt; t þ hÞ denotes the covariate path from time t to t þ h, then Lancaster’s strict exogeneity condition is P½Xðt; t þ hÞ j T b t þ h; XðtÞ\u0003 ¼ P½Xðt; t þ hÞ j XðtÞ\u0003 ð20:19Þ for all t b 0, h > 0. Actually, when condition (20.19) holds, Lancaster says fxðtÞ: t > 0g is ‘‘exogenous.’’ We prefer the name ‘‘strictly exogenous’’ because condition (20.19) is closely related to the notions of strict exogeneity that we have encoun- tered throughout this book. Plus, it is important to see that condition (20.19) has nothing to do with contemporaneous endogeneity: by deﬁnition, the covariates are sequentially exogenous (see Section 11.1.1) because, by specifying l½t; XðtÞ\u0003, we are conditioning on current and past covariates. Equation (20.19) applies to covariates whose entire path is well-deﬁned whether or not the agent is in the initial state. One such class of covariates, called external covariates by Kalbﬂeisch and Prentice (1980), has the feature that the covariate path is independent of whether any particular agent has or has not left the initial state. In modeling time until arrest, these covariates might include law enforcement per capita in the person’s city of residence or the city unemployment rate. Other covariates are not external to each agent but have paths that are still deﬁned after the agent leaves the initial state. For example, marital status is well-deﬁned be- fore and after someone is arrested, but it is possibly related to whether someone has been arrested. Whether marital status satisﬁes condition (20.19) is an empirical issue. Chapter 20 692", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 699, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p700::c0", "text": "The deﬁnition of strict exogeneity in condition (20.19) cannot be applied to time- varying covariates whose path is not deﬁned once the agent leaves the initial state. Kalbﬂeisch and Prentice (1980) call these internal covariates. Lancaster (1990, p. 28) gives the example of job tenure duration, where a time-varying covariate is wage paid on the job: if a person leaves the job, it makes no sense to deﬁne the future wage path in that job. As a second example, in modeling the time until a former prisoner is arrested, a time-varying covariate at time t might be wage income in the previous month, t \u0001 1. If someone is arrested and reincarcerated, it makes little sense to deﬁne future labor income. It is pretty clear that internal covariates cannot satisfy any reasonable strict exo- geneity assumption. This fact will be important in Section 20.4 when we discuss esti- mation of duration models with unobserved heterogeneity and grouped duration data. We will actually use a slightly di¤erent notion of strict exogeneity that is directly relevant for conditional maximum likelihood estimation. Nevertheless, it is in the same spirit as condition (20.19). With time-varying covariates there is not, strictly speaking, such a thing as a pro- portional hazard model. Nevertheless, it has become common in econometrics to call a hazard of the form l½t; xðtÞ\u0003 ¼ k½xðtÞ\u0003l0ðtÞ ð20:20Þ a proportional hazard with time-varying covariates. The function multiplying the baseline hazard is usually k½xðtÞ\u0003 ¼ exp½xðtÞb\u0003; for notational reasons, we show this depending only on xðtÞ and not on past covariates [which can always be included in xðtÞ]. We will discuss estimation of these models, without the strict exogeneity as- sumption, in Section 20.4.2. In Section 20.4.3, when we multiply equation (20.20) by unobserved heterogeneity, strict exogeneity becomes very important. The log-logistic hazard is also easily modiﬁed to have time-varying covariates. One way to include time-varying covariates parametrically is l½t; xðtÞ\u0003 ¼ exp½xðtÞb\u0003ata\u00011=f1 þ exp½xðtÞb\u0003tag We will see how to estimate a and b in Section 20.4.2. 20.3 Analysis of Single-Spell Data with Time-Invariant Covariates We assume that the population of interest is individuals entering the initial state during a given interval of time, say ½0; b\u0003, where b > 0 is a known constant. (Naturally, ‘‘individual’’ can be replaced with any population unit of interest, such as ‘‘family’’ or ‘‘ﬁrm.’’) As in all econometric contexts, it is very important to be explicit about the Duration Analysis 693", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 700, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p701::c0", "text": "underlying population. By convention, we let zero denote the earliest calendar date that an individual can enter the initial state, and b is the last possible date. For ex- ample, if we are interested in the population of U.S. workers who became unem- ployed at any time during 1998, and unemployment duration is measured in years (with .5 meaning half a year), then b ¼ 1. If duration is measured in weeks, then b ¼ 52; if duration is measured in days, then b ¼ 365; and so on. In using the methods of this section, we typically ignore the fact that durations are often grouped into discrete intervals—for example, measured to the nearest week or month—and treat them as continuously distributed. If we want to explicitly recog- nize the discreteness of the measured durations, we should treat them as grouped data, as we do in Section 20.4. We restrict attention to single-spell data. That is, we use, at most, one completed spell per individual. If, after leaving the initial state, an individual subsequently reenters the initial state in the interval ½0; b\u0003, we ignore this information. In addition, the covariates in the analysis are time invariant, which means we collect covariates on individuals at a given point in time—usually, at the beginning of the spell—and we do not re-collect data on the covariates during the course of the spell. Time-varying covariates are more naturally handled in the context of grouped duration data in Section 20.4. We study two general types of sampling from the population that we have de- scribed. The most common, and the easiest to handle, is ﬂow sampling. In Section 20.3.3 we brieﬂy consider various kinds of stock sampling. 20.3.1 Flow Sampling With ﬂow sampling, we sample individuals who enter the state at some point during the interval ½0; b\u0003, and we record the length of time each individual is in the initial state. We collect data on covariates known at the time the individual entered the initial state. For example, suppose we are interested in the population of U.S. workers who became unemployed at any time during 1998, and we randomly sample from U.S. male workers who became unemployed during 1998. At the beginning of the unem- ployment spell we might obtain information on tenure in last job, wage on last job, gender, marital status, and information on unemployment beneﬁts. There are two common ways to collect ﬂow data on unemployment spells. First, we may randomly sample individuals from a large population, say, all working-age individuals in the United States for a given year, say, 1998. Some fraction of these people will be in the labor force and will become unemployed during 1998—that is, enter the initial state of unemployment during the speciﬁed interval—and this group of people who become unemployed is our random sample of all workers who become Chapter 20 694", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 701, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p702::c0", "text": "unemployed during 1998. Another possibility is retrospective sampling. For example, suppose that, for a given state in the United States, we have access to unemployment records for 1998. We can then obtain a random sample of all workers who became unemployed during 1998. Flow data are usually subject to right censoring. That is, after a certain amount of time, we stop following the individuals in the sample, which we must do in order to analyze the data. (Right censoring is the only kind that occurs with ﬂow data, so we will often refer to right censoring as ‘‘censoring’’ in this and the next subsection.) For individuals who have completed their spells in the initial state, we observe the exact duration. But for those still in the initial state, we only know that the duration lasted as long as the tracking period. In the unemployment duration example, we might follow each individual for a ﬁxed length of time, say, two years. If unemployment spells are measured in weeks, we would have right censoring at 104 weeks. Alter- natively, we might stop tracking individuals at a ﬁxed calendar date, say, the last week in 1999. Because individuals can become unemployed at any time during 1998, calendar-date censoring results in censoring times that di¤er across individuals. 20.3.2 Maximum Likelihood Estimation with Censored Flow Data For a random draw i from the population, let ai A ½0; b\u0003 denote the time at which in- dividual i enters the initial state (the ‘‘starting time’’), let t\u0004 i denote the length of time in the initial state (the duration), and let xi denote the vector of observed covariates. We assume that t\u0004 i has a continuous conditional density f ðt j xi; yÞ, t b 0, where y is the vector of unknown parameters. Without right censoring we would observe a random sample on ðai; t\u0004 i ; xiÞ, and estimation would be a standard exercise in conditional maximum likelihood. To ac- count for right censoring, we assume that the observed duration, ti, is obtained as ti ¼ minðt\u0004 i ; ciÞ ð20:21Þ where ci is the censoring time for individual i. In some cases, ci is constant across i. For example, suppose t\u0004 i is unemployment duration for person i, measured in weeks. If the sample design speciﬁes that we follow each person for at most two years, at which point all people remaining unemployed after two years are censored, then c ¼ 104. If we have a ﬁxed calendar date at which we stop tracking individuals, the cen- soring time di¤ers by individual because the workers typically would become unem- ployed on di¤erent calendar dates. If b ¼ 1 year and we censor everyone at two years from the start of the study, the censoring times could range from 52 to 104 weeks.) We assume that, conditional on the covariates, the true duration is independent of the starting point, ai, and the censoring time, ci: Duration Analysis 695", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 702, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p703::c0", "text": "Dðt\u0004 i j xi; ai; ciÞ ¼ Dðt\u0004 i j xiÞ ð20:22Þ where Dð\u0002 j \u0002Þ denotes conditional distribution. Assumption (20.22) clearly holds when ai and ci are constant for all i, but it holds under much weaker assumptions. Sometimes ci is constant for all i, in which case assumption (20.22) holds when the duration is independent of the starting time, conditional on xi. If there are seasonal e¤ects on the duration—for example, unemployment durations that start in the summer have a di¤erent expected length than durations that start at other times of the year—then we may have to put dummy variables for di¤erent starting dates in xi to ensure that assumption (20.22) holds. This approach would also ensure that as- sumption (20.22) holds when a ﬁxed calendar date is used for censoring, implying that ci is not constant across i. Assumption (20.22) holds for certain nonstandard censoring schemes, too. For example, if an element of xi is education, assumption (20.22) holds if, say, individuals with more education are censored more quickly. Under assumption (20.22), the distribution of t\u0004 i given ðxi; ai; ciÞ does not depend on ðai; ciÞ. Therefore, if the duration is not censored, the density of ti ¼ t\u0004 i given ðxi; ai; ciÞ is simply f ðt j xi; yÞ. The probability that ti is censored is Pðt\u0004 i b ci j xiÞ ¼ 1 \u0001 Fðci j xi; yÞ where Fðt j xi; yÞ is the conditional cdf of t\u0004 i given xi. Letting di be a censoring indi- cator (di ¼ 1 if uncensored, di ¼ 0 if censored), the conditional likelihood for obser- vation i can be written as f ðti j xi; yÞdi½1 \u0001 Fðti j xi; yÞ\u0003ð1\u0001diÞ ð20:23Þ Importantly, neither the starting times, ai, nor the length of the interval, b, plays a role in the analysis. [In fact, in the vast majority of treatments of ﬂow data, b and ai are not even introduced. However, it is important to know that the reason ai is not relevant for the analysis of ﬂow data is the conditional independence assumption in equation (20.22).] By contrast, the censoring times ci do appear in the likelihood for censored observations because then ti ¼ ci. Given data on ðti; di; xiÞ for a random sample of size N, the maximum likelihood estimator of y is obtained by maximizing X N i¼1 fdi log½ f ðti j xi; yÞ\u0003 þ ð1 \u0001 diÞ log½1 \u0001 Fðti j xi; yÞ\u0003g ð20:24Þ For the choices of f ð\u0002 j x; yÞ used in practice, the conditional MLE regularity conditions—see Chapter 13—hold, and the MLE is ﬃﬃﬃﬃ N p -consistent and asymptoti- cally normal. [If there is no censoring, the second term in expression (20.24) is simply dropped.] Chapter 20 696", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 703, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p704::c0", "text": "Because the hazard function can be expressed as in equation (20.15), once we specify f , the hazard function can be estimated once we have the MLE, ^y. For ex- ample, the Weibull distribution with covariates has conditional density f ðt j xi; yÞ ¼ expðxibÞata\u00011 exp½\u0001expðxibÞta\u0003 ð20:25Þ where xi contains unity as its ﬁrst element for all i. [We obtain this density from Ex- ample 20.3 with g replaced by expðxibÞ.] The hazard function in this case is simply lðt; xÞ ¼ expðxbÞata\u00011. Example 20.5 (Weibull Model for Recidivism Duration): Let durat be the length of time, in months, until an inmate is arrested after being released from prison. Although the duration is rounded to the nearest month, we treat durat as a continu- ous variable with a Weibull distribution. We are interested in how certain covariates a¤ect the hazard function for recidivism, and also whether there is positive or nega- tive duration dependence, once we have conditioned on the covariates. The variable workprg—a binary indicator for participation in a prison work program—is of par- ticular interest. The data in RECID.RAW, which comes from Chung, Schmidt, and Witte (1991), are ﬂow data because it is a random sample of convicts released from prison during the period July 1, 1977, through June 30, 1978. The data are retrospective in that they were obtained by looking at records in April 1984, which served as the common censoring date. Because of the di¤erent starting times, the censoring times, ci, vary from 70 to 81 months. The results of the Weibull estimation are in Table 20.1. In interpreting the estimates, we use equation (20.17). For small ^bj, we can multi- ply the coe‰cient by 100 to obtain the semielasticity of the hazard with respect to xj. (No covariates appear in logarithmic form, so there are no elasticities among the ^bj.) For example, if tserved increases by one month, the hazard shifts up by about 1.4 percent, and the e¤ect is statistically signiﬁcant. Another year of education reduces the hazard by about 2.3 percent, but the e¤ect is insigniﬁcant at even the 10 percent level against a two-sided alternative. The sign of the workprg coe‰cient is unexpected, at least if we expect the work program to have positive beneﬁts after the inmates are released from prison. (The result is not statistically di¤erent from zero.) The reason could be that the program is ine¤ective or that there is self-selection into the program. For large ^bj, we should exponentiate and subtract unity to obtain the proportion- ate change. For example, at any point in time, the hazard is about 100½expð:477Þ \u0001 1\u0003 ¼ 61:1 percent greater for someone with an alcohol problem than for someone without. Duration Analysis 697", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 704, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p705::c0", "text": "The estimate of a is .806, and the standard error of ^a leads to a strong rejection of H0: a ¼ 1 against H0: a < 1. Therefore, there is evidence of negative duration de- pendence, conditional on the covariates. This means that, for a particular ex-convict, the instantaneous rate of being arrested decreases with the length of time out of prison. When the Weibull model is estimated without the covariates, ^a ¼ :770 (se ¼ :031), which shows slightly more negative duration dependence. This is a typical ﬁnding in applications of Weibull duration models: estimated a without covariate tends to be less than the estimate with covariates. Lancaster (1990, Section 10.2) contains a the- oretical discussion based on unobserved heterogeneity. When we are primarily interested in the e¤ects of covariates on the expected duration (rather than on the hazard), we can apply a censored Tobit analysis to the Table 20.1 Weibull Estimation of Criminal Recidivism Explanatory Variable Coe‰cient (Standard Error) workprg .091 (.091) priors .089 (.013) tserved .014 (.002) felon \u0001.299 (.106) alcohol .447 (.106) drugs .281 (.098) black .454 (.088) married \u0001.152 (.109) educ \u0001.023 (.019) age \u0001.0037 (.0005) constant \u00013.402 (0.301) Observations 1,445 Log likelihood \u00011,633.03 ^a .806 (.031) Chapter 20 698", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 705, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p706::c0", "text": "log of the duration. A Tobit analysis assumes that, for each random draw i, logðt\u0004 i Þ given xi has a Normalðxid; s2Þ distribution, which implies that t\u0004 i given xi has a log- normal distribution. (The ﬁrst element of xi is unity.) The hazard function for a log- normal distribution, conditional on x, is lðt; xÞ ¼ h½ðlog t \u0001 xdÞ=s\u0003=st, where hðzÞ 1 fðzÞ=½1 \u0001 FðzÞ\u0003, fð\u0002Þ is the standard normal probability density function (pdf ), and Fð\u0002Þ is the standard normal cdf. The lognormal hazard function is not monotonic and does not have the proportional hazard form. Nevertheless, the estimates of the dj are easy to interpret because the model is equivalent to logðt\u0004 i Þ ¼ xid þ ei ð20:26Þ where ei is independent of xi and normally distributed. Therefore, the dj are semielasticities—or elasticities if the covariates are in logarithmic form—of the covariates on the expected duration. The Weibull model can also be represented in regression form. When t\u0004 i given xi has density (20.25), expðxibÞðt\u0004 i Þa is independent of xi and has a unit exponential distribution. Therefore, its natural log has a type I extreme value distribution; there- fore, we can write a logðt\u0004 i Þ ¼ \u0001xib þ ui, where ui is independent of xi and has density gðuÞ ¼ expðuÞ expfexpð\u0001uÞg. The mean of ui is not zero, but, because ui is indepen- dent of xi, we can write logðt\u0004 i Þ exactly as in equation (20.26), where the slope coef- ﬁcents are given by dj ¼ \u0001bj=a, and the intercept is more complicated. Now, ei does not have a normal distribution, but it is independent of xi with zero mean. Censoring can be handled by maximum likelihood estimation. The estimated coe‰cients can be compared with the censored Tobit estimates described previously to see if the esti- mates are sensitive to the distributional assumption. In Example 20.5, we can obtain the Weibull estimates of the dj as ^dj ¼ \u0001^bj=^a. (Some econometrics packages, such as Stata, allow direct estimation of the dj and provide standard errors.) For example, ^ddrugs ¼ \u0001:281=:806A\u0001:349. When the lognormal model is used, the coe‰cient on drugs is somewhat smaller in magnitude, about \u0001.298. As another example, ^dage ¼ :0046 in the Weibull estimation and ^dage ¼ :0039 in the lognormal estimation. In both cases, the estimates have t statistics over six. For obtaining estimates on the expected duration, the Weibull and lognormal models give similar results. [Interestingly, the lognormal model ﬁts the data notably better, with log likelihood ¼ \u00011,597.06. This result is consistent with the ﬁndings of Chung, Schmidt, and Witte (1991).] Sometimes we begin by specifying a parametric model for the hazard conditional on x and then use the formulas from Section 20.2 to obtain the cdf and density. This approach is easiest when the hazard leads to a tractable duration distribution, but there is no reason the hazard function must be of the proportional hazard form. Duration Analysis 699", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 706, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p707::c0", "text": "Example 20.6 (Log-Logistic Hazard with Covariates): A log-logistic hazard func- tion with covariates is lðt; xÞ ¼ expðxbÞata\u00011=½1 þ expðxbÞta\u0003 ð20:27Þ where x1 1 1. From equation (20.14) with g ¼ expðxbÞ, the cdf is Fðt j x; yÞ ¼ 1 \u0001 ½1 þ expðxbÞta\u0003\u00011; t b 0 ð20:28Þ The distribution of logðt\u0004 i Þ given xi is logistic with mean \u0001a\u00011 logfexpðxbÞg ¼ \u0001a\u00011xb and variance p2=ð3a2Þ. Therefore, logðt\u0004 i Þ can be written as in equation (20.26) where ei has a zero mean logistic distribution and is independent of xi and d ¼ \u0001a\u00011b. This is another example where the e¤ects of the covariates on the mean duration can be obtained by an OLS regression when there is no censoring. With censoring, the distribution of ei must be accounted for using the log likelihood in expression (20.24). 20.3.3 Stock Sampling Flow data with right censoring are common, but other sampling schemes are also used. With stock sampling we randomly sample from individuals that are in the initial state at a given point in time. The population is again individuals who enter the ini- tial state during a speciﬁed interval, ½0; b\u0003. However, rather than observe a random sample of people ﬂowing into the initial state, we can only obtain a random sample of individuals that are in the initial state at time b. In addition to the possibility of right censoring, we may also face the problem of left censoring, which occurs when some or all of the starting times, ai, are not observed. For now, we assume that (1) we observe the starting times ai for all individuals we sample at time b and (2) we can follow sampled individuals for a certain length of time after we observe them at time b. We also allow for right censoring. In the unemployment duration example, where the population comprises workers who became unemployed at some point during 1998, stock sampling would occur if we randomly sampled from workers who were unemployed during the last week of 1998. This kind of sampling causes a clear sample selection problem: we necessarily exclude from our sample any individual whose unemployment spell ended before the last week of 1998. Because these spells were necessarily shorter than a year, we can- not just assume that the missing observations are randomly missing. The sample selection problem caused by stock sampling is essentially the same situation we faced in Section 17.3, where we covered the truncated regression model. Therefore, we will call this the left truncation problem. Kiefer (1988) calls it length- biased sampling. Chapter 20 700", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 707, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p708::c0", "text": "Under the assumptions that we observe the ai and can observe some spells past the sampling date b, left truncation is fairly easy to deal with. With the exception of replacing ﬂow sampling with stock sampling, we make the same assumptions as in Section 20.3.2. To account for the truncated sampling, we must modify the density in equation (20.23) to reﬂect the fact that part of the population is systematically omitted from the sample. Let ðai; ci; xi; tiÞ denote a random draw from the population of all spells starting in ½0; b\u0003. We observe this vector if and only if the person is still in the initial state at time b, that is, if and only if ai þ t\u0004 i b b or t\u0004 i b b \u0001 ai, where t\u0004 i is the true duration. But, under the conditional independence assumption (20.22), Pðt\u0004 i b b \u0001 ai j ai; ci; xiÞ ¼ 1 \u0001 Fðb \u0001 ai j xi; yÞ ð20:29Þ where Fð\u0002 j xi; yÞ is the cdf of t\u0004 i given xi, as before. The correct conditional density function is obtained by dividing equation (20.23) by equation (20.29). In Problem 20.5 you are asked to adapt the arguments in Section 17.3 to also allow for right censoring. The log-likelihood function can be written as X N i¼1 fdi log½ f ðti j xi; yÞ\u0003 þ ð1 \u0001 diÞ log½1 \u0001 Fðti j xi; yÞ\u0003 \u0001 log½1 \u0001 Fðb \u0001 ai j xi; yÞ\u0003g ð20:30Þ where, again, ti ¼ ci when di ¼ 0. Unlike in the case of ﬂow sampling, with stock sampling both the starting dates, ai, and the length of the sampling interval, b, appear in the conditional likelihood function. Their presence makes it clear that specifying the interval ½0; b\u0003 is important for analyzing stock data. [Lancaster (1990, p. 183) es- sentially derives equation (20.30) under a slightly di¤erent sampling scheme; see also Lancaster (1979).] Equation (20.30) has an interesting implication. If observation i is right censored at calendar date b—that is, if we do not follow the spell after the initial data collection— then the censoring time is ci ¼ b \u0001 ai. Because di ¼ 0 for censored observations, the log likelihood for such an observation is log½1 \u0001 Fðci j xi; yÞ\u0003 \u0001 log½1 \u0001 Fðb \u0001 ai j xi; yÞ\u0003 ¼ 0. In other words, observations that are right censored at the data collection time provide no information for estimating y, at least when we use equation (20.30). Consequently, the log likelihood in equation (20.30) does not identify y if all units are right censored at the interview date: equation (20.30) is identically zero. The intuition for why equation (20.30) fails in this case is fairly clear: our data consist only of ðai; xiÞ, and equation (20.30) is a log likelihood that is conditional on ðai; xiÞ. E¤ec- tively, there is no random response variable. Duration Analysis 701", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 708, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p709::c0", "text": "Even when we censor all observed durations at the interview date, we can still es- timate y, provided—at least in a parametric context—we specify a model for the conditional distribution of the starting times, Dðai j xiÞ. (This is essentially the prob- lem analyzed by Nickell, 1979.) We are still assuming that we observe the ai. So, for example, we randomly sample from the pool of people unemployed in the last week of 1998 and ﬁnd out when their unemployment spells began (along with covariates). We do not follow any spells past the interview date. (As an aside, if we sample un- employed people during the last week of 1998, we are likely to obtain some obser- vations where spells began before 1998. For the population we have speciﬁed, these people would simply be discarded. If we want to include people whose spells began prior to 1998, we need to redeﬁne the interval. For example, if durations are mea- sured in weeks and if we want to consider durations beginning in the ﬁve-year period prior to the end of 1998, then b ¼ 260.) For concreteness, we assume that Dðai j xiÞ is continuous on ½0; b\u0003 with density kð\u0002 j xi; hÞ. Let si denote a sample selection indicator, which is unity if we observe random draw i, that is, if t\u0004 i b b \u0001 ai. Estimation of y (and h) can proceed by apply- ing CMLE to the density of ai conditional on xi and si ¼ 1. [Note that this is the only density we can hope to estimate, as our sample only consists of observations ðai; xiÞ when si ¼ 1.] This density is informative for y even if h is not functionally related to y (as would typically be assumed) because there are some durations that started and ended in ½0; b\u0003; we simply do not observe them. Knowing something about the start- ing time distribution gives us information about the duration distribution. (In the context of ﬂow sampling, when h is not functionally related to y, the density of ai given xi is uninformative for estimating y; in other words, ai is ancillary for y.) In Problem 20.6 you are asked to show that the density of ai conditional on observing ðai; xiÞ is pða j xi; si ¼ 1Þ ¼ kða j xi; hÞ½1 \u0001 Fðb \u0001 a j xi; yÞ\u0003=Pðsi ¼ 1 j xi; y; hÞ ð20:31Þ 0 < a < b, where Pðsi ¼ 1 j xi; y; hÞ ¼ ð b 0 ½1 \u0001 Fðb \u0001 u j xi; yÞ\u0003kðu j xi; hÞ du ð20:32Þ [Lancaster (1990, Section 8.3.3) essentially obtains the right-hand side of equation (20.31) but uses the notion of backward recurrence time. The argument in Problem 20.6 is more straightforward because it is based on a standard truncation argument.] Once we have speciﬁed the duration cdf, F, and the starting time density, k, we can use conditional MLE to estimate y and h: the log likelihood for observation i is just the log of equation (20.31), evaluated at ai. If we assume that ai is independent of Chapter 20 702", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 709, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p710::c0", "text": "xi and has a uniform distribution on ½0; b\u0003, the estimation simpliﬁes somewhat; see Problem 20.6. Allowing for a discontinuous starting time density kð\u0002 j xi; hÞ does not materially a¤ect equation (20.31). For example, if the interval [0,1] represents one year, we might want to allow di¤erent entry rates over the di¤erent seasons. This would correspond to a uniform distribution over each subinterval that we choose. We now turn to the problem of left censoring, which arises with stock sampling when we do not actually know when any spell began. In other words, the ai are not observed, and therefore neither are the true durations, t\u0004 i . However, we assume that we can follow spells after the interview date. Without right censoring, this assump- tion means we can observe the time in the current spell since the interview date, say, ri, which we can write as ri ¼ t\u0004 i þ ai \u0001 b. We still have a left truncation problem because we only observe ri when t\u0004 i > b \u0001 ai, that is, when ri > 0. The general approach is the same as with the earlier problems: we obtain the density of the vari- able that we can at least partially observe, ri in this case, conditional on observing ri. Problem 20.8 asks you to ﬁll in the details, accounting also for possible right censoring. We can easily combine stock sampling and ﬂow sampling. For example, in the case that we observe the starting times, ai, suppose that, at time m < b, we sample a stock of individuals already in the initial state. In addition to following spells of individuals already in the initial state, suppose we can randomly sample individuals ﬂowing into the initial state between times m and b. Then we follow all the individuals appearing in the sample, at least until right censoring. For starting dates after m ðai b mÞ, there is no truncation, and so the log likelihood for these observations is just as in equation (20.24). For ai < m, the log likelihood is identical to equation (20.30) except that m replaces b. Other combinations are easy to infer from the preceding results. 20.3.4 Unobserved Heterogeneity One way to obtain more general duration models is to introduce unobserved hetero- geneity into fairly simple duration models. In addition, we sometimes want to test for duration dependence conditional on observed covariates and unobserved heteroge- neity. The key assumptions used in most models that incorporate unobserved heter- ogeneity are that (1) the heterogeneity is independent of the observed covariates, as well as starting times and censoring times; (2) the heterogeneity has a distribution known up to a ﬁnite number of parameters; and (3) the heterogeneity enters the hazard function multiplicatively. We will make these assumptions. In the context of single-spell ﬂow data, it is di‰cult to relax any of these assumptions. (In the special case of a lognormal duration distribution, we can relax assumption 1 by using Tobit methods with endogenous explanatory variables; see Section 16.6.2.) Duration Analysis 703", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 710, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p711::c0", "text": "Before we cover the general case, it is useful to cover an example due to Lancaster (1979). For a random draw i from the population, a Weibull hazard function condi- tional on observed covariates xi and unobserved heterogeneity vi is lðt; xi; viÞ ¼ vi expðxibÞata\u00011 ð20:33Þ where xi1 1 1 and vi > 0. [Lancaster (1990) calls equation (20.33) a conditional haz- ard, because it conditions on the unobserved heterogeneity vi. Technically, almost all hazards in econometrics are conditional because we almost always condition on observed covariates.] Notice how vi enters equation (20.33) multiplicatively. To identify the parameters a and b we need a normalization on the distribution of vi; we use the most common, EðviÞ ¼ 1. This implies that, for a given vector x, the average hazard is expðxbÞata\u00011. An interesting hypothesis is H0: a ¼ 1, which means that, conditional on xi and vi, there is no duration dependence. In the general case where the cdf of t\u0004 i given ðxi; viÞ is Fðt j xi; vi; yÞ, we can obtain the distribution of t\u0004 i given xi by integrating out the unobserved e¤ect. Because vi and xi are independent, the cdf of t\u0004 i given xi is Gðt j xi; y; rÞ ¼ ðy 0 Fðt j xi; v; yÞhðv; rÞ dv ð20:34Þ where, for concreteness, the density of vi, hð\u0002; rÞ, is assumed to be continuous and depends on the unknown parameters r. From equation (20.34) the density of t\u0004 i given xi, gðt j xi; y; rÞ, is easily obtained. We can now use the methods of Sections 20.3.2 and 20.3.3. For ﬂow data, the log-likelihood function is as in equation (20.24), but with Gðt j xi; y; rÞ replacing Fðt j xi; yÞ and gðt j xi; y; rÞ replacing f ðt j xi; yÞ. We should assume that Dðt\u0004 i j xi; vi; ai; ciÞ ¼ Dðt\u0004 i j xi; viÞ and Dðvi j xi; ai; ciÞ ¼ DðviÞ; these assumptions ensure that the key condition (20.22) holds. The methods for stock sampling described in Section 20.3.3 also apply to the integrated cdf and density. If we assume gamma-distributed heterogeneity—that is, vi @ Gammaðd; dÞ, so that EðviÞ ¼ 1 and VarðviÞ ¼ 1=d—we can ﬁnd the distribution of t\u0004 i given xi for a broad class of hazard functions with multiplicative heterogeneity. Suppose that the hazard function is lðt; xi; viÞ ¼ vikðt; xiÞ, where kðt; xÞ > 0 (and need not have the propor- tional hazard form). For simplicity, we suppress the dependence of kð\u0002; \u0002Þ on un- known parameters. From equation (20.7), the cdf of t\u0004 i given ðxi; viÞ is Fðt j xi; viÞ ¼ 1 \u0001 exp \u0001vi ð t 0 kðs; xiÞ ds \u0001 \u0002 1 1 \u0001 exp½\u0001vixðt; xiÞ\u0003 ð20:35Þ where xðt; xiÞ 1 Ð t 0 kðs; xiÞ ds. We can obtain the cdf of t\u0004 i given xi by using equation (20.34). The density of vi is hðvÞ ¼ ddvd\u00011 expð\u0001dvÞ=GðdÞ, where VarðviÞ ¼ 1=d and Chapter 20 704", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 711, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p712::c0", "text": "Gð\u0002Þ is the gamma function. Let xi 1 xðt; xiÞ for given t. Then ðy 0 expð\u0001xivÞddvd\u00011 expð\u0001dvÞ=GðdÞ dv ¼ ½d=ðd þ xiÞ\u0003d ðy 0 ðd þ xiÞdvd\u00011 exp½\u0001ðd þ xiÞv\u0003=GðdÞ dv ¼ ½d=ðd þ xiÞ\u0003d ¼ ð1 þ xi=dÞ\u0001d where the second-to-last equality follows because the integrand is the Gamma ðd; d þ xiÞ density and must integrate to unity. Now we use equation (20.34): Gðt j xiÞ ¼ 1 \u0001 ½1 þ xðt; xiÞ=d\u0003\u0001d ð20:36Þ Taking the derivative of equation (20.36) with respect to t, using the fact that kðt; xiÞ is the derivative of xðt; xiÞ, yields the density of t\u0004 i given xi as gðt j xiÞ ¼ kðt; xiÞ½1 þ xðt; xiÞ=d\u0003\u0001ðd\u00011Þ ð20:37Þ The function kðt; xÞ depends on parameters y, and so gðt j xÞ should be gðt j x; y; dÞ. With censored data the vector y can be estimated along with d by using the log- likelihood function in equation (20.24) (again, with G replacing F ). With the Weibull hazard in equation (20.33), xðt; xÞ ¼ expðxbÞta, which leads to a very tractable analysis when plugged into equations (20.36) and (20.37); the resulting duration distribution is called the Burr distribution. In the log-logistic case with kðt; xÞ ¼ expðxbÞata\u00011½1 þ expðxbÞta\u0003\u00011, xðt; xÞ ¼ log½1 þ expðxbÞta\u0003. These equa- tions can be plugged into the preceding formulas for a maximum likelihood analysis. Before we end this section, we should recall why we might want to explicitly in- troduce unobserved heterogeneity when the heterogeneity is assumed to be indepen- dent of the observed covariates. The strongest case is seen when we are interested in testing for duration dependence conditional on observed covariates and unobserved heterogeneity, where the unobserved heterogeneity enters the hazard multiplicatively. As carefully exposited by Lancaster (1990, Section 10.2), ignoring multiplicative heterogeneity in the Weibull model results in asymptotically underestimating a. Therefore, we could very well conclude that there is negative duration dependence conditional on x, whereas there is no duration dependence ða ¼ 1Þ conditional on x and v. In a general sense, it is somewhat heroic to think we can distinguish between dura- tion dependence and unobserved heterogeneity when we observe only a single cycle for each agent. The problem is simple to describe: because we can only estimate the distribution of T given x, we cannot uncover the distribution of T given ðx; vÞ unless Duration Analysis 705", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 712, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p713::c0", "text": "we make extra assumptions, a point Lancaster (1990, Section 10.1) illustrates with an example. Therefore, we cannot tell whether the hazard describing T given ðx; vÞ exhibits duration dependence. But, when the hazard has the proportional hazard form lðt; x; vÞ ¼ vkðxÞl0ðtÞ, it is possible to identify the function kð\u0002Þ and the baseline hazard l0ð\u0002Þ quite generally (along with the distribution of v). See Lancaster (1990, Section 7.3) for a presentation of the results of Elbers and Ridder (1982). Recently, Horowitz (1999) has demonstrated how to nonparametrically estimate the baseline hazard and the distribution of the unobserved heterogeneity under fairly weak assumptions. When interest centers on how the observed covariates a¤ect the mean duration, explicitly modeling unobserved heterogeneity is less compelling. Adding unobserved heterogeneity to equation (20.26) does not change the mean e¤ects; it merely changes the error distribution. Without censoring, we would probably estimate b in equation (20.26) by OLS (rather than MLE) so that the estimators would be robust to dis- tributional misspeciﬁcation. With censoring, to perform maximum likelihood, we must know the distribution of t\u0004 i given xi, and this depends on the distribution of vi when we explicitly introduce unobserved heterogeneity. But introducing unobserved heterogeneity is indistinguishable from simply allowing a more ﬂexible duration dis- tribution. 20.4 Analysis of Grouped Duration Data Continuously distributed durations are, strictly speaking, rare in social science appli- cations. Even if an underlying duration is properly viewed as being continuous, mea- surements are necessarily discrete. When the measurements are fairly precise, it is sensible to treat the durations as continuous random variables. But when the mea- surements are coarse—such as monthly, or perhaps even weekly—it can be impor- tant to account for the discreteness in the estimation. Grouped duration data arise when each duration is only known to fall into a certain time interval, such as a week, a month, or even a year. For example, unemployment durations are often measured to the nearest week. In Example 20.2 the time until next arrest is measured to the nearest month. Even with grouped data we can generally estimate the parameters of the duration distribution. The approach we take here to analyzing grouped data summarizes the information on staying in the initial state or exiting in each time interval in a sequence of binary outcomes. (Kiefer, 1988; Han and Hausman, 1990; Meyer, 1990; Lancaster, 1990; McCall, 1994; and Sueyoshi, 1995, all take this approach.) In e¤ect, we have a panel data set where each cross section observation is a vector of binary responses, along Chapter 20 706", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 713, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p714::c0", "text": "with covariates. In addition to allowing us to treat grouped durations, the panel data approach has at least two additional advantages. First, in a proportional hazard speciﬁcation, it leads to easy methods for estimating ﬂexible hazard functions. Sec- ond, because of the sequential nature of the data, time-varying covariates are easily introduced. We assume ﬂow sampling so that we do not have to address the sample selection problem that arises with stock sampling. We divide the time line into M þ 1 inter- vals, ½0; a1Þ; ½a1; a2Þ; . . . ; ½aM\u00011; aMÞ; ½aM; yÞ, where the am are known constants. For example, we might have a1 ¼ 1; a2 ¼ 2; a3 ¼ 3, and so on, but unequally spaced intervals are allowed. The last interval, ½aM; yÞ, is chosen so that any duration fall- ing into it is censored at aM: no observed durations are greater than aM. For a ran- dom draw from the population, let cm be a binary censoring indicator equal to unity if the duration is censored in interval m, and zero otherwise. Notice that cm ¼ 1 implies cmþ1 ¼ 1: if the duration was censored in interval m, it is still censored in in- terval m þ 1. Because durations lasting into the last interval are censored, cMþ1 1 1. Similarly, ym is a binary indicator equal to unity if the duration ends in the mth in- terval and zero otherwise. Thus, ymþ1 ¼ 1 if ym ¼ 1. If the duration is censored in interval m ðcm ¼ 1Þ, we set ym 1 1 by convention. As in Section 20.3, we allow individuals to enter the initial state at di¤erent calen- dar times. In order to keep the notation simple, we do not explicitly show the con- ditioning on these starting times, as the starting times play no role under ﬂow sampling when we assume that, conditional on the covariates, the starting times are independent of the duration and any unobserved heterogeneity. If necessary, starting- time dummies can be included in the covariates. For each person i, we observe ðyi1; ci1Þ; . . . ; ðyiM; ciMÞ, which is a balanced panel data set. To avoid confusion with our notation for a duration (T for the random variable, t for a particular outcome on T ), we use m to index the time intervals. The string of binary indicators for any individual is not unrestricted: we must observe a string of zeros followed by a string of ones. The important information is the interval in which yim becomes unity for the ﬁrst time, and whether that represents a true exit from the initial state or censoring. 20.4.1 Time-Invariant Covariates With time-invariant covariates, each random draw from the population consists of information on fðy1; c1Þ; . . . ; ðyM; cMÞ; xg. We assume that a parametric hazard function is speciﬁed as lðt; x; yÞ, where y is the vector of unknown parameters. Let T denote the time until exit from the initial state. While we do not fully observe T, either we know which interval it falls into, or we know whether it was censored in a Duration Analysis 707", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 714, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p715::c0", "text": "particular interval. This knowledge is enough to obtain the probability that ym takes on the value unity given ðym\u00011; . . . ; y1Þ, ðcm; . . . ; c1Þ, and x. In fact, by deﬁnition this probability depends only on ym\u00011, cm, and x, and only two combinations yield probabilities that are not identically zero or one. These probabilities are Pðym ¼ 0 j ym\u00011 ¼ 0; x; cm ¼ 0Þ ð20:38Þ Pðym ¼ 1 j ym\u00011 ¼ 0; x; cm ¼ 0Þ; m ¼ 1; . . . ; M ð20:39Þ (We deﬁne y0 1 0 so that these equations hold for all m b 1.) To compute these probabilities in terms of the hazard for T, we assume that the duration is condition- ally independent of censoring: T is independent of c1; . . . ; cM, given x ð20:40Þ This assumption allows the censoring to depend on x but rules out censoring that depends on unobservables, after conditioning on x. Condition (20.40) holds for ﬁxed censoring or completely randomized censoring. (It may not hold if censoring is due to nonrandom attrition.) Under assumption (20.40) we have, from equation (20.9), Pðym ¼ 1 j ym\u00011 ¼ 0; x; cm ¼ 0Þ ¼ Pðam\u00011 a T < am j T b am\u00011; xÞ ¼ 1 \u0001 exp \u0001 ð am am\u00011 lðs; x; yÞ ds \u0001 \u0002 1 1 \u0001 amðx; yÞ ð20:41Þ for m ¼ 1; 2; . . . ; M, where amðx; yÞ 1 exp \u0001 ð am am\u00011 lðs; x; yÞ ds \u0001 \u0002 ð20:42Þ Therefore, Pðym ¼ 0 j ym\u00011 ¼ 0; x; cm ¼ 0Þ ¼ amðx; yÞ ð20:43Þ We can use these probabilities to construct the likelihood function. If, for observation i, uncensored exit occurs in interval mi, the likelihood is Y mi\u00011 h¼1 ahðxi; yÞ \" # ½1 \u0001 amiðxi; yÞ\u0003 ð20:44Þ The ﬁrst term represents the probability of remaining in the initial state for the ﬁrst mi \u0001 1 intervals, and the second term is the (conditional) probability that T falls into interval mi. [Because an uncensored duration must have mi a M, expression (20.44) Chapter 20 708", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 715, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p716::c0", "text": "at most depends on a1ðxi; yÞ; . . . ; aMðxi; yÞ.] If the duration is censored in interval mi, we know only that exit did not occur in the ﬁrst mi \u0001 1 intervals, and the likelihood consists of only the ﬁrst term in expression (20.44). If di is a censoring indicator equal to one if duration i is uncensored, the log like- lihood for observation i can be written as X mi\u00011 h¼1 log½ahðxi; yÞ\u0003 þ di log½1 \u0001 amiðxi; yÞ\u0003 ð20:45Þ The log likelihood for the entire sample is obtained by summing expression (20.45) across all i ¼ 1; . . . ; N. Under the assumptions made, this log likelihood represents the density of ðy1; . . . ; yMÞ given ðc1; . . . ; cMÞ and x, and so the conditional maxi- mum likelihood theory covered in Chapter 13 applies directly. The various ways of estimating asymptotic variances and computing test statistics are available. To implement conditional MLE, we must specify a hazard function. One hazard function that has become popular because of its ﬂexibility is a piecewise-constant proportional hazard: for m ¼ 1; . . . ; M, lðt; x; yÞ ¼ kðx; bÞlm; am\u00011 a t < am ð20:46Þ where kðx; bÞ > 0 [and typically kðx; bÞ ¼ expðxbÞ]. This speciﬁcation allows the hazard to be di¤erent (albeit constant) over each time interval. The parameters to be estimated are b and l, where the latter is the vector of lm, m ¼ 1; . . . ; M. {Because durations in ½aM; yÞ are censored at aM, we cannot estimate the hazard over the interval ½aM; yÞ.} As an example, if we have unemployment duration measured in weeks, the hazard can be di¤erent in each week. If the durations are sparse, we might assume a di¤erent hazard rate for every two or three weeks (this assumption places restrictions on the lm). With the piecewise-constant hazard and kðx; bÞ ¼ expðxbÞ, for m ¼ 1; . . . ; M, we have amðx; yÞ 1 exp½\u0001expðxbÞlmðam \u0001 am\u00011Þ\u0003 ð20:47Þ Remember, the am are known constants (often am ¼ m) and not parameters to be estimated. Usually the lm are unrestricted, in which case x does not contain an intercept. The piecewise-constant hazard implies that the duration distribution is discontin- uous at the endpoints, whereas in our discussion in Section 20.2, we assumed that the duration had a continuous distribution. A piecewise-continuous distribution causes no real problems, and the log likelihood is exactly as speciﬁed previously. Alter- natively, as in Han and Hausman (1990) and Meyer (1990), we can assume that T Duration Analysis 709", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 716, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p717::c0", "text": "has a proportional hazard as in equation (20.16) with continuous baseline hazard, l0ð\u0002Þ. Then, we can estimate b along with the parameters ð am am\u00011 l0ðsÞ ds; m ¼ 1; 2; . . . ; M In practice, the approaches are the same, and it is easiest to just assume a piecewise- constant proportional hazard, as in equation (20.46). Once the lm have been estimated along with b, an estimated hazard function is easily plotted: graph ^lm at the midpoint of the interval ½am\u00011; amÞ, and connect the points. Without covariates, maximum likelihood estimation of the lm leads to a well- known estimator of the survivor function. Rather than derive the MLE of the survi- vor function, it is easier to motivate the estimator from the representation of the survivor function as a product of conditional probabilities. For m ¼ 1; . . . ; M, the survivor function at time am can be written as SðamÞ ¼ PðT > amÞ ¼ Y m r¼1 PðT > ar j T > ar\u00011Þ ð20:48Þ [Because a0 ¼ 0 and PðT > 0Þ ¼ 1, the r ¼ 1 term on the right-hand side of equation (20.48) is simply PðT > a1Þ.] Now, for each r ¼ 1; 2; . . . ; M, let Nr denote the number of people in the risk set for interval r: Nr is the number of people who have neither left the initial state nor been censored at time ar\u00011, which is the beginning of interval r. Therefore, N1 is the number of individuals in the initial random sample; N2 is the number of individuals who did not exit the initial state in the ﬁrst interval, less the number of individuals censored in the ﬁrst interval; and so on. Let Er be the number of people observed to leave in the rth interval—that is, in the interval ½ar\u00011; arÞ. A consistent estimator of PðT > ar j T > ar\u00011Þ is ðNr \u0001 ErÞ=Nr, r ¼ 1; 2; . . . ; M. [We must use the fact that the censoring is ignorable in the sense of assumption (20.40), so that there is no sample selection bias in using only the uncensored observations.] It follows from equation (20.48) that a consistent estimator of the survivor function at time am is ^SðamÞ ¼ Y m r¼1 ½ðNr \u0001 ErÞ=Nr\u0003; m ¼ 1; 2; . . . ; M ð20:49Þ This is the Kaplan-Meier estimator of the survivor function (at the points a1; a2; . . . ; aM). Lancaster (1990, Section 8.2) contains a proof that maximum likelihood estimation of the lm (without covariates) leads to the Kaplan-Meier estimator of the Chapter 20 710", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 717, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p718::c0", "text": "survivor function. If there are no censored durations before time am, ^SðamÞ is simply the fraction of people who have not left the initial state at time am, which is obviously consistent for PðT > amÞ ¼ SðamÞ. In the general model, we do not need to assume a proportional hazard speciﬁcation within each interval. For example, we could assume a log-logistic hazard within each interval, with di¤erent parameters for each m. Because the hazard in such cases does not depend on the covariates multiplicatively, we must plug in values of x in order to plot the hazard. Sueyoshi (1995) studies such models in detail. If the intervals ½am\u00011; amÞ are coarser than the data—for example, unemployment is measured in weeks, but we choose ½am\u00011; amÞ to be four weeks for all m—then we can specify nonconstant hazards within each interval. The piecewise-constant hazard corresponds to an exponential distribution within each interval. But we could specify, say, a Weibull distribution within each interval. See Sueyoshi (1995) for details. 20.4.2 Time-Varying Covariates Deriving the log likelihood is more complicated with time-varying covariates, espe- cially when we do not assume that the covariates are strictly exogenous. Nevertheless, we will show that, if the covariates are constant within each time interval ½am\u00011; amÞ, the form of the log likelihood is the same as expression (20.45), provided xi is replaced with xim in interval m. For the population, let x1; x2; . . . ; xM denote the outcomes of the covariates in each of the M time intervals, where we assume that the covariates are constant within an interval. This assumption is clearly an oversimpliﬁcation, but we cannot get very far without it (and it reﬂects how data sets with time-varying covariates are usually constructed). When the covariates are internal and are not necessarily deﬁned after exit from the initial state, the deﬁnition of the covariates in the time intervals is irrelevant; but it is useful to list covariates for all M time periods. We assume that the hazard at time t conditional on the covariates up through time t depends only on the covariates at time t. If past values of the covariates matter, they can simply be included in the covariates at time t. The conditional independence assumption on the censoring indicators is now stated as DðT j T b am\u00011; xm; cmÞ ¼ DðT j T b am\u00011; xmÞ; m ¼ 1; . . . ; M ð20:50Þ This assumption allows the censoring decision to depend on the covariates during the time interval (as well as past covariates, provided they are either included in xm or do not a¤ect the distribution of T given xm). Under this assumption, the probability of exit (without censoring) is Duration Analysis 711", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 718, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p719::c0", "text": "Pðym ¼ 1 j ym\u00011 ¼ 0; xm; cm ¼ 0Þ ¼ Pðam\u00011 a T < am j T b am\u00011; xmÞ ¼ 1 \u0001 exp \u0001 ð am am\u00011 lðs; xm; yÞ ds \u0001 \u0002 1 1 \u0001 amðxm; yÞ ð20:51Þ We can use equation (20.51), along with Pðym ¼ 0 j ym\u00011 ¼ 0; xm; cm ¼ 0Þ ¼ amðxm; yÞ, to build up a partial log likelihood for person i. As we discussed in Section 13.8, this is only a partial likelihood because we are not necessarily modeling the joint distri- bution of ðy1; . . . ; yMÞ given fðx1; c1Þ; . . . ; ðcM; xMÞg. For someone censored in interval m, the information on the duration is contained in yi1 ¼ 0; . . . ; yi;m\u00011 ¼ 0. For someone who truly exits in interval m, there is addi- tional information in yim ¼ 1. Therefore, the partial log likelihood is given by ex- pression (20.45), but, to reﬂect the time-varying covariates, ahðxi; yÞ is replaced by ahðxih; yÞ and amiðxi; yÞ is replaced by amiðxi;mi; yÞ. Each term in the partial log likelihood represents the distribution of ym given ðym\u00011; . . . ; y1Þ, ðxm; . . . ; x1Þ, and ðcm; . . . ; c1Þ. [Most of the probabilities in this con- ditional distribution are either zero or one; only the probabilities that depend on y are shown in expression (20.45).] Therefore, the density is dynamically complete, in the terminology of Section 13.8.3. As shown there, the usual maximum likelihood variance matrix estimators and statistics are asymptotically valid, even though we need not have the full conditional distribution of y given ðx; cÞ. This result would change if, for some reason, we chose not to include past covariates when in fact they a¤ect the current probability of exit even after conditioning on the current covariates. Then the robust forms of the statistics covered in Section 13.8 should be used. In most duration applications we want dynamic completeness. If the covariates are strictly exogenous and if the censoring is strictly exogenous, then the partial likelihood is the full conditional likelihood. The precise strict exoge- neity assumption is DðT j T b am\u00011; x; cÞ ¼ DðT j T b am\u00011; xmÞ; m ¼ 1; . . . ; M ð20:52Þ where x is the vector of covariates across all time periods and c is the vector of cen- soring indicators. There are two parts to this assumption. Ignoring the censoring, assumption (20.52) means that neither future nor past covariates appear in the haz- ard, once current covariates are controlled for. The second implication of assumption (20.52) is that the censoring is also strictly exogenous. With time-varying covariates, the hazard speciﬁcation lðt; xm; yÞ ¼ kðxm; bÞlm; am\u00011 a t < am ð20:53Þ Chapter 20 712", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 719, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p720::c0", "text": "m ¼ 1; . . . ; M, is still attractive. It implies that the covariates have a multiplicative e¤ect in each time interval, and it allows the baseline hazard—the part common to all members of the population—to be ﬂexible. Meyer (1990) essentially uses the speciﬁcation (20.53) to estimate the e¤ect of un- employment insurance on unemployment spells. McCall (1994) shows how to allow for time-varying coe‰cients when kðxm; bÞ ¼ expðxmbÞ. In other words, b is replaced with bm, m ¼ 1; . . . ; M. 20.4.3 Unobserved Heterogeneity We can also add unobserved heterogeneity to hazards speciﬁed for grouped data, even if we have time-varying covariates. With time-varying covariates and unob- served heterogeneity, it is di‰cult to relax the strict exogeneity assumption. Also, with single-spell data, we cannot allow general correlation between the unobserved heterogeneity and the covariates. Therefore, we assume that the covariates are strictly exogenous conditional on unobserved heterogeneity and that the unobserved hetero- geneity is independent of the covariates. The precise assumptions are given by equation (20.52) but where unobserved het- erogeneity, v, appears in both conditioning sets. In addition, we assume that v is in- dependent of ðx; cÞ (which is a further sense in which the censoring is exogenous). In the leading case of the piecewise-constant baseline hazard, equation (20.53) becomes lðt; v; xm; yÞ ¼ vkðxm; bÞlm; am\u00011 a t < am ð20:54Þ where v > 0 is a continuously distributed heterogeneity term. Using the same rea- soning as in Sections 20.4.1 and 20.4.2, the density of ðyi1; . . . ; yiMÞ given ðvi; xi; ciÞ is Y mi\u00011 h¼1 ahðvi; xih; yÞ \" # ½1 \u0001 amiðvi; xi;mi; yÞ\u0003di ð20:55Þ where di ¼ 1 if observation i is uncensored. Because expression (20.55) depends on the unobserved heterogeneity, vi, we cannot use it directly to consistently estimate y. However, because vi is independent of ðxi; ciÞ, with density gðv; dÞ, we can integrate expression (20.55) against gð\u0002 ; dÞ to obtain the density of ðyi1; . . . ; yiMÞ given ðxi; ciÞ. This density depends on the observed data—ðmi; di; xiÞ—and the parameters y and d. From this density, we construct the conditional log likelihood for observation i, and we can obtain the conditional MLE, just as in other nonlinear models with unob- served heterogeneity—see Chapters 15, 16, and 19. Meyer (1990) assumes that the distribution of vi is gamma, with unit mean, and obtains the log-likelihood function Duration Analysis 713", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 720, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p721::c0", "text": "in closed form. McCall (1994) analyzes a heterogeneity distribution that contains the gamma as a special case. It is possible to consistently estimate b and l without specifying a parametric form for the heterogeneity distribution; this approach results in a semiparametric maxi- mum likelihood estimator. Heckman and Singer (1984) ﬁrst showed how to perform this method with a Weibull baseline hazard, and Meyer (1990) proved consistency when the hazard has the form (20.54). The estimated heterogeneity distribution is discrete and, in practice, has relatively few mass points. The consistency argument works by allowing the number of mass points to increase with the sample size. Computation is a di‰cult issue, and the asymptotic distribution of the semiparametric maximum likelihood estimator has not been worked out. 20.5 Further Issues The methods we have covered in this chapter have been applied in many contexts. Nevertheless, there are several important topics that we have neglected. 20.5.1 Cox’s Partial Likelihood Method for the Proportional Hazard Model Cox (1972) suggested a partial likelihood method for estimating the parameters b in a proportional hazard model without specifying the baseline hazard. The strength of Cox’s approach is that the e¤ects of the covariates can be estimated very generally, provided the hazard is of the form (20.16). However, Cox’s method is intended to be applied to ﬂow data as opposed to grouped data. If we apply Cox’s methods to grouped data, we must confront the practically important issue of individuals with identical observed durations. In addition, with time-varying covariates, Cox’s method evidently requires the covariates to be strictly exogenous. Estimation of the hazard function itself is more complicated than the methods for grouped data that we covered in Section 20.4. See Amemiya (1985, Chapter 11) and Lancaster (1990, Chapter 9) for treatments of Cox’s partial likelihood estimator. 20.5.2 Multiple-Spell Data All the methods we have covered assume a single spell for each sample unit. In other words, each individual begins in the initial state and then either is observed leaving the state or is censored. But at least some individuals might have multiple spells, especially if we follow them for long periods. For example, we may observe a person who is initially unemployed, becomes employed, and then after a time becomes unemployed again. If we assume constancy across time about the process driving Chapter 20 714", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 721, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p722::c0", "text": "unemployment duration, we can use multiple spells to aid in identiﬁcation, particu- larly in models with heterogeneity that can be correlated with time-varying covari- ates. Chamberlain (1985) and Honore´ (1993b) contain identiﬁcation results when multiple spells are observed. Chamberlain allowed for correlation between the het- erogeneity and the time-varying covariates. Multiple-spell data are also useful for estimating models with unobserved hetero- geneity when the regressors are not strictly exogenous. Ham and Lalonde (1996) give an example in which participation in a job training program can be related to past unemployment duration, even though eligibility is randomly assigned. See also Wooldridge (2000c) for a general framework that allows feedback to future explan- atory variables in models with unobserved heterogeneity. 20.5.3 Competing Risks Models Another important topic is allowing for more than two possible states. Competing risks models allow for the possibility that an individual may exit into di¤erent alter- natives. For example, a person working full-time may choose to retire completely or work part-time. Han and Hausman (1990) and Sueyoshi (1992) contain discussions of the assumptions needed to estimate competing risks models, with and without unobserved heterogeneity. Problems 20.1. Use the data in RECID.RAW for this problem. a. Using the covariates in Table 20.1, estimate equation (20.26) by censored Tobit. Verify that the log-likelihood value is \u00011,597.06. b. Plug in the mean values for priors, tserved, educ, and age, and the values workprg ¼ 0, felon ¼ 1, alcohol ¼ 1, drugs ¼ 1, black ¼ 0, and married ¼ 0, and plot the estimated hazard for the lognormal distribution. Describe what you ﬁnd. c. Using only the uncensored observations, perform an OLS regression of log(durat) on the covariates in Table 20.1. Compare the estimates on tserved and alcohol with those from part a. What do you conclude? d. Now compute an OLS regression using all data—that is, treat the censored observations as if they are uncensored. Compare the estimates on tserved and alcohol from those in parts a and c. 20.2. Use the data in RECID.RAW to answer these questions: Duration Analysis 715", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 722, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p723::c0", "text": "a. To the Weibull model, add the variables super (¼1 if release from prison was supervised) and rules (number of rules violations while in prison). Do the coe‰- cient estimates on these new variables have the expected signs? Are they statistically signiﬁcant? b. Add super and rules to the lognormal model, and answer the same questions as in part a. c. Compare the estimated e¤ects of the rules variable on the expected duration for the Weibull and lognormal models. Are they practically di¤erent? 20.3. Consider the case of ﬂow sampling, as in Section 20.3.2, but suppose that all durations are censored: di ¼ 1, i ¼ 1; . . . ; N. a. Write down the log-likelihood function when all durations are censored. b. Find the special case of the Weibull distribution in part a. c. Consider the Weibull case where xi only contains a constant, so that Fðt; a; bÞ ¼ 1 \u0001 exp½\u0001expðbÞta\u0003. Show that the Weibull log likelihood cannot be maximized for real numbers ^b and ^a. d. From part c, what do you conclude about estimating duration models from ﬂow data when all durations are right censored? e. If the duration distribution is continuous, ci > b > 0 for some constant b, and Pðt\u0004 i < tÞ > 0 for all t > 0, is it likely, in a large random sample, to ﬁnd that all durations have been censored? 20.4. Suppose that, in the context of ﬂow sampling, we observe covariates xi, the censoring time ci, and the binary indicator di (¼1 if the observation is uncensored). We never observe t\u0004 i . a. Show that the conditional likelihood function has the binary response form. What is the binary ‘‘response’’? b. Use the Weibull model to demonstrate the following when we only observe whether durations are censored: if the censoring times ci are constant, the parameters b and a are not identiﬁed. [Hint: Consider the same case as in Problem 20.3c, and show that the log likelihood depends only on the constant expðbÞca, where c is the common censoring time.] c. Use the lognormal model to argue that, provided the ci vary across i in the popu- lation, the parameters are generally identiﬁed. [Hint: In the binary response model, what is the coe‰cient on logðciÞ?] Chapter 20 716", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 723, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p724::c0", "text": "20.5. In this problem you are to derive the log likelihood in equation (20.30). As- sume that ci > b \u0001 ai for all i, so that we always observe part of each spell after the sampling date, b. In what follows, we supress the parameter vector, y. a. For b \u0001 ai < t < ci, show that Pðt\u0004 i at j xi; ai; ci; si ¼ 1Þ ¼ ½Fðt j xiÞ\u0001Fðb\u0001ai j xiÞ\u0003= ½1 \u0001 Fðb \u0001 ai j xiÞ\u0003. b. Use part a to obtain the density of t\u0004 i conditional on ðxi; ai; ci; si ¼ 1Þ for b \u0001 ai < t < ci. c. Show that Pðti ¼ ci j xi; ai; ci; si ¼ 1Þ ¼ ½1 \u0001 Fðci j xiÞ\u0003=½1 \u0001 Fðb \u0001 ai j xiÞ\u0003. d. Explain why parts b and c lead to equation (20.30). 20.6. Consider the problem of stock sampling where we do not follow spells after the sampling date, b, as described in Section 20.3.3. Let Fð\u0002 j xiÞ denote the cdf of t\u0004 i given xi, and let kð\u0002 j xiÞ denote the continuous density of ai given xi. We drop de- pendence on the parameters for most of the derivations. Assume that t\u0004 i and ai are independent conditional on xi. a. Let si denote a selection indicator, so that si ¼ 1ðt\u0004 i b b \u0001 aiÞ. For any 0 < a < b, show that Pðai a a; si ¼ 1 j xiÞ ¼ ð a 0 kðu j xiÞ½1 \u0001 Fðb \u0001 u j xiÞ\u0003 du b. Derive equation (20.32). {Hint: Pðsi ¼ 1 j xiÞ ¼ Eðsi j xiÞ ¼ E½Eðsi j ai; xiÞ j xi\u0003, and Eðsi j ai; xiÞ ¼ Pðt\u0004 i b b \u0001 ai j xiÞ.} c. For 0 < a < b, what is the cdf of ai given xi and si ¼ 1? Now derive equation (20.31). d. Take b ¼ 1, and assume that the starting time distribution is uniform on ½0; 1\u0003 (independent of xi). Find the density (20.31) in this case. e. For the setup in part d, assume that the duration cdf has the Weibull form, 1 \u0001 exp½\u0001expðxibÞta\u0003. What is the log likelihood for observation i? 20.7. Consider the original stock sampling problem that we covered in Section 20.3.3. There, we derived the log likelihood (20.30) by conditioning on the starting times, ai. This approach is convenient because we do not have to specify a distribu- tion for the starting times. But suppose we have an acceptable model for kð\u0002 j xi; hÞ, the (continuous) density of ai given xi. Further, we maintain assumption (20.22) and assume Dðai j ci; xiÞ ¼ Dðai j xiÞ. a. Show that the log-likelihood function conditional on xi, which accounts for trun- cation, is Duration Analysis 717", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 724, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p725::c0", "text": "X N i¼1 fdi log½ f ðti j xi; yÞ\u0003 þ ð1 \u0001 diÞ log½1 \u0001 Fðti j xi; yÞ\u0003 þ log½kðai j xi; hÞ\u0003 \u0001 log½Pðsi ¼ 1 j xi; y; hÞ\u0003g ð20:56Þ where Pðsi ¼ 1 j xi; y; hÞ is given in equation (20.32). b. Discuss the trade-o¤s in using equation (20.30) or the log likelihood in (20.56). 20.8. In the context of stock sampling, where we are interested in the population of durations starting in ½0; b\u0003, suppose that we interview at date b, as usual, but we do not observe any starting times. {This assumption raises the issue of how we know individual i’s starting time is in the speciﬁed interval, ½0; b\u0003. We assume that the in- terval is deﬁned to make this condition true for all i.} Let r\u0004 i ¼ ai þ t\u0004 i \u0001 b, which can be interpreted as the calendar date at which the spell ends minus the interview date. Even without right censoring, we observe r\u0004 i only if r\u0004 i > 0, in which case r\u0004 i is simply the time in the spell since the interview date, b. Assume that t\u0004 i and ai are independent conditional on xi. a. Show that for r > 0, the density of r\u0004 i given xi is hðr j xi; y; hÞ 1 ð b 0 kðu j xi; hÞf ðr þ b \u0001 u j xi; yÞ du where, as before, kða j xi; hÞ is the density of ai given xi and f ðt j xi; yÞ is the duration density. b. Let q > 0 be a ﬁxed censoring time after the interview date, and deﬁne ri ¼ minðr\u0004 i ; qÞ. Find Pðri ¼ q j xiÞ in terms of the cdf of r\u0004 i , say, Hðr j xi; y; hÞ. c. Use parts a and b, along with equation (20.32), to show that the log likelihood conditional on observing ðri; xiÞ is di log½hðri j xi; y; hÞ\u0003 þ ð1 \u0001 diÞ log½1 \u0001 Hðri j xi; y; hÞ\u0003 \u0001 log ð b 0 ½1 \u0001 Fðb \u0001 u j xi; yÞ\u0003kðu j xi; hÞ du \u0004 \u0005 ð20:57Þ where di ¼ 1 if observation i has not been right censored. d. Simplify the log likelihood from part c when b ¼ 1 and kða j xi; hÞ is the uniform density on ½0; 1\u0003. 20.9. Consider the Weibull model with multiplicative heterogeneity, as in equation (20.33), where vi takes on only two values, 1=r and 0, with probabilities r and 1 \u0001 r, Chapter 20 718", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 725, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p726::c0", "text": "respectively, where 0 < r < 1. This parameterization imposes the normalization EðviÞ ¼ 1. You can think of a situation where there are only two types of people, type A ðvi ¼ 0Þ and type B ðvi ¼ 1=rÞ. a. Show that, as the di¤erence between the two types grows, the probability of being type B must shrink to zero. b. Find the cdf of t\u0004 i given xi. c. Find the log-likelihood function for observation i in terms of a, b, and r. 20.10. Let 0 < a1 < a2 < \u0002 \u0002 \u0002 < aM\u00011 < aM be a positive, increasing set of con- stants, and let T be a nonnegative random variable with PðT > 0Þ ¼ 1. a. Show that, for any m ¼ 1; . . . ; M, PðT > amÞ ¼ PðT > am j T > am\u00011ÞPðT > am\u00011Þ. b. Use part a to prove equation (20.48). Duration Analysis 719", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 726, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p727::c0", "text": "References Abrevaya, J. (1997), ‘‘The Equivalence of Two Estimators for the Fixed E¤ects Logit Model,’’ Economics Letters 55, 41–43. Ahn, H., and J. L. Powell (1993), ‘‘Semiparametric Estimation of Censored Selection Models with a Nonparametric Selection Mechanism,’’ Journal of Econometrics 58, 3–29. Ahn, S. C., and P. Schmidt (1995), ‘‘E‰cient Estimation of Models for Dynamic Panel Data,’’ Journal of Econometrics 68, 5–27. Ai, C. (1997), ‘‘A Semiparametric Maximum Likelihood Estimator,’’ Econometrica 65, 933–963. Aitchison, J., and S. D. Silvey (1958), ‘‘Maximum-Likelihood Estimation of Parameters Subject to Con- straints,’’ Annals of Mathematical Statistics 29, 813–828. Altonji, J. G., and L. M. Segal (1996), ‘‘Small-Sample Bias in GMM Estimation of Covariance Struc- tures,’’ Journal of Business and Economic Statistics 14, 353–366. Amemiya, T. (1973), ‘‘Regression Analysis When the Dependent Variable Is Truncated Normal,’’ Econo- metrica 41, 997–1016. Amemiya, T. (1974), ‘‘The Nonlinear Two-Stage Least-Squares Estimator,’’ Journal of Econometrics 2, 105–110. Amemiya, T. (1985), Advanced Econometrics. Cambridge, MA: Harvard University Press. Andersen, E. B. (1970), ‘‘Asymptotic Properties of Conditional Maximum Likelihood Estimators,’’ Journal of the Royal Statistical Society, Series B, 32, 283–301. Anderson, T. W., and C. Hsiao (1982), ‘‘Formulation and Estimation of Dynamic Models Using Panel Data,’’ Journal of Econometrics 18, 67–82. Andrews, D. W. K. (1989), ‘‘Power in Econometric Applications,’’ Econometrica 57, 1059–1090. Angrist, J. D. (1990), ‘‘Lifetime Earnings and the Vietnam Era Draft Lottery: Evidence from Social Secu- rity Administrative Records,’’ American Economic Review 80, 313–336. Angrist, J. D. (1991), ‘‘Instrumental Variables Estimation of Average Treatment E¤ects in Econometrics and Epidemiology,’’ National Bureau of Economic Research Technical Working Paper Number 115. Angrist, J. D. (1998), ‘‘Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants,’’ Econometrica 66, 249–288. Angrist, J. D., and G. W. Imbens (1995), ‘‘Two-Stage Least Squares Estimation of Average Causal E¤ects in Models with Variable Treatment Intensity,’’ Journal of the American Statistical Association 90, 431–442. Angrist, J. D., G. W. Imbens, and D. B. Rubin (1996), ‘‘Identiﬁcation and Causal E¤ects Using Instru- mental Variables,’’ Journal of the American Statistical Association 91, 444–455. Angrist, J. D., and A. B. Krueger (1991), ‘‘Does Compulsory School Attendance A¤ect Schooling and Earnings?’’ Quarterly Journal of Economics 106, 979–1014. Angrist, J. D., and V. Lavy (1999), ‘‘Using Maimonides’ Rule to Estimate the E¤ect of Class Size on Scholastic Achievement,’’ Quarterly Journal of Economics 114, 533–575. Angrist, J. D., and W. K. Newey (1991), ‘‘Overidentiﬁcation Tests in Earnings Functions with Fixed E¤ects,’’ Journal of Business and Economic Statistics 9, 317–323. Arellano, M. (1987), ‘‘Computing Robust Standard Errors for Within-Groups Estimators,’’ Oxford Bulle- tin of Economics and Statistics 49, 431–434. Arellano, M., and S. R. Bond (1991), ‘‘Some Speciﬁcation Tests for Panel Data: Monte Carlo Evidence and an Application to Employment Equations,’’ Review of Economic Studies 58, 277–298. Arellano, M., and O. Bover (1995), ‘‘Another Look at the Instrumental Variables Estimation of Error- Component Models,’’ Journal of Econometrics 68, 29–51. Arellano, M., and B. E. Honore´ (in press), ‘‘Panel Data: Some Recent Developments,’’ Handbook of Econometrics, Volume 5, ed. E. Leamer and J. J. Heckman. Amsterdam: North Holland. Ashenfelter, O., and A. B. Krueger (1994), ‘‘Estimates of the Economic Return to Schooling from a New Sample of Twins,’’ American Economic Review 84, 1157–1173.", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 727, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p728::c0", "text": "Ashenfelter, O., and C. E. Rouse (1998), ‘‘Income, Schooling, and Ability: Evidence from a New Sample of Identical Twins,’’ Quarterly Journal of Economics 113, 253–284. Ayers, I., and S. D. Levitt (1998), ‘‘Measuring Positive Externalities from Unobservable Victim Precau- tion: An Empirical Analysis of Lojack,’’ Quarterly Journal of Economics 108, 43–77. Baltagi, B. H. (1981), ‘‘Simultaneous Equations with Error Components,’’ Journal of Econometrics 17, 189–200. Baltagi, B. H. (1995), Econometric Analysis of Panel Data. New York: Wiley. Baltagi, B. H., and Q. Li (1995), ‘‘Testing AR(1) Against MA(1) Disturbances in an Error Component Model,’’ Journal of Econometrics 68, 133–151. Barnow, B., G. Cain, and A. Goldberger (1980), ‘‘Issues in the Analysis of Selectivity Bias,’’ Evaluation Studies 5, 42–59. Bartik, T. J. (1987), ‘‘The Estimation of Demand Parameters in Hedonic Price Models,’’ Journal of Polit- ical Economy 95, 81–88. Bassett, G., and R. Koenker (1978), ‘‘Asymptotic Theory of Least Absolute Error Regression,’’ Journal of the American Statistical Association 73, 618–622. Bassi, L. J. (1984), ‘‘Estimating the E¤ect of Job Training Programs with Non-Random Selection,’’ Review of Economics and Statistics 66, 36–43. Bates, C. E., and H. White (1993), ‘‘Determination of Estimators with Minimum Asymptotic Covariances Matrices,’’ Econometric Theory 9, 633–648. Bera, A. K., and C. R. McKenzie (1986), ‘‘Alternative Forms and Properties of the Score Test,’’ Journal of Applied Statistics 13, 13–25. Berndt, E. R., B. H. Hall, R. E. Hall, and J. A. Hausman (1974), ‘‘Estimation and Inference in Nonlinear Structural Models,’’ Annals of Economic and Social Measurement 3, 653–666. Bhargava, A., L. Franzini, and W. Narendranathan (1982), ‘‘Serial Correlation and the Fixed E¤ects Model,’’ Review of Economic Studies 49, 533–549. Biddle, J. E., and D. S. Hamermesh (1990), ‘‘Sleep and the Allocation of Time,’’ Journal of Political Economy 98, 922–943. Billingsley, P. (1979), Probability and Measure. New York: John Wiley. Blackburn, M., and D. Neumark (1992), ‘‘Unobserved Ability, E‰ciency Wages, and Interindustry Wage Di¤erentials,’’ Quarterly Journal of Economics 107, 1421–1436. Blundell, R., and S. Bond (1998), ‘‘Initial Conditions and Moment Restrictions in Dynamic Panel Data Models,’’ Journal of Econometrics 87, 115–144. Blundell, R., R. Gri‰th, and F. Windmeijer (1998), ‘‘Individual E¤ects and Dynamics in Count Data Models,’’ mimeo, Institute of Fiscal Studies, London. Bound, J., D. A. Jaeger, and R. M. Baker (1995), ‘‘Problems with Instrumental Variables Estimation When the Correlation between the Instruments and Endogenous Explanatory Variables Is Weak,’’ Journal of the American Statistical Association 90, 443–450. Breusch, T. S., G. E. Mizon, and P. Schmidt (1989), ‘‘E‰cient Estimation Using Panel Data,’’ Econo- metrica 57, 695–700. Breusch, T. S., and A. R. Pagan (1979), ‘‘A Simple Test for Heteroskedasticity and Random Coe‰cient Variation,’’ Econometrica 50, 987–1007. Breusch, T. S., and A. R. Pagan (1980), ‘‘The LM Test and Its Applications to Model Speciﬁcation in Econometrics,’’ Review of Economic Studies 47, 239–254. Breusch, T., H. Qian, P. Schmidt, and D. Wyhowski (1999), ‘‘Redundancy of Moment Conditions,’’ Journal of Econometrics 91, 89–111. Bronars, S. G., and J. Grogger (1994), ‘‘The Economic Consequences of Unwed Motherhood: Using Twin Births as a Natural Experiment,’’ American Economic Review 84, 1141–1156. References 722", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 728, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p729::c0", "text": "Brown, B. W., and M. B. Walker (1995), ‘‘Stochastic Speciﬁcation in Random Production Models of Cost- Minimizing Firms,’’ Journal of Econometrics 66, 175–205. Buchinsky, M. (1994), ‘‘Changes in the U.S. Wage Structure: Application of Quantile Regression,’’ Econ- ometrica 62, 405–458. Buchinsky, M., and J. Hahn (1998), ‘‘An Alternative Estimator for the Censored Quantile Regression Model,’’ Econometrica 66, 653–671. Butler, J. S., and R. A. Mo‰tt (1982), ‘‘A Computationally E‰cient Quadrature Procedure for the One- Factor Multinomial Probit Model,’’ Econometrica 50, 761–764. Cameron, A. C., and P. K. Trivedi (1986), ‘‘Econometric Models Based on Count Data: Comparisons and Applications of Some Estimators and Tests,’’ Journal of Applied Econometrics 1, 29–53. Cameron, A. C., and P. K. Trivedi (1998), Regression Analysis of Count Data. Cambridge: Cambridge University Press. Card, D. (1995), ‘‘Using Geographic Variation in College Proximity to Estimate the Return to Schooling,’’ in Aspects of Labour Market Behavior: Essays in Honour of John Vanderkamp, ed. L. N. Christophides, E. K. Grant, and R. Swidinsky. Toronto: University of Toronto Press, 201–222. Case, A. C., and L. F. Katz (1991), ‘‘The Company You Keep: The E¤ects of Family and Neighborhood on Disadvantaged Youths,’’ National Bureau of Economic Research Working Paper Number 3705. Chamberlain, G. (1980), ‘‘Analysis of Covariance with Qualitative Data,’’ Review of Economic Studies 47, 225–238. Chamberlain, G. (1982), ‘‘Multivariate Regression Models for Panel Data,’’ Journal of Econometrics 18, 5–46. Chamberlain, G. (1984), ‘‘Panel Data,’’ in Handbook of Econometrics, Volume 2, ed. Z. Griliches and M. D. Intriligator. Amsterdam: North Holland, 1247–1318. Chamberlain, G. (1985), ‘‘Heterogeneity, Omitted Variable Bias, and Duration Dependence,’’ in Longitu- dinal Analysis of Labor Market Data, ed. J. J. Heckman and B. Singer. Cambridge: Cambridge University Press, 3–38. Chamberlain, G. (1987), ‘‘Asymptotic E‰ciency in Estimation with Conditional Moment Restrictions,’’ Journal of Econometrics 34, 305–334. Chamberlain, G. (1992a), ‘‘E‰ciency Bounds for Semiparametric Regression,’’ Econometrica 60, 567–596. Chamberlain, G. (1992b), ‘‘Comment: Sequential Moment Restrictions in Panel Data,’’ Journal of Business and Economic Statistics 10, 20–26. Chesher, A., and R. Spady (1991), ‘‘Asymptotic Expansions of the Information Matrix Test Statistic,’’ Econometrica 59, 787–815. Chung, C.-F., and A. Goldberger (1984), ‘‘Proportional Projections in Limited Dependent Variable Models,’’ Econometrica 52, 531–534. Chung, C.-F., P. Schmidt, and A. D. Witte (1991), ‘‘Survival Analysis: A Survey,’’ Journal of Quantitative Criminology 7, 59–98. Cornwell, C., P. Schmidt, and D. Wyhowski (1992), ‘‘Simultaneous Equations Models and Panel Data,’’ Journal of Econometrics 51, 151–181. Cornwell, C., and D. Trumball (1994), ‘‘Estimating the Economic Model of Crime with Panel Data,’’ Re- view of Economics and Statistics 76, 360–366. Cosslett, S. R. (1981), ‘‘E‰cient Estimation of Discrete-Choice Models,’’ in Structural Analysis of Discrete Data with Econometric Applications, ed. C. F. Manski and D. McFadden. Cambridge, MA: MIT Press, 51–111. Cosslett, S. R. (1993), ‘‘Estimation from Endogenously Stratiﬁed Samples,’’ in Handbook of Statistics, Volume 11, ed. G. S. Maddala, C. R. Rao, and H. D. Vinod. Amsterdam: North Holland, 1–43. Costa, D. L. (1995), ‘‘Pensions and Retirements: Evidence from Union Army Veterans,’’ Quarterly Journal of Economics 110, 297–319. References 723", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 729, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p730::c0", "text": "Cox, D. R. (1972), ‘‘Regression Models and Life Tables,’’ Journal of the Royal Statistical Society, Series B, 34, 187–220. Cragg, J. (1971), ‘‘Some Statistical Models for Limited Dependent Variables with Applications to the De- mand for Durable Goods,’’ Econometrica 39, 829–844. Cragg, J. (1983), ‘‘More E‰cient Estimation in the Presence of Heteroskedasticity of Unknown Form,’’ Econometrica 51, 751–763. Cragg, J. G., and S. G. Donald (1996), ‘‘Inferring the Rank of a Matrix,’’ Journal of Econometrics 76, 223– 250. Currie, J., and N. Cole (1993), ‘‘Welfare and Child Health: The Link between AFDC Participation and Birth Weight,’’ American Economic Review 83, 971–983. Currie, J., and D. Thomas (1995), ‘‘Does Head Start Make a Di¤erence?’’ American Economic Review 85, 341–364. Cutler, D. M., and E. L. Glaeser (1997), ‘‘Are Ghettos Good or Bad?’’ Quarterly Journal of Economics 112, 827–872. Davidson, J. (1994), Stochastic Limit Theory. Oxford: Oxford University Press. Davidson, R., and J. G. MacKinnon (1984), ‘‘Convenient Speciﬁcation Tests for Logit and Probit Models,’’ Journal of Econometrics 24, 241–262. Davidson, R., and J. G. MacKinnon (1985), ‘‘Heteroskedasticity-Robust Tests in Regression Directions,’’ Annale de l’INSE´ E´ 59/60, 183–218. Davidson, R., and J. G. MacKinnon (1992), ‘‘A New Form of the Information Matrix Test,’’ Econo- metrica 60, 145–147. Davidson, R., and J. G. MacKinnon (1993), Estimation and Inference in Econometrics. New York: Oxford University Press. Deaton, A. (1995), ‘‘Data and Econometric Tools for Development Analysis,’’ in Handbook of Develop- ment Economics, Volume 3A, ed. J. Berhman and T. N. Srinivasan. Amsterdam: North Holland, 1785– 1882. Dehejia, R. H., and S. Wahba (1999), ‘‘Causal E¤ects in Non-Experimental Studies: Evaluating the Eval- uation of Training Programs,’’ Journal of the American Statistical Association 94, 1053–1062. Donald, S. G., and H. J. Paarsch (1996), ‘‘Identiﬁcation, Estimation, and Testing in Parametric Empirical Models of Auctions within the Independent Private Values Paradigm,’’ Econometric Theory 12, 517–567. Downes, T. M., and S. M. Greenstein (1996), ‘‘Understanding the Supply Decisions of Nonproﬁts: Mod- eling the Location of Private Schools,’’ Rand Journal of Economics 27, 365–390. Dustmann, C., and M. E. Rochina-Barrachina (2000), ‘‘Selection Correction in Panel Data Models: An Application to Labour Supply and Wages,’’ mimeo, Department of Economics, University College London. Eicker, F. (1967), ‘‘Limit Theorems for Regressions with Unequal and Dependent Errors,’’ Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 1, 59–82. Berkeley: University of California Press. Elbers, C., and G. Ridder (1982), ‘‘True and Spurious Duration Dependence: The Identiﬁability of the Proportional Hazard Model,’’ Review of Economic Studies 49, 403–410. El Sayyad, G. M. (1973), ‘‘Bayesian and Classical Analysis of Poisson Regression,’’ Journal of the Royal Statistical Society, Series B, 35, 445–451. Engle, R. F. (1982), ‘‘Autoregressive Conditional Heteroskedasticity with Estimates of the Variance of U.K. Inﬂation,’’ Econometrica 50, 987–1008. Engle, R. F. (1984), ‘‘Wald, Likelihood Ratio, and Lagrange Multiplier Statistics in Econometrics,’’ in Handbook of Econometrics, Volume 2, ed. Z. Griliches and M. D. Intriligator. Amsterdam: North Holland, 776–828. Epple, D. (1987), ‘‘Hedonic Prices and Implicit Markets: Estimated Demand and Supply Functions for Di¤erentiated Products,’’ Journal of Political Economy 95, 59–80. References 724", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 730, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p731::c0", "text": "Estrella, A. (1998), ‘‘A New Measure of Fit for Equations with Dichotomous Dependent Variables,’’ Journal of Business and Economic Statistics 16, 198–205. Evans, W. N., W. E. Oates, and R. M. Schwab (1992), ‘‘Measuring Peer Group E¤ects: A Study of Teenage Behavior,’’ Journal of Political Economy 100, 966–991. Evans, W. N., and R. M. Schwab (1995), ‘‘Finishing High School and Starting College: Do Catholic Schools Make a Di¤erence?’’ Quarterly Journal of Economics 110, 941–974. Fin, T., and P. Schmidt (1984), ‘‘A Test of the Tobit Speciﬁcation Against an Alternative Suggested by Cragg,’’ Review of Economics and Statistics 66, 174–177. Fisher, F. M. (1965), ‘‘Identiﬁability Criteria in Nonlinear Systems: A Further Note,’’ Econometrica 33, 197–205. Foster, A. D., and M. R. Rosenzweig (1995), ‘‘Learning by Doing and Learning from Others: Human Capital and Technical Change in Agriculture,’’ Journal of Political Economy 103, 1176–1209. Friedberg, L. (1998), ‘‘Did Unilateral Divorce Raise Divorce Rates? Evidence from Panel Data,’’ Ameri- can Economic Review 88, 608–627. Gallant, A. R. (1987), Nonlinear Statistical Models. New York: Wiley. Gallant, A. R., and H. White (1988), A Uniﬁed Theory of Estimation and Inference for Nonlinear Dynamic Models. New York: Blackwell. Garen, J. (1984), ‘‘The Returns to Schooling: A Selectivity Bias Approach with a Continuous Choice Variable,’’ Econometrica 52, 1199–1218. Geronimus, A. T., and S. Korenman (1992), ‘‘The Socioeconomic Consequences of Teen Childbearing Reconsidered,’’ Quarterly Journal of Economics 107, 1187–1214. Geweke, J., and M. P. Keane (in press), ‘‘Computationally Intensive Methods for Integration in Econom- ics,’’ Handbook of Econometrics, Volume 5, ed. E. Leamer and J. J. Heckman. Amsterdam: North Holland. Goldberger, A. S. (1968), Topics in Regression Analysis. New York: Macmillan. Goldberger, A. S. (1972), ‘‘Structural Equation Methods in the Social Sciences,’’ Econometrica 40, 979– 1001. Goldberger, A. S. (1981), ‘‘Linear Regression after Selection,’’ Journal of Econometrics 15, 357–366. Goldberger, A. S. (1991), A Course in Econometrics. Cambridge, MA: Harvard University Press. Gordy, M. B. (1999), ‘‘Hedging Winner’s Curse with Multiple Bids: Evidence from the Portuguese Trea- sury Bill Auction,’’ Review of Economics and Statistics 81, 448–465. Gourieroux, C., A. Monfort, and C. Trognon (1984a), ‘‘Pseudo–Maximum Likelihood Methods: Theory,’’ Econometrica 52, 681–700. Gourieroux, C., A. Monfort, and C. Trognon (1984b), ‘‘Pseudo–Maximum Likelihood Methods: Appli- cations to Poisson Models,’’ Econometrica 52, 701–720. Greene, W. (1997), Econometric Analysis. New York: Macmillan, 3rd edition. Gregory, A. W., and M. R. Veall (1985), ‘‘On Formulating Wald Tests for Nonlinear Restrictions,’’ Econometrica 53, 1465–1468. Griliches, Z., B. H. Hall, and J. A. Hausman (1978), ‘‘Missing Data and Self-Selection in Large Panels,’’ Annale de l’INSE´ E´ 30/31, 137–176. Griliches, Z., and J. A. Hausman (1986), ‘‘Errors in Variables in Panel Data,’’ Journal of Econometrics 31, 93–118. Griliches, Z., and W. M. Mason (1972), ‘‘Education, Income and Ability,’’ Journal of Political Economy, Part II, 80, S74–S103. Gronau, R. (1974), ‘‘Wage Comparisons—A Selectivity Bias,’’ Journal of Political Economy 82, 1119–1143. Gruber, J., and J. M. Poterba (1994), ‘‘Tax Incentives and the Decision to Purchase Health Insurance: Evidence from the Self-Employed,’’ Quarterly Journal of Economics 109, 701–733. References 725", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 731, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p732::c0", "text": "Gurmu, S., and P. K. Trivedi (1994), ‘‘Recent Developments in Models of Event Counts: A Survey,’’ University of Virginia Department of Economics Discussion Paper Number 261. Haavelmo, T. (1943), ‘‘The Statistical Implications of a System of Simultaneous Equations,’’ Econometrica 11, 1–12. Hagy, A. P. (1998), ‘‘The Demand for Child Care Quality: An Hedonic Price Approach,’’ Journal of Human Resources 33, 683–710. Hahn, J. (1998), ‘‘On the Role of the Propensity Score in E‰cient Semiparametric Estimation of Average Treatment E¤ects,’’ Econometrica 66, 315–331. Hahn, J. (1999), ‘‘How Informative is the Initial Condition in the Dynamic Panel Data Model with Fixed E¤ects?’’ Journal of Econometrics 93, 309–326. Hajivassiliou, V. A. (1993), ‘‘Simulation Estimation Methods for Limited Dependent Variable Models,’’ in Handbook of Statistics, Volume 11, ed. G. S. Maddala, C. R. Rao, and H. D. Vinod. Amsterdam: North Holland, 519–543. Hajivassiliou, V. A., and P. A. Ruud (1994), ‘‘Classical Estimation Methods for LDV Models Using Sim- ulation,’’ in Handbook of Econometrics, Volume 4, ed. R. F. Engle and D. McFadden. Amsterdam: North Holland, 2383–2441. Hall, A. (1987), ‘‘The Information Matrix Test for the Linear Model,’’ Review of Economic Studies 54, 257–263. Hall, P. (1994), ‘‘Methodology and Theory for the Bootstrap,’’ in Handbook of Econometrics, Volume 4, ed. R. F. Engle and D. McFadden. Amsterdam: North Holland, 2341–2381. Ham, J. C., and R. J. Lalonde (1996), ‘‘The E¤ect of Sample Selection and Initial Conditions in Duration Models: Evidence from Experimental Data on Training,’’ Econometrica 64, 175–205. Hamilton, J. D. (1994), Time Series Analysis. Princeton, NJ: Princeton University Press. Han, A. K., and J. A. Hausman (1990), ‘‘Flexible Parametric Estimation of Duration and Competing Risk Models,’’ Journal of Applied Econometrics 5, 1–28. Hansen, L. P. (1982), ‘‘Large Sample Properties of Generalized Method of Moments Estimators,’’ Econo- metrica 50, 1029–1054. Ha¨rdle, W., and O. Linton (1994), ‘‘Applied Nonparametric Methods,’’ in Handbook of Econometrics, Volume 4, ed. R. F. Engle and D. McFadden. Amsterdam: North Holland, 2295–2339. Hausman, J. A. (1978), ‘‘Speciﬁcation Tests in Econometrics,’’ Econometrica 46, 1251–1271. Hausman, J. (1983), ‘‘Speciﬁcation and Estimation of Simultaneous Equations Models,’’ in Handbook of Econometrics, Volume 1, ed. Z. Griliches and M. D. Intriligator. Amsterdam: North Holland, 391–448. Hausman, J. A., B. H. Hall, and Z. Griliches (1984), ‘‘Econometric Models for Count Data with an Ap- plication to the Patents-R&D Relationship,’’ Econometrica 52, 909–938. Hausman, J. A., and D. L. McFadden (1984), ‘‘A Speciﬁcation Test for the Multinomial Logit Model,’’ Econometrica 52, 1219–1240. Hausman, J. A., W. K. Newey, and W. E. Taylor (1987), ‘‘E‰cient Estimation and Identiﬁcation of Simultaneous Equation Models with Covariance Restrictions,’’ Econometrica 55, 849–874. Hausman, J. A., and W. E. Taylor (1981), ‘‘Panel Data and Unobservable Individual E¤ects,’’ Econo- metrica 49, 1377–1398. Hausman, J. A., and D. A. Wise (1977), ‘‘Social Experimentation, Truncated Distributions, and E‰cient Estimation,’’ Econometrica 45, 319–339. Hausman, J. A., and D. A., Wise (1978), ‘‘A Conditional Probit Model for Qualitative Choice: Discrete Decisions Recognizing Interdependence and Heterogeneous Preferences,’’ Econometrica 46, 403–426. Hausman, J. A., and D. A. Wise (1981), ‘‘Stratiﬁcation on an Endogenous Variable and Estimation: The Gary Income Maintenance Experiment,’’ in Structural Analysis of Discrete Data with Econometric Appli- cations, ed. C. F. Manski and D. McFadden. Cambridge, MA: MIT Press, 365–391. References 726", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 732, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p733::c0", "text": "Heckman, J. J. (1976), ‘‘The Common Structure of Statistical Models of Truncation, Sample Selection, and Limited Dependent Variables and a Simple Estimator for Such Models,’’ Annals of Economic and Social Measurement 5, 475–492. Heckman, J. J. (1978), ‘‘Dummy Endogenous Variables in a Simultaneous Equations System,’’ Econo- metrica 46, 931–960. Heckman, J. J. (1979), ‘‘Sample Selection Bias as a Speciﬁcation Error,’’ Econometrica 47, 153–161. Heckman, J. J. (1981), ‘‘The Incidental Parameters Problem and the Problem of Initial Conditions in Estimating a Discrete Time–Discrete Data Stochastic Process,’’ in Structural Analysis of Discrete Data with Econometric Applications, ed. C. F. Manski and D. McFadden. Cambridge, MA: MIT Press, 179–195. Heckman, J. J. (1992), ‘‘Randomization and Social Program Evaluation,’’ in Evaluating Welfare and Train- ing Programs, ed. C. F. Manski and I. Garﬁnkel. Cambridge, MA: Harvard University Press, 201–230. Heckman, J. J. (1997), ‘‘Instrumental Variables: A Study of Implicit Behavioral Assumptions Used in Making Program Evaluations,’’ Journal of Human Resources 32, 441–462. Heckman, J. J., and V. J. Hotz (1989), ‘‘Choosing among Alternative Nonexperimental Methods for Estimating the Impact of Social Programs: The Case of Manpower Training,’’ Journal of the American Statistical Association 84, 862–875. Heckman, J. J., H. Ichimura, and P. Todd (1997), ‘‘Matching as an Econometric Evaluation Estimator,’’ Review of Economic Studies 65, 261–294. Heckman, J. J., L. Lochner, and C. Taber (1998), ‘‘General-Equilibrium Treatment E¤ects,’’ American Economic Review 88, 381–386. Heckman, J. J., and R. Robb (1985), ‘‘Alternative Methods for Evaluating the Impact of Interventions,’’ in Longitudinal Analysis of Labor Market Data, ed. J. J. Heckman and B. Singer. New York: Cambridge University Press, 156–245. Heckman, J. J., and B. Singer (1984), ‘‘A Method for Minimizing the Impact of Distributional Assump- tions in Econometric Models for Duration Data,’’ Econometrica 52, 271–320. Heckman, J. J., and E. Vytlacil (1998), ‘‘Instrumental Variables Methods for the Correlated Random Co- e‰cient Model,’’ Journal of Human Resources 33, 974–987. Hendry, D. F. (1984), ‘‘Monte Carlo Experimentation in Econometrics,’’ in Handbook of Econometrics, Volume 2, ed. Z. Griliches and M. D. Intriligator. Amsterdam: North Holland, 937–976. Hirano, K., G. W. Imbens, and G. Ridder (2000), ‘‘E‰cient Estimation of Average Treatment E¤ects Using the Estimated Propensity Score,’’ mimeo, UCLA Department of Economics. Holzer, H., R. Block, M. Cheatham, and J. Knott (1993), ‘‘Are Training Subsidies E¤ective? The Michi- gan Experience,’’ Industrial and Labor Relations Review 46, 625–636. Honore´, B. E. (1992), ‘‘Trimmed LAD and Least Squares Estimation of Truncated and Censored Regres- sion Models with Fixed E¤ects,’’ Econometrica 60, 533–565. Honore´, B. E. (1993a), ‘‘Orthogonality Conditions for Tobit Models with Fixed E¤ects and Lagged De- pendent Variables,’’ Journal of Econometrics 59, 35–61. Honore´, B. E. (1993b), ‘‘Identiﬁcation Results for Duration Models with Multiple Spells,’’ Review of Economic Studies 60, 241–246. Honore´, B. E., and E. Kyriazidou (2000a), ‘‘Panel Data Discrete Choice Models with Lagged Dependent Variables,’’ Econometrica 68, 839–874. Honore´, B. E., and E. Kyriazidou (2000b), ‘‘Estimation of Tobit-Type Models with Individual Speciﬁc E¤ects,’’ Econometric Reviews 19, 341–366. Honore´, B. E., E. Kyriazidou, and C. Udry (1997), ‘‘Estimation of Type 3 Tobit Models Using Symmetric Trimming and Pairwise Comparisons,’’ Journal of Econometrics 76, 107–128. Horowitz, J. L. (1992), ‘‘A Smoothed Maximum Score Estimator for the Binary Response Model,’’ Econ- ometrica 60, 505–531. References 727", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 733, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p734::c0", "text": "Horowitz, J. L. (1993), ‘‘Semiparametric and Nonparametric Estimation of Quantal Response Models,’’ in Handbook of Statistics, Volume 11, ed. G. S. Maddala, C. R. Rao, and H. D. Vinod. Amsterdam: North Holland, 45–72. Horowitz, J. L. (1999), ‘‘Semiparametric Estimation of a Proportional Hazard Model with Unobserved Heterogeneity,’’ Econometrica 67, 1001–1028. Horowitz, J. L. (in press), ‘‘The Bootstrap,’’ Handbook of Econometrics, Volume 5, ed. E. Leamer and J. J. Heckman. North Holland: Amsterdam. Horowitz, J. L., and C. F. Manski (1998), ‘‘Censoring of Outcomes and Regressors Due to Survey Non- response: Identiﬁcation and Estimation Using Weights and Imputations,’’ Journal of Econometrics 84, 37– 58. Horvitz, D., and D. Thompson (1952), ‘‘A Generalization of Sampling without Replacement from a Finite Population,’’ Journal of the American Statistical Association 47, 663–685. Hoxby, C. M. (1994), ‘‘Does Competition among Public Schools Beneﬁt Students and Taxpayers?’’ National Bureau of Economic Research Working Paper Number 4979. Hoxby, C. M. (1996), ‘‘How Teachers’ Unions A¤ect Education Production,’’ Quarterly Journal of Eco- nomics 111, 671–718. Hsiao, C. (1986), Analysis of Panel Data. Cambridge: Cambridge University Press. Huber, P. J. (1967), ‘‘The Behavior of Maximum Likelihood Estimates under Nonstandard Conditions,’’ in Proceedings of the Fifth Berkeley Symposium in Mathematical Statistics, Volume 1. Berkeley: University of California Press, 221–233. Ichimura, H. (1993), ‘‘Semiparametric Least Squares (SLS) and Weighted SLS Estimation of Single-Index Models,’’ Journal of Econometrics 58, 71–120. Im, K. S., S. C. Ahn, P. Schmidt, and J. M. Wooldridge (1999), ‘‘E‰cient Estimation of Panel Data Models with Strictly Exogenous Explanatory Variables,’’ Journal of Econometrics 93, 177–201. Imbens, G. W. (1992), ‘‘An E‰cient Method of Moments Estimator for Discrete Choice Models with Choice-Based Sampling,’’ Econometrica 60, 1187–1214. Imbens, G. W., and J. D. Angrist (1994), ‘‘Identiﬁcation and Estimation of Local Average Treatment E¤ects,’’ Econometrica 62, 467–476. Imbens, G. W., and T. Lancaster (1996), ‘‘E‰cient Estimation and Stratiﬁed Sampling,’’ Journal of Econometrics 74, 289–318. Kahn, S., and K. Lang (1988), ‘‘E‰cient Estimation of Structural Hedonic Systems,’’ International Eco- nomic Review 29, 157–166. Kakwani, N. (1967), ‘‘The Unbiasedness of Zellner’s Seemingly Unrelated Regressions Equation Estima- tors,’’ Journal of the American Statistical Association 62, 141–142. Kalbﬂeisch, J. D., and R. L. Prentice (1980), The Statistical Analysis of Failure Time Data. New York: Wiley. Kane, T. J., and C. E. Rouse (1995), ‘‘Labor-Market Returns to Two- and Four-Year Colleges,’’ American Economic Review 85, 600–614. Kao, C. (1999), ‘‘Spurious Regression and Residual-Based Tests for Cointegration in Panel Data,’’ Journal of Econometrics 90, 1–44. Keane, M. P. (1993), ‘‘Simulation Estimation for Panel Data Models with Limited Dependent Variables,’’ in Handbook of Statistics, Volume 11, ed. G. S. Maddala, C. R. Rao, and H. D. Vinod. Amsterdam: North Holland, 545–571. Keane, M. P., and R. A. Mo‰tt (1998), ‘‘A Structural Model of Multiple Welfare Participation and Labor Supply,’’ International Economic Review 39, 553–589. Keane, M. P., and D. E. Runkle (1992), ‘‘On the Estimation of Panel Data Models with Serial Correlation When Instruments Are Not Strictly Exogenous,’’ Journal of Business and Economic Statistics 10, 1–9. References 728", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 734, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p735::c0", "text": "Keane, M. P., and K. I. Wolpin (1997), ‘‘The Career Decisions of Young Men,’’ Journal of Political Economy 105, 473–522. Kiefer, N. M. (1980), ‘‘Estimation of Fixed E¤ect Models for Time Series of Cross-Sections with Arbitrary Intertemporal Covariance,’’ Journal of Econometrics 14, 195–202. Kiefer, N. M. (1988), ‘‘Economic Duration Data and Hazard Functions,’’ Journal of Economic Literature 26, 646–679. Kiefer, N. M. (1989), ‘‘The ET Interview: Arthur S. Goldberger,’’ Econometric Theory 5, 133–160. Kiel, K. A., and K. T. McClain (1995), ‘‘House Prices during Siting Decision Stages: The Case of an In- cinerator from Rumor through Operation,’’ Journal of Environmental Economics and Management 28, 241–255. Kinal, T. W. (1980), ‘‘The Existence of Moments of k-Class Estimators,’’ Econometrica 48, 241–249. Kinal, T., and K. Lahiri (1993), ‘‘On the Estimation of Simultaneous Error Components Models with an Application to a Model of Developing Country Foreign Trade,’’ Journal of Applied Econometrics 8, 81–92. Klein, R. W., and R. H. Spady (1993), ‘‘An E‰cient Semiparametric Estimator for Discrete Choice Models,’’ Econometrica 61, 387–421. Koenker, R. (1981), ‘‘A Note on Studentizing a Test for Heteroskedasticity,’’ Journal of Econometrics 17, 107–112. Koenker, R., and G. Bassett (1978), ‘‘Regression Quantiles,’’ Econometrica 46, 33–50. Krueger, A. B. (1993), ‘‘How Computers Have Changed the Wage Structure: Evidence from Microdata, 1984–1989,’’ Quarterly Journal of Economics 108, 33–60. Kyriazidou, E. (1997), ‘‘Estimation of a Panel Data Sample Selection Model,’’ Econometrica 65, 1335– 1364. Lahiri, K., and P. Schmidt (1978), ‘‘On the Estimation of Triangular Structural Systems,’’ Econometrica 46, 1217–1221. Lancaster, T. (1979), ‘‘Econometric Methods for the Duration of Unemployment,’’ Econometrica 47, 939– 956. Lancaster, T. (1990), The Econometric Analysis of Transition Data. Cambridge: Cambridge University Press. LeCam, L. (1953), ‘‘On Some Asymptotic Properties of Maximum Likelihood Estimates and Related Bayes Estimates,’’ University of California Publications in Statistics 1, 277–328. Lemieux, T. (1998), ‘‘Estimating the E¤ects of Unions on Wage Inequality in a Panel Data Model with Comparative Advantage and Nonrandom Selection,’’ Journal of Labor Economics 16, 261–291. Levine, P. B., T. A. Gustafson, and A. D. Velenchik (1997), ‘‘More Bad News for Smokers? The E¤ects of Cigarette Smoking on Wages,’’ Industrial and Labor Relations Review 50, 493–509. Levitt, S. D. (1996), ‘‘The E¤ect of Prison Population Size on Crime Rates: Evidence from Prison Over- crowding Legislation,’’ Quarterly Journal of Economics 111, 319–351. Levitt, S. D. (1997), ‘‘Using Electoral Cycles in Police Hiring to Estimate the E¤ect of Police on Crime,’’ American Economic Review 87, 270–290. Lewbel, A. (1998), ‘‘Semiparametric Latent Variable Model Estimation with Endogenous or Mismeasured Regressors,’’ Econometrica 66, 105–121. MacKinnon, J. G., and H. White (1985), ‘‘Some Heteroskedasticity Consistent Covariance Matrix Esti- mators with Improved Finite Sample Properties,’’ Journal of Econometrics 29, 305–325. MaCurdy, T. E. (1982), ‘‘The Use of Time Series Processes to Model the Error Structure of Earnings in a Longitudinal Data Analysis,’’ Journal of Econometrics 18, 83–114. Maddala, G. S. (1983), Limited Dependent and Qualitative Variables in Econometrics. Cambridge: Cam- bridge University Press. References 729", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 735, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p736::c0", "text": "Maloney, M. T., and R. E. McCormick (1993), ‘‘An Examination of the Role That Intercollegiate Athletic Participation Plays in Academic Achievement: Athlete’s Feats in the Classroom,’’ Journal of Human Resources 28, 555–570. Manski, C. F. (1975), ‘‘Maximum Score Estimation of the Stochastic Utility Model of Choice,’’ Journal of Econometrics 3, 205–228. Manski, C. F. (1987), ‘‘Semiparametric Analysis of Random E¤ects Linear Models from Binary Panel Data,’’ Econometrica 55, 357–362. Manski, C. F. (1988), Analog Estimation Methods in Econometrics. New York: Chapman and Hall. Manski, C. F. (1996), ‘‘Learning about Treatment E¤ects from Experiments with Random Assignment of Treatments,’’ Journal of Human Resources 31, 709–733. Manski, C. F., and S. Lerman (1977), ‘‘The Estimation of Choice Probabilities from Choice-Based Sam- ples,’’ Econometrica 45, 1977–1988. Manski, C. F., and D. McFadden (1981), ‘‘Alternative Estimators and Sample Designs for Discrete Choice Analysis,’’ in Structural Analysis of Discrete Data with Econometric Applications, ed. C. F. Manski and D. McFadden. Cambridge, MA: MIT Press, 2–50. McCall, B. P. (1994), ‘‘Testing the Proportional Hazards Assumption in the Presence of Unmeasured Heterogeneity,’’ Journal of Applied Econometrics 9, 321–334. McCullagh, P., and J. A. Nelder (1989), Generalized Linear Models, second edition. New York: Chapman and Hall. McDonald, J. B. (1996), ‘‘An Application and Comparison of Some Flexible Parametric and Semi- Parametric Qualitative Response Models,’’ Economics Letters 53, 145–152. McDonald, J. F., and R. A. Mo‰tt (1980), ‘‘The Uses of Tobit Analysis,’’ Review of Economics and Sta- tistics 62, 318–321. McFadden, D. L. (1974), ‘‘Conditional Logit Analysis of Qualitative Choice Analysis,’’ in Frontiers in Econometrics, ed. P. Zarembka. New York: Academic Press, 105–142. McFadden, D. L. (1978), ‘‘Modeling the Choice of Residential Location,’’ in Spatial Interaction Theory and Residential Location, ed. A. Karlqvist. Amsterdam: North Holland, 75–96. McFadden, D. L. (1981), ‘‘Econometric Models of Probabilistic Choice,’’ in Structural Analysis of Discrete Data with Econometric Applications, ed. C. F. Manski and D. McFadden. Cambridge, MA: MIT Press, 198–272. McFadden, D. L. (1984), ‘‘Econometric Analysis of Qualitative Response Models,’’ in Handbook of Econometrics, Volume 2, ed. Z. Griliches and M. D. Intriligator. Amsterdam: North Holland, 1395–1457. McFadden, D. L. (1987), ‘‘Regression Based Speciﬁcation Tests for the Multinomial Logit Model,’’ Jour- nal of Econometrics 34, 63–82. Meyer, B. D. (1990), ‘‘Unemployment Insurance and Unemployment Spells,’’ Econometrica 58, 757–782. Meyer, B. D. (1995), ‘‘Natural and Quasi-Experiments in Economics,’’ Journal of Business and Economic Statistics 13, 151–161. Meyer, B. D., W. K. Viscusi, and D. L. Durbin (1995), ‘‘Workers’ Compensation and Injury Duration: Evidence from a Natural Experiment,’’ American Economic Review 85, 322–340. Model, K. E. (1993), ‘‘The E¤ect of Marijuana Decriminalization on Hospital Emergency Drug Episodes: 1975–1978,’’ Journal of the American Statistical Association 88, 737–747. Mo‰tt, R. A. (1996), ‘‘Identiﬁcation of Causal E¤ects Using Instrumental Variables: Comment,’’ Journal of the American Statistical Association 91, 462–465. Mo‰tt, R., J. Fitzgerald, and P. Gottschalk (1999), ‘‘Sample Attrition in Panel Data: The Role of Selec- tion on Observables,’’ Annale d’Economie et de Statistique 55/56, 129–152. Montgomery, E., K. Shaw, and M. E. Benedict (1992), ‘‘Pensions and Wages: An Hedonic Price Theory Approach,’’ International Economic Review 33, 111–128. References 730", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 736, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p737::c0", "text": "Moon, C.-G. (1988), ‘‘Simultaneous Speciﬁcation Test in a Binary Logit Model: Skewness and Hetero- skedasticity,’’ Communications in Statistics 17, 3361–3387. Moulton, B. (1990), ‘‘An Illustration of a Pitfall in Estimating the E¤ects of Aggregate Variables on Micro Units,’’ Review of Economics and Statistics 72, 334–338. Mroz, T. A. (1987), ‘‘The Sensitivity of an Empirical Model of Married Women’s Hours of Work to Economic and Statistical Assumptions,’’ Econometrica 55, 765–799. Mullahy, J. (1997), ‘‘Instrumental-Variable Estimation of Count Data Models: Applications to Models of Cigarette Smoking Behavior,’’ Review of Economics and Statistics 79, 586–593. Mundlak, Y. (1978), ‘‘On the Pooling of Time Series and Cross Section Data,’’ Econometrica 46, 69–85. Newey, W. K. (1984), ‘‘A Method of Moments Interpretation of Sequential Estimators,’’ Economics Let- ters 14, 201–206. Newey, W. K. (1985), ‘‘Maximum Likelihood Speciﬁcation Testing and Conditional Moment Tests,’’ Econometrica 53, 1047–1070. Newey, W. K. (1990), ‘‘E‰cient Instrumental Variables Estimation of Nonlinear Models,’’ Econometrica 58, 809–837. Newey, W. K. (1993), ‘‘E‰cient Estimation of Models with Conditional Moment Restrictions,’’ in Hand- book of Statistics, Volume 11, ed. G. S. Maddala, C. R. Rao, and H. D. Vinod. Amsterdam: North Hol- land, 419–454. Newey, W. K. (1994), ‘‘The Asymptotic Variance of Semiparametric Estimators,’’ Econometrica 62, 1349– 1382. Newey, W. K., and D. McFadden (1994), ‘‘Large Sample Estimation and Hypothesis Testing,’’ in Hand- book of Econometrics, Volume 4, ed. R. F. Engle and D. McFadden. Amsterdam: North Holland, 2111– 2245. Newey, W. K., and K. D. West (1987), ‘‘A Simple, Positive Semi-Deﬁnite Heteroskedasticity and Auto- correlation Consistent Covariance Matrix,’’ Econometrica 55, 703–708. Nickell, S. (1979), ‘‘Estimating the Probability of Leaving Unemployment,’’ Econometrica 47, 1249– 1266. Nijman, T., and M. Verbeek (1992), ‘‘Nonresponse in Panel Data: The Impact on Estimates of a Life Cycle Consumption Function,’’ Journal of Applied Econometrics 7, 243–257. Orme, C. (1990), ‘‘The Small Sample Performance of the Information Matrix Test,’’ Journal of Econo- metrics 46, 309–331. Pagan, A. R. (1984), ‘‘Econometric Issues in the Analysis of Regressions with Generated Regressors,’’ In- ternational Economic Review 25, 221–247. Pagan, A. R., and F. Vella (1989), ‘‘Diagnostic Tests for Models Based on Individual Data: A Survey,’’ Journal of Applied Econometrics 4, S29–59. Page, M. (1995), ‘‘Racial and Ethnic Discrimination in Urban Housing Markets: Evidence from a Recent Audit Study,’’ Journal of Urban Economics 38, 183–206. Papke, L. E. (1991), ‘‘Interstate Business Tax Di¤erentials and New Firm Location,’’ Journal of Public Economics 45, 47–68. Papke, L. E. (1994), ‘‘Tax Policy and Urban Development: Evidence From the Indiana Enterprise Zone Program,’’ Journal of Public Economics 54, 37–49. Papke, L. E. (1998), ‘‘How Are Participants Directing Their Participant-Directed Individual Account Pension Plans?’’ American Economic Review 88, 212–216. Papke, L. E., and J. M. Wooldridge (1996), ‘‘Econometric Methods for Fractional Response Variables with an Application to 401(k) Plan Participation Rates,’’ Journal of Applied Econometrics 11, 619–632. Pesaran, M. H., and R. J. Smith (1995), ‘‘Estimating Long-Run Relationships from Dynamic Heteroge- neous Panels,’’ Journal of Econometrics 68, 79–113. References 731", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 737, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p738::c0", "text": "Phillips, P. C. B., and H. R. Moon (1999), ‘‘Linear Regression Limit Theory for Nonstationary Panel Data,’’ Econometrica 67, 1057–1111. Phillips, P. C. B., and J. Y. Park (1988), ‘‘On the Formulation of Wald Tests for Nonlinear Restrictions,’’ Econometrica 56, 1065–1083. Polachek, S., and M.-K. Kim (1994), ‘‘Panel Estimates of the Gender Earnings Gap: Individual-Speciﬁc Intercepts and Individual-Speciﬁc Slope Models,’’ Journal of Econometrics 61, 23–42. Porter, J. R. (1999), ‘‘Semiparametric E‰ciency in Maximum Likelihood Variance Estimation,’’ mimeo, Harvard University Department of Economics. Powell, J. L. (1984), ‘‘Least Absolute Deviations Estimation for the Censored Regression Model,’’ Journal of Econometrics 25, 303–325. Powell, J. L. (1986), ‘‘Symmetrically Trimmed Least Squares Estimation for Tobit Models,’’ Econometrica 54, 1435–1460. Powell, J. L. (1994), ‘‘Estimation of Semiparametric Models,’’ in Handbook of Econometrics, Volume 4, ed. R. F. Engle and D. McFadden. Amsterdam: North Holland, 2443–2521. Powell, J. L., J. H. Stock, and T. M. Stoker (1989), ‘‘Semiparametric Estimation of Weighted Average Derivatives,’’ Econometrica 57, 1403–1430. Qian, H., and P. Schmidt (1999), ‘‘Improved Instrumental Variables and Generalized Method of Moments Estimators,’’ Journal of Econometrics 91, 145–169. Quah, D. (1994), ‘‘Exploiting Cross-Section Variations for Unit Root Inference in Dynamic Data,’’ Eco- nomics Letters 44, 9–19. Quandt, R. E. (1983), ‘‘Computational Problems and Methods,’’ in Handbook of Econometrics, Volume 1, ed. Z. Griliches and M. D. Intriligator. Amsterdam: North Holland, 699–764. Ramsey, J. B. (1969), ‘‘Tests for Speciﬁcation Errors in Classical Linear Least Squares Regression Analy- sis,’’ Journal of the Royal Statistical Society, Series B, 31, 350–371. Rao, C. R. (1948), ‘‘Large Sample Tests of Hypotheses Involving Several Parameters with Applications to Problems of Estimation,’’ Proceedings of the Cambridge Philosophical Society 44, 50–57. Rivers, D., and Q. H. Vuong (1988), ‘‘Limited Information Estimators and Exogeneity Tests for Simulta- neous Probit Models,’’ Journal of Econometrics 39, 347–366. Robins, J. A., A. Rotnitzky, and L. Zhao (1995), ‘‘Analysis of Semiparametric Regression Models for Repeated Outcomes in the Presence of Missing Data,’’ Journal of the American Statistical Association 90, 106–121. Romer, D. (1993), ‘‘Openness and Inﬂation: Theory and Evidence,’’ Quarterly Journal of Economics 108, 869–903. Rose, N. L. (1990), ‘‘Proﬁtability and Product Quality: Economic Determinants of Airline Safety Perfor- mance,’’ Journal of Political Economy 98, 944–961. Rosenbaum, P. R., and D. B. Rubin (1983), ‘‘The Central Role of the Propensity Score in Observational Studies for Causal E¤ects,’’ Biometrika 70, 41–55. Rouse, C. E. (1995), ‘‘Democratization or Diversion? The E¤ect of Community Colleges on Educational Attainment,’’ Journal of Business and Economic Statistics 13, 217–224. Rubin, D. B. (1974), ‘‘Estimating Causal E¤ects of Treatments in Randomized and Nonrandomized Studies,’’ Journal of Education Psychology 66, 688–701. Rudin, W. (1976), Principles of Mathematical Analysis, 3rd edition. New York: McGraw-Hill. Ruud, P. (1983), ‘‘Su‰cient Conditions for Consistency of Maximum Likelihood Estimation Despite Misspeciﬁcation of Distribution,’’ Econometrica 51, 225–228. Ruud, P. (1984), ‘‘Tests of Speciﬁcation in Econometrics,’’ Econometric Reviews 3, 211–242. Ruud, P. (1986), ‘‘Consistent Estimation of Limited Dependent Variable Models Despite Misspeciﬁcation of Distribution,’’ Journal of Econometrics 32, 157–187. References 732", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 738, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p739::c0", "text": "Sander, W. (1992), ‘‘The E¤ect of Women’s Schooling on Fertility,’’ Economics Letters 40, 229–233. Schmidt, P. (1976), Econometrics. New York: Marcel-Dekker. Schmidt, P. (1990), ‘‘Three-Stage Least Squares with Di¤erent Instruments for Di¤erent Equations,’’ Journal of Econometrics 43, 389–394. Shapiro, M. D. (1984), ‘‘The Permanent Income Hypothesis and the Real Interest Rate: Some Evidence from Panel Data,’’ Economics Letters 14, 93–100. Shea, J. (1995), ‘‘Union Contracts and the Life-Cycle/Permanent Income Hypothesis,’’ American Economic Review 85, 186–200. Smith, R., and R. Blundell (1986), ‘‘An Exogeneity Test for a Simultaneous Equation Tobit Model with an Application to Labor Supply,’’ Econometrica 54, 679–685. Solon, G. (1985), ‘‘Comment on ‘Beneﬁts and Limitations of Panel Data’ by C. Hsiao,’’ Econometric Reviews 4, 183–186. Staiger, D., and J. H. Stock (1997), ‘‘Instrumental Variables Regression with Weak Instruments,’’ Econo- metrica 65, 557–586. Stoker, T. M. (1986), ‘‘Consistent Estimation of Scaled Coe‰cients,’’ Econometrica 54, 1461–1481. Stoker, T. M. (1992), Lectures on Semiparametric Econometrics. Louvain-la-Neuve, Belgium: CORE Lec- ture Series. Strauss, J., and D. Thomas (1995), ‘‘Human Resources: Empirical Modeling of Household and Family Decisions,’’ in Handbook of Development Economics, Volume 3A, ed. J. Berhman and T. N. Srinivasan. Amsterdam: North Holland, 1883–2023. Sueyoshi, G. T. (1992), ‘‘Semiparametric Proportional Hazards Estimation of Competing Risks Models with Time-Varying Covariates,’’ Journal of Econometrics 51, 25–58. Sueyoshi, G. T. (1995), ‘‘A Class of Binary Response Models for Grouped Duration Data,’’ Journal of Applied Econometrics 10, 411–431. Tauchen, G. (1985), ‘‘Diagnostic Testing and Evaluation of Maximum Likelihood Models,’’ Journal of Econometrics 30, 415–443. Tauchen, G. (1986), ‘‘Statistical Properties of Generalized Method-of-Moments Estimators of Structural Parameters Obtained from Financial Market Data,’’ Journal of Business and Economic Statistics 4, 397– 416. Terza, J. V. (1998), ‘‘Estimating Count Models with Endogenous Switching: Sample Selection and Endogenous Treatment E¤ects,’’ Journal of Econometrics 84, 129–154. Theil, H. (1983), ‘‘Linear Algebra and Matrix Methods in Econometrics,’’ in Handbook of Econometrics, Volume 1, ed. Z. Griliches and M. D. Intriligator. Amsterdam: North Holland, 5–65. Thomas, D., J. Strauss, and M.-H. Henriques (1990), ‘‘Child Survival, Height for Age and Household Characteristics in Brazil,’’ Journal of Development Economics 33, 197–234. Tobin, J. (1956), ‘‘Estimation of Relationships for Limited Dependent Variables,’’ Econometrica 26, 24–36. Ullah, A., and H. D. Vinod (1993), ‘‘General Nonparametric Regression Estimation and Testing in Econometrics,’’ in Handbook of Statistics, Volume 11, ed. G. S. Maddala, C. R. Rao, and H. D. Vinod. Amsterdam: North Holland, 85–116. van der Klaauw, W. (1996), ‘‘Female Labour Supply and Marital Status Decisions: A Life-Cyle Model,’’ Review of Economic Studies 63, 199–235. Vella, F. (1992), ‘‘Simple Tests for Sample Selection Bias in Censored and Discrete Choice Models,’’ Journal of Applied Econometrics 7, 413–421. Vella, F. (1998), ‘‘Estimating Models with Sample Selection Bias: A Survey,’’ Journal of Human Resources 33, 127–169. Vella, F., and M. Verbeek (1998), ‘‘Whose Wages Do Unions Raise? A Dynamic Model of Unionism and Wage Rate Determination for Young Men,’’ Journal of Applied Econometrics 13, 163–183. References 733", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 739, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p740::c0", "text": "Vella, F., and M. Verbeek (1999), ‘‘Estimating and Interpreting Models with Endogenous Treatment E¤ects,’’ Journal of Business and Economic Statistics 17, 473–478. Verbeek, M., and T. Nijman (1992), ‘‘Testing for Selectivity Bias in Panel Data Models,’’ International Economic Review 33, 681–703. Verbeek, M., and T. Nijman (1996), ‘‘Incomplete Panels and Selection Bias,’’ in L. Matyas and P. Sevestre, eds., The Econometrics of Panel Data. Amsterdam: Kluwer Academic Publishers, 449–490. Vuong, Q. (1989), ‘‘Likelihood Ratio Tests for Model Selection and Nonnested Hypotheses,’’ Econo- metrica 57, 307–333. Wald, A. (1940), ‘‘The Fitting of Straight Lines If Both Variables Are Subject to Error,’’ Annals of Math- ematical Statistics 11, 284–300. White, H. (1980a), ‘‘Nonlinear Regression on Cross Section Data,’’ Econometrica 48, 721–746. White, H. (1980b), ‘‘A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity,’’ Econometrica 48, 817–838. White, H. (1982a), ‘‘Maximum Likelihood Estimation of Misspeciﬁed Models,’’ Econometrica 50, 1–26. White, H. (1982b), ‘‘Instrumental Variables Regression with Independent Observations,’’ Econometrica 50, 483–499. White, H. (1984), Asymptotic Theory for Econometricians. Orlando, FL: Academic Press. White, H. (1994), Estimation, Inference and Speciﬁcation Analysis. Cambridge: Cambridge University Press. Wolak, F. A. (1991), ‘‘The Local Nature of Hypothesis Tests Involving Inequality Constraints in Non- linear Models,’’ Econometrica 59, 981–995. Wooldridge, J. M. (1990), ‘‘A Uniﬁed Approach to Robust, Regression-Based Speciﬁcation Tests,’’ Econometric Theory 6, 17–43. Wooldridge, J. M. (1991a), ‘‘On the Application of Robust, Regression-Based Diagnostics to Models of Conditional Means and Conditional Variances,’’ Journal of Econometrics 47, 5–46. Wooldridge, J. M. (1991b), ‘‘Speciﬁcation Testing and Quasi-Maximum Likelihood Estimation,’’ Journal of Econometrics 48, 29–55. Wooldridge, J. M. (1992), ‘‘Some Alternatives to the Box-Cox Regression Model,’’ International Economic Review 33, 935–955. Wooldridge, J. M. (1994), ‘‘Estimation and Inference for Dependent Processes,’’ in Handbook of Econo- metrics, Volume 4, ed. R. F. Engle and D. L. McFadden. Amsterdam: North-Holland, 2639–2738. Wooldridge, J. M. (1995a), ‘‘Selection Corrections for Panel Data Models under Conditional Mean Inde- pendence Assumptions,’’ Journal of Econometrics 68, 115–132. Wooldridge, J. M. (1995b), ‘‘Score Diagnostics for Linear Models Estimated by Two Stage Least Squares,’’ in Advances in Econometrics and Quantitative Economics, ed. G. S. Maddala, P. C. B. Phillips, and T. N. Srinivasan. Oxford: Blackwell, 66–87. Wooldridge, J. M. (1996), ‘‘Estimating Systems of Equations with Di¤erent Instruments for Di¤erent Equations,’’ Journal of Econometrics 74, 387–405. Wooldridge, J. M. (1997a), ‘‘Multiplicative Panel Data Models without the Strict Exogeneity Assump- tion,’’ Econometric Theory 13, 667–678. Wooldridge, J. M. (1997b), ‘‘On Two Stage Least Squares Estimation of the Average Treatment E¤ect in a Random Coe‰cient Model,’’ Economics Letters 56, 129–133. Wooldridge, J. M. (1997c), ‘‘Quasi-Likelihood Methods for Count Data,’’ in Handbook of Applied Econometrics, Volume 2, ed. M. H. Pesaran and P. Schmidt. Oxford: Blackwell, 352–406. Wooldridge, J. M. (1998), ‘‘Selection Corrections with a Censored Selection Variable,’’ mimeo, Michigan State University Department of Economics. References 734", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 740, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "Wooldridge - Cross-section and Panel Data::p741::c0", "text": "Wooldridge, J. M. (1999a), ‘‘Distribution-Free Estimation of Some Nonlinear Panel Data Models,’’ Jour- nal of Econometrics 90, 77–97. Wooldridge, J. M. (1999b), ‘‘Asymptotic Properties of Weighted M-Estimators for Variable Probability Samples,’’ Econometrica 67, 1385–1406. Wooldridge, J. M. (1999c), ‘‘Estimating Average Partial E¤ects under Conditional Moment Independence Assumptions,’’ mimeo, Michigan State University Department of Economics. Wooldridge, J. M. (2000a), Introductory Econometrics: A Modern Approach. Cincinnati, OH: South- Western. Wooldridge, J. M. (2000c), ‘‘A Framework for Estimating Dynamic, Unobserved E¤ects Panel Data Models with Possible Feedback to Future Explanatory Variables,’’ Economics Letters 68, 245–250. Wooldridge, J. M. (2000d), ‘‘Inverse Probability Weighted M-Estimators for Sample Selection, Attrition, and Stratiﬁcation,’’ mimeo, Michigan State University Department of Economics. Wooldridge, J. M. (2000e), ‘‘The Initial Conditions Problem for Dynamic, Nonlinear Panel Data Models with Unobserved Heterogeneity,’’ mimeo, Michigan State University Department of Economics. Wooldridge, J. M. (2000f ), ‘‘Instrumental Variables Estimation of the Average Treatment E¤ect in the Correlated Random Coe‰cient Model,’’ mimeo, Michigan State University Department of Economics. Wooldridge, J. M. (2001), ‘‘Asymptotic Properties of Weighted M-Estimators for Standard Stratiﬁed Samples.’’ Econometric Theory 17, 451–470. Zeger, S. L., K.-Y. Liang, and P. S. Albert (1988), ‘‘Models for Longitudinal Data: A Generalized Esti- mating Equation Approach,’’ Biometrics 44, 1049–1060. Zeldes, S. P. (1989), ‘‘Consumption and Liquidity Constraints: An Empirical Investigation,’’ Journal of Political Economy 97, 305–346. Zellner, A. (1962), ‘‘An E‰cient Method of Estimating Seemingly Unrelated Regressions and Tests of Aggregation Bias,’’ Journal of the American Statistical Association 57, 500–509. Ziliak, J. P. (1997), ‘‘E‰cient Estimation with Panel Data When Instruments Are Predetermined: An Empirical Comparison of Moment-Condition Estimators,’’ Journal of Business and Economic Statistics 15, 419–431. Ziliak, J. P., and T. J. Kniesner (1998), ‘‘The Importance of Sample Attrition in Life Cycle Labor Supply Estimation,’’ Journal of Human Resources 33, 507–530. Ziliak, J. P., B. Wilson, and J. Stone (1999), ‘‘Spatial Dynamics and Heterogeneity in the Cyclicality of Real Wages,’’ Review of Economics and Statistics 81, 227–236. References 735", "meta": {"source_type": "paper", "topic": ["econometrics"], "title": "Econometric Analysis of Cross Section and Panel Data", "page": 741, "path": "/Users/taka/research_rag/data/papers/econometrics/Wooldridge - Cross-section and Panel Data.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p1::c0", "text": "Journal of Machine Learning Research 23 (2022) 1-55 Submitted 8/21; Revised 1/22; Published 4/22 Policy Evaluation and Temporal–Diﬀerence Learning in Continuous Time and Space: A Martingale Approach Yanwei Jia yj2650@columbia.edu Department of Industrial Engineering and Operations Research Columbia University New York, NY 10027, USA Xun Yu Zhou xz2574@columbia.edu Department of Industrial Engineering and Operations Research & The Data Science Institute Columbia University New York, NY 10027, USA Editor: Csaba Szepesvari Abstract We propose a uniﬁed framework to study policy evaluation (PE) and the associated tempo- ral diﬀerence (TD) methods for reinforcement learning in continuous time and space. We show that PE is equivalent to maintaining the martingale condition of a process. From this perspective, we ﬁnd that the mean–square TD error approximates the quadratic variation of the martingale and thus is not a suitable objective for PE. We present two methods to use the martingale characterization for designing PE algorithms. The ﬁrst one minimizes a “martingale loss function”, whose solution is proved to be the best approximation of the true value function in the mean–square sense. This method interprets the classical gradi- ent Monte-Carlo algorithm. The second method is based on a system of equations called the “martingale orthogonality conditions” with test functions. Solving these equations in diﬀerent ways recovers various classical TD algorithms, such as TD(λ), LSTD, and GTD. Diﬀerent choices of test functions determine in what sense the resulting solutions approx- imate the true value function. Moreover, we prove that any convergent time-discretized algorithm converges to its continuous-time counterpart as the mesh size goes to zero, and we provide the convergence rate. We demonstrate the theoretical results and corresponding algorithms with numerical experiments and applications. Keywords: continuous time and space, reinforcement learning, policy evaluation, tem- poral diﬀerence, martingale 1. Introduction Policy evaluation (PE) is a crucial step in most critic-related reinforcement learning (RL) algorithms such as actor-critic algorithms and policy iteration. Its objective is to esti- mate/predict the value function of a given policy using samples, generally without knowl- edge about the environment. Existing PE methods have predominantly been limited to discrete-time problems with ﬁnite-state Markov decision processes (MDPs). For instance, Monte Carlo methods use samples to estimate expectations assuming the whole sample trajectories can be repeatedly presented for training; hence they are compatible with oﬄine c⃝2022 Yanwei Jia and Xun Yu Zhou. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v23/21-0947.html.", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 1, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p2::c0", "text": "Jia and Zhou learning. The most popular PE methods are based on the temporal diﬀerence (TD) error. These are incremental learning procedures driven by the error between temporally succes- sive predictions. Sutton (1988) argues that predictions of the TD methods are both more accurate and easier to compute than other methods. More importantly, these methods can learn the value in real-time before a task terminates; hence it can be used both online and oﬄine (Sutton and Barto, 2018). Despite the fast development and vast applications, there are two major limitations in the current study on RL in general and on PE in particular. First, most algorithms are developed for MDPs, and little attention has been paid to problems with continuous time and space. The few existing studies in the continuous setting have been largely restricted to deterministic systems; see for example Baird (1993); Doya (2000); Fr´emaux et al. (2013); Vamvoudakis and Lewis (2010) and Lee and Sutton (2021), where the state processes follow ordinary diﬀerential equations (ODEs) and there are no environmental noises. In particular, Baird (1993) and Doya (2000) are the ﬁrst to propose some continuous-time versions of the TD methods. In real life, however, there are abundant examples in which an agent can or indeed needs to interact with a random environment at ultra-high frequency, e.g., high- frequency stock trading, autonomous driving, and robots navigation. Second, while there have been numerous PE algorithms proposed using function approximation such as residual gradient, gradient Monte Carlo, and TD(λ), they were usually devised in heuristic and ad hoc manners and their underlying objectives were not always clearly stated.1 Although many of them are proved to be convergent, the limiting functions are not always interpreted properly especially if the function approximators do not contain the true solutions. In short, there seems a lack of a uniﬁed framework to study PE and there is need for a continuous time and space perspective, from which many well-known algorithms appear as discretizations. The goal of this paper is to bridge these gaps by providing a uniﬁed theoretical un- derpinning of PE in continuous time and space with general Markov diﬀusion processes. Instead of discretizing time, state, and action from the start and then applying the existing discrete techniques and results, we carry out all our theoretical analysis for the continuous setting and discretize time only at the ﬁnal, algorithmic stage. The advantage of doing so is two-fold. On one hand, as Doya (2000) argues, the control performance with this approach will be smoother and the right granularity for discretization will be guided by the function approximation. On the other hand, and indeed more importantly, for analyses in a continuous setting, we have plenty of well-developed tools at our disposal including those of stochastic calculus, diﬀerential equations, and stochastic control, which, in turn, will provide better interpretability/explainability to the underlying learning technologies. Stochastic optimization in continuous time and space, also known as stochastic control, has a long history starting from the 1960s. However, its theory is model-based, namely, the system dynamics and the objective functions are assumed to be given and known. The problem can then be solved by well-established approaches such as Pontryagin’s maximum principle and Bellman’s dynamic programming. For full accounts of the stochastic control theory see, e.g., Yong and Zhou (1999) and Fleming and Soner (2006). On the other hand, to our best knowledge, the study on model-free RL for diﬀusion processes started only recently. Wang et al. (2020) propose an entropy-regularized, stochastic relaxed control 1. See Appendix A, Table 1, for a list of names of existing PE algorithms for MDPs. 2", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 2, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p3::c0", "text": "Policy Evaluation and TD Learning in Continuous Time formulation for trading oﬀexploration and exploitation in continuous time and space, and derive the continuous version of the Boltzmann distribution (Gibbs measure) as the optimal exploratory policy. When the problem is linear–quadratic (LQ), namely the dynamic is linear and the payoﬀis quadratic in state and action, the optimal strategy specializes to Gaussian exploration. Wang and Zhou (2020) apply this general theory to a mean–variance ﬁnancial portfolio selection problem, which is inherently of an LQ structure, and design an algorithm for extensive simulation and empirical experiments. Dai et al. (2020) further consider the equilibrium mean–variance strategies addressing the time-inconsistent issue of the problem. Guo et al. (2022) extend the formulation and results of Wang et al. (2020) to mean-ﬁeld games. Gao et al. (2020) use the idea of Wang et al. (2020) to a non-learning problem – simulated annealing for nonconvex optimization formulated as controlling the temperature of a Langevin diﬀusion. For PE, there are generally two aspects one should address. First and more funda- mentally, one speciﬁes a mathematical objective against which a learning task is evaluated. Usually, such an objective is described by either an optimization problem (to minimize a loss/error function) or a system of equations. Second and on the implementation front, one designs an algorithm to achieve the objective. Many papers have contributed to the second aspect, namely, to develop more eﬃcient numerical solvers to accelerate convergence, reduce variance, or save computational cost; see, e.g., Xu et al. (2002); Liu et al. (2016); Du et al. (2017). In contrast to that line of research, the present paper focuses on the ﬁrst aspect aiming at building a uniﬁed theoretical framework for PE. We propose and analyze several common objectives in the continuous setting, and demonstrate that they generate continu- ous counterparts of some of the best-known PE algorithms for MDPs. This not only leads to PE algorithms for the continuous problems but also provides additional foundations for the discrete ones. As our algorithms designed for continuous setting are discretized in time for implementation, their convergence with a ﬁxed discretization mesh size has been already established by existing results. Moreover, we show that, as the discretization gets ﬁner, the limiting point of a convergent discrete-time algorithm also converges to the corresponding solution to the continuous problem, and we further provide the convergence rate. The entire theoretical analysis of the current paper is premised upon the fact that the value function along the state process combined with the accumulated running payoﬀis a martingale. This martingality naturally gives rise to a target for oﬄine learning: the value of the martingale at any given time is the least square estimate of that at the terminal time. On the other hand, the martingality leads to orthogonality conditions that in turn generate algorithms corresponding to many existing well-known TD algorithms for MDPs. A similar martingale condition can also be derived for discrete-time MDPs, which is equivalent to the so-called Bellman equation or the Bellman consistency. In Appendix C we provide such a derivation. However, to our best knowledge, in the existing RL literature such a condition has not been explicitly presented – even if it is rather straightforward to deduce – nor has it been employed to study PE. Instead, the Bellman equation has been the predominant tool to devise PE algorithms. We demonstrate that the change of perspective from the Bellman equation to the martingality is crucial in our analysis. Speciﬁcally, our main contributions can be summarized as follows: (i) We show that the continuous analogue of the na¨ıve residual gradient method, which minimizes the mean-square TD error (Barnard, 1993; Baird, 1995; Doya, 2000; Wang 3", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 3, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p4::c0", "text": "Jia and Zhou and Zhou, 2020), converges to the minimizer of the quadratic variation of the afore- mentioned martingale. It is, therefore, inconsistent with the learning objective. This in turn provides a theoretical explanation why the method is not a desired approach for PE when the environment is stochastic. (ii) We propose a martingale loss function based on the total mean-square error between the said martingale process and its terminal value. We prove that minimizing such a loss function is equivalent to minimizing the mean-square error between the approxi- mate value function and the true one. This loss function is implementable on samples, and justiﬁes the Monte Carlo PE with function approximation (Sutton and Barto, 2018) in the classical MDP and RL literature. (iii) We provide a uniﬁed perspective to interpret TD errors and the related algorithms, including TD(λ), least square TD (LSTD), and gradient TD (GTD and its variants), based on the martingale orthogonality conditions. Speciﬁcally, by introducing a ﬁ- nite number of suitable test functions to these conditions, the learning problem is transformed into a system of equations called moment conditions. From this vantage point, we realize that TD(λ) is nothing but to directly apply stochastic approxima- tion to solve such equations, LSTD is to solve them explicitly when they form a linear system, and GTD methods are to solve various quadratic forms of the moment condi- tions. In addition, diﬀerent choices of the test functions determine in what sense the true value function is approximated. For example, TD(λ) essentially correspond to diﬀerent test functions for diﬀerent values of λ, and hence may converge to diﬀerent limits. For reader’s easy reference, we present Table 1 in Appendix A summarizing popular PE methods and algorithms, and the interpretations we will have discovered in this paper in terms of the objectives and the convergent limits of the algorithms. As the conditional expectation in the expression of the value function is connected to both a partial diﬀerential equation (PDE) through the Feynman–Kac formula (Karatzas and Shreve, 2014) and to a backward stochastic diﬀerential equation (BSDE) through a martingale representation theorem (El Karoui et al., 1997), the results of the current paper have natural implications on applying machine learning methods to numerically solve (high- dimensional) PDEs in search of breaking the “curse of dimensionality”. The latter has been a hotly pursued topic lately; see for example Raissi (2018) and Hur´e et al. (2019). Han et al. (2018) propose a deep learning approach to solving PDEs by solving the associated BSDEs via simulation. All these works need to assume that the coeﬃcients of the PDEs are known. The results of our paper shed light on solving PDEs by PE methodologies in a data-driven way, in view of the intimate connection among PDEs, BSDEs and PE for Markov diﬀusion process.2 The rest of the paper proceeds as follows. In Section 2, we formulate the PE problem in continuous time and space and present the martingale characterization of the value function. In Section 3, we extend the classical mean-square TD error to the continuous setting and show why it is not a proper objective when the environment is stochastic through 2. It is interesting to note that there seems to be less research on solving recursive Bellman-like equations using MDPs, even though the same curse of dimensionality exists for discrete-time equations. 4", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 4, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p5::c0", "text": "Policy Evaluation and TD Learning in Continuous Time simple simulated counter-examples and theoretical analysis. In Section 4, we propose several objectives for PE from the martingale perspective, based on which we recover and interpret some well-studied PE algorithms. We also present numerical experiments for demonstration. Section 5 is devoted to some extensions of our problem formulation along with applications to option-like payoﬀs and linear-quadratic problems. In Section 6 we discuss the choice of test functions and the way to do function approximation from the algorithmic perspective. We conclude in Section 7. Appendix contains some supplementary materials and all the proofs.3 2. Problem Formulation and Preliminaries Throughout this paper, by convention all vectors are column vectors unless otherwise spec- iﬁed, and Rk is the space of all k-dimensional vectors (hence k × 1 matrices). Let A and B be two matrices of the same size. We denote by A ◦B the inner product between A and B, by |A| the Euclidean/Frobenius norm of A, and write A2 := AA⊤, where A⊤is A’s transpose. A general continuous-time RL problem can be formulated under the stochastic control framework with controlled Itˆo’s stochastic diﬀerential equations (SDEs), analogous to MDPs in discrete time. However, since this paper concerns only a part (though a crucial part) of the RL problem, namely policy evaluation (PE) under a ﬁxed control policy, we will formulate the problem without the control variable, which is the continuous-time counterpart of the Markov reward process (MRP) in discrete time.4 Let d, m be given positive integers, T > 0, and b : [0, T]×Rd 7→Rd and σ : [0, T]×Rd 7→ Rd×m be given functions. The state (or feature) dynamic follows a Markov diﬀusion process governed by an SDE: dXs = b(s, Xs)ds + σ(s, Xs)dWs, (1) such that for any given initial time–state pair (t, x) ∈[0, T] × Rd, the SDE (1) with Xt = x admits a solution X = {Xs, t ≤s ≤T} on a certain ﬁltered probability space (Ω, F, P; {Fs}s≥t) along with a standard {Fs}s≥t-adapted m-dimensional Brownian motion W = {Ws, s ≥t}. Note here we are concerned with the weak solution which includes the ﬁltered probability space and the Brownian motion as part of the solution. See Karatzas and Shreve (2014) for various notions of solutions to an SDE. Assuming the weak solution of (1) is unique (i.e. all the solutions have identical proba- bility distribution, even if possibly with diﬀerent sample paths), we deﬁne the value function J(t, x) = E \u0014Z T t r(s, Xs)ds + h(XT ) Xt = x \u0015 , (2) where r is an (instantaneous) reward (cost) function (i.e. rate of reward/cost conditioned on time and state) and h the lump-sum reward (cost) function applied at the end of the planning period, T. 3. The codes to reproduce our results are publicly available at https://www.dropbox.com/sh/ 5vyaw0yognhcabf/AACsArMcNmEuSwpXxcRq-qT1a?dl=0. 4. PE sometimes is also referred to as the prediction problem. A general stochastic control formulation of RL can be founded in Wang et al. (2020), which will also be reviewed in Appendix B. 5", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 5, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p6::c0", "text": "Jia and Zhou Unlike most RL problems that are formulated in an inﬁnite planning horizon (known as continuing tasks), the current paper mainly focuses on a ﬁnite horizon setting (known as episodic tasks).5 Finite horizons reﬂect limited lifespans of real-life tasks, e.g., a trader sells a ﬁnancial contract with a maturity date, a robot ﬁnishes a task before a deadline, and a video gamer strives to pass a checkpoint given a time limit. The PE task is, for a ﬁxed given policy (which is suppressed in the formulation above due to the reason we stated earlier), to devise a numerical procedure to ﬁnd J(t, x) as a function of (t, x) using multiple sample trajectories of the process {s, Xs, r(s, Xs)}t≤s≤T , where {Xs, t ≤s ≤T} is the solution to (1), without the knowledge of the model parameters (the functional forms of b, σ, r, h). Hence we cover the settings of on-policy (i.e., the samples are generated under a target policy)6, episodic (i.e., the same learning task is repeated for many episodes/multiple trajectories), oﬄine (i.e., the approximated function is updated after a full episode/trajectory has been run) and online (i.e., the approximated function is updated in real time as we go). We emphasize that for a ﬁnite-horizon problem, it is generally too ambitious to expect an eﬀective algorithm that learns from a single trajectory with no resets, due to the limited sample size. Learning with a single trajectory is usually done in an inﬁnite horizon setting. We make the following standard regularity assumptions on the coeﬃcients of (1) and the reward function (2) to ensure the theoretical well-posedness of the problem: Assumption 1 The following conditions hold true: (i) b, σ, r, h are all continuous functions in their respective arguments; (ii) b, σ are uniformly Lipschitz in x, i.e., for ϕ = b, σ, there exists a constant C > 0 such that |ϕ(t, x) −ϕ(t, x′)| ≤C|x −x′|, ∀t ∈[0, T], x, x′ ∈Rd; (iii) b, σ have linear growth in x, i.e., for ϕ = b, σ, there exists a constant C > 0 such that |ϕ(t, x)| ≤C(1 + |x|), ∀(t, x) ∈[0, T] × Rd; (iv) r and h both have polynomial growth in x, i.e., there exist constants C > 0 and µ ≥1 such that |r(t, x)| ≤C(1 + |x|µ), |h(x)| ≤C(1 + |x|µ), ∀(t, x) ∈[0, T] × Rd. Under Assumption 1(i)-(iii), the SDE (1) admits a unique strong solution (and hence a unique weak solution) whose moments of any given order are uniformly bounded; see, e.g., Karatzas and Shreve (2014). The unique existence of a weak solution alone requires much weaker assumptions; see e.g. Stroock and Varadhan (1979), but we will not pursue along 5. We will brieﬂy discuss the inﬁnite horizon case with exponentially discounted payoﬀin Subsection 5.1. 6. Sutton et al. (2008) uses “behavioral policy” to describe the policy to generate observations and “target policy” to describe the policy we want to evaluate. Oﬀ-policy means training on data from a behavioral policy in order to learn the value of a target policy, and on-policy means that the behavioral policy coincides with the target policy in learning. 6", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 6, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p7::c0", "text": "Policy Evaluation and TD Learning in Continuous Time that line. On the other hand, Assumption 1(iv) is to ensure that J(t, x) is ﬁnite for any (t, x). We now recall some existing results on Markov diﬀusion processes underpinning the theoretical analysis in this paper. First, J can be characterized by a PDE based on the celebrated Feynman–Kac formula (Karatzas and Shreve, 2014):7 \u001a LJ(t, x) + r \u0000t, x \u0001 = 0, (t, x) ∈[0, T) × Rd, J(T, x) = h(x), (3) where LJ(t, x) := ∂J ∂t (t, x) + b \u0000t, x \u0001 ◦∂J ∂x(t, x) + 1 2σ2\u0000t, x \u0001 ◦∂2J ∂x2 (t, x) is known as the inﬁnitesimal generator associated with the diﬀusion process (1). Here, ∂J ∂x ∈Rd is the gradient, and ∂2J ∂x2 ∈Rd×d is the Hessian. The above PDE would be fully speciﬁed had the model been completely known.8 If the state space has a dimension up to 4 (i.e. d ≤4), the equations can be eﬃciently solved numerically by methods such as Monte-Carlo and ﬁnite element algorithms. Unfortunately, in many practical applications the model parameters are not known, nor is the dimension small. Here, to avoid unnecessary technicality, we assume Assumption 2 The PDE (3) admits a classical solution J ∈C1,2([0, T) × Rd) satisfying the polynomial growth condition, i.e., there exist constants C > 0 and µ ≥1 such that |J(t, x)| ≤C(1 + |x|µ), ∀(t, x) ∈[0, T] × Rd. Second, the PDE (3) is related to the following forward–backward stochastic diﬀerential equation (FBSDE): \u001a dXs = b(s, Xs)ds + σ(s, Xs)dWs, s ∈[t, T]; Xt = x, dYs = −r \u0000s, Xs \u0001 ds + ZsdWs, s ∈[t, T]; YT = h(XT ). (4) Its solution, {(Xs, Ys, Zs), t ≤s ≤T}, has the following representations in terms of J: Ys = J(s, Xs), Zs = ∂J ∂x(s, Xs)⊤σ \u0000s, Xs \u0001 , s ∈[t, T]. (5) The above relationship can be easily seen by applying Itˆo’s formula to J; for details see El Karoui et al. (1997). For any ﬁxed (t, x) ∈[0, T] × Rd and {Xs, t ≤s ≤T} solving the ﬁrst equation of (4), deﬁne Ms := J(s, Xs) + Z s t r \u0000s′, Xs′\u0001 ds′ ≡Ys + Z s t r \u0000s′, Xs′\u0001 ds′, s ∈[t, T]. (6) The following result is the theoretical foundation of this paper, which characterizes the value function J by the martingality of M. 7. This PDE is a spacial case of the (nonlinear) Hamilton-Jacobi-Bellman (HJB) equation in continuous- time stochastic control when the control variable is ﬁxed. 8. Some of this PDE’s theoretical properties, such as existence, uniqueness, and regularity, have been well studied in terms of viscosity solution; see, e.g., Crandall et al. (1992); Beck et al. (2021). 7", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 7, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p8::c0", "text": "Jia and Zhou Proposition 1 Suppose Assumptions 1 and 2 hold. For any ﬁxed (t, x) ∈[0, T] × Rd and {Xs, t ≤s ≤T} solving the ﬁrst equation of (4), the process M = {Ms, t ≤s ≤T} is a square-integrable martingale. Conversely, if there is a continuous function ˜J such that for all (t, x) ∈[0, T] × Rd, ˜ M = { ˜ Ms, t ≤s ≤T} is a square-integrable martingale, where ˜ Ms := ˜J(s, Xs) + R s t r \u0000s′, Xs′\u0001 ds′, and ˜J(T, x) = h(x), then ˜J ≡J on [0, T] × Rd. This proposition inspires a martingale approach to PE in continuous-time RL, which will be developed in this paper. Essentially, the approach exploits the equivalence between PE (Feynman–Kac formula) and the martingality. Finally, for a square-integrable semi-martingale M = {Mt, 0 ≤t ≤T}, its quadratic variation process, denoted by ⟨M⟩= {⟨M⟩t, 0 ≤t ≤T}, is deﬁned to be (Karatzas and Shreve, 2014) ⟨M⟩t = lim ||∆||→0 K−1 X i=0 (Mτi −Mτi−1)2 < ∞, where ∆: 0 = τ0 < · · · < τK = t is an arbitrary partition of the interval [0, t], and ||∆|| = max1≤i≤K{τi −τi−1} is the largest mesh size. For M deﬁned by (6), we have ⟨M⟩t = ⟨Y ⟩t = Z t 0 |Zs|2ds, t ∈[0, T]. (7) Introduce L2 F([0, T]) = \u001a κ = {κt, 0 ≤t ≤T} is real-valued and Ft-progressively measurable : E Z T 0 κ2 t dt < ∞ \u001b . It is a Hilbert space with L2-norm ||κ||L2 = (E R T 0 κ2 t dt) 1 2 . More generally, for any semi- martingale Y = {Ys, s ≥0}, we denote L2 F([0, T]; Y ) = \u001a κ = {κt, 0 ≤t ≤T} : κ is Ft-progressively measurable and E Z T 0 |κt|2d⟨Y ⟩t < ∞ \u001b . 3. Temporal Diﬀerence Error in Continuous Time In this section, we ﬁrst review Doya (2000)’s TD error approach for deterministic dynamics and then explain why we can not extend that approach to the stochastic setting. 3.1 Doya’s TD algorithm for deterministic dynamics Many RL algorithms for discrete-time MDPs use TD error to evaluate policies. Doya (2000) extends the TD approach to RL with continuous time and space, albeit only for deterministic dynamics. For readers’ convenience and for highlighting the key diﬀerences between deterministic and stochastic settings, we brieﬂy review Doya (2000)’s approach here. In our setting with σ = 0 (and hence all the expectations are dropped), Doya’s approach starts with the obvious identity J(t, Xt) = Z t′ t r(s, Xs)ds + J(t′, Xt′), t′ ∈(t, T]. (8) 8", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 8, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p9::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Rearranging this equation and dividing both sides by t′ −t, we obtain J(t′, Xt′) −J(t, Xt) t′ −t + 1 t′ −t Z t′ t r(s, Xs)ds = 0. (9) Letting t′ →t on the left hand side motivates the deﬁnition of the TD error rate:9 δt := ˙Jt + rt, (10) where ˙Jt := d dtJ(t, Xt) is the total derivative of J along (t, Xt), and rt := r(t, Xt). The function approximation approach widely employed for PE ﬁrst approximates J by a parametric family of functions Jθ (upon using linear spans of basis functions or neural networks, or taking advantage of any known or plausible structure of the underlying prob- lem), with θ ∈Θ ⊆RL. Henceforth, we always use θ-superscripted functions to denote those corresponding to the parameterized functions. For instance, δθ t := ˙Jθ t + rt. Doya (2000) determines θ by minimizing the mean–square TD error (MSTDE) MSTDE(θ) := 1 2 Z T 0 |δθ t |2dt ≡1 2 Z T 0 ˙Jθ t + rt 2dt, (11) in view of the fact that this error ought to be zero theoretically. To approximate and compute MSTDE(θ), we discretize [0, T] into small intervals [ti, ti+1], i = 0, 1, · · · , K −1, with an equal length ∆t, where t0 = 0 and tK = T. This leads to MSTDE(θ) ≈1 2 K−1 X i=0 \u0012Jθ(ti+1, Xti+1) −Jθ(ti, Xti) ti+1 −ti + rti \u00132 ∆t =: MSTDE∆t(θ). (12) A gradient descent algorithm is then applied to obtain the minimizer θ∗of MSTDE∆t which in turn determines J(t, x) = Jθ∗(t, x). Namely, θ ←θ−α K−1 X i=0 \u0012Jθ(ti+1, Xti+1) −Jθ(ti, Xti) ti+1 −ti +rti \u0013\u0012∂Jθ ∂θ (ti+1, Xti+1)−∂Jθ ∂θ (ti, Xti) \u0013 , (13) where α is the learning rate (step size). This updating rule is also referred to as the na¨ıve residual gradient method (Barnard, 1993; Baird, 1995). The above algorithm is stated in the oﬄine setting; namely, one uses the whole sample trajectory to update θ. However, TD-learning is often advocated for online learning: instead of observing the full sample path over [0, T], one updates the estimate of the value function at each discrete time point using all available historical information. Take the most popular one-step method for example. With the time discretization described above, this method updates θ by θ ←θ −α \u0012Jθ(ti+1, Xti+1) −Jθ(ti, Xti) ti+1 −ti + rti \u0013\u0012∂Jθ ∂θ (ti+1, Xti+1) −∂Jθ ∂θ (ti, Xti) \u0013 . 9. Doya (2000) still refers this term as “TD error”, while we add “rate” in its deﬁnition to reﬂects that it is indeed the instantaneous temporal diﬀerence at a given time t. However, we will use both terms interchangeably in this paper. 9", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 9, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p10::c0", "text": "Jia and Zhou Notice that this increment is just one term in that of (13). The most important feature of these TD-based algorithms that makes them imple- mentable for learning is that one can observe the payoﬀs rti and the states Xti, and hence can compute Jθ(ti, Xti), i = 0, 1, · · · , K −1, through samples, without having to know the model parameters. 3.2 Mean-square TD error for stochastic dynamics If we are to extend the MSTDE approach na¨ıvely from Doya (2000)’s deterministic setting to the current stochastic (diﬀusion) setting, then we ﬁrst note that the following equation, which is similar to (8), holds J(t, Xt) = E \"Z t′ t r(s, Xs)ds + J(t′, Xt′) Ft # . t′ ∈(t, T]. (14) This equation is called Bellman’s consistency. Then E \" J(t′, Xt′) −J(t, Xt) t′ −t + 1 t′ −t Z t′ t r(s, Xs)ds # = 0. (15) We may then be tempted to deﬁne a stochastic version of the TD error rate as in (10). Unfortunately, the path-wise total derivative ˙Jt = d dtJ(t, Xt) no longer exists in the current diﬀusion case; hence, the TD error rate δt is not well deﬁned now. This issue stems from the intrinsic non-diﬀerentiability of (non-degenerate) diﬀusion processes. For instance, it is well-known that with probability one, the sample trajectory of a Brownian motion is nowhere diﬀerentiable. To facilitate our analysis without getting overly technical, we make the following regu- larity assumption on the value function approximators Jθ we use in this paper: Assumption 3 Jθ(t, x) is a suﬃciently smooth function of (t, x, θ) so that all the deriva- tives required exist in the classical sense. Moreover, for all θ ∈Θ, Jθ(·, X·), LJθ(·, X·) + r·, ∂Jθ ∂x (·, X·)⊤σ(·, X·) ∈L2 F([0, T]), and their L2-norms are continuous functions of θ. Given an approximator Jθ, a theoretically well-deﬁned error based on (14) in continuous time is the so-called Bellman’s error rate: lim t′→t+ 1 t′ −tE \"Z t′ t r(s, Xs)ds + Jθ(t′, Xt′) −Jθ(t, Xt) Ft # = LJθ(t, Xt) + r \u0000t, Xt \u0001 . (16) This can be derived by applying Itˆo’s formula to Jθ(t, Xt). If there is no randomness in the environment, the conditional expectation in (16) van- ishes and hence Bellman’s error coincides with TD error (10) in the deterministic case. In a non-degenerate stochastic environment, however, only Bellman’s error {LJθ(t, Xt) + r \u0000t, Xt \u0001 , 0 ≤t ≤T} is well deﬁned on sample trajectories. Note that this error is zero everywhere for the true value function, according to (3). So it seems natural to set a PE objective to minimize Bellman’s error. Unfortunately, this error accounts for the condi- tional expectation and thus represents the average of temporal diﬀerences over inﬁnitely 10", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 10, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p11::c0", "text": "Policy Evaluation and TD Learning in Continuous Time many sample trajectories that are distributed according to the SDE (1). Therefore, the knowledge about the state dynamics is required in computing the conditional expectation or, equivalently, in applying the operator L. This knowledge is nevertheless unknown to the agent in our RL setting. In other words, in sharp contrast to the TD error, Bellman’s error and its discretization version cannot be computed with only samples in a black-box environment. On the other hand, even though MSTDE does not exist theoretically in the continuous- time stochastic setting, we can still deﬁne and compute its discretization version, in a way analogous to (12): MSTDE∆t(θ) :=1 2E \"K−1 X i=0 \u0012Jθ(ti+1, Xti+1) −Jθ(ti, Xti) ti+1 −ti + rti \u00132 ∆t # . (17) Indeed, Wang and Zhou (2020) use this version to develop a PE algorithm for the mean– variance problem. A natural question is whether minimizing MSTDE∆t(θ) (or equivalently applying the stochastic version of the residual gradient algorithm (13)) will lead to the correct solution in the stochastic environment. The answer is unfortunately negative, as illustrated by the following example. Example 1 Let us ﬁnd a function that represents the conditional expectation J(t, x) = E[X1|Xt = x], where Xt = Wt is a Brownian motion. This is probably the simplest example possible. Because the Brownian motion is a martingale, we know the ground truth solution J(t, x) = x. Pretending we do not know this solution, we proceed to minimize MSTDE∆t(θ) to learn J based on the simulated sample paths of the state process X ≡W = {Wt, 0 ≤ t ≤1}, which is a standard Brownian motion starting from W0 = 0. We ﬁrst use a parameterized family Jθ(t, x) = [θ(1 −t) + 1]x to approximate J. This family contains the true function when θ = 0. The discretized MSTDE is MSTDE∆t(θ) = 1 2E   K−1 X i=0 \u0000θ(1 −ti+1) + 1 \u0001 Xti+1 − \u0000θ(1 −ti) + 1 \u0001 Xti ti+1 −ti !2 ∆t  . We apply the stochastic gradient decent (SGD) with the updating rule θ ←θ−α K−1 X i=0 \u0012\u0000θ(1 −ti+1) + 1 \u0001 Xti+1 − \u0000θ(1 −ti) + 1 \u0001 Xti ti+1 −ti \u0013 \u0002 (1 −ti+1)Xti+1 −(1 −ti)Xti \u0003 . In our simulation we use multiple independent episodes for training. We take the time grid size as ∆t = 0.01, initialize the parameter to be θ(0) = −1, and apply the above updating rule with the learning rate α = 0.01. Figure 1 illustrates the convergence of θ to θ∗ MSTDE = −3 2 which is not the true so- lution θtrue = 0. In other words, the value function is not correctly learned by MSTDE. Equivalently, it does not solve the PDE (3) or the FBSDE (4) correctly. 11", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 11, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p12::c0", "text": "Jia and Zhou Figure 1: The paths of parameters over episodes with diﬀerent objectives for Example 1. The true solution is θtrue = 0. Applying SGD to minimize MSTDE∆t leads to θ∗ MSTDE = −3 2. Applying SGD to minimize the martingale loss function generates the correct solution. We repeat the experiment for 100 times to calculate the standard deviations, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. 3.3 Theoretical characterization of mean-square TD error To understand theoretically why taking the objective of minimizing MSTDE does not work for stochastic problems, recall the processes (X, Y, Z) and the martingale M deﬁned through (4)–(6) in which we take t = 0. Then K−1 X i=0 \u0012J(tt+1, Xti+1) −J(ti, Xti) ti+1 −ti + rti \u00132 ∆t = 1 ∆t K−1 X i=0 \u0012 J(ti+1, Xti+1) −J(ti, Xti) + Z ti+1 ti rtsds + O \u0000(∆t)2\u0001\u00132 ≈1 ∆t⟨M⟩T = 1 (∆t)2 ⟨Y ⟩T = 1 ∆t Z T 0 |Zt|2dt, which is not zero, unlike in the deterministic setting. Hence, minimizing the MSTDE is wrong, because it is equivalent to minimizing the expected quadratic variation of the martingale M, which should not be minimized as the objective for estimating the value function.10 Take Example 1 again: Example 1 (Continued) Let us use the previously taken parameterized family Y θ t = Jθ(t, Xt) = [θ(1 −t) + 1]Xt to approximate J. Then dMθ t ≡dY θ t = [θ(1 −t) + 1]dWt, 10. A related notion in ﬁnancial econometrics is the realized variance of a time series, which is proved to be an unbiased estimate of the integrated variance or the quadratic variation, see, e.g., Barndorﬀ-Nielsen and Shephard (2002). 12", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 12, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p13::c0", "text": "Policy Evaluation and TD Learning in Continuous Time leading to ⟨Mθ⟩1 = Z 1 0 \u0002 1 + θ(1 −t) \u00032dt = 1 3θ2 + θ + 1, which is minimized at θ∗= −3 2, instead of the desired value θtrue = 0. This theoretical value matches the simulation result reported in Figure 1. We now present a slightly more involved example, one that includes a running reward term. Example 2 We seek a function representing the conditional expectation J(t, x) = E[X2 1 − R 1 t ds|Xt = x] where Xt = Wt is a Brownian motion. Theoretically, the problem is equivalent to solving the following BSDE: dYt = dt + ZtdWt, Y1 = X2 1. The true solution is Yt = X2 t , Zt = 2Xt, namely, J(t, x) = x2. If we use a parameterized family Y θ t = Jθ(t, Xt) = [θ0(1 −t) + 1]X2 t + θ1(1 −t)Xt + θ2(1 −t) to approximate J, then the desired parameter values are θtrue = (0, 0, 0). Let us compute the quadratic variation of Mθ t := Y θ t −t. By Itˆo’s lemma and replacing Xt by Wt, we obtain dMθ t = (−θ2 −θ1Wt −θ0W 2 t −1)dt + \u0002 2 \u0000θ0(1 −t) + 1 \u0001 Wt + θ1(1 −t) \u0003 dWt. Then its expected quadratic variation is E[⟨Mθ⟩1] =E Z 1 0 \u0002 2 \u0000θ0(1 −t) + 1 \u0001 Wt + θ1(1 −t) \u00032dt = Z 1 0 h 4 \u0000θ0(1 −t) + 1 \u00012t + θ2 1(1 −t)2i dt =4 \u0012 1 12θ2 0 + 1 3θ0 + 1 2 \u0013 + 1 3θ2 1, . which attains minimum at θ∗ 0 = −2, θ∗ 1 = 0. Here, the parameter θ2 is not present in the expected quadratic variation, and hence remains undetermined. However, due to numerical errors in computing the TD error, we can determine θ2 by minimizing the high-order small term in the quadratic variation, given the minimizer, (θ∗ 0, θ∗ 1), of the leading term. To do this, recall we have the following expansion: (dMθ t )2 = · · · |{z} leading-order term (dWt)2 + · · · |{z} high-order small term (dt)2 + · · · |{z} mean-zero term dWtdt. So, parameters will be determined ﬁrst through the leading term in the quadratic variation. Parameters that do not show up in the leading term have much smaller but non-negligible impact on the TD error, which can be determined through the second term in the above. Finally, the mean-zero term can be ignored because it will be averaged out. Therefore, in the current example, θ2 will be determined through minimizing the fol- lowing: E Z 1 0 (−θ2 −θ∗ 1Wt −θ∗ 0W 2 t −1)2dt = Z 1 0 h (θ2 + 1)2 + 2(θ2 + 1)θ∗ 0t + 3θ∗ 0 2t2i dt. 13", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 13, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p14::c0", "text": "Jia and Zhou The minimizer is θ∗ 2 = 0. So, optimal parameters to minimize the MSTDE are θ∗ MSTDE = (−2, 0, 0) and hence the resulting learned function is J(t, x) = (2t−1)x2. However, the true function is J(t, x) = x2.11 We now verify this analysis by simulation. The discretized mean-square TD error is MSTDE∆t(θ) = 1 2E   K−1 X i=0 Jθ(ti+1, Xti+1) −Jθ(ti, Xti) ti+1 −ti −1 !2 ∆t  . We initialize the parameter to be θ(0) = (−1, −1, −1), and use the SGD algorithm. The learning rate is taken as 0.01. The result, shown in Figure 2, is consistent with the above theoretical analysis, which incidentally justiﬁes our scheme of determining some of the parameters through the high-order term. Figure 2: The paths of parameters over episodes with diﬀerent objectives for Example 2. The true solution is θtrue = (0, 0, 0). Applying SGD to minimize mean-square TD error leads to θ∗ MSTDE = (−2, 0, 0). Applying SGD to minimize the martingale loss function leads to the desired solution. We repeat the experiment for 100 times to calculate the standard deviations of the predicted parameters, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. Next, we present a general result stipulating that any algorithm minimizing MSTDE∆t indeed converges to the minimizer of the quadratic variation of Mθ. First, it follows from Itˆo’s lemma that dMθ t = h LJθ(t, Xt) + rt i dt + \u0012∂Jθ(t, Xt) ∂x \u0013⊤ σ(t, Xt)dWt. Hence d⟨Mθ⟩t ≡ \u0000dMθ t \u00012 = \"\u0012∂Jθ(t, Xt) ∂x \u0013⊤ σ(t, Xt) #2 (dWt)2 | {z } leading-order term: quadratic variation + h LJθ(t, Xt) + rt i2 (dt)2 + (· · · )dWtdt | {z } high-order small term . (18) 11. Even though the two parameters θ∗ 1, θ∗ 2 agree with the correct ones, this seems to be a coincidence and the ﬁnal learned value function still deviates from the true one. 14", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 14, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p15::c0", "text": "Policy Evaluation and TD Learning in Continuous Time The ﬁrst term on the right hand side is the leading order term since (dWt)2 = dt. Therefore, minimizing the left hand side is essentially to minimize this leading term. Before presenting the theorem that formalizes this result, let us note that in the time- discretization throughout this paper, Xti, i = 0, 1, · · · , K, are discrete observations of the continuous-time process X which is the exact solution to (1), instead of its approximation resulting from any numerical approximation scheme (such as the ones in Kloeden and Platen, 1992). So, in our paper the only approximation happens in evaluating the cumulative reward between two consecutive observations – we use the instantaneous reward rti observed at time ti multiplied by the length of the time window to approximate the total reward: rti∆t ≈ R ti+1 ti rtsds – and in calculating the integral on [0, T] by a discrete sum with the forward Euler scheme. Clearly, the error of approximating cumulative reward is 0 if the running reward is a constant. When it is not a constant, the convergence rate of the approximation requires some conditions, which we put forward as an assumption. Assumption 4 There exist constants C > 0 and µ1, µ2, µ3 ≥0, such that |r(t′, x′) −r(t, x)| ≤C|t′ −t|µ1|x′ −x|µ2(|x′|µ3 + |x|µ3), ∀t′, t ∈[0, T], x′, x ∈Rd. Theorem 2 Suppose Assumptions 1, 2, and 3 hold. Let θ∗ MSTDE(∆t) ∈arg min θ∈Θ MSTDE∆t(θ) and assume that θ∗ MSTDE := lim∆t→0 θ∗ MSTDE(∆t) exists. Then θ∗ MSTDE ∈arg min θ∈Θ E Z T 0 \u0012∂Jθ(t, Xt) ∂x \u0013⊤ σ(t, Xt) 2 dt. Moreover, if Assumption 4 also holds true, then E Z T 0 ∂Jθ∗ MSTDE(∆t)(t, Xt) ∂x !⊤ σ(t, Xt) 2 dt−min θ∈Θ E Z T 0 \u0012∂Jθ(t, Xt) ∂x \u0013⊤ σ(t, Xt) 2 dt ≤C∆t, for some constant C. In contrast, the true value function J solves the PDE (3) which corresponds to the coeﬃcient of the (dt)2 term in (18). So once again the parameters should minimize the mean-square Bellman’s error (which as discussed earlier depends on the model parameters and hence any algorithm trying to accomplish it is not implementable). This shows a fundamental discrepancy between the objective of MSTDE and that of PE in the stochastic diﬀusion environment. The undesirability of the na¨ıve residual gradient or MSTDE has actually been noticed in the discrete-time MDP literature. For example, Sutton et al. (2009) point out the sim- ilar problem of MSTDE and present a simple counterexample in an adsorbing three-state Markov chain. Sutton and Barto (2018, p.272) comment that “by penalizing all TD errors it (MSTDE) achieves something more like temporal smoothing than accurate prediction,” 15", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 15, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p16::c0", "text": "Jia and Zhou although the authors stop short of explaining why it achieves temporal smoothing. Our theory conﬁrms this intuition by a rigorous analysis showing that, in the diﬀusion setting, the MSTDE minimizer is primarily determined through minimizing quadratic variation. As quadratic variation measures the smoothness of a diﬀusion process, the value function process {Jθ(t, Xt), 0 ≤t ≤T} has the smoothest trajectory at θ = θ∗ MSTDE. 3.4 Online mean-square TD error algorithms So far our discussions have been focused on the oﬄine setting. However, TD is often used for online learning. The question now is whether an online algorithm may correct the errors arising from MSTDE. Let us take the one-step online method for illustration. Precisely, suppose the time discretization is 0 = t0 < t1 < · · · < tK = T. At each time ti+1, i = 0, · · · , K −1, this method updates θ by the following SGD algorithm: θ ←θ −α \u0012Jθ(ti+1, Xti+1) −Jθ(ti, Xti) ti+1 −ti + rti \u0013\u0012∂Jθ ∂θ (ti+1, Xti+1) −∂Jθ ∂θ (ti, Xti) \u0013 , and then loop over all episodes. Since multiple episodes are used, this procedure, by and large, can be viewed as drawing samples in the time direction uniformly (since the learning rate is constant, one does not diﬀerentiate among diﬀerent times). Therefore, such a one-step updating rule is equivalent to solving min θ Et∼U(0,T) \"\u0012Jθ(t + ∆t, Xt+∆t) −Jθ(t, Xt) ∆t + rt \u00132# ≈min θ 1 ∆t Z T 0 E[(dMθ t )2] ≈min θ E[ Z T 0 d⟨Mθ⟩t] = min θ E[⟨Mθ⟩T ], where U(0, T) is the uniform distribution on [0, T]. This is the same optimization problem as the oﬄine learning when we use the whole trajectory; hence, theoretically, it will lead to the same undesired solution that is determined by Theorem 2. We revisit Examples 1 and 2 and implement the above online algorithm to minimize the mean-square TD error. Figures 3 and 4 show the results of the learned parameters respectively. As predicted by our analysis, they again converge to the same wrong solutions that are determined by minimizing quadratic variation. 4. Martingale Perspective and Approach It follows from the previous section that MSTDE is not the right objective/loss function for PE in continuous-time stochastic RL. In this section we propose and analyze several other objective functions or criteria all based on the martingality of the process M, and connect some of them to well-studied alternative TD algorithms for MDPs. 4.1 Oﬄine learning: Martingale loss function In this subsection, we propose a loss function that uses full sample trajectories and is therefore applicable for oﬄine learning, and test the corresponding algorithm’s performance. 16", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 16, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p17::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Figure 3: The paths of parameters over episodes with diﬀerent objectives under the online setting for Example 1. The true solution is θtrue = 0. Applying SGD to minimize one-step MSTDE∆t online leads to θ∗ MSTDE = −3 2. CTD(0) and CTD(1) lead to the desired solution. We repeat the experiment for 100 times to calculate the standard deviations, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. Figure 4: The path of parameters over episodes for diﬀerent objectives under the online setting for Example 2. The true solution is θtrue = (0, 0, 0). Based on our analysis of quadratic variation, the minimizer is θ∗ MSTDE = (−2, 0, 0). CTD(0) and CTD(1) lead to the desired solution. We repeat the experiment for 100 times to calculate the standard deviations, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. 17", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 17, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p18::c0", "text": "Jia and Zhou Let the state process be {Xt, 0 ≤t ≤T}. Recall that the square-integrable martingality of Mt = J(t, Xt) + R t 0 r(s, Xs)ds characterizes the correct value function. The martingale condition is further equivalent to Mt = E[MT |Ft], for all t ∈[0, T], which in turn stipulates that the process at any given time prior to the terminal time T is the expectation of the terminal value conditional on all the information available at that time. However, a fundamental property of the conditional expectation yields that Mt minimizes the L2-error between MT and any Ft-measurable random variables, namely, Mt ≡E[MT |Ft] = arg min ξ is Ft-measurable E|MT −ξ|2, for all t ∈[0, T]. This property inspires the following loss function, termed the martingale loss function: ML(θ) :=1 2||MT −Mθ · ||2 L2 = 1 2E Z T 0 |MT −Mθ t |2dt ≈1 2E \u0014 K−1 X i=0 \u0012 h(XT ) −Jθ(ti, Xti) + Z T ti r(s, Xs)ds \u00132 ∆t \u0015 ≈1 2E \u0014 K−1 X i=0 \u0012 h(XtK) −Jθ(ti, Xti) + K−1 X j=i r(tj, Xtj)∆t \u00132 ∆t \u0015 =1 2E \u0014 K−1 X i=0 \u0012 h(XtK) + K−1 X j=0 r(tj, Xtj)∆t −Jθ(ti, Xti) − i−1 X j=0 r(tj, Xtj)∆t \u00132 ∆t \u0015 = : ML∆t(θ), (19) where 0 = t0 < t1 < · · · < tK = T is a mesh grid in time. Note that this loss function does not rely on the knowledge of the functional forms of b, σ, r or h.12 As long as we can observe the accumulated reward Pi−1 j=0 r(tj, Xtj) along with the ﬁnal reward h(XT ), the loss function can be implemented with the expectation replaced by the average over sample trajectories. This loss function uses the whole trajectory to calculate the diﬀerence between the predicted value function and the realized reward-to-go, minimizing which naturally leads to an unbiased estimation. This approach is the continuous-time analogue of the so-called Monte Carlo policy evaluation with function approximation (Sutton and Barto, 2018) in the classical MDP and RL literature. It is primarily for oﬄine learning where one observes multiple trajectories oﬄine and updates estimate after observing one full trajectory. The martingale loss objective is not of a TD type; it does not compare the approximate function values at two consecutive times. To explain the diﬀerence between the martingale loss function and the mean-square TD error, let us assume that the running reward r ≡0 for simplicity. In this case, Mt = J(t, Xt) is a martingale. The martingale loss considers the diﬀerence values of J between any time instance and the ﬁnal time, J(XT ) −J(ti, Xti) = 12. In particular, MT = h(XT ) + R T 0 rsds does not depend on the parameter θ and can be directly observed from samples as the total reward obtained over [0, T]. 18", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 18, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p19::c0", "text": "Policy Evaluation and TD Learning in Continuous Time h(XT ) −J(ti, Xti), while the TD concerns the diﬀerence between two consecutive time instances, J(ti+1, Xti+1)−J(ti, Xti). The intuition why the former leads to the right solution is that it always compares the current value of J with that of the ﬁnal time, h(XT ), which is observable and thus can serve as an ultimate and correct target. In fact, instead of thinking of J(t, x) as a bivariate function of time t and space x, in any numerical procedure one is essentially looking for K functions of x, denoted by Ji(·) = J(ti, ·), where i = 0, · · · , K −1, with JK(x) = h(x) known and given. Therefore, the martingale loss is the aggregated error between Ji and JK = h, minimizing which also minimizes the error between Ji and JK for each i. As a result, each Ji converges to the correct value. In contrast, the mean-square TD error represents the aggregated intertemporal L2 error between Ji and Ji+1. When computing this error, each Ji except J0 shows up twice in |Ji −Ji+1|2 and |Ji −Ji−1|2; so the resulting function Ji will be twisted away from the true value, leading to a wrong solution. Finally, we can apply SGD to minimize the proposed martingale loss function and the updating rule is given by θ ←θ + α K−1 X i=0 \u0012 h(XtK) + K−1 X j=i r(tj, Xtj)∆t −Jθ(ti, Xti) \u0013∂Jθ ∂θ (ti, Xti)∆t. (20) Let us call this the ML (martingale loss) algorithm, which is the counterpart of the gradient Monte Carlo in classical RL with MDP, when G(ti) := h(XtK)+PK−1 j=i r(tj, Xtj)∆t is taken as the Monte Carlo target at each ti (Sutton and Barto, 2018). We apply this algorithm to numerically solve Examples 1 and 2, and ﬁnd that it leads to the true solution; see Figures 1 and 2. In our implementation, the initial value is the same as before and the learning rate is tuned for smoother convergence. In particular, the initial learning rate is set to be 0.01 and decays according to (♯episode)−0.67 where ♯episode denotes the number of episode.13 The next theorem states that minimizing the martingale loss function is equivalent to minimizing the mean-square error between the estimated value function Jθ and the true value function J. This error is known as the mean-square value error (MSVE): MSVE(θ) = E Z T 0 |J(t, Xt) −Jθ(t, Xt)|2dt. (21) Theorem 3 It holds that arg min θ∈Θ ML(θ) = arg min θ∈Θ MSVE(θ). Moreover, under Assumptions 1, 2, and 3, as ∆t →0, any convergent subsequence of the minimizer of the discretized martingale loss function θ∗ ML(∆t) ∈arg minθ∈Θ ML∆t(θ) converges to the minimizer of martingale loss function; that is lim ∆t→0 θ∗ ML(∆t) = θ∗ ML ∈arg min θ∈Θ ML(θ) = arg min θ∈Θ MSVE(θ). 13. This decay schedule satisﬁes the usual requirement on the decay rate of the learning rate for the conver- gence of SGD algorithms. Note here our purpose is not to optimize convergence rates, but to conﬁrm the limiting point for a convergent algorithm. Tuning the learning rate is not crucial to our results, as long as the algorithm does converge. 19", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 19, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p20::c0", "text": "Jia and Zhou Furthermore, if in addition Assumption 4 holds, then ML (θ∗ ML(∆t)) −min θ∈Θ ML(θ) ≤C(∆t)min{1,µ1+ µ2 2 } for some constant C > 0. Clearly, MSVE is a theoretically sound loss function for learning. However, by itself it does not lead to an implementable algorithm because the true value J is not observable from data. Theorem 3 strengthens the theoretical foundation of the martingale loss function that is implementable. Moreover, this theorem establishes the convergence together with the convergence rate of applying any convergent algorithm developed for minimizing discrete- time martingale loss as the time step tends to zero. Therefore, it also provides a theoretical foundation for implementing the discretization procedure. We illustrate this result with the following examples. Example 3 Consider the same learning problem in Example 1, but with a diﬀerent pa- rameterized value function given by Jθ(t, x) = θx3. Recall Xt = Wt is a Brownian motion. The main diﬀerence between this example and the previous ones is that now the paramet- ric family does not contain the true solution. Indeed, it does not even satisfy the correct terminal condition that Jθ(1, w) = x, which could happen in more complex problems when the terminal payoﬀfunctions are unknown. Recall the true value function is J(t, x) = x. Let us compute the MSVE: E Z 1 0 |J(t, Xt) −Jθ(t, Xt)|2dt = E Z 1 0 \u0000Wt −θW 3 t \u00012dt = Z 1 0 (t −6θt2 + 15θ2t3)dt. The minimizer is θ∗= 4 15. According to Theorem 3, minimizing the martingale loss function should converge to this solution. Example 4 Consider the same learning problem in Example 1, with the parameterized value function Jθ(t, x) = x + (1 −t)eθx−1 2 θ2t+θ. Recall Xt = Wt is a Brownian motion. This time it satisﬁes the terminal condition, but still does not contain the true solution. Let us compute the MSVE: E Z 1 0 |J(t, Xt) −Jθ(t, Xt)|2dt = E Z 1 0 (1 −t)2e2θWt−θ2t+2θdt = Z 1 0 (1 −t)2eθ2t+2θdt = −e2θ(2 −2eθ2 + 2θ2 + θ4) θ6 . The minimizer is θ∗≈−2.12568. According to Theorem 3, minimizing the proposed mar- tingale loss function should converge to this solution. We test the numerical solutions to Examples 3 and 4 by applying our ML algorithms with SGD. The initial learning rate is taken as 0.001 and decays as (♯episode)−0.67. Figures 5 and 6 conﬁrm the result of Theorem 3. 20", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 20, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p21::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Figure 5: ML and CTD(λ) methods converge to diﬀerent points for Example 3. Applying ML algorithm leads to θ∗ ML = 4 15, which is the minimizer of MSVE. CTD methods converge to θ∗ moment = 0, which is the solution to the moment condition. In this case, the moment conditions associated with CTD(0) and CTD(1) have the same solution so the two algorithms converge to the same point. We repeat the experiment for 100 times to calculate the standard deviations, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. 4.2 Online and oﬄine learning: TD based on martingale orthogonality conditions We have proposed a martingale loss function to interpret the Monte Carlo PE. This approach requires the whole sample trajectory over [0, T]; so it is inherently oﬄine and is diﬃcult to extend to the online setting where only historical samples are available when one updates the approximated function in real time. Classically, TD learning was introduced as a remedy to enable online learning. However, based on our previous discussion, the mean-square TD error is not the correct objective function to learn the value function even though it can indeed be implemented online. In this section, we propose a diﬀerent approach, again based on the martingality of the process M, that generates the continuous-time counterparts of several well-studied TD algorithms and that works both online and oﬄine. This approach starts with noting that M being a square-integrable martingale implies E Z T 0 ξtdMt = 0, (22) for any ξ ∈L2 F([0, T], M) (called a test function).14 In fact, the following result shows that this is a necessary and suﬃcient condition for the parameterized process Mθ t to be a martingale. Proposition 4 In general, a diﬀusion process Mθ ∈L2 F([0, T]) is a martingale if and only if E R T 0 ξtdMθ t = 0 for any ξ ∈L2 F([0, T], Mθ). In the current setting, E R T 0 ξtdMθ t = E R T 0 ξt \u0002 LJθ(t, Xt) + rt \u0003 dt. 14. It would be more appropriate to call it a test process because ξ needs to be generally an adapted stochastic process. However, we use the more common term “test function”. 21", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 21, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p22::c0", "text": "Jia and Zhou Figure 6: ML and CTD(λ) methods converge to diﬀerent points with diﬀerent values of λ for Example 4. Applying ML algorithm leads to θ∗ ML ≈−2.12568, which is the minimizer of MSVE. CTD(0) converges to θ∗ moment,CTD(0) ≈−1.83923, which is the solution to the moment condition associated with the choice of test function for CTD(0). CTD(1) converges to θ∗ moment,CTD(1) ≈−2.12568, which is the solution to the moment condition associated with the choice of test function for CTD(1). Because of the diﬀerent choices of test functions, the two CTD algorithms converge to diﬀerent points. It is a coincidence that ML and CTD(1) converge to the same point. We repeat the experiment for 100 times to calculate the standard deviations, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. We call (22) a family of martingale orthogonality conditions. In theory, one should vary all possible test functions and thus this family has inﬁnitely many equations. For numerical approximation methods, however, we can choose ﬁnitely many test functions in special forms. Notice that, for a parametric family {Jθ : θ ∈Θ ⊂RL}, in principle, we need at least L equations as our martingale orthogonality conditions in order to fully determine θ. For example, we can take ξt = ∂Jθ ∂θ (t, Xt) ∈RL. (Here, and henceforth, ξt may be vector-valued and (22) is accordingly a vector-valued equation or a system of equations.) In statistics and econometrics, a problem of the type (22) with a ﬁnite number of equations is also referred to as moment conditions, and a systematic way to analyze and solve it is known as the generalized methods of moments (GMM); see, e.g., Hansen (1982). To devise algorithms based on this theory, we need to answer the following questions: How to choose these ﬁnite number of test functions? And how to solve the resulting moment conditions in an eﬀective and eﬃcient way? It turns out that answering these two questions suitably in our continuous setting gives rise to algorithms that correspond to several well- known conventional TD learning algorithms in discrete setting. • Choose ξt = ∂Jθ ∂θ (t, Xt), and use stochastic approximation (Robbins and Monro, 1951) to update parameters after a whole episode (oﬄine): θ ←θ+α Z T 0 ∂Jθ ∂θ (t, Xt)dMθ t ≈θ+α K−1 X i=0 ∂Jθ ∂θ (ti, Xti) \u0000Jθ(ti+1, Xti+1)−Jθ(ti, Xti)+rti∆t \u0001 , 22", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 22, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p23::c0", "text": "Policy Evaluation and TD Learning in Continuous Time or to update parameters at every time step (online): θ ←θ + α∂Jθ ∂θ (t, Xt)dMθ t ≈θ + α∂Jθ ∂θ (ti, Xti) \u0000Jθ(ti+1, Xti+1) −Jθ(ti, Xti) + rti∆t \u0001 . These algorithms correspond to the TD(0) algorithm (Sutton, 1988). • Choose ξt = R t 0 λt−s ∂Jθ ∂θ (s, Xs)ds, where 0 < λ ≤1, and use stochastic approximation to update parameters after one episode: θ ←θ + α Z T 0 Z t 0 λt−s ∂Jθ ∂θ (s, Xs)dsdMθ t ≈θ + α K−1 X i=0 i X j=0 ∆tλ(i−j)∆t ∂Jθ ∂θ (tj, Xtj) \u0000Jθ(ti+1, Xti+1) −Jθ(ti, Xti) + rti∆t \u0001 , or to update parameters at every time step: θ ←θ + α Z t 0 λt−s ∂Jθ ∂θ (s, Xs)dsdMθ t ≈θ + α i X j=0 ∆tλ(i−j)∆t ∂Jθ ∂θ (tj, Xtj) \u0000Jθ(ti+1, Xti+1) −Jθ(ti, Xti) + rti∆t \u0001 . These algorithms correspond to the TD(λ) algorithm (Sutton, 1988). Here the param- eter λ dictates how much weight to be put on historical predictions in the procedure. When λ = 1, it puts equal weight on all the past predictions. The smaller λ becomes, the more weight on more recent predictions. When λ = 0, past predictions do not matter, resulting in the TD(0) algorithm. It should be noted that TD(0) and TD(λ) algorithms are not exactly gradient based; rather, they use stochastic approximation as a ﬁrst-order method to solve (22). In the literature they are also referred to as semi-gradient TD algorithms (Sutton and Barto, 2018) because they do include a part of the gradient. • Choose ξt = ∂Jθ ∂θ (t, Xt) and take the approximated value function to be a linear combination of a series of basis functions: Jθ(t, x) = PL j=1 θjφj(t, x). Then ∂Jθ ∂θ (t, x) = φ(t, x) := (φ1(t, x), · · · , φL(t, x))⊤∈RL. In this case, (22) becomes a system of linear equatins in θ and can be solved explicitly as θ = − \u0014 E Z T 0 φ(t, Xt) \u0000dφ(t, Xt)⊤\u0001\u0015−1 E Z T 0 rtφ(t, Xt)dt, assuming the matrix inverse exists. The expectation can be estimated using sample average across multiple trajectories. Hence, if there are M episodes, we have θ = − \u0012 1 M M X k=1 Z T 0 φ(t, X(k) t ) \u0000dφ(t, X(k) t )⊤\u0001\u0013−1\u0012 1 M M X k=1 Z T 0 r(k) t φ(t, X(k) t )dt \u0013 ≈− \u0012 1 M M X k=1 K−1 X i=0 φ(ti, X(k) ti ) \u0000φ(ti+1, X(k) ti+1)⊤−φ(ti, X(k) ti )⊤\u0001\u0013−1\u0012 1 M M X k=1 K−1 X i=0 r(k) ti φ(ti, X(k) ti )∆t \u0013 , 23", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 23, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p24::c0", "text": "Jia and Zhou where the superscript (k) signiﬁes that the corresponding observations are taken from the k-th episode. If there is only one trajectory up to time t = tj, then we estimate the parameter using long-time average (under certain ergodicity condition) to obtain θ = − \u00121 t Z t 0 φ(s, Xs) \u0000dφ(s, Xs)⊤\u0001\u0013−1\u00121 t Z t 0 rsφ(s, Xs)ds \u0013 ≈− \u00121 j j−1 X i=0 φ(ti, Xti) \u0000φ(ti+1, Xti+1)⊤−φ(ti, Xti)⊤\u0001\u0013−1\u00121 j j−1 X i=0 rtiφ(ti, Xti)∆t \u0013 . These algorithms correspond to the (linear) least square TD(0), or LSTD(0), algo- rithms (Bradtke and Barto, 1996). LSTD and its variants (Boyan, 2002) are often discussed in the context of linear function approximation. Despite the name of “least square”, it does not solve any minimization problem per se; instead it uses the linear structure to obtain the exact solution to (22) and then use sample average to estimate the expectation. Xu et al. (2002) and Geramifard et al. (2006) develop a more eﬃcient way to implement this solution in a recursive way. The reason why it is called “least square” comes from the instrumental variable approach to regression problems (Ljung and S¨oderstr¨om, 1983).15 Bradtke and Barto (1996) show that the basis functions in LSTD are indeed instrumental variables. • We choose ξt = ∂Jθ ∂θ (t, Xt), and minimize the GMM objective function GMM(θ) = 1 2E \u0014Z T 0 ξtdMθ t \u0015⊤ AE \u0014Z T 0 ξtdMθ t \u0015 , where A is a given matrix. Diﬀerent choices of A lead to a variety of algorithms corresponding to what are broadly called gradient TD (GTD) algorithms for MDPs. For example, taking A = I, the identity matrix, corresponds to GTD(0) by Sutton et al. (2009). Another choice is A = [E R T 0 ξtξ⊤ t dt]−1. In this case, the gradient of the objective in θ is (noting ξt also depends on θ) E \u0014Z T 0 d \u0012∂Mθ ∂θ (t, Xt) \u0013 ξ⊤ t \u0015 u + E \"Z T 0 ∂ξt ∂θ ⊤ dMθ t # u −E \u0014Z T 0 u⊤ξt ∂ξt ∂θ (t, Xt)⊤udt \u0015 , where u := [E R T 0 ξtξ⊤ t dt]−1E[ R T 0 ξtdMθ t ] and ∂ξt ∂θ is the Jacobian matrix. In particular, ∂ξt ∂θ = ∂2Jθ ∂θ2 (t, Xt) is the Hessian matrix and hence is symmetric. When Jθ(t, x) = PL j=1 θjφj(t, x) is the linear span of basis functions, the last two terms of the gradient will vanish because ∂ξt ∂θ = ∂2Jθ ∂θ2 (t, Xt) = 0. Two GTD algorithms, called GTD2 and TDC (Sutton et al., 2008, 2009), apply stochastic approximation at two diﬀerent time scales to update u and θ respectively. 15. Instrumental variables are widely used in statistics and econometrics to estimate causal relationship when exploratory variables are endogenous. A necessary condition for being a instrumental variable is that it must be uncorrelated with the residual term. In the context of TD learning, the residual term is the TD error. 24", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 24, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p25::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Speciﬁcally, in both algorithms, u is estimated with long-term average: u ←u + αu h ξtdMθ t −ξtξ⊤ t u∆t i ≈u + αu \u0002 ξti(Mθ ti+1 −Mθ ti) −ξtiξ⊤ ti u∆t \u0003 , and then θ is updated with two diﬀerent one-step sampling methods. The GTD2 algorithm proceeds as follows: θ ←θ −αθ \" d \u0012∂Mθ ∂θ (t, Xt) \u0013 ξ⊤ t u + ∂ξt ∂θ ⊤ dMθ t u −u⊤ξt ∂ξt ∂θ ⊤ u∆t # ≈θ −αθ \u0014 \u0012∂Mθ ∂θ (ti+1, Xti+1) −∂Mθ ∂θ (ti, Xti) \u0013 ξ⊤ ti u + ∂ξ ∂θ(ti, Xti)⊤(Mθ ti+1 −Mθ ti)u −u⊤ξti ∂ξ ∂θ(ti, Xti)⊤u∆t \u0015 . The TDC algorithm observes that ∂Jθ ∂θ (t, Xt) = ξt and hence updates θ by θ ←θ −αθ \" ξtdMθ t + ξt′ξ⊤ t u∆t + ∂ξt ∂θ ⊤ dMθ t u −u⊤ξt ∂ξt ∂θ ⊤ u∆t # ≈θ −αθ \u0014 ξti(Mθ ti+1 −Mθ ti) + ξti+1ξ⊤ ti u∆t + ∂ξ ∂θ(ti, Xti)⊤(Mθ ti+1 −Mθ ti)u −u⊤ξti ∂ξ ∂θ(ti, Xti)⊤u∆t \u0015 . GTD(0), GTD2 and TDC are gradient based methods as well as typical GMM meth- ods to minimize a quadratic form of the conditions (22), where expectations are es- timated using long term averages as in Hansen et al. (1996). Sutton et al. (2008, 2009) and Maei et al. (2009) study stochastic approximation and incremental imple- mentation of the gradient of quadratic functions for linear and non-linear function approximation respectively. All the above methods can be classiﬁed into two types. The ﬁrst type applies stochastic approximation to solve the moment conditions directly, like TD(λ). This is the classical TD learning. The second type follows GMM to minimize a quadratic function of the moment conditions by computing its gradient and approximating the expectation by either long- term average or one long sample trajectory. We call it the GTD method, following Sutton et al. (2008). LSTD is limited to linear approximation only and hence can be considered as a special case of the ﬁrst type when the moment conditions can be explicitly solved so the only computation needed is to estimate the expectation using samples. It should be noted that although the goal of this paper is to devise PE algorithms for the continuous setting, the actual implementations of the various algorithms described above are all discrete-time with a ﬁxed mesh size ∆t. These algorithms correspond to some discrete- time versions of the moment conditions. So natural and important theoretical questions are whether such an algorithm converges to the solution of the continuous-time version of the respective moment conditions as ∆t →0 and, if yes, what the convergence rate is. The next two theorems answer these questions. 25", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 25, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p26::c0", "text": "Jia and Zhou Henceforth we impose the following assumption on the test functions used for moment conditions. Assumption 5 A test function ξ = {ξt, 0 ≤t ≤T} is an RL′-valued adapted process satisfying |ξ| ∈L2 F([0, T]; Mθ) and E[|ξt′ −ξt|2] ≤C(θ)|t′ −t|α for any t, t′ ∈[0, T], where C(θ) is a continuous function of θ and 0 < α ≤2 is a given constant. The following is about the convergence of the TD type algorithms when ∆t →0. Theorem 5 Denote by θ∗ moment(∆t) the solution to the discrete-time moment conditions with mesh size ∆t: E K−1 X i=0 ξti(Mθ ti+1 −Mθ ti) = 0. Then, under Assumptions 1, 2, 3, and 5, as ∆t →0, any convergent subsequence of θ∗ moment(∆t) converges to the solution to the continuous-time moment conditions (22); that is, θ∗ moment := lim ∆t→0 θ∗ moment(∆t) solves (22). Moreover, if in addition Assumption 4 holds, then |E Z T 0 ξtdMθ∗ moment(∆t) t | ≤C(∆t)min{ α 2 ,µ1+ µ2 2 } for some constant C. The next theorem is on the convergence of the GTD type algorithms when ∆t →0. Theorem 6 Let the discretized GMM objective function be GMM∆t(θ) := 1 2E \"K−1 X i=0 ξθ ti(Mθ ti+1 −Mθ ti) #⊤ A∆tE \"K−1 X i=0 ξθ ti(Mθ ti+1 −Mθ ti) # , where A∆t is a discretized approximation of A satisfying |A∆t −A| ≤˜C(θ)|∆t|β, with ˜C(θ) being a continuous function of θ and β > 0 a constant.16 Then, under Assumptions 1, 2, 3, and 5, as ∆t →0, any convergent subsequence of the minimizer of the discretized GMM objective function θ∗ GMM(∆t) ∈arg minθ∈Θ GMM∆t(θ) converges to the minimizer of the continuous GMM objective function; that is, lim ∆t→0 θ∗ GMM(∆t) = θ∗ GMM ∈arg min θ∈Θ GMM(θ). Moreover, if in addition Assumption 4 holds, then GMM (θ∗ GMM(∆t)) −min θ∈Θ GMM(θ) ≤C(∆t)min{ α 2 ,µ1+ µ2 2 ,β} for some constant C. 16. When A is a constant as in GTD(0), A∆t = A. When A = [E R T 0 ξtξ⊤ t dt]−1 as in GTD2 and TDC, A∆t := [E PK−1 i=0 ξtiξ⊤ ti∆t]−1 is the discretization approximation of this integral. 26", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 26, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p27::c0", "text": "Policy Evaluation and TD Learning in Continuous Time From now on, to distinguish our algorithms from their existing discrete-time counter- parts, we will add a preﬁx “C”, signifying “continuous”, to the names of the algorithms. For instance, we will call them CTD(λ), CLSTD, and so on. The next important question is in what sense the aforementioned methods approximate the correct value function. First, a convergent CTD(0) or CTD(λ) algorithm should con- verge to the solution to the moment conditions (22) based on the respective choices of test functions. Intuitively, such an algorithm searches for one particular Bellman’s error process LJθ(t, Xt)+rt within the parametric family such that it is orthogonal to the underlying test functions. These TD learning methods are usually easy to implement and work eﬀectively in many applications. To demonstrate, we re-compute the problems in Examples 1 and 2 using online CTD(0) and CTD(1) algorithms with stochastic approximation. The learning rate is chosen as 0.01. Both algorithms converge to the correct values; see Figures 3 and 4. However, a caveat is that these algorithms may not always work. On one hand, due to possible misspeciﬁcation of the parametric family, solutions to the moment conditions may not exist, in which case the algorithms will not converge; see Example 5 below where the test function is not properly chosen. On the other hand, as the following continuations of Examples 3 and 4 illustrate, even if the solution to the moment conditions exists uniquely and an algorithm converges, the resulting solution may vary depending on the choice of test functions. Example 5 Consider the same learning problem in Example 1, now with the parameterized value function Jθ(t, x) = x + (1 −t)eθx−1 2 θ2t[(θ + 1)2 + 1]. Recall Xt = Wt is a Brownian motion. This family does not contain the true solution. If we choose the test function to be the constant 1 and use CTD(0), a convergent algorithm should solve 0 = E Z 1 0 eθWt−1 2 θ2t[(θ + 1)2 + 1]dt = (θ + 1)2 + 1. However, there is no solution to the above equation. Our implementation of CTD(0) indeed generates a divergent sequence of iterates; see Figure 7. On the other hand, we can use the martingale loss function to get a solution. Indeed, it follows from Theorem 3 that the ML algorithm is equivalent to minimizing E Z 1 0 |J(t, Xt) −Jθ(t, Xt)|2dt = E Z 1 0 (1 −t)2e2θWt−θ2t[(θ + 1)2 + 1]2dt = Z 1 0 (1 −t)2eθ2tdt[(θ + 1)2 + 1]2 = −2 −2eθ2 + 2θ2 + θ4 θ6 [(θ + 1)2 + 1]2, whose minimizer is around −0.875301. The implementation of ML conﬁrms this theoretical prediction; see Figure 7. Example 3 (Continued) We revisit this example where Jθ(t, x) = θx3. Recall Xt = Wt is a Brownian motion. There is no running reward so Mθ t = θX3 t and dMθ t = 3θW 2 t dWt + 3θWtdt. Hence, any test function that is not identically 0 leads to the only solution θ∗= 0. As a result, any convergent CTD algorithm should converge to 0, yielding a value function Jθ∗(t, x) = 0 that is signiﬁcantly deviated from the true one J(t, x) = x. See Figure 5 for the CTD(0) and CTD(1) experiment results. 27", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 27, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p28::c0", "text": "Jia and Zhou Example 4 (Continued) Consider the parameterized value function Jθ(t, x) = x + (1 − t)eθx−1 2 θ2t+θ. Recall Xt = Wt is a Brownian motion. In this case, dJθ(t, Xt) = dWt + (1 − t)θeθWt−1 2 θ2t+θdWt −eθWt−1 2 θ2t+θdt. If we use the one-step or one-episode CTD(0) algorithm with ξt = ∂Jθ ∂θ (t, Xt) = (1 − t)eθXt−1 2 θ2t+θ(Xt −θt + 1), then the moment condition (22) becomes 0 =E[ Z 1 0 (1 −t)eθWt−1 2 θ2t+θ(Wt −θt + 1)eθWt−1 2 θ2t+θdt] = Z 1 0 (1 −t)(1 + tθ)e(2+tθ)θdt = e2θ[2 −θ + θ2 −θ3 + eθ2(−2 + θ + θ2)] θ5 . This equation has a unique solution θ ≈−1.83923. A convergent CTD(0) algorithm should converge to this point, which is however diﬀerent from the solution produced by the mar- tingale loss function approach. If we use the one-step or one-episode CTD(1) algorithm with ξt = R t 0 ∂Jθ ∂θ (s, Xs)ds = R t 0(1 −s)eθWs−1 2 θ2s+θ(Ws −θs + 1)ds, then the moment condition (22) is 0 =E[ Z 1 0 Z t 0 (1 −τ)eθWτ−1 2 θ2τ+θ(Wτ −θτ + 1)dτeθWt−1 2 θ2t+θdt] = Z 1 0 Z t 0 E \u0002 eθWτ−1 2 θ2τ(Wτ −θτ + 1)E[eθWt−1 2 θ2t|Fτ] \u0003 (1 −τ)e2θdτdt = Z 1 0 Z t 0 E[e2θWτ (Wτ −θτ + 1)](1 −τ)e−θ2τ+2θdτdt = Z 1 0 Z t 0 (1 + τθ)(1 −τ)eθ2τ+2θdτdt = e2θ[6 + 2eθ2(−3 + θ + θ2) −(−1 + θ)θ(−2 + 2θ + θ3)] θ7 . There is a unique solution θ ≈−2.12568, to which a convergent CTD(1) algorithm con- verges. This solution coincides with the one by the martingale loss function approach. The implementations of the above algorithms are reported in Figure 6, which are con- sistent with the theoretical analysis. When the parametric family is a linear span of some basis functions, the unique solution that solves the moment conditions is theoretically guaranteed under very mild conditions, which is numerically generated by the CLSTD algorithm. More generally, all the CGTD methods aim to minimize some quadratic forms of the moment conditions regardless of whether the existence and/or uniqueness of the solution to the conditions holds, and hence usually produce more robust results. Moreover, these methods have a clear geometric interpretation. Recall that the true value function minimizes Bellman’s error to zero. The space of approximate linear functions may not contain the true function, but the CGTD algorithms minimize the projection of Bellman’s error onto the linear space. This intuition is formalized in Sutton et al. (2009) and Maei et al. (2009), who show that the discrete- time GTD minimizers, instead of directly approximating the value function, minimize the so-called mean-square projected Bellman’s error (MSPBE). Here, we present a continuous- time version of the result with a more general choice of the test functions. 28", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 28, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p29::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Theorem 7 For L′ linearly independent test functions ξθ,(1), · · · , ξθ,(L′) ∈L2 F([0, T]), de- note by Πθ the projection operator onto the linear space spanned by {ξθ,(1), · · · , ξθ,(L′)}. Then 1 2E \u0014Z T 0 ξθ t dMθ t \u0015⊤\u0014 E Z T 0 ξθ t (ξθ t )⊤dt \u0015−1 E \u0014Z T 0 ξθ t dMθ t \u0015 =1 2E \u0014Z T 0 \u0000LJθ(t, Xt) + rt \u0001 ξθ t dt \u0015⊤\u0014 E Z T 0 ξθ t (ξθ t )⊤dt \u0015−1 E \u0014Z T 0 \u0000LJθ(t, Xt) + rt \u0001 ξθ t dt \u0015 =1 2||Πθ \u0000LJθ(·, X·) + r· \u0001 ||2 L2 =: MSPBE(θ). Recall Example 5 in which the moment condition admits no solution due to the choice of the test function and hence CTD methods such as CTD(0) will not converge. We now illustrate that CGTD does lead to a solution that is the minimizer of MSPBE. Example 5 (Continued) Consider the same learning problem in Example 1 with the parameterized value function Jθ(t, x) = x + (1 −t)eθx−1 2 θ2t[(θ + 1)2 + 1]. Recall Xt = Wt is a Brownian motion. If we choose the test function to be the constant 1, the projection of eθWt−1 2 θ2t[(θ + 1)2 + 1] onto the subspace spanned by the test function 1 is (θ + 1)2 + 1. Theorem 7 yields that CGTD2 or CTDC algorithms should minimize E R 1 0 (θ +1)2 +1 2dt, whose minimizer is −1. We implement CTD(0) and CGTD2 (along with ML) and show the results in Figure 7. In our implementation, the initial learning rate for all the three algorithms is 0.01 and decays as (♯episode)−0.67. The results conﬁrm our theoretical analysis. Figure 7: Minimizing martingale loss function and CGTD methods converge to the respective minimizers, while CTD(0) diverges, for Example 5. Applying SGD to minimize the martingale loss function leads to θ∗ ML ≈−0.875301, CTD(0) does not converge since there is no solution to the moment condition, and CGTD method leads to θ∗ MSPBE = −1, which is equivalent to minimizing the mean-square projected Bellman’s error. We repeat the experiment for 100 times to calculate the standard deviations, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. 29", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 29, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p30::c0", "text": "Jia and Zhou 5. Extensions and Applications In this section, we extend the martingale characterization to the case with an exponential discount factor, and discuss the non-Markovian setting through an example of a fractional Brownian motion. Then we present two applications – a problem with option-like payoﬀ and a linear-quadratic problem in inﬁnite time horizon. 5.1 Extension to discounted case In many applications the payoﬀfunctions involve discounting. We now extend our analysis for PE to such a case. Note that, in this case, if we let T →∞, then we will have an inﬁnite time horizon problem. We modify the value function of (2) to J(t, x) =E \u0014Z T t e−ρ(s−t)r(s, Xs)ds + e−ρ(T−t)h(XT ) Xt = x \u0015 , (23) where the discount rate ρ > 0 is known and given. The PDE (3) is revised to \u001a LJ(t, x) + r \u0000t, x \u0001 = ρJ(t, x), (t, x) ∈[0, T) × Rd, J(T, x) = h(x), (24) and the FBSDE (4) becomes \u001a dXs = b(s, Xs)ds + σ(s, Xs)dWs, s ∈[t, T]; Xt = x, dYs = −e−ρ(s−t)r \u0000s, Xs \u0001 ds + ZsdWs, s ∈[t, T]; YT = e−ρ(T−t)h(XT ), (25) whereas the relationship (5) is now Ys = e−ρ(s−t)J(s, Xs), Zs = e−ρ(s−t) ∂J ∂x(s, Xs)⊤σ \u0000s, Xs \u0001 , s ∈[t, T]. (26) Finally Ms := e−ρ(s−t)J(s, Xs)+ Z s t e−ρ(s′−t)r \u0000s′, Xs′\u0001 ds′ ≡Ys+ Z s t e−ρ(s′−t)r \u0000s′, Xs′\u0001 ds′, s ∈[t, T] (27) is a martingale. Fixing the initial time t = 0, the above analysis suggests that the martingale loss function should be E Z T 0 |MT −Mθ t |2dt =E Z T 0 e−ρT h(XT ) −e−ρtJθ(t, Xt) + Z T t e−ρsr \u0000s, Xs \u0001 ds 2 dt ≈E \u0014 K−1 X i=0 \u0012 e−ρT h(XT ) −e−ρtiJθ(ti, Xti) + K−1 X j=i e−ρtjr(tj, Xtj)∆t \u00132 ∆t \u0015 . . On the other hand, dMθ t = e−ρtdJθ t −ρe−ρtJθ t dt + e−ρtrtdt = e−ρt(dJθ t + rtdt −ρJθ t dt); hence the martingale orthogonality condition (22) is modiﬁed to E Z T 0 ξt(dJθ t + rtdt −ρJθ t dt) = 0, (28) 30", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 30, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p31::c0", "text": "Policy Evaluation and TD Learning in Continuous Time for any test function ξ ∈L2 F([0, T]). Note here the discount factor has been absorbed by the test function and thus omitted. If we set T = ∞in (23) and assume the payoﬀdoes not depend on t, then the problem becomes J(x) = E \u0014Z ∞ 0 e−ρtr(Xt)dt|X0 = x \u0015 . This type of problems occur when the time horizon is suﬃciently long or indeﬁnite. Note that in this case the value function does not depend on time explicitly. As a result, there is no longer a terminal condition; instead, it is usually replaced by a growth condition such as E[e−ρtJ(Xt)] →0 as t →∞. As the martingale loss function requires full trajectories, it may not be directly applicable for the inﬁnite-horizon problem where we are obviously not able to observe a whole sample until “inﬁnity”. However, the martingale loss function can still be deﬁned by truncating at a suﬃciently long time T with an artiﬁcial terminal condition e−ρT h(XT ) = 0. Therefore, in the episodic setup with repeatedly presented ﬁnite training sets, we can still learn the value function by minimizing the martingale loss function. However, with a very long horizon, people are more interested in online learning with no reset by observing a single trajectory. As a result, TD learning is a better choice. All the previously discussed TD learning methods with suitable test functions can be applied based on the conditions (28). 5.2 Extension to non-Markovian setting A key assumption of the current paper is that the state process X is Markovian. Indeed, the Markov property determines that the value function J, deﬁned through (2), is a function of the current state x, instead of the whole past history of X. However, the martingale perspective may extend beyond the Markovian setting. While a general non-Markovian PE theory goes beyond the scope of this paper and warrants a full separate research, here we use an example to illustrate. Recall in Example 1, the state process X = W is a Brownian motion, which is Markov- nian. Now we consider instead a fractional Brownian motion W H with the Hurst index H. When H = 1 2, W H reduces to a Brownian motion; but when H ̸= 1 2, it is well known to be a non-Markov process. For basic theory and applications of fractional Brownian motions, see e.g. Mandelbrot and Van Ness (1968). For a non-Markov process, the value function is a functional of the current time t and the entire state trajectory up to t. For example, E[W H T |Ft] = W H t − Z t 0 ΨH(T, s|t)dW H s , where ΨH(T, s|t) = −sin \u0000π(H −1 2) \u0001 π s 1 2 −H(t −s) 1 2 −H Z T t zH−1 2 (z −t)H−1 2 z −s dz; see Sottinen and Viitasaari (2017). For Example 1 with Xt = Wt replaced by Xt = W H t , the only modiﬁcation we need is to introduce the value function J(t, Xt∧·) that is a functional of the past trajectory with 31", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 31, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p32::c0", "text": "Jia and Zhou the terminal condition J(1, X1∧·) = E[X1|X1∧·] = X1. In our experiment, we use a two- layer fully connected neural network plus an LSTM type of recurrent neural network to approximate such a path-dependent functional satisfying the terminal condition: Jθ(t, Xt∧·) = Xt + (1 −t) NNθ \u0010 LSTMθ(t, Xt∧·) \u0011 , and then minimize the martingale loss function or apply the CTD(0) algorithm to update the parameters. Here LSTMθ maps a sequence of time-series data (Xt0, · · · , Xtk, · · · , XtK) to a sequence of output (Yt0, · · · , Ytk, · · · , XtK) recursively where the time-tk output Ytk depends on the past trajectory (Xt0, · · · , Xtk). For details of this network structure, see Hochreiter and Schmidhuber (1997). We then compare the mean-square value errors between the learned functional and the ground truth solution with both algorithms. Figure 8 shows the trend of convergence of both the ML and CTD(0) methods, although the convergence is not as sharp as in the other Markov examples (most likely due to the non-Markovian setting and a fairly general neural network used). We also see that CTD(0) performs better than ML in this case. Figure 8: The mean-square value errors of the learned value functional with ML and CTD(0) algorithm for a fractional Brownian motion with Hurst index H = 0.75. The MSVEs are evaluated with 1000 independent trajectories and standard deviations are computed, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. 5.3 Option-like payoﬀ We apply the theory developed so far to evaluate J(t, x) = E[e−r(T−t)h(XT )|Xt = x], (29) where X = {Xt, 0 ≤t ≤T} is the state process. This type of evaluation occurs in option pricing in which X is the underlying stock price process and h is the payoﬀfunction (usually known and given) at the maturity T. 32", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 32, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p33::c0", "text": "Policy Evaluation and TD Learning in Continuous Time In our simulation, we generate X from a geometric Brownian motion dXt Xt = (r −q)dt + σdWt, and take h as a call option payoﬀ h(x) = (x −K)+. Moreover, we set T = 1, K = 1, r = 0.01, q = 0, σ = 0.3. The price process is generated from X0 = 1. The value function has a theoretical (ground truth) form given by the Black–Scholes formula J(t, x) = e−r(T−t)[e(r−q)(T−t)xΦ(d+) −KΦ(d−)], where d± = log(x/K) + (r −q ± 1 2σ2)(T −t) σ √ T −t , and Φ is the distribution function of the standard normal. For learning the value function in our numerical experiment, we parameterize it by Jθ(t, x) = (x −K)+ + (T −t) NNθ(t, x), where NNθ is a general bivariate neural network taking both time and space as inputs. This particular form is inspired by that of the payoﬀ function along with the fact that time to maturity, T −t, is critical in pricing an option. We use both ML and online CTD(0) in our experiment, with the martingale loss function being E Z T 0 e−rT (XT −K)+ −e−rtJθ(t, Xt) 2dt. In our implementation, we use a simple three-layer fully connected neural network with softplus activation function and 128 and 64 neurons, that is, NNθ(u) = θ5a \u0012 θ3a \u0000θ1u + θ2 \u0001 + θ4 \u0013 + θ6, a(x) = log(1 + ex), where θ1 ∈R128×2, θ2 ∈R128×1, θ3 ∈R64×128, θ4 ∈R64×1, θ5 ∈R64×1, θ6 ∈R. Since now we have hundreds of parameters and the functional forms are complex, instead of comparing the learned parameters, we assess the performance of learning by the following three errors: J(0, x0) −Jθ(0, x0), E Z T 0 J(t, Xt) −Jθ(t, Xt) 2dt, E Z T 0 ∂J ∂x(t, Xt) −∂Jθ ∂x (t, Xt) 2dt, where θ is the vector of optimized parameters obtained, and J is the ground truth value function. In these errors, the ﬁrst one is in terms of price diﬀerence at the initial time t = 0, and the second one in terms of the averaged total price diﬀerences over time. The last one concerns the accuracy in determining ∂J ∂x, the so-called “Delta” of the option which is the quantity of the underlying stock needed to hedge the option risk. The PDE (24) satisﬁed by J is nothing else than the well-known Black-Scholes PDE: ∂J ∂t + (r −q)x∂J ∂x + 1 2σ2x2 ∂2J ∂x2 −rJ = 0, J(T, x) = (x −K)+. 33", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 33, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p34::c0", "text": "Jia and Zhou Hence, as discussed earlier, PE can also be regarded as an alternative method to solve such a PDE. This in turn presents a benchmark in our experiment for comparison purpose, which is the deep learning method in Han et al. (2018) called the deep BSDE method for solving PDEs. Note that their method requires the perfect knowledge about the model parameters or, equivalently, not only samples of {Xt, 0 ≤t ≤T} but also samples of {Wt, 0 ≤t ≤T} that drives the state process. When implementing the deep BSDE method, we apply a neural network with the same structure to keep the computational cost at the similar level. We use the Adam algorithm for optimization with one trajectory for each episode so that the number of training sample trajectories is also kept the same.17 Figure 9 shows the comparison. For the ﬁrst two criteria, the errors by the two PE methods developed in this paper, (oﬄine) ML and online CTD(0), both converge to zero very quickly, while it takes some time for those with the BSDE method to be close to zero. For the last criterion, the errors by the PE methods remain close to zero and keeps almost ﬂat from the start, while the BSDE method oscillates dramatically at the start before converging to 0. Indeed, we have shown that minimizing the martingale loss function is equivalent to approximating the value function itself in the mean–square sense, without concerning at all the derivatives of the function. In contrast, the deep BSDE method strives also to learn the derivative term (the Zt term in FBSDE (25)) directly and hence requires more knowledge about the system. This example shows that PE methods can be used to learn the function itself eﬀectively but may not provide an accurate approximation to the derivative value. In particular, in terms of estimating the value function, ML achieves the smallest error and CTD(0) is only slightly behind due to its online setting; but deep BSDE can ultimately learns the derivative. As such, PE methods provide more ﬂexibility for users with tasks such as solving a PDE. It also suggests that for continuous-time RL one should avoid methods relying on the derivatives of the estimated value function. Finally, we point out that the purpose of this example is to compare our methods with the deep PDE/BSDE method. Because (29) holds in the risk-neutral world where data cannot be actually observed, our method cannot be used directly to evaluate an option price. It remains an interesting problem to price options based on physical probability and the real-world data. 5.4 Inﬁnite time horizon linear-quadratic problem Consider the following value function J(x) = E \u0014Z ∞ 0 e−ρtr(Xt)dt|X0 = x \u0015 , where X = {Xt, 0 ≤t < ∞} is the state process. In our simulation, we generate X from an Ornstein–Uhlenbeck (OU) process dXt = a(b −Xt)dt + σdWt, 17. The Adam algorithm is proposed in Kingma and Ba (2014) and is considered to be an improvement over the vanilla SGD algorithm. We follow Han et al. (2018) to apply the Adam algorithm. Implementation of neutral networks is through Tensorﬂow 2. All the computations are conducted on an Intel(R) Core(TM) i7-1065G7 CPU @ 1.30GHz 1.50 GHz Windows laptop. 34", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 34, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p35::c0", "text": "Policy Evaluation and TD Learning in Continuous Time 0 200 400 600 800 1000 episode 0 0.5 1 1.5 2 ML CTD(0) Deep BSDE Figure 9: Comparison of learned value functions by ML, online CTD(0) and deep BSDE methods. From left to right, we show the errors against the true solutions in terms of |J(0, x0) −Jθ(0, x0)|, E R T 0 J(t, Xt) −Jθ(t, Xt) 2dt, E R T 0 ∂J ∂x(t, Xt) −∂Jθ ∂x (t, Xt) 2dt re- spectively. These expectations are evaluated using 5000 independent trajectories. Standard deviations are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. which is well-known to converge to its unique stationary distribution when a > 0 and σ > 0. And we take r to be a quadratic function: r(x) = 1 2x2 + qx. This is a discounted linear-quadratic (LQ) control problem in inﬁnite time horizon. By the standard stochastic control theory via dynamic programming (Yong and Zhou, 1999, Chapter 6) we can compute the value function explicitly as J(x) = 1 2Ax2 + Bx + C, where A = 1 ρ + 2a, B = abA + q ρ + a , C = abB + 1 2σ2A ρ . We set a = 1, b = 1, σ = 0.5, ρ = 1.5, q = 1, X0 = 0, and simulate the trajectories up to T = 2 × 105. In our experiment, we parameterize the value function by Jθ(x) = 1 2θ0x2 + θ1x + θ2. We implement CTD(0), CLSTD(0), and CGTD2 algorithms and report the results in Figure 10. Since the parametric functions lie in a linear space, CLSTD(0) explicitly solves the moment conditions, and hence converges fastest. CGTD2 converges faster than CTD(0) because the former is a true gradient-based algorithm. 6. Some Algorithmic Aspects In this section we discuss two problems from the algorithmic perspective: the choice of test functions and the way to perform function approximation. 6.1 Choice of test functions One of the most important implications of the martingale viewpoint is the introduction of the test functions. In Subsection 4.2, we show that the choice of test functions determines in what sense the true value function is approximated and, hence, a same algorithm with diﬀerent test functions may converge diﬀerently, as illustrated in Examples 3 – 5. While this characterization remains abstract in theory and provides little guidance on how to actually 35", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 35, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p36::c0", "text": "Jia and Zhou Figure 10: Comparison of learned parameters with diﬀerent online TD algo- rithms. All the algorithms converge to the correct value function. Among them, CLSTD(0) converges the fastest and CTD(0) the slowest. We repeat the experiment for 100 times to calculate the standard deviations, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. select test functions, here we present a simple example to demonstrate how test functions may aﬀect the convergence from an algorithmic perspective. Consider the same LQ problem in Subsection 5.4, with a = 0, σ = 1, and q = 0. In this case, Xt = X0 + Wt now becomes a Brownian motion, which has no stationary distributions.18 As before, we parameterize the value function by Jθ = 1 2ρx2 +θ. This parametric family contains the true value function with θtrue = 1 2ρ2 . The conventional choice of the test function in TD(0) is ξt = ∂Jθ ∂θ (Xt) = 1, leading to the following updating rule on θ: θ ←θ + α[Jθ(Xt+∆t) −Jθ(Xt) + r(Xt)∆t −ρJθ(Xt)∆t]. Denote by θt the learned parameter value at time t. Then, at the continuous-time limit, θt satisﬁes an SDE (ignoring the learning rate constant α): dθt = dJθt(Xt) + r(Xt)dt −ρJθt(Xt)dt. By Itˆo’s lemma, dJθt(Xt) = Xt ρ dWt + 1 2ρdt; hence dθt = \u0012 1 2ρ −ρθt \u0013 dt + Xt ρ dWt. Suppose the initial guess of θ is θ0. Then E[θt] = 1 2ρ2 (2θ0ρ2e−ρt + 1 −e−ρt) → 1 2ρ2 = θtrue as t →∞. That is, asymptotically, the conventional choice of the test function indeed leads to an unbi- ased estimate. Let us now calculate Var(θt), the variance of θt. Set zt = θt−1 2ρ2 (2θ0ρ2e−ρt+ 1 −e−ρt), which satisﬁes the SDE: dzt = −ρztdt + Xt ρ dWt, z0 = 0. 18. Such non-stationary processes are common in practice. Due to the presence of the discount factor, the corresponding LQ problem is still well-posed. 36", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 36, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p37::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Itˆo’s lemma provides dz2 t = 2zt[−ρztdt + Xt ρ dWt] + X2 t ρ2 dt, zt = 0. Hence Var(θt) = E[z2 t ] = 1 4ρ4 \u0000e−2ρt −1 + ρt −2ρX2 0e−2ρt + 2ρX2 0 \u0001 →∞, as t →∞. So, the conventionally chosen test function does not produce a consistent estimator of θ due to the blow-up in variance, which in turn is caused by the non-stationarity of the underlying state process – a Brownian motion in this example – whose variance grows linearly in time. However, this issue can be resolved by selecting a tailored test function. Recall the CTD(0) algorithm with a general test function ξt updates θ by θ ←θ + αξt h Jθ(Xt+∆t) −Jθ(Xt) + r(Xt)∆t −ρJθ(Xt)∆t i . Applying the same SDE approximation, we derive dθt = ξt \u0012 1 2ρ −ρθt \u0013 dt + ξt Xt ρ dWt. Intuitively, to reduce the variance of θt, we need to choose a test function that can cancel the growing trend in variance. There are many choices to achieve this goal, but a simple one is to take ξt = 1 |Xt|+1 so that the volatility term above is bounded. We call this a “tailored choice” of test function for this particular LQ problem. The cost of this variance reduction method is the introduction of some bias in the mean as some correlation enters into the drift term. Figure 11 visualizes the result of a simulation study that conﬁrms our analysis. With the conventional test function ξt = 1, even though the average of the learned parameter values across diﬀerent experiments tends to be close to the true value, these values become more volatile as time grows larger. On the other hand, with our tailored test function ξt = 1 |Xt|+1, the variance is reduced dramatically, though the average is slightly oﬀfrom the true value. Overall, the study we provide in this subsection shows the promise of our martingale framework in designing more eﬃcient algorithms with suitable choice of test functions, which may at the same time extend the existing literature on RL algorithms for MDPs. 6.2 Function approximation: global vs sectional For a ﬁnite-horizon problem, the value function J(t, x) is a bivariate function of time t and state x. Hitherto we have used a global approximator Jθ in the sense that we use the same parameter θ when approximating J(t, ·) by Jθ(t, ·). Another way of function approximation is sectional, namely, we approximate J(t, ·) by Jθt(t, ·) where the parameter θt may be time-varying. More precisely, let the time discretization be ﬁxed with the grid points 0 = t0 < t1 < · · · < tK = T, and let Jθ0 0 (x), · · · , JθK K (x) approximate the value function at these points, namely, Jθi i (x) ≈J(ti, x), i = 0, 1, · · · , K. To compare these two methods of function approximation, the ﬁrst thing to note is that the number of parameters to learn grows linearly in the number of time steps with the 37", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 37, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p38::c0", "text": "Jia and Zhou Figure 11: Comparison of the learned parameters under the conventional test function and the tailored test function. Conventional CTD(0) refers to the algorithm using test function ξt = ∂Jθ ∂θ (Xt) = 1, and Tailored CTD(0) refers to the one using test function ξt = 1 |Xt|+1. In the simulation, the problem primitives are a = 0, σ = 1, q = 0, ρ = 1.5, the initial state is X0 = 0 and initial guess of the parameter θ is θ0 = 1. The true parameter is θtrue = 2 9 ≈0.22. The learning rate is α = 0.1. We repeat the experiment for 100 times to calculate the standard deviations, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. sectional approximation, while remains the same with the global one. Hence, the latter has an edge in terms of computational cost when a ﬁner time grid is used. Second, and indeed more importantly, the sectional approximation may become problematic for online learning. To see this, suppose we are now at (ti, Xti) in the online setting. Applying the idea of the conventional TD(0) algorithm, one can update the parameter θi−1 by θi−1 ←θi−1 + α∂Jθi−1 i−1 ∂θi−1 (Xti−1) h Jθi i (Xti) −Jθi−1 i−1 (Xti−1) i . The question is how to update θk for k = i, i + 1, · · · , K without knowing the future states Xtk? It seems the best we could do is to update θk according to θk ←θk + α∂Jθk k ∂θk (Xti−1) h Jθi i (Xti) −Jθi−1 i−1 (Xti−1) i . (30) This form is less intuitive because we use the current and past states to update parameters for future value functions. In contrast, the global parameterization views the value function as a whole; hence a temporal advancement naturally leads to an update of the whole bivariate function, including a prediction into the future as well as an updated evaluation of the past. Finally, we run a simulation for Example 1 to compare the learning results of the two function approximation approaches, both in oﬄine learning (using martingale loss function) and online learning (using CTD(0) for the global approximation and (30) for the sectional one). Recall that the ground truth is J(t, x) = x, and we have used the global approximation 38", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 38, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p39::c0", "text": "Policy Evaluation and TD Learning in Continuous Time with Jθ(t, x) = [θ(1−t)+1]x. For the sectional approximation, we consider a simple form of Jθi i (x) = θix, with unknown parameters θ0, · · · , θK−1 while it is known that θK = 1 based on the terminal condition. We evaluate the performance of the diﬀerent approximation approaches by MSVE as deﬁned in (21). For the global approximation, this error is E Z T 0 |J(t, Xt) −Jθ(t, Xt)|2dt = E Z T 0 θ2(1 −t)2W 2 t dt = 1 12θ2. For the sectional approximation, this error is calculated by E K−1 X i=0 |J(ti, Xti) −Jθi i (Xti)|2∆t = K−1 X i=0 (θi −1)2ti∆t. The results are presented in Figure 12. For this simple example, the number of unknown parameters in the sectional approach is small so the diﬀerence in computational cost is insigniﬁcant. Otherwise, we observe that the global approximation performs similarly as the sectional one in the oﬄine setting (the ML method), but signiﬁcantly better in the online setting (the CTD(0) method). Figure 12: Comparison of the mean-square value errors of the learned value func- tion using globel and sectional approximation methods with ML and CTD(0) algorithms. The initial guess is θ = −1 for global approximation and θi = ti, 0 ≤i ≤K−1, for sectional approximation so that the two methods are initialized to be the same function. The learning rate is α = 0.01. We repeat the experiment for 100 times to calculate the standard deviations, which are represented as the shaded areas. The width of each shaded area is twice the corresponding standard deviation. 7. Conclusions In this paper, we provide a uniﬁed theoretical framework for studying PE in RL with con- tinuous time and space. The theory is premised upon the observation that PE is equivalent 39", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 39, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p40::c0", "text": "Jia and Zhou to enforcing the martingality of a stochastic process. Many existing and popular PE algo- rithms, which somewhat scatter around in the MDP literature, can ﬁnd a common ground through this “martingale lens”. These algorithms can be modiﬁed for solving PE in the continuous setting for actual implementation. The martingale perspective is potentially useful for studying many important problems related to PE that have been well developed in the discrete setting but remain open in the continuous setting, including oﬀ-policy evaluation, state-action value estimation, and convergence analysis. Furthermore, it may inspire new questions that have not been posed by traditional RL research. For example, how to “optimally” choose test functions and how their choices aﬀect the convergence rate in both discrete and continuous settings. Finally, PE is formulated for Itˆo processes in this paper, mainly because such a process has convenient and well-studied properties and can reasonably model many real-life dy- namics. The martingale view, however, is generalizable beyond Itˆo processes such as jump diﬀusions, non-Markov processes and semi-martingales. Acknowledgments We are grateful for comments from the seminar participants at University of Southern California, Boston University, Imperial College, University of Connecticut, the International Seminar on SDEs and Related Topics, the Joint Seminar by AIFT and Columbia University and the World Online Seminars on Machine Learning in Finance, and from the participants at the 6th Berlin Workshop for Young Researchers in Mathematical Finance and the IMSI Workshop on Advances in Optimal Decision Making under Uncertainty. We thank Jerome Detemple, Steven Kou, Huyˆen Pham, Moris Strub, Wenpin Tang, Nizar Touzi, Renyuan Xu and Jianfeng Zhang for helpful discussions and comments on the paper. We are also indebted to the Action Editor and three anonymous referees for constructive comments which have led to an improved version of the paper. Zhou gratefully acknowledges ﬁnancial support through a start-up grant and the Nie Center for Intelligent Asset Management at Columbia University. 40", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 40, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p41::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Appendix A: A Summary of Popular PE Methods The following Table 1 summarizes popular PE methods and algorithms, and the interpre- tations we have discovered in this paper in terms of objectives (loss/error functions to be minimized or equations to be solved) and limiting points of convergent algorithms. Method Representative algorithms Online Objective Converging point Monte Carloa gradient Monte Carlo No minimize martingale loss function minimizers of mean-square value function error Residual gradientb na¨ıve residual gradient Yes minimize mean-square TD error minimizers of quadratic variation Semi-gradient TD learningc TD(λ) LSTD(λ) Yes solve moment conditions zeros to moment conditions Gradient TD learningd GTD(0) GTD2 TDC Yes minimize quadratic form of moment conditions minimizers of mean-square projected Bellman error a. Sutton and Barto (2018). b. Baird (1995). c. Sutton (1988); Bradtke and Barto (1996). This terminology is taken from Sutton and Barto (2018, Chapter 9). d. Sutton et al. (2008, 2009). Table 1: Summary of popular PE methods in RL literature. The table summarizes diﬀerent PE methods. The ﬁrst three columns indicate the names of the methods, those of the representative algorithms, and whether applicable online and/or oﬄine. The last two columns reveal the objectives and the converging points of the corresponding algorithms. Appendix B: Stochastic Control Formulation of RL Let d, m, n be given positive integers, T > 0, and b : [0, T] × Rd × Rn 7→Rd and σ : [0, T] × Rd × Rn 7→Rd×m be given functions. A stochastic control problem is to control the state (or feature) dynamic governed by an SDE: dXs = b \u0000s, Xs, u(s, Xs) \u0001 dt + σ \u0000s, Xs, u(s, Xs) \u0001 dWs, s ∈[0, T], (31) where u : (t, x) ∈[0, T] × Rd 7→u(t, x) ∈U is a given (measurable) feedback control policy, with U ⊆Rn being the action space representing the constraints on an agent’s decisions (controls or actions). 41", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 41, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p42::c0", "text": "Jia and Zhou Given a policy u and an initial time–state pair (t, x) ∈[0, T]×Rd, let {Xt,x,u s , t ≤s ≤T} be the solution to (31) with Xt = x. The value function under the policy u is J(t, x; u) =E \u0014Z T t r(s, Xt,x,u s , u(s, Xt,x,u s ))ds + h(Xt,x,u T ) Xt,x,u t = x \u0015 , (32) where r : [0, T] × Rd × Rn 7→R and h : Rd 7→R are given reward functions. A policy u is called admissible if (31) has a unique weak solution and (32) is ﬁnite for any (t, x) ∈[0, T] × Rd. A typical RL problem is to maximize (minimize) J(t, x; u) over all admissible policies u. In the classical (model-based) stochastic control literature, the functional forms of b, σ, r and h are known, and there are well-developed theories to solve the problem; see, e.g., Yong and Zhou (1999); Fleming and Soner (2006). However, in the RL context, these functional forms are typically unknown, although in some applications that of h may be known because it may be interpreted as a given target the agent speciﬁes (e.g. in option pricing h is the payoﬀfunction of an option, which is typically given and known; see Subsection 5.3). The PE task as a part of the general RL problem is, for a given policy u, to devise a numerical procedure to ﬁnd J(t, x; u) as a function of (t, x) using multiple sample trajecto- ries of the process {s, Xt,x,u s , r \u0000s, Xt,x,u s , u(t, Xt,x,u s ) \u0001 }t≤s≤T , without the knowledge of the model parameters (the functional forms of b, σ, r, h). If we suppress u, which is ﬁxed in PE, then we recover the formulation (1)–(2). Note that the formulation also covers the “exploratory” setting of Wang et al. (2020) in which the admissible control policies are probability-distribution-valued, because the value function therein is of the same form as (32) under a ﬁxed distributional control policy. Appendix C: Martingale in Discrete-time Markov Reward Processes We show that there is also a martingale property in the classical discrete-time RL MDP formulation. To be consistent with the main setting of this paper, we consider only the ﬁnite horizon episodic tasks; the inﬁnite horizon continuing tasks can be studied similarly. Let X = {Xt, t = 0, 1, · · · , T} be a discrete-time Markov process adapted to {Ft}t=0,1,··· ,T in a ﬁltered probability space (Ω, P, F, {Ft}t=0,1,··· ,T ). One is interested in ﬁnding the value function v deﬁned by v(t, x) = E \"T−1 X s=t r(s, Xs) + h(XT ) Xt = x # , where r(t, x) is the expected reward at time t conditioned on being at state x, and h is the ﬁnal reward. When the state space is ﬁnite and discrete, X is referred to as a Markov reward process (MRP) or alternatively as an MDP with a ﬁxed policy. When the state space is inﬁnite or typically continuous, it is usually called a semi-MRP or a semi-MDP. 42", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 42, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p43::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Set Mt = v(t, Xt) + Pt−1 s=0 r(s, Xs) with MT = h(XT ) + PT−1 s=0 r(s, Xs). Then, for any t = 0, 1, · · · , T −1, by Markov property, we obtain E[Mt+1|Ft] = t X s=0 r(s, Xs) + E[v(t + 1, Xt+1)|Ft] = t−1 X s=0 r(s, Xs) + E[r(t, Xt) + v(t + 1, Xt+1)|Ft] = t−1 X s=0 r(s, Xs) + v(t, Xt) = Mt, where the last equality is due to E[r(t, Xt) + v(t + 1, Xt+1)|Ft] = E[r(t, Xt) + v(t + 1, Xt+1)|Xt] = v(t, Xt), which is the well-known Bellman equation for a discrete-time MRP. So, M being a martingale is equivalent to the value function satisfying the Bellman equation, which in turn can be used to characterize PE. From this martingale perspective, we can develop parallel approaches such as the martingale loss function and the martingale orthogonality condition that will recover various conventional PE algorithms for discete- time MRPs. Appendix D: Proofs of Statements Proof of Proposition 1 Proof To show M is a martingale, observe that based on (2), we have Ms = E \u0014Z T s r \u0000s′, Xs′\u0001 ds′ + h(XT )|Xs \u0015 + Z s t r \u0000s′, Xs′\u0001 ds′ = E[MT |Fs], where we have used the Markov property of the process {Xs, t ≤s ≤T}. This establishes that M is a martingale. Conversely, if ˜ M is a martingale, then ˜ Ms = E[ ˜ MT |Fs], which is equivalent to ˜J(s, Xs) = E hR T s r \u0000s′, Xs′\u0001 ds′ + ˜J(T, XT )|Fs i = E hR T s r \u0000s′, Xs′\u0001 ds′ + h(XT )|Fs i = J(s, Xs), s ∈[t, T]. Letting s = t, we conclude ˜J(t, x) = J(t, x). Proof of Theorem 2 We ﬁrst present two lemmas that will be useful for the proof of Theorem 2 and also other theorems later. 43", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 43, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p44::c0", "text": "Jia and Zhou Lemma 8 Let fh(x) = f(x) + rh(x), where f is a continuous function and rh converges to 0 uniformly on any compact set as h →0. (a) Suppose x∗ h ∈arg minx fh(x) ̸= ∅and limh→0 x∗ h = x∗. Then x∗∈arg minx f(x). Moreover, if there exists α > 0 such that |rh(x)| ≤Chα for some constant C, then |f(x∗ h) −f(x∗)| ≤2Chα. (b) Suppose fh(x∗ h) = 0 and limh→0 x∗ h = x∗. Then f(x∗) = 0. Moreover, if there exists α > 0 such that |rh(x)| ≤Chα for some constant C, then |f(x∗ h)| ≤Chα. Proof (a) For any y, we have f(x∗ h) + rh(x∗ h) = fh(x∗ h) ≤fh(y). The sequence {x∗ h} forms a compact set; hence rh(x∗ h) →0 as h →0. Letting h →0, since x∗ h →x∗and f is continuous, we obtain f(x∗) ≤f(y). Since y is arbitrary, x∗∈arg minx f(x). Moreover, we have 0 ≤f(x∗ h) −f(x∗) = fh(x∗ h) −rh(x∗ h) −fh(x∗) + rh(x∗) ≤−rh(x∗ h) + rh(x∗) ≤2Chα. (b) Since f(x∗ h) + rh(x∗ h) = fh(x∗ h) = 0, |f(x∗ h)| = |rh(x∗ h)|. The sequence {x∗ h} forms a compact set; hence rh(x∗ h) →0 as h →0. Letting h →0, since x∗ h →x∗and f is continuous, we obtain |f(x∗)| = 0. The second statement is straightforward since |f(x∗ h)| = |rh(x∗ h)| ≤Chα. Lemma 9 Under Assumptions 1 and 4, we have E \u0014 Z t+h t |r(s, Xs) −r(t, Xt)|2 ds \u0015 ≤Ch2µ1+µ2+1. Proof By Assumption 4, for s ∈[t, t + h], we have |r(s, Xs) −r(t, Xt)|2 ≤Ch2µ1|Xs −Xt|2µ2(|Xs|2µ3 + |Xt|2µ3). When µ2 > 0, we take p > 1 suﬃciently large such that 2µ2p ≥1, and q > 1 such that 1 p + 1 q = 1. Under Assumption 1, we have the usual moment estimate of the solution to an SDE, e.g., Yong and Zhou (1999, Chapter 1, Theorem 6.1). Together with H¨older’s inequality, 44", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 44, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p45::c0", "text": "Policy Evaluation and TD Learning in Continuous Time we have E \u0014 Z t+h t |Xs −Xt|2µ2(|Xs|2µ3 + |Xt|2µ3)ds \u0015 ≤ \u0012 E \u0014 Z t+h t |Xs −Xt|2µ2pds \u0015\u00131/p \u0012 E \u0014 Z t+h t (|Xs|2µ3 + |Xt|2µ3)qds \u0015\u00131/q ≤C \u0012 E \u0014 Z t+h t |Xs −Xt|2µ2pds \u0015\u00131/p \u0012 h max t≤s≤t+h E \u0002 |Xs|2µ3q\u0003\u00131/q ≤C \u0012Z t+h t (s −t)µ2pds \u00131/p h1/q ≤Ch(µ2p+1)/ph1/q = Chµ2+1. When µ2 = 0, the above inequality also holds true as E \u0014 R t+h t |Xs −Xt|2µ2(|Xs|2µ3 + |Xt|2µ3)ds \u0015 ≤Ch. Now we are already to prove Theorem 2. Proof By Itˆo’s lemma, we have K−1 X i=0 \u0012Jθ(tt+1, Xti+1) −Jθ(ti, Xti) ti+1 −ti + rti \u00132 ∆t = 1 ∆t K−1 X i=0 \u0012 Jθ(ti+1, Xti+1) −Jθ(ti, Xti) + Z ti+1 ti rtids \u00132 = 1 ∆t K−1 X i=0 \u0012 Z ti+1 ti [LJθ(s, Xs) + rti]ds + Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs)dWs \u00132 = 1 ∆t K−1 X i=0 (\u0012 Z ti+1 ti [LJθ(s, Xs) + rti]ds \u00132 + \u0012 Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs)dWs \u00132 + 2 \u0012 Z ti+1 ti [LJθ(s, Xs) + rti]ds \u0013\u0012 Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs)dWs \u0013) . Itˆo’s isometry implies E \u0014\u0012 Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs)dWs \u00132\u0015 = E Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs) 2 ds. Thus, MSTDE∆t(θ) = 1 ∆tE Z T 0 \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs) 2ds + 1 ∆t K−1 X i=0 E \u0014\u0012 Z ti+1 ti [LJθ(s, Xs) + rti]ds \u00132\u0015 + 2 ∆t K−1 X i=0 E \u0014\u0012 Z ti+1 ti [LJθ(s, Xs) + rti]ds \u0013\u0012 Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs)dWs \u0013\u0015 . 45", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 45, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p46::c0", "text": "Jia and Zhou We write MSTDE∆t(θ)∆t = QV(θ) + R(θ), where QV(θ) := E Z T 0 \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs) 2ds and R(θ) := K−1 X i=0 E \u0014\u0012 Z ti+1 ti [LJθ(s, Xs) + rti]ds \u00132 + 2 \u0012 Z ti+1 ti [LJθ(s, Xs) + rti]ds \u0013\u0012 Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs)dWs \u0013\u0015 . We apply Cauchy-Schwarz inequality and obtain |R(θ)| ≤ K−1 X i=0 E Z ti+1 ti [LJθ(s, Xs) + rti]2ds(∆t)2 + 2 K−1 X i=0 ( E \u0014\u0012 Z ti+1 ti [LJθ(s, Xs) + rti]ds \u00132\u0015 E \u0014\u0012 Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs)dWs \u00132\u0015)1/2 ≤ K−1 X i=0 E Z ti+1 ti [LJθ(s, Xs) + rti]2ds(∆t)2 + 2 K−1 X i=0 ( \u0014 E Z ti+1 ti [LJθ(s, Xs) + rti]2ds(∆t)2 \u0015 \" E Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs) 2ds # )1/2 =(∆t)2E Z T 0 [LJθ(s, Xs) + ¯rs]2ds + 2∆t K−1 X i=0 ( \u0014 E Z ti+1 ti [LJθ(s, Xs) + ¯rs]2ds \u0015 \" E Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs) 2ds # )1/2 ≤(∆t)2E Z T 0 [LJθ(s, Xs) + ¯rs]2ds + 2∆t ( K−1 X i=0 E Z ti+1 ti [LJθ(s, Xs) + ¯rs]2ds )1/2( K−1 X i=0 E Z ti+1 ti \u0012∂Jθ ∂x \u0013⊤ σ(s, Xs) 2ds )1/2 =(∆t)2E Z T 0 [LJθ(s, Xs) + ¯rs]2ds + 2∆t ( E Z T 0 [LJθ(s, Xs) + ¯rs]2ds )1/2p QV(θ), where ¯rs := rti for the unique i such that ti ≤s < ti+1. It follows from the triangle inequality that ( E Z T 0 [LJθ(s, Xs) + ¯rs]2ds )1/2 = ||LJθ(·, X·) + ¯r·||L2 ≤||LJθ(·, X·) + r·||L2 + ||r −¯r||L2. 46", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 46, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p47::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Hence, |R(θ)| ≤(∆t)2\u0000||LJθ(·, X·) + r·||L2 + ||r −¯r||L2 \u00012 + 2∆t \u0000||LJθ(·, X·) + r·||L2 + ||r −¯r||L2 \u0001p QV (θ) ≤4(∆t)2\u00002 MSBE(θ) + ||r −¯r||2 L2 \u0001 + 2∆t p QV(θ) \u0000p 2 MSBE(θ) + ||r −¯r||L2 \u0001 , where MSBE(θ) = ||LJθ(·, X·) + r·||2 L2 is the mean-square Bellman error. Because ¯r is a simple process approximating r, we have ||r −¯r||L2 →0 as ∆t →0, which is independent of θ. For an arbitrary compact set Γ, Assumption 3 yields that MSBE(θ) and QV(θ) are both continuous functions of θ; hence supθ∈Γ MSBE(θ) and supθ∈Γ QV(θ) are both ﬁnite. Consequently, sup θ∈Γ |R(θ)| ≤4(∆t)2\u00002 sup θ∈Γ MSBE(θ) + ||r −¯r||2 L2 \u0001 + 2∆t r sup θ∈Γ QV(θ) \u0000r 2 sup θ∈Γ MSBE(θ) + ||r −¯r||L2 \u0001 →0 as ∆t →0. The desired result now follows from Lemma 8. Moreover, under Assumption 4, it follows from Lemma 9 that ||r −¯r||2 L2 = K−1 X i=0 E Z ti+1 ti (r(s, Xs) −r(ti, Xti))2 ds ≤K(∆t)2µ1+µ2+1 = (∆t)2µ1+µ2. Therefore, our analysis above yields sup θ∈Γ |R(θ)| ≤C1∆t + C2(∆t)2 + C3(∆t)µ1+µ2/2+1 + C4(∆t)2µ1+µ2+2, where the leading term in the right hand side is O(∆t). The desired result again follows from Lemma 8. Proof of Theorem 3 Proof Since Mt = J(t, Xt) + R t 0 r(s, Xs)ds is a martingale, we have ML(θ) =E Z T 0 |MT −Mθ t |2dt =E Z T 0 (MT −Mt + Mt −Mθ t )2dt =E Z T 0 [(MT −Mt)2 + (Mt −Mθ t )2 + 2(MT −Mt)(Mt −Mθ t )]dt =E Z T 0 (MT −Mt)2dt + E Z T 0 (Mt −Mθ t )2dt + 2 Z T 0 E \u0010 (Mt −Mθ t )E[(MT −Mt)|Ft] \u0011 dt =E Z T 0 |J(t, Xt) −Jθ(t, Xt)|2dt + E Z T 0 |MT −Mt|2dt. 47", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 47, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p48::c0", "text": "Jia and Zhou The second term does not rely on θ. This proves our ﬁrst statement. Next, let us estimate the diﬀerence between the continuous-time and the discretized martingale loss functions. Denote m(t, θ) = E[(MT −Mθ t )2] = E[ \u0000h(XT ) −Jθ(t, Xt) + Z T t rsds \u00012], and ∆˜ Mθ ti = h(XT ) −Jθ(ti, Xti) + K−1 X j=i r(tj, Xtj)∆t = h(XT ) −Jθ(t, Xt) + Z T t ¯rsds, where ¯rs := rti for the unique i such that ti ≤s < ti+1. Then ML(θ) −ML∆t(θ) = Z T 0 m(t, θ)dt − K−1 X i=0 m(ti, θ)∆t + K−1 X i=0 m(ti, θ)∆t −ML∆t(θ) = K−1 X i=0 Z ti+1 ti [m(t, θ) −m(ti, θ)]dt + ∆t K−1 X i=0 E[(MT −Mθ ti)2 −(∆˜ Mθ ti)2]. The ﬁrst term is bounded by K−1 X i=0 Z ti+1 ti [m(t, θ) −m(ti, θ)]dt ≤ K−1 X i=0 sup t∈[0,T] |∂m ∂t (t, θ)| Z ti+1 ti (t−ti)dt = T 2 sup t∈[0,T] |∂m ∂t (t, θ)|∆t. To estimate the second term, recall that MT −Mθ ti −∆˜ Mθ ti = Z T ti (rs −¯rs)ds. Hence E[(MT −Mθ ti)2 −(∆˜ Mθ ti)2] = E[2 Z T ti (rs −¯rs)ds(MT −Mθ ti) − \u0000 Z T ti (rs −¯rs)ds \u00012] ≤2m(ti, θ) 1 2 (E[( Z T ti (rs −¯rs)ds)2]) 1 2 + E[( Z T ti (rs −¯rs)ds)2] ≤2T sup t∈[0,T] |m(t, θ)| 1 2 ||r −¯r||L2 + T 2||r −¯r||2 L2. Therefore, we have proved | ML(θ) −ML∆t(θ)| ≤T 2 sup t∈[0,T] |∂m ∂t (t, θ)|∆t + 2T 2 sup t∈[0,T] |m(t, θ)| 1 2 ||r −¯r||L2 + T 3||r −¯r||2 L2. For an arbitrary compact set Γ, under Assumption 3, supt∈[0,T],θ∈Γ |∂m ∂t (t, θ)|+supt∈[0,T],θ∈Γ |m(t, θ)| < ∞, and ||r −¯r||L2 →0. Hence as ∆t →0, sup θ∈Γ | ML(θ) −ML∆t(θ)| →0. 48", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 48, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p49::c0", "text": "Policy Evaluation and TD Learning in Continuous Time By Lemma 8, we obtain the desired conclusion. Moreover, under Assumption 4, it follows from Lemma 9 and the proof of Theorem 2 that sup θ∈Γ | ML(θ) −ML∆t(θ)| ≤C1∆t + C2(∆t)µ1+µ2/2 + C3(∆t)2µ1+µ2, where the leading term in the right hand side is O \u0010 (∆t)min{1,µ1+ µ2 2 }\u0011 . The desired result again follows from Lemma 8. Proof of Proposition 4 Proof The “only if” part is evident. To prove the “if” part, assume that dMθ t = Atdt + BtdWt. In particular, in our case, At = LJθ(t, Xt) + rt and Bt = (∂Jθ ∂x )⊤σ(t, Xt). A, B ∈ L2 F([0, T]) follows from Assumption 3. For any 0 ≤s < s′ ≤T, take ξt = sgn(At) if t ∈[s, s′] and ξt = 0 otherwise. Then 0 = E Z s′ s ξtdMθ t = E Z s′ s (|At|dt + ξtBtdWt) = E Z s′ s |At|dt, where the expectation of the second term vanishes because |ξB| ≤|B| ∈L2 F([0, T]) and hence E R · 0 ξtBtdWt is a martingale. This yields At = 0 almost surely, and thus Mθ is a martingale. 49", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 49, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p50::c0", "text": "Jia and Zhou Proof of Theorem 5 Proof Based on Lemma 8, it suﬃces to examine the diﬀerence E Z T 0 ξtdMθ t −E K−1 X i=0 ξti(Mθ ti+1 −Mθ ti) = E K−1 X i=0 Z ti+1 ti (ξt −ξti)dMθ t + E K−1 X i=0 ξti Z ti+1 ti (rs −rti)ds ≤E K−1 X i=0 Z ti+1 ti |ξt −ξti| · |LJθ(t, Xt) + rt|dt + E \u0014 K−1 X i=0 ξ2 ti !1/2 K−1 X i=0 \u0012Z ti+1 ti (rs −rti)ds \u00132!1/2 \u0015 ≤ K−1 X i=0 \u0000E Z ti+1 ti |ξt −ξs|2dt \u00011/2 \u0012 E Z ti+1 ti \u0000LJθ(t, Xt) + rt \u00012dt \u00131/2 E \u0014 K−1 X i=0 ξ2 ti !1/2 K−1 X i=0 \u0012Z ti+1 ti (rs −rti)2ds \u0013 ∆t !1/2 \u0015 ≤ K−1 X i=0 \u0012 E Z ti+1 ti \u0000LJθ(t, Xt) + rt \u00012dt \u00131/2\u0012 Z ti+1 ti C(t −ti)αdt \u00131/2 + (∆t)1/2||r −¯r||L2 K−1 X i=0 E[ξ2 ti] !1/2 ≤ K−1 X i=0 \u0012 E Z ti+1 ti \u0000LJθ(t, Xt) + rt \u00012dt \u00131/2r C 1 + α(∆t) 1+α 2 + ||r −¯r||L2||¯ξ||L2 ≤ \u0012 K−1 X i=0 E Z ti+1 ti \u0000LJθ(t, Xt) + rt \u00012dt \u00131/2 K1/2 r C 1 + α(∆t) 1+α 2 + ||¯ξ||L2(∆t)µ1+µ2/2 ≤||LJθ(·, X·) + r·||L2 r CT 1 + α(∆t) α 2 + ||¯ξ||L2(∆t)µ1+µ2/2. Hence, for an arbitrary compact set Γ, under Assumption 3, we have sup θ∈Γ E Z T 0 ξtdMθ t −E K−1 X i=0 ξti(Mθ ti+1 −Mθ ti) ≤sup θ∈Γ ||LJθ(·, X·) + r·||L2 r CT 1 + α(∆t) α 2 + sup θ∈Γ ||¯ξ||L2(∆t)µ1+µ2/2 →0, as ∆t →0. Since the leading term above is O \u0000(∆t)min{α/2, µ1+µ2/2}\u0001 , we obtain the convergence rate in view of Lemma 8. 50", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 50, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p51::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Proof of Theorem 6 We ﬁrst prove an error estimate of the following form: (b + ∆b)⊤(D + ∆D)(b + ∆b) −b⊤Db = D ◦[(b + ∆b)(b + ∆b)⊤−bb⊤] + ∆D ◦(b + ∆b)(b + ∆b)⊤ ≤|D||(b + ∆b)(b + ∆b)⊤−bb⊤| + |∆D||(b + ∆b)(b + ∆b)⊤| =|D||∆b∆b⊤+ b∆b⊤+ ∆bb⊤| + |∆D||b + ∆b|2 =|D||∆b|2 + 2|D||b||∆b| + 2|∆D||b|2 + 2|∆D||∆b|2. Based on the proof of Theorem 5, we have that for an arbitrary compact set Γ, sup θ∈Γ E Z T 0 ξtdMθ t −E K−1 X i=0 ξti(Mθ ti+1 −Mθ ti) ≤sup θ∈Γ ||LJθ(·, X·) + r·||L2 r CT 1 + α(∆t) α 2 + sup θ∈Γ ||¯ξ||L2(∆t)µ1+µ2/2 →0. Given that |A∆t −A| ≤˜C(θ)|∆t|β, we get sup θ∈Γ | GMM∆t(θ) −GMM(θ)| →0, as ∆t →0. By Lemma 8, we obtain the desired results. Moreover, based on the error estimate of the quadratic form, we obtain sup θ∈Γ | GMM∆t(θ) −GMM(θ)| ≤C h O \u0010 (∆t)α/2\u0011 + O \u0010 (∆t)µ1+µ2/2\u0011 + O \u0010 (∆t)β\u0011 + o \u0010 (∆t)α/2 + (∆t)µ1+µ2/2 + (∆t)β\u0011i , where the leading term is O \u0000(∆t)min{α/2, µ1+µ2/2, β}\u0001 . In particular, when A = h E R T 0 ξθ t (ξθ t )⊤dt i−1 and A∆t = h E PK−1 i=0 ξθ ti(ξθ ti)⊤∆t i−1 , we claim the condition |A∆t −A| ≤˜C(θ)|∆t|β holds true. To see this, recall that |(D+∆D)−1−D−1| = | ∞ X k=0 (D−1∆D)kD−1−D−1| ≤ ∞ X k=0 |D−1∆D||(D−1∆D)kD−1| ≤ |D−1|2|∆D| 1 −|D−1||∆D|. 51", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 51, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p52::c0", "text": "Jia and Zhou Thus, it suﬃces to estimate the diﬀerence E Z T 0 ξθ t (ξθ t )⊤dt −E K−1 X i=0 ξθ ti(ξθ ti)⊤∆t = E K−1 X i=0 Z ti+1 ti [ξθ t (ξθ t )⊤−ξθ ti(ξθ ti)⊤]dt ≤ K−1 X i=0 Z ti+1 ti E[ξθ t (ξθ t )⊤−ξθ ti(ξθ ti)⊤]dt ≤ K−1 X i=0 Z ti+1 ti E[|ξθ t −ξθ ti|2 + 2|ξθ t −ξθ ti||ξθ t |]dt ≤ K−1 X i=0 Z ti+1 ti C(θ)(t −ti)αdt + 2 K−1 X i=0 Z ti+1 ti E[|ξθ t −ξθ ti||ξθ t |]dt ≤TC(θ) 1 + α (∆t)α + 2 K−1 X i=0 \u0000E Z ti+1 ti |ξθ t −ξθ ti|2dt \u00011/2\u0000E Z ti+1 ti |ξθ t |2dt \u00011/2 ≤TC(θ) 1 + α (∆t)α + 2 r C(θ) 1 + α(∆t) 1 2 + α 2 K−1 X i=0 \u0000E Z ti+1 ti |ξθ t |2dt \u00011/2 ≤TC(θ) 1 + α (∆t)α + 2 r TC(θ) 1 + α (∆t) α 2 ||ξθ||L2. Proof of Theorem 7 Proof Denote by ⟨κ, ˜κ⟩L2 := E R T 0 κt˜κtdt the inner product in L2 F([0, T]). It follows from the property of projection that ⟨κ −Πθκ, ξθ,(j)⟩L2 = 0 for any κ ∈L2 F([0, T]) and all j = 1, · · · , L′. As a stochastic process, LJθ(·, X·) + r· ∈L2 F([0, T]). Write Πθ \u0000LJθ(·, X·) + r· \u0001 = L′ X i=1 α(i)(θ)ξθ,(i) · =: α(θ)⊤ξθ · . Then ⟨Πθ \u0000LJθ(·, X·) + r· \u0001 , Πθ \u0000LJθ(·, X·) + r· \u0001 ⟩L2 = X 1≤i,j≤L′ α(i)(θ)α(j)(θ)⟨ξθ,(i) · , ξθ,(j) · ⟩L2 = α(θ)⊤Aθα(θ), where the ij-th entry of the L′ × L′ matrix Aθ is ⟨ξθ,(i) · , ξθ,(j) · ⟩L2. On the other hand, E \u0014Z T 0 \u0010 Πθ \u0010 LJθ(·, X·) + r· \u0011\u0011 ξθ t dt \u0015 = E \u0014Z T 0 \u0000LJθ(t, Xt) + rt \u0001 ξθ t dt \u0015 = Aθα(θ). 52", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 52, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p53::c0", "text": "Policy Evaluation and TD Learning in Continuous Time Therefore, 1 2E \u0014Z T 0 \u0000LJθ(t, Xt) + rt \u0001 ξθ t dt \u0015⊤\u0014 E Z T 0 ξθ t (ξθ t )⊤dt \u0015−1 E \u0014Z T 0 \u0000LJθ(t, Xt) + rt \u0001 ξθ t dt \u0015 =1 2α(θ)⊤Aθ(Aθ)−1Aθα(θ) =1 2E Z T 0 Πθ \u0000LJθ(·, X·) + r· \u0001 2 dt = 1 2||Πθ \u0000LJθ(·, X·) + r· \u0001 ||2 L2 = MSPBE(θ). References L. C. Baird. Advantage updating. Technical report, Write Lab Wright-Patterson Air Force Base, OH 45433-7301, USA, 1993. L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning Proceedings 1995, pages 30–37. Elsevier, 1995. E. Barnard. Temporal-diﬀerence methods and Markov models. IEEE Transactions on Systems, Man, and Cybernetics, 23(2):357–365, 1993. O. E. Barndorﬀ-Nielsen and N. Shephard. Estimating quadratic variation using realized variance. Journal of Applied Econometrics, 17(5):457–477, 2002. C. Beck, M. Hutzenthaler, and A. Jentzen. On nonlinear feynman–kac formulas for viscosity solutions of semilinear parabolic partial diﬀerential equations. Stochastics and Dynamics, 21(08):2150048, 2021. J. A. Boyan. Technical update: Least-squares temporal diﬀerence learning. Machine Learn- ing, 49(2):233–246, 2002. S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal diﬀerence learning. Machine Learning, 22(1):33–57, 1996. M. G. Crandall, H. Ishii, and P.-L. Lions. User’s guide to viscosity solutions of second order partial diﬀerential equations. Bulletin of the American mathematical society, 27(1):1–67, 1992. M. Dai, Y. Dong, and Y. Jia. Learning equilibrium mean-variance strategy. SSRN preprint SSRN:3770818, 2020. K. Doya. Reinforcement learning in continuous time and space. Neural Computation, 12 (1):219–245, 2000. S. S. Du, J. Chen, L. Li, L. Xiao, and D. Zhou. Stochastic variance reduction methods for policy evaluation. In International Conference on Machine Learning, pages 1049–1058. PMLR, 2017. 53", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 53, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p54::c0", "text": "Jia and Zhou N. El Karoui, S. Peng, and M. C. Quenez. Backward stochastic diﬀerential equations in ﬁnance. Mathematical Finance, 7(1):1–71, 1997. W. H. Fleming and H. M. Soner. Controlled Markov processes and viscosity solutions, volume 25. Springer Science & Business Media, 2006. N. Fr´emaux, H. Sprekeler, and W. Gerstner. Reinforcement learning using a continuous time actor-critic framework with spiking neurons. PLoS Computational Biology, 9(4): e1003024, 2013. X. Gao, Z. Q. Xu, and X. Y. Zhou. State-dependent temperature control for Langevin diﬀusions. SIAM Journal on Control and Optimization, pages 1–26, 2020. A. Geramifard, M. Bowling, and R. S. Sutton. Incremental least-squares temporal diﬀerence learning. In Proceedings of the National Conference on Artiﬁcial Intelligence, volume 21, page 356. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006. X. Guo, R. Xu, and T. Zariphopoulou. Entropy regularization for mean ﬁeld games with learning. Mathematics of Operations Research, 2022. J. Han, A. Jentzen, and W. E. Solving high-dimensional partial diﬀerential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018. L. P. Hansen. Large sample properties of generalized method of moments estimators. Econo- metrica, pages 1029–1054, 1982. L. P. Hansen, J. Heaton, and A. Yaron. Finite-sample properties of some alternative GMM estimators. Journal of Business & Economic Statistics, 14(3):262–280, 1996. S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. Advances in neural information processing systems, pages 473–479, 1997. C. Hur´e, H. Pham, and X. Warin. Some machine learning schemes for high-dimensional nonlinear PDEs. arXiv preprint arXiv:1902.01599, 33, 2019. I. Karatzas and S. Shreve. Brownian Motion and Stochastic Calculus, volume 113. Springer, 2014. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. P. E. Kloeden and E. Platen. Numerical Solution of Stochastic Diﬀerential Equations. Springer, 1992. J. Lee and R. S. Sutton. Policy iterations for reinforcement learning problems in continuous time and space – Fundamental theory and methods. Automatica, 126:109421, 2021. B. Liu, J. Liu, M. Ghavamzadeh, S. Mahadevan, and M. Petrik. Proximal gradient temporal diﬀerence learning algorithms. In IJCAI, pages 4195–4199, 2016. 54", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 54, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "PolicyOptimizationTDlearninginContinuoustime::p55::c0", "text": "Policy Evaluation and TD Learning in Continuous Time L. Ljung and T. S¨oderstr¨om. Theory and practice of recursive identiﬁcation. MIT press, 1983. H. R. Maei, C. Szepesvari, S. Bhatnagar, D. Precup, D. Silver, and R. S. Sutton. Convergent temporal-diﬀerence learning with arbitrary smooth function approximation. In NIPS, pages 1204–1212, 2009. B. B. Mandelbrot and J. W. Van Ness. Fractional Brownian motions, fractional noises and applications. SIAM Review, 10(4):422–437, 1968. M. Raissi. Deep hidden physics models: Deep learning of nonlinear partial diﬀerential equations. Journal of Machine Learning Research, 19(1):932–955, 2018. H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400–407, 1951. T. Sottinen and L. Viitasaari. Prediction law of fractional Brownian motion. Statistics & Probability Letters, 129:155–166, 2017. D. W. Stroock and S. R. S. Varadhan. Multidimensional diﬀusion processes, volume 233 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathe- matical Sciences]. Springer-Verlag, Berlin-New York, 1979. R. S. Sutton. Learning to predict by the methods of temporal diﬀerences. Machine learning, 3(1):9–44, 1988. R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. Cambridge, MA: MIT Press, 2018. R. S. Sutton, C. Szepesv´ari, and H. R. Maei. A convergent o(n) temporal-diﬀerence algo- rithm for oﬀ-policy learning with linear function approximation. In NIPS, 2008. R. S. Sutton, H. R. Maei, D. Precup, S. Bhatnagar, D. Silver, C. Szepesv´ari, and E. Wiewiora. Fast gradient-descent methods for temporal-diﬀerence learning with lin- ear function approximation. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 993–1000, 2009. K. G. Vamvoudakis and F. L. Lewis. Online actor–critic algorithm to solve the continuous- time inﬁnite horizon optimal control problem. Automatica, 46(5):878–888, 2010. H. Wang and X. Y. Zhou. Continuous-time mean–variance portfolio selection: A reinforce- ment learning framework. Mathematical Finance, 30(4):1273–1308, 2020. H. Wang, T. Zariphopoulou, and X. Y. Zhou. Reinforcement learning in continuous time and space: A stochastic control approach. Journal of Machine Learning Research, 21 (198):1–34, 2020. X. Xu, H.-g. He, and D. Hu. Eﬃcient reinforcement learning using recursive least-squares methods. Journal of Artiﬁcial Intelligence Research, 16:259–292, 2002. J. Yong and X. Y. Zhou. Stochastic Controls: Hamiltonian Systems and HJB Equations. New York, NY: Spinger, 1999. 55", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "PolicyOptimizationTDlearninginContinuoustime", "page": 55, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/PolicyOptimizationTDlearninginContinuoustime.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p1::c0", "text": "Finance and Stochastics (2021) 26:103–129 https://doi.org/10.1007/s00780-021-00467-2 Reinforcement learning and stochastic optimisation Sebastian Jaimungal1 Received: 31 January 2021 / Accepted: 19 October 2021 © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2021 Abstract At the heart of ﬁnancial mathematics lie stochastic optimisation problems. Traditional approaches to solving such problems, while applicable to broad classes of models, require specifying a model to complete the analysis and obtain implementable re- sults. Even then, the curse of dimensionality challenges the viability of conventional methods to settings of practical relevance. In contrast, machine learning, and rein- forcement learning (RL) particularly, promises to learn from data and overcome the curse of dimensionality simultaneously. This article touches on several approaches in the extant literature that are well positioned to merge our traditional techniques with RL. Keywords Stochastic optimisation · Stochastic games · Reinforcement learning · Machine learning Mathematics Subject Classiﬁcation (2020) 93E20 · 93E35 · 91G60 · 91G80 JEL Classiﬁcation C02 · G11 · G12 1 Introduction There are many interesting and exciting research directions around us, including (but not limited to) optimal transport, algorithmic trading and market microstructure, mean-ﬁeld games, credit and systemic risk, model robustness, portfolio optimisation, and of course machine learning (ML). While I am keenly interested in many of these topics, I had to pick one for this article, and in my opinion, ML is touching all aspects of ﬁnancial mathematics. The outlook that I provide here is far from complete (and The author would like to acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) under the funding reference numbers RGPIN-2018-05705 and RGPAS-2018-522715. \u0002 S. Jaimungal sebastian.jaimungal@utoronto.ca 1 Department of Statistical Sciences, University of Toronto, 700 University Avenue, Toronto, Ontario, Canada", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 1, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p2::c0", "text": "104 S. Jaimungal I apologise for any omissions that I may have made), nor is it a “call to arms” to drop whatever it is you are doing and start working on ML-related problems. Instead, it is a biased view of what I deem are several exciting developments in ML that I suspect will and can signiﬁcantly impact our craft both within academia and industry. I have a love/hate relationship with ML – from one angle, there are numerous suc- cess stories in using ML techniques to solve problems ranging over portfolio optimi- sation, implied volatility surface modelling, algorithmic trading, mean-ﬁeld games, and robo-advising, among many others. Nevertheless, when trying to glean the essen- tial insights from these “successes”, it is difﬁcult to ascertain what new insights stem from ML applications in our ﬁeld. A proponent may look at the empirical evidence of published works and preprints and say “We can now solve problems A, B and C,... using ML methodology, and we could not before!” A critic would say “Sure, we have some numerical results that work on particular examples of problems A, B and C,..., but what, other than the speciﬁc solution to this particular problem, have I learnt? What is the general structure of the solution? How may I interpret this result? How robust is the result of the particular ML model architecture?” I would argue that both views are correct. There are many successes, and many more will come along in surprising areas with impactful results from which our community may beneﬁt. However, there is a gnawing feeling that often in these lines of work, we create tools to solve speciﬁc problems that, while relevant, do not provide fundamental insight. An important takeaway, however, from ML approaches is that, in the words of Tukey [113]: “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” ML methods and tools are vast and varied; thus, I must make some judicious choices on what to cover. As stochastic optimisation is a cornerstone of almost all aspects of ﬁnancial mathematics and underpins much of our craft, I focus here on reinforcement learning (RL), which aims to solve variants of stochastic optimisation problems without making strong assumptions on the underlying stochastic dynamics or how the environment responds to agents’ actions. I explore several methods and provide a few ideas for combining ML techniques with more traditional stochastic modelling techniques. I do not, however, proclaim to have the answers on how any of the ideas can be made precise. I do, nonetheless, hope that the exposition serves as a motivation for posing new problem formulations and acts as a catalyst for ideas of blending ML and ﬁnancial mathematics, in contrast to merely borrowing ideas from ML to solve speciﬁc ﬁnancial mathematics problems. The remainder of this article starts with an overview of several deep learning meth- ods for RL, and the subsequent sections touch on speciﬁc problems and discuss some potential ideas for blending ML techniques with stochastic analysis tools. The reader should be aware that the article is written with the lens that approximately solving problems that more accurately reﬂect data is an endeavour worth taking on. The arti- cle is neither rigorous nor complete; instead, the hope is to introduce ideas from the ML world that we, as ﬁnancial mathematicians, should be made aware of and which hopefully spark insights into coupling (and formalising) ML techniques and ideas with those more familiar to us.", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 2, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p3::c0", "text": "Reinforcement learning and stochastic optimisation 105 Fig. 1 Directed graph representation of the state–action–reward evolution 2 Deep reinforcement learning This section provides an overview of deep RL approaches that I believe we as ﬁnan- cial mathematicians should be aware of, and hopefully improve upon by blending these ideas with our traditional approaches. In the simplest RL setting, the evolution of the environment, action and reward may be viewed as in the directed graph shown in Fig. 1. The system evolves (stochas- tically) from state xt into state xt+1, but that transition is affected by the agent’s actions at, which itself depend on the state xt, and the triplet (xt,xt+1,at) affects the agent’s reward rt = r(xt,at). We assume the dynamics are Markov (and stationary) in the state variables for ease of notation and let A ⊆Rm denote the set of allowable actions and X ⊆Rn the set of possible states. The agent’s goal is to determine the mapping from states to actions (called the policy) that maximises (discounted) total rewards or average running sum of total re- wards. Thus, the agent’s goal is to solve a stochastic control problem in discrete time, and when the state evolution is Markov, it is known as a Markov decision process (MDP); see Puterman [98, Chap. 4]. Numerous questions in ﬁnancial mathematics fall within this class of problems ranging from portfolio optimisation to algorithmic trading to hedging. In standard stochastic control, however, the stochastic evolution, the effect of ac- tions and the rewards are all pre-speciﬁed and known. On the other hand, many RL approaches aim to perform the optimisation online (while making observations) and do so with little, or no strong, assumptions on the dynamics of the environment or the response the environment has from the agent’s actions. Our conventional tools cannot handle such a “model-free” approach that learns from data. Even with spe- ciﬁc models, traditional tools can solve only limited cases in closed form, and if the state dimension is above 3, our standard numerical tools face serious challenges. RL promises to tackle these difﬁculties simultaneously and head-on. To describe the RL approach, we denote the total discounted reward at time t by Rπ t = \u0002∞ s=t γ s−trπ t , where γ ∈(0,1) is the discount factor, rπ t is the random reward at time t under policy π, a mapping from states to actions, and the superscripts are present to remind the reader that the state path, actions and rewards depend on this policy. The policy may be deterministic or random, i.e., the policy may dictate precisely what action to take at a point in the state space (a deterministic policy), or it may dictate only the distribution over actions to take at a point in the state space (a randomised policy).", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 3, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p4::c0", "text": "106 S. Jaimungal The agent’s value function associated with such a control problem is V (x) = max π∈A E[Rπ t |xt = x] and does not depend on t due to the stationarity assumption. There is a slight abuse of notation here, as we use A to denote both the set of allowable actions and the set of policies – the context should it make clear which version is relevant. The dynamic programming principle implies that V satisﬁes the Bellman equation V (x) = max π∈A E[rπ t + γ V (xπ t+1)|xt = x]. In RL, it proves useful (as it leads to efﬁcient learning algorithms) to deﬁne the action–value function which measures the “quality” of taking a speciﬁc action a, and hence is called the Q-function, as Q(x,a) = E[ra t + γ V (xa t+1)|xt = x], x ∈X,a ∈A. Here, a is an arbitrary action taken at time t, xa t+1 denotes the one-step evolution of the state under action a, and ra t the corresponding reward. As V (x) = maxa Q(x,a), the action–value function satisﬁes a Bellman-like equation, namely Q(x,a) = E \u0003 ra t + γ max a′∈AQ(xa t+1,a′) \u0004\u0004\u0004xt = x \u0005 . (2.1) This (ﬁxed-point) equation is the starting point for a multitude of RL methods, in- cluding e.g. Monte Carlo methods, policy iteration, state–action–reward–state–action (SARSA), and Q-learning (see Sutton and Barto [109, Chaps. 4–6] for an overview of these methods). 2.1 Tabular Q-learning One of the most straightforward approaches to solving the Bellman equation (2.1) is tabular Q-learning – which refers to the case when the action and state space are either discrete or, if continuous, approximated to be discrete. In this case, the Q-function is simply a table of values (see Table 1) where, e.g., each row represents the quality of all possible actions for a particular state. Q-learning aims to estimate Q(x,a) directly from observations of 4-tuples (xt,at,rat t ,xat t+1). Under certain conditions, stochastic approximation methods show that the iterative update scheme (see Watkins and Dayan [118], Tsitsiklis [112]) Q(xt,at) ←Q(xt,at) + αt \u0006\u0007 rat t + γ max a′ Q(xat t+1,a′) \b −Q(xt,at) , (2.2) Table 1 Tabular Q-function is a table of quality of actions for particular states Q(x1,a1) Q(x1,a2) Q(x1,a3) Q(x2,a1) Q(x2,a2) Q(x2,a3) Q(x3,a1) Q(x3,a2) Q(x3,a3) Q(x4,a1) Q(x4,a2) Q(x4,a3)", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 4, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p5::c0", "text": "Reinforcement learning and stochastic optimisation 107 Initialise: initialise Q(·,·) and replay buffer B 1 repeat 2 observe state xτ ; 3 with probability ε, select aτ ∈A at random, otherwise set aτ = argmaxa∈A Q(xτ ,a); 4 take action aτ ; 5 receive reward raτ τ and new state xτ+1; 6 add (xτ ,aτ ,raτ τ ,xτ+1) to replay buffer B; 7 update Q-function Q(xτ ,aτ ) ←Q(xτ ,aτ ) + ατ \u0006 raτ τ + γ max a′∈A Q(xaτ τ+1,a′) −Q(xτ ,aτ ) ; 8 for M iterations do 9 sample (xt,at,rat t ,xt+1) from B; 10 update Q-function Q(xt,at) ←Q(xt,at) + αt \u0006 rat t + γ max a′∈A Q(xat t+1,a′) −Q(xt,at) ; 11 end 12 until converged; 13 return Q(·,·) Algorithm 1: The Dyna-Q learning algorithm with αt > 0, \u0002 t αt = ∞, \u0002 t α2 t < ∞, converges to the correct ﬁxed point. The ar- row in (2.2) represents replacing the element on the left with the value on the right. This iterative scheme should be understood as taking (sequential) samples of states xt and, taking what is known as ε-greedy actions at, observing the resulting reward rat t and the new state xat t+1 that the system evolves into, and then updating Q(xt,at) according to (2.2). An ε-greedy action consists of sampling a Bernoulli random vari- able H (with success probabilities that tend to zero as the number of iterations tends to inﬁnity) and if H = 1, selecting a random action from the allowable set of actions, if H = 0, selecting an optimal action from the current estimate of the Q-function, i.e., at = argmaxa′∈A Q(xt,a′). An alternative to the basic ε-greedy strategy is to sample actions using Boltzmann weights eQ(xt,a)/\u0002 a′∈A eQ(xt,a′) over actions. Such a ran- domisation of actions is key to RL methods as it allows the agent to explore the state space while simultaneously exploiting what has been learnt to be optimal thus far – the so-called exploration/exploitation tradeoff which appears again and again. This basic form of Q-learning updates the Q-function at each state–action pair only whenever that state–action pair is visited. As a result, it tends not to work very well, and there are many improvements in the extant literature. One simple but ef- fective improvement is to use the Dyna-Q learning approach which employs a replay buffer. A replay buffer consists of histories of the set of states, action and rewards (xt,at,rat t ,xt+1), called a 4-tuple. In the simplest form of Dyna-Q learning, the up- date rule (2.2) is applied to samples from the replay buffer in addition to realisations. This resampling results in improved estimates of the Q-function at previously visited state–action pairs. Algorithm 1 summarises the steps of the approach. When the state and action space cannot be made approximately discrete, or di- mensions are large, tabular Q-learning and its Dyna-Q learning cousin fail. In these settings, function approximation approaches, including deep Q-networks (DQN) (see Mnih et al. [88]), double DQN (DDQN) (see van Hasselt et al. [114]), and dueling DQN (see Wang et al. [117]) are utilised and we discuss these approaches next.", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 5, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p6::c0", "text": "108 S. Jaimungal Fig. 2 A schematic of a feed-forward ANN 2.2 Deep Q-networks Before delving into the details of deep Q-networks, it is instructive to brieﬂy in- troduce artiﬁcial neural networks (ANNs, also known simply as NNs). Cybenko [35], Hornik et al. [62], Hornik [61] have shown that ANNs are universal function approximators, i.e., ANNs can approximate arbitrarily well any continuous function with compact support. The proofs, however, are not constructive and do not provide the ANN that achieves a particular accuracy for a given target function. Nonetheless, ANNs have proved exceedingly useful. An ANN is simply a sequence of nonlinear maps from inputs to outputs, constructed from afﬁne transformations and so-called activation functions; see Fig. 2. The intermediate output values at each step in the sequential map are called hid- den layers. Letting h(k) denote the values at hidden layer k (k = 0 denotes the input layer and k = K the output layer), the nonlinear map may be written sequentially as h(k) = (a(k) ◦p(k))(h(k−1)), k = 1,...,K, where p(k) denotes an afﬁne transfor- mation and a(k) denotes an activation function1 for layer k. Deep learning (DL) ap- proaches use ANNs for various modelling ingredients. The term “deep” in DL refers to having many hidden layers. The term “learning” in DL refers to estimating the network’s parameters, consisting of the weights and biases that form the afﬁne trans- formation between layers (when learning, the activation functions and architecture of the ANN are held ﬁxed). At this point, it is worth commenting that while proponents pitch DL approaches as being “model-free”, this is not a wholly fair or transparent statement. The “model” in DL approaches is the ANN architecture used to connect inputs to outputs. There are numerous architectures (for example a feed-forward, a recurrent neural or a con- volution neural net, etc.), and even when the general framework is speciﬁed, there are a host of “hyper-parameters” that require tuning – how many nodes in a layer, how many layers, what is the learning rate, what optimiser is being used, and so on. 1Typical activation functions are sigmoids a(x) = (1 + e−x)−1, rectiﬁed linear units (RELU) a(x) = x+, and softplus a(x) = log(1 + ex), but there are numerous others.", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 6, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p7::c0", "text": "Reinforcement learning and stochastic optimisation 109 Deep Q-networks (DQN) (see Mnih et al. [88]) are a form of function approxima- tion for the Q-function where an ANN parametrises it, and we write Q(x,a;θ), with θ denoting the network parameters. Rather than using the stochastic approximation update rule (2.2), the update rule minimises a loss function that aims to enforce the Bellman equation (2.1). Speciﬁcally, the loss function over a mini-batch B of data is L(θ) = i∈B \u0006 rai i + γ max a′∈AQ(xai i+1,a′;θ) −Q(xi,ai;θ) 2 . (2.3) The network is updated by taking a so-called (stochastic) gradient descent2 (GD) step θ ←θ −η∇θL(θ), sampling a new mini-batch, and repeating until the parame- ters converge. The term “stochastic” GD refers to the fact that only a mini-batch of data, rather than all data, is being used to compute the gradients and hence update the parameters. The hyper-parameter η is called the learning rate and should be cho- sen judiciously. In practice, different learning rates in powers of ten are tested and the one which results in the best convergence is selected. Often, the learning rate is tuned downwards as the iteration progresses. The gradient ∇θL(θ) is obtained via a computationally efﬁcient method known as backpropagation (see e.g. Chauvin and Rumelhart [32, Chap. 1]). The convergence of a slightly modiﬁed DQN approach (known as ﬁtted Q-iteration (FQI), see Riedmiller [100]) is established in Fan et al. [40]. The loss function (2.3) often leads to over-estimates of the Q-function, resulting in sub-optimal actions. To circumvent this issue, double DQN (DDQN), see van Hasselt et al. [114], utilises the idea of a target network ϑ and main network θ, and instead minimises (over θ) the loss L(θ;ϑ) = i∈B \u0006\u0007 rai i + γ Q(xai t+1,a∗ i ;ϑ) \b −Q(xi,ai;θ) 2 , (2.4) where a∗ i = argmax a′∈A Q(xai t+1,a′;θ). In this manner, the optimal action is chosen using the main network θ, while the “quality” of that action is evaluated using the target network ϑ. After several mini- batch GD steps, the target network is replaced by the main network and the process continues until it converges. This approach has been shown to increase stability in the optimal strategies. As a demonstration of the applicability of these methods to ﬁnancial problems, we show results from Cartea et al. [25], where the authors implement DDQN for op- timal trading in foreign exchange (FX) triplets. An FX triplet consists of exchange rates between three currencies, e.g. EURUSD, GBPUSD, EURGBP. The FX triplets 2There are many other optimisation update rules such as momentum (see Polyak [97], Sutskever et al. [108]) (which exponentially smooths the gradients) and Adam (see Kingma and Ba [70]) (a combination of momentum and root-mean-squared propagation).", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 7, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p8::c0", "text": "110 S. Jaimungal Fig. 3 Optimal trading strategy in FX triplets using DDQN over one day. Each column corresponds to the optimal action in one of the triplet pairs, as a function of inventory in \u0003 and \u0003$ FX rate while the \u0003£ rate and position in £ are held ﬁxed are cointegrated and trading frictions prevent the trader from instantly changing po- sitions. This corresponds to a system with ﬁve state variables, and even if the state dynamics are known, a numerical PDE approach to the stochastic control problem would not be feasible. Figure 3 (modiﬁed from [25]) shows that the DDQN algo- rithm, while not being aware of the FX dynamics, learns to take advantage of cointe- gration while properly managing inventories in all three currencies. Other examples of DDQN in ﬁnancial modelling include Ning et al. [93], Dabérius et al. [36], Kumar [74]. Applications of path signatures and ML to an optimal execution problem may be found in Cartea et al. [26]. Next, the dueling DQN approach (see Wang et al. [117]) breaks the action–value function into two terms – the value function and the advantage function – and models each with separate ANNs. Explicitly, Q(x,a;α,β) = V (x;α) + A(x,a;β), with network parameters α and β. Often the average of A over a is subtracted to aid with identiﬁability. Using the above form of the Q-function, the loss function becomes L(α,β) = i∈B \u0006 rai i + γ max a′∈AA(xi+1,a′;β) −A(xi,ai;β) + \u0007 γ V (xi+1;α) −V (xi;α) \b 2 .", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 8, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p9::c0", "text": "Reinforcement learning and stochastic optimisation 111 Learning proceeds by taking mini-batch GD steps by ﬁrst freezing the advantage network β and “learning” the value network α, and then freezing the value network α and “learning” the value network β, over several iterations. Then, one samples a new mini-batch and repeats the process. The term dueling networks stems from the presence of the two networks and that learning proceeds by minimising the loss while one network is held ﬁxed. This separation of the Q-function in terms of value and advantage functions will reappear when we study multi-agent games in Sect. 6. The dueling framework may be viewed as a sort of generalised adversarial network (GAN); see Goodfellow et al. [50]. GANs are essentially min–max games where an actor generates data from a model, and a critic assesses how close that generated data is to test data. More speciﬁcally, GANs may be viewed as a game between two play- ers, where one player (the actor) controls a generator Gθ and aims to minimise a loss LG and a second player (the critic) controls a discriminator Dϑ and aims to minimise a loss LD. Each loss function depends on the other’s control. More speciﬁcally, the goals may be written as ﬁnding θ∗∈argmin θ∈ LG(θ,ϑ∗) and ϑ∗∈argmin ϑ∈Θ LD(θ∗,ϑ). The dueling DQN approach may thus be seen as a GAN where the loss functions are identical and the networks for advantage and value function are the actor and critic. 2.3 Policy gradient methods The DQN-based approaches require seeking for argmaxa∈A Q(x,a;θ) and therefore become infeasible when the action space is itself continuous. To circumvent this is- sue, policy gradient (PG) methods were developed in Silver et al. [104], and extended to deterministic policy gradient (DPG) methods in Lillicrap et al. [84]. DPG is based on the following set of observations. First, with F(x′) := P[x1 ≤x′], deﬁne the (un- normalised) discounted state distribution from following the policy π as ρπ(x) = X ∞ t=1 γ t−1P[xt = x|x1 = x′,π]dF(x′). Then the value function V (π) associated with a policy π, after taking the expectation over the initial state, may be written as V (π) = Eπ[r(X,aπ(X))] (note this is the expectation of the one-step reward r), where Eπ[·] means expectation over the (un- normalised) discounted state distribution ρπ, where X ∼ρπ. If we parametrise the (deterministic) policy by θ, we may write V (πθ) = \u000e X ρπθ (x)r(x,a(x;θ))dx. DPG uses a GD step on policies and updates the policy in the direction of ∇θV (πθ). Sutton et al. [110] show that ∇θV (πθ) = X ρπθ (x)∇θQ \u0007 x,a(x;θ);θ \b dx = Eπθ \u000f ∇θQ \u0007 x,a(x;θ);θ \b\u0010 . (2.5) The surprising aspect of this result is that while the (un-normalised) discounted state distribution ρπθ depends on the policy parameters, its gradients do not show up in", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 9, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p10::c0", "text": "112 S. Jaimungal the value function’s gradient. When the policy is parametrised by a deep ANN, the procedure is known as deep DPG (DDPG). This observation is often used in tandem with actor–critic methods with four net- works: θ,θ′ determine the policies (the actor and actor’s target), and ϑ,ϑ′ determine the Q-functions (the critic and critic’s target). The RL algorithm proceeds as follows. Observe a mini-batch of states (xt)t∈B, take actions (at = a(xt;θ))t∈B (sometimes noise is added to allow additional exploration), observe the new states (xt+1)t∈B and rewards (rt)t∈B. Use these 4-tuples to update the critic’s network ϑ by taking a GD step with loss function L(ϑ;θ′,ϑ′) = t∈B \u0006 rt + γ Q \u0007 xt+1,a(xt+1;θ′);ϑ′\b −Q(xt,at;ϑ) 2 , and update the actor’s policy network θ using DPG via θ ←θ −η N t∈B ∇θQ \u0007 xt,a(xt;θ);ϑ \b , where the summation equals the mini-batch sample average of E[∇θV (πθ)] in (2.5). The target networks are updated according to the rules θ′ ←α θ + (1 −α)θ′ and ϑ′ ←α ϑ + (1 −α)ϑ′, which allows the target networks to slowly progress towards the main networks. One immediate application area for DDPG are portfolio optimisation problems in settings where analytical methods fail (even if the model is fully speciﬁed). For ex- ample, one can consider including investment goals, see Nevins [90] (which are es- sentially probability constraints), or encoding stochastic benchmarks, as in Al-Aradi and Jaimungal [2, 3] (that make assumptions so that solutions are in closed form) or Ni et al. [91] who use ANNs to parametrise strategies but perform optimisation using straight forward backpropagation and mini-batch GD updates. As an explicit example, let us consider Pesenti and Jaimungal [95] who investigate how to minimise a distortion risk measure (e.g. conditional value-at-risk) on terminal wealth subject to two constraints: (i) strategies are self-ﬁnancing and satisfy a budget constraint, and (ii) strategies result in terminal wealth distributions that lie within a Wasserstein ball around a given benchmark’s terminal wealth. That is, they seek min π∈A− 1 0 F −1 Xπ T (u)γ (u)du subject to inf χ∈\u000f(FXπ T ,FXδ T ) R2 |y1 −y2|2 χ(dy1,dy2) 1 2 ≤ε, where Xδ is a benchmark portfolio, γ : [0,1] →R+ a distortion weight function (with \u000e 1 0 γ (u)du = 1), and \u000f(F1,F2) denotes all couplings between distribution functions F1 and F2. This problem is far more challenging than outright portfolio allocation. Nonetheless, DDPG methods can be applied, and Fig. 4 shows the results of an implementation when asset prices follow a stochastic volatility model. Even", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 10, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p11::c0", "text": "Reinforcement learning and stochastic optimisation 113 Fig. 4 Example portfolio optimisation where the goal is to minimise a particular distortion risk measure while remaining within a Wasserstein ball around a given benchmark in this rather complex environment, the approach is able to improve the risk mea- sure from −10.1 to −14.23 while respecting the constraint, and produces ﬁnancially sound results. For further examples and generalisations see [69] who develop a deep learning approach for solving a wide class of robust risk-averse reinforcement learn- ing problems. 3 Exploratory controls In Wang et al. [115] and Wang and Zhou [116], the authors introduce a line of work that aims to show why the exploration–exploitation tradeoff is valuable for RL prob- lems (in continuous time and space). They suppose that agents control the distribution over actions (akin to the randomised strategies discussed in the previous sections) in a manner that is modulated by time and state, i.e., the agent controls a ﬂow of measures πt that is Markov in the state. Further, if the “deterministic” control problem has a state evolution (under an action process u) dxt = b(xt,ut)dt + σ(xt,ut)dWt, then the relaxed control problem (as the randomised policy version of the problem is called) satisﬁes the SDE dxt = A b(xt,u)πt(u)dudt + \u0011 A σ 2(xt,u)πt(u)dudWt. This has the interpretation of the instantaneous drift and variance being randomised over the action distribution. The agents’ performance criterion is also modiﬁed to include an entropy term, so that V (x) = inf π∈D E \u0012 ∞ 0 e−ρ t A \u0007 f (xt,u) + λlogπt(u) \b πt(u)dudt \u0004\u0004\u0004\u0004xt = x \u0013 . The function f plays the usual role of a running cost, while the second term is an entropy term that encourages exploration. Interestingly, in the linear–quadratic", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 11, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p12::c0", "text": "114 S. Jaimungal Gaussian (LQG) case, Wang et al. [115] demonstrate that the optimal measure ﬂow is Gaussian where (i) the mean equals the optimal solution in the deterministic policy case (states are mapped uniquely to actions as in classical stochastic control), and (ii) the standard deviation is proportional to λ. Hence, the strength of the entropy term controls variability around the control that the agent would deem to be optimal if they did not explore. These results are generalised to mean-ﬁeld game (MFG) settings in Guo et al. [55] and Firoozi and Jaimungal [46]. The latter also studies the heterogeneous agents case, provides a proof of the ε-Nash equilibria property for the exploratory problem, and establishes that the existence of the solutions to the “classical” and exploratory LQG MFG systems are equivalent. None of these works, however, provide a basis for a model-free approach to ex- ploration/exploitation. One idea perhaps worth exploring is to consider a batch RL paradigm, whereby an LQG model is learnt from observing the environment, the agent employs the exploratory optimal control, and then updates the estimated LQG model with new data, applies the new exploratory optimal controls, updates the esti- mated LQG model with new data, and so on. One issue with this approach is that the real systems are likely not LQG, and while the procedure may converge, it need not provide near-optimal solutions. Alternatively, the ideas in the next section – where a fully nonlinear system is viewed as an LQG system along perturbed sub-optimal paths that are then updated – together with the exploratory control approaches here may prove a viable alternative. 4 Iterative LQG control The iterative linear quadratic regulator (iLQR) introduced in Li and Todorov [81] looks at deterministic control problems, where the state equation is known but non- linear, and provides update rules to improve upon a nominal control by performing an expansion on the state along the previous (sub-optimal) trajectory. These ideas were extended in Todorov and Li [111] to stochastic control problems as follows. Suppose the controlled state satisﬁes the SDE dxt = f (xt,ut)dt + σ dWt and the agent aims to minimise the performance criterion J(u) = E \u0012 T 0 g(xt,ut)dt \u0013 , (4.1) where f and g are nonlinear. The focus of this example is on a controlled drift with running costs alone. The extension to controlled volatility and a terminal cost could follow along similar lines of thought. Next, [111] supposes that we have a current estimate of the “optimal” control ¯u and the corresponding controlled state ¯x (obtained by ignoring the diffusion term). We aim to improve upon this estimate by expanding around the estimate and solving a new linear–quadratic problem.", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 12, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p13::c0", "text": "Reinforcement learning and stochastic optimisation 115 To this end, write xt = ¯xt + δxt and ut = ¯ut + δut and consider δu as the new control (where ¯u is held ﬁxed) that controls δx. Then expand to ﬁrst order, f (¯xt + δxt, ¯ut + δut) ≈f (¯xt, ¯ut) + \u0007 ∇f (¯xt, ¯ut) \b⊤δyt, and similarly expand to second order, g(xt,ut) ≈g(¯xt, ¯ut) + \u0007 ∇g(¯xt, ¯ut) \b⊤δyt + 1 2δy⊤ t Hg(¯xt, ¯ut)δyt, where δyt = (δxt,δut)⊤and Hg denotes the Hessian of g. Next, the SDE for δx may be written as d(δxt) = ( ¯ft + ¯fx,t δxt + ¯fu,t δut)dt + σ dWt −d ¯xt, where ¯ft = f (¯xt, ¯ut), ¯fx,t = ∂xf (¯xt, ¯ut) and ¯fu,t = ∂uf (¯xt, ¯ut), which correspond to the drift along the previous approximately optimal path, and its sensitivity to state and control. As the state dynamics is afﬁne in the modiﬁed state and control (δx and δu) and the running cost is at most quadratic in the modiﬁed state and control, the optimal modiﬁcation of the control will be afﬁne,3 so that we may write δu∗ t = at + bt δx∗ t for some (known) stochastic processes (at) and (bt) that depend on the previous es- timate of the “optimal” control and corresponding path ( ¯x and ¯u) – obtained e.g. by solving the dynamic programming equation, a linear forward–backward SDE (FBSDE) stemming from the stochastic Pontryagin maximum principle, or a varia- tional analysis approach to optimising (4.1) with the perturbed dynamics and running cost. Thus, the process u∗ t = ¯ut + δu∗ t provides an improved control and we may iterate the procedure until it converges. This iterative procedure appears to work well in several settings, and when the nonlinear dynamics is not known, approaches for estimating approximate linear dy- namics along observed paths have been suggested in e.g. Gu et al. [53]. It would be interesting to further develop these approaches for applications in ﬁnance, e.g. prop- agator models of nonlinear impact (see Gatheral [47]), trading signals in Cartea and Jaimungal [24], Lehalle and Neuman [80], and latent factors in Casgrain and Jaimun- gal [28]. The idea of coupling variants of iLQG with exploratory controls as outlined in Sect. 3 and its generalisation to MFGs (see Guo et al. [55], Firoozi and Jaimun- gal [46]) is intriguing. Making all of these procedures precise, and ﬁnding conditions under which the algorithm converges to the true solution, is a challenging but fruitful line of research that would go a long way. 3This is despite the fact that the perturbed model is not linear–quadratic due to the presence of the stochas- tic path from the previous iteration of the control appearing in both the state dynamics and the cost func- tional.", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 13, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p14::c0", "text": "116 S. Jaimungal 5 Deep BSDE for stochastic control Stochastic control problems are intimately connected with FBSDEs, and this section brieﬂy discusses deep learning approaches for solving such equations. In settings where the dynamics of the stochastic control system is known, the deep BSDE ap- proach (see E et al. [39], Han and Long [57]) has been shown to be a successful alternative to least-squares-regression approaches. The idea is as follows. Suppose that the state process satisﬁes the SDE dxt = b \u0007 t,xt,α(t,xt) \b dt + σ(t,xt)dWt, where α represents the agent’s control, and the agent has the performance criterion J α(t,x) = E \u0012 T t f \u0007 s,xs,α(s,xs) \b ds + g(xT ) \u0013 and value function J(t,x) = minα J α(t,x). Then, under certain conditions (see e.g. Pham [96, Chap. 6]), the optimisation problem may be rewritten as the solution to the FBSDE xt = x0 + t 0 b \u0007 s,xs,α∗(s,xs) \b ds + t 0 σ(s,xs)dWs, (5.1) yt = g(xT ) + T t H(s,xs,ys)ds − T t zs dWs, (5.2) where H(t,x,y) = mina(b(t,x,a)y + f (t,x,a)) is the system’s Hamiltonian, and we set α∗(t,xt) = argmina(b(t,xt,a)yt + f (t,xt,a)), yt = J(t,xt), as well as zt = σ(t,xt)∂xJ(t,xt). The deep BSDE approach to the optimisation problem pro- ceeds by ﬁrst performing an Euler discretisation of (5.1) and (5.2) leading to xtk+1 = xtk + b \u0007 tk,xtk,α∗(tk,xtk) \b \u0014tk + σ(tk,xtk)\u0014Wtk, ytk+1 = ytk −H(tk,xtk,ytk)\u0014tk + ztk \u0014Wtk, for 0 = t0 < t1 < ··· < tN = T . The method then assumes that y0 = Y(x0;θ) and ztk = Z(xtk;ϑk), k = 0,...,N −1, where θ and ϑk parametrise ANN approximations for the mapping from state to adjoint (at time 0) and co-adjoint (at all discrete time points). For ﬁxed ANNs, sample paths of x,y,z may be generated, and the goal is to minimise the loss function L(θ;ϑ) = 1 M M m=1 \u0007 y(m) T −g(x(m) T ) \b2 over θ and ϑ, which aims to enforce the terminal condition constraint. Naturally, one may also take the PDE approach for solving control problems. An early work in this direction is Sirignano and Spiliopoulos [106], where the solution to the PDE is approximated by an ANN. For an overview and a number of ﬁnancial applications, we refer to Al-Aradi et al. [1]. Germain et al. [48] provide an overview", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 14, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p15::c0", "text": "Reinforcement learning and stochastic optimisation 117 of a number of other approaches. See Huré et al. [68] for extensions to variational inequalities as well as reﬂected BSDEs and Saporito and Zhang [102] for extensions to path-dependent PDEs. An interesting challenge is to extend these ideas to the case when the state dynam- ics themselves are not known. One simple idea is to introduce a prior on a set P of models, solve for the optimal controls for each model, using ε-greedy actions where the randomisation is over models using the prior distribution, make observations that update your prior, and repeat. This requires having a good set of models that you are conﬁdent reﬂects the environment; how to go beyond that and directly learn from the environment is a real challenge. Perhaps tying this into exploratory control in Sect. 3 and iLQG methods in Sect. 4 would prove useful. Also, making use of DPG methods (see Sect. 2.3) may prove helpful in accelerating parameter learning. A ﬁnal direction could be to assume that the coefﬁcients of the SDE are themselves parametrised by ANNs – so-called neural SDEs as in Li et al. [82], Gierjatowicz et al. [49] (see also Cuchiero et al. [34] for neural ODEs). Then solve for the optimal controls using the deep BSDE approach, take these optimal actions to the environment and observe over several epochs, after which you update the neural SDE and repeat. 6 Multi-agent games Multi-agent games arise in many contexts in ﬁnancial mathematics, including algo- rithmic trading (Firoozi and Caines [45], Casgrain and Jaimungal [27], Cardaliaguet and Lehalle [15], Huang et al. [67], Lehalle and Mouzouni [79], Casgrain and Jaimun- gal [29]), energy markets (Shrivats et al. [103], Féron et al. [43], Bouveret et al. [9]), and systemic risk (Carmona et al. [19], Bo and Capponi [7], Borovykh et al. [8]). Much research effort has been placed on developing the theory and application of mean-ﬁeld games (MFGs) as a means to approximate Nash equilibria for multi-agent games by their inﬁnite population limits. The MFG methodology was originally de- veloped in Huang et al. [65, 66] and Lasry and Lions [76, 77, 78], and many exten- sions and generalisations exist, including the probabilistic approach to MFGs (Car- mona and Delarue [17, 18]), MFGs with common noise (Cardaliaguet et al. [14]), major–minor MFGs (Huang [64]), and the master equation approach (Cardaliaguet et al. [14]). As only very few MFGs are solvable in closed form, there has been a ﬂurry of activity from the ﬁnancial mathematics community in developing solutions to MFGs and mean-ﬁeld type control (MFC) problems using variations of Q-learning adapted to the MFG and MFC settings, including Yang et al. [120], Carmona et al. [20], Carmona and Laurière [21], Carmona et al. [22, 23], Guo et al. [54], Gu et al. [51, 52]. Due to limited space, the details of these approaches are omitted, and this section instead focuses on approaches for solving ﬁnite-player, as opposed to the inﬁnite- population-limit, versions of multi-agent games using RL. A Q-learning-based approach for obtaining Nash equilibria in general-sum sto- chastic games ﬁrst appears in Hu and Wellman [63]. They prove convergence of their algorithm for games with ﬁnite state and action spaces; however, their approach is computationally infeasible for all but the simplest examples. The main computational", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 15, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p16::c0", "text": "118 S. Jaimungal bottleneck is the need to repeatedly compute a local Nash equilibrium over states (a stage game), which is an NP-hard operation in general. Moreover, the method proposed in [63] does not extend to games where agents choose continuous-valued controls or to games with either high-dimensional states or with a large (but ﬁnite) number of players. Alternatively, Casgrain et al. [31] combine the iLQG framework of Todorov and Li [111], its actor–critic variation in Gu et al. [53] and the Nash Q-learning algorithm of Hu and Wellman [63] to produce an algorithm that can learn Nash equilibria in these more complex and practically relevant settings. First, in the ﬁnite-player setting, each agent has their own Q-function which is a function of the states of agents and their own control. The analogue of the Bellman equation is Qi(x;u) = E[ri,t + γ Nu′Qi(xt+1;u′)], where Nu′ represents the static stage-game operator and i enumerates agents. We can try a deep learning approach to solve for the ﬁxed point and minimise a loss akin to (2.4); however, this is computationally infeasible due to the necessity of solving the stage game at each iteration and at all points in the action–state space. Thus, Casgrain et al. [31] propose to write the Q-function in terms of value functions Vi(x;ϑ) and advantage functions Ai(x,u;θ), where the advantage functions are linear–quadratic in controls, but generally nonlinear in states, e.g. Ai(x,u;θ) = − ui −μθ i (x) u−i −μθ −i(x) ⊤ P θ i (x) ui −μθ i (x) u−i −μθ −i(x) + \u0007 u−i −μθ −i(x) \b⊤\u0015θ i (x). For each x, P θ i (x) is a positive deﬁnite matrix, μθ i (x), μθ −i(x) and \u0015θ i (x) are vectors, and all are parametrised by an ANN with parameters θ. This formulation allows the stage game to be solved in closed form (given the network parameters), and we have by construction that the strategy attaining NuQ(x,u;θ,ϑ) is μ(x) and at the equi- libria NuQ(x,u;θ,ϑ) = V (x;ϑ). Hence, the loss function (over a mini-batch B) becomes L(ϑ,θ) = t∈B \u0007 V (xt;ϑ) + A(xt,ut;θ) −rt −γ V (xt+1;ϑ) \b2, which may be optimised in an actor–critic fashion by alternating between GD steps in θ and ϑ. To illustrate a ﬁnancial application of these methods, we take an example from algorithmic trading with multiple agents under nonlinear trading impact; see Casgrain et al. [31]. The price dynamics are \u0014St = κ (θ −St) + b sgn \u0006 1 N N i=1 \u0014qt,i \u0014 \u0015 \u0015 \u0016 \u0004\u0004\u0004\u0004 1 N N i=1 \u0014qt,i \u0004\u0004\u0004\u0004 \u0014T + σ √ \u0014T ξt. Here, \u0014qt,i represents agent i’s trades (changes in inventory) at t and ξt ∼N(0,1) represents the one-step noisy innovations. Figure 5 (modiﬁed from [31]) shows re- sults of applying the algorithm to a system with 5 agents. The results demonstrate", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 16, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p17::c0", "text": "Reinforcement learning and stochastic optimisation 119 Fig. 5 Optimal trade execution heatmaps in (a)–(c) as a function of time, inventory, price and average inventory of other agents. Within each panel, subplots correspond to price levels $6,$8,...,$14 from left to right. The dotted lines show the threshold where the agent switches from buying to selling. Panel (d) shows sample inventory paths and corresponding price paths. Solid lines represent agents’ inventory paths, dotted lines represent asset price paths that while the optimisation algorithm is unaware of the environment’s dynamics, it ﬁnds an equilibrium that corresponds closely to the mean-ﬁeld solution obtained in Casgrain and Jaimungal [27, 29] which generalise the results in Cartea and Jaimun- gal [24], Casgrain and Jaimungal [28]. The ﬁgure shows that agents, while ending the trading period with zero inventory, are attracted to a stochastic trend which depends on the asset price path, i.e., they are responding to price signals and account for the actions of aggregate trading. Another approach, which requires solving at each stage two BSDEs, is deep ﬁcti- tious play in Han and Hu [56] (which is proved to converge in Han and Long [57]). The central idea is as follows: 1. Make a (Markov) initial assumption on the optimal strategy for all players. 2. Each player optimises against this (ﬁxed) estimated strategy. This step is solved using a deep FBSDE approach (as described in Sect. 5). 3. Players update their value functions according to the previous estimate of the op- timal strategy. 4. Players update their policy by minimising the Hamiltonian using these new esti- mates. Once again, it may be possible to integrate the ideas of “model-free” learning de- scribed in this section, or iterative methods (as in Sect. 4) together with deep ﬁctitious play, to go beyond requiring the speciﬁcation of model dynamics and to learn directly", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 17, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p18::c0", "text": "120 S. Jaimungal from data. Also, extending the ideas in Carmona et al. [20], Carmona and Laurière [21], Carmona et al. [22, 23], Gu et al. [51, 52], Guo et al. [54] to cases with major– minor agents and/or common noise, and graphon models of MFGs (see Caines and Huang [12], Carmona et al. [16]) would be very interesting. As well, recently, [13] look at principal-agent mean ﬁeld games using deep learning techniques. 7 Reinforced deep Markov models The approaches described thus far do not rely on speciﬁc models of data. There are many situations, however, where “model-driven” approaches have proved to be ad- vantageous. The term model-driven here does not mean e.g. a particular SDE for the system dynamics, but rather a ﬂexible DL model with additional structure that re- ﬂects aspects of the system being analysed without being too prescriptive. Kurutach et al. [75] argue that when deep neural networks are used to learn both the model and the policy, the learnt policy tends to exploit regions where there is actually little data, and thus induces instability in training. Instead, the authors propose to utilise an en- semble of models, which simultaneously accounts for uncertainty and regularises the problem. This section looks at one class of models known as deep Markov models (DMMs) that, as they contain features that resemble stochastic volatility as well as latent factors (both important aspects of ﬁnancial data), are poised to be useful for ﬁnancial modelling. DMMs (see Krishnan et al. [72]) are generalisations of hidden Markov mod- els (HMMs) to include nonlinear state-dependence between the latent (unobserved) states themselves and between latent states and observables. They may be viewed as in the graphical model in Fig. 6. For example, Gaussian DMMs (see Krishnan et al. [73]) make the assumption that zt+1 P∼N \u0007 μθ z(zt),\u0018θ z (zt) \b , (7.1) xt P∼N \u0007 μθ x(zt),\u0018θ x(zt) \b , (7.2) where x,z are visible and latent states, respectively, and the means and covariance matrices are parametrised by ANNs with parameters θ. If the means are afﬁne and the covariances constant, the setup reduces to a Kalman ﬁlter model; hence the nonlinear mapping in a DMM distinguishes it from a Kalman ﬁlter model. The emission dis- tribution (i.e., that of xt|zt) may be chosen to suit the needs of the modelling setting; e.g., if x is categorical, then a conditional multi-class logistic model parametrised by an ANN, P[xt = i|zt] = ewθ i (zt)/\u0002K k=1 ewθ k (zt), is useful, or if x is continuous and has support on R+, then a conditional Gamma distribution with shape aθ(zt) and scale bθ(zt) is useful, etc. Fig. 6 Directed graph representation of a DMM", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 18, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p19::c0", "text": "Reinforcement learning and stochastic optimisation 121 As z is not visible, learning in such models requires the posterior distribution of z given observations x. This is generally intractable in such nonlinear models, and instead variational approximation (VA) techniques (see e.g. Ormerod and Wand [94] for an overview) are often employed. The essential idea behind VA techniques is to approximate an intractable posterior distribution with a parametrised (but rich enough) alternative distribution Q (which should be viewed as a distribution over z1:T conditional on observations xobs 1:T ) and maximise what is called the evidence lower bound (ELBO) (also known as the variational lower bound) L(xobs 1:T ) = EQ\u000f P[x1:T = xobs 1:T |z1:T ] \u0010 −EQ \u0012 log dQ dP \u0013 . Here and in the remainder of this section, we use the slice notation where x1:T denotes the vector x1,x2,...,xT and similarly for z, xobs. A simple application of Jensen’s inequality shows that the ELBO provides a lower bound for the log-likelihood, and when the relative entropy vanishes, the ELBO equals the log-likelihood. An example of a VA for the Gaussian DMM is to assume an encoder zt+1 Q∼N \u0007 μϑ z (zt,x1:T ),\u0018ϑ z (zt,x1:T ) \b , (7.3) where ϑ is a new ANN that parametrises the mean and covariance and takes the previous latent state and all observations as an input. Using all visible inputs is sim- ilar to the Kalman smoother. Instead of using the whole path of x, we can also use summaries (which is more memory-efﬁcient) of the path of x up to time t, e.g. the output of a forward–backward long-short-term memory network (LSTM) or a gated recurrent unit (GRU). This shows the ﬂexibility in DMM architectures. For the VA in (7.3), and in many other cases, the ELBO may be written out explicitly as L(xobs 1:T ) = −1 2 T −1 t=0 EQ \u0012 dx log(2π) + logdet\u0018θ x(zt+1) + \u0007 xobs t+1 −μθ x(zt+1) \b⊤\u0007 \u0018θ x(zt+1) \b−1\u0007 xobs t+1 −μθ x(zt+1) \b + log det\u0018θ z (zt) det\u0018ϑz (zt,xobs 1:T ) + dz −Tr \u0006\u0007 \u0018θ z (zt) \b−1 \u0018ϑ z (zt,xobs 1:T ) − \u0007 μθ z(zt) −μϑ z (zt,xobs 1:T ) \b⊤\u0007 \u0018θ z (zt) \b−1\u0007 μθ z(zt) −μϑ z (zt,xobs 1:T ) \b\u0013 . Learning proceeds in an actor–critic fashion by simulating mini-batches of paths from (7.3) (with ϑ frozen), taking GD steps in θ over several mini-batches, and then freez- ing ϑ and taking GD steps over ϑ, to maximise the ELBO and hence (hope) to max- imise the log-likelihood. The DMM may be used to develop models for sequential data and account for la- tent factors that modulate the dynamics. Such an approach is very pertinent to ﬁnan- cial modelling, and it would be interesting to develop e.g. interest rate term structure models, models of volatility indices, volatility surfaces, FX rates, and so on.", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 19, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p20::c0", "text": "122 S. Jaimungal Fig. 7 Directed graph representation of a reinforced deep Markov model The DMM setup was not initially envisioned for controlled systems. However, one modiﬁcation (along the lines of Ferreira [44] who studies an optimal execution problem) that allows controls is to include actions and rewards as part of the graphical model as in Fig. 7. Such models are termed reinforced DMM (RDMM). RDMMs contain three network parameters θ, ϕ and ϑ. The ﬁrst is the generative network θ in the DMM (7.1), (7.2) – which is now supplemented by a reward output and the effect of actions on the latent and visible states via zt+1 P∼N \u0007 μθ z(zt,at),\u0018θ z (zt,at) \b , xt P∼N \u0007 μθ x(zt,at−1),\u0018θ x(zt,at−1) \b , rt P∼N \u0007 μθ r (zt,at),\u0018θ r (zt,at) \b . The second is the action network ϕ, which depends only on the visible states (or summaries through e.g. an LSTM or GRU) via, for example, at P∼N \u0007 μϕ a(xt),\u0018ϕ a (xt) \b , (7.4) and the third is the VA approximation to the posterior network ϑ, which now must account for actions and rewards via, for example, zt+1 Q∼N \u0007 μϑ z (zt,x1:T ,r1:T ,a1:T ),\u0018ϑ z (zt,x1:T ,r1:T ,a1:T ) \b . In the above, the posterior can once again use summaries rather than entire paths of x,r,a. Optimisation proceeds in a batch and iterative manner by 1. freezing the action network ϕ from (7.4), running (sub-optimal) strategies, and alternatively optimising over the generative model network θ and the VA approx- imation network ϑ to maximise the ELBO; 2. freezing the generative model and VA approximation networks θ and ϑ, and max- imising expected rewards over the action network ϕ using simulated paths (this replaces the Dyna-Q learning replay buffer) from the model learnt in the previous step; 3. repeating from step 1 until we get convergence. In this manner, the setup very much resembles a GAN. It would be interesting to develop DPG outlined in Sect. 2.3 to accelerate the policy iteration part of the", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 20, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p21::c0", "text": "Reinforcement learning and stochastic optimisation 123 algorithm in step 2. Perhaps there may be a way to extend the iLQG approach outlined in Sect. 4 to the case with latent factors as well. It is tantalising to view DMM models as discrete versions of so-called neural SDEs (see Li et al. [82, 83], Gierjatowicz et al. [49], Cuchiero et al. [33]), where the drift and volatility of a process x may be outputs of an ANN, dxt = μθ(t,xt)dt + σ θ(t,xt)dWt. In the case of an RDMM, however, the neural SDE is speciﬁed on the latent process z. The visible state may be viewed as realisation of a measure ﬂow that is parametrised by an ANN, with inputs being the latent neural SDE state and samples from the mea- sure ﬂow being the observations. These learnt dynamics may then be used in tandem with the iLQG methodology to obtain iteratively improved closed-form correction strategies. 8 Concluding remarks There are many interesting problems related to ML/DL in ﬁnancial mathematics other than those covered in this article. I touch upon a few of these strands here in the concluding remarks, but only in passing, as it is impossible to do justice to the volume of work in such limited space. Hedging and pricing of contingent claims are some of the original and funda- mental questions in ﬁnancial mathematics on which signiﬁcant progress has already been achieved, including Black and Scholes [6], Merton [87], Harrison and Kreps [58], Harrison and Pliska [59], Kreps [71], Delbaen and Schachermayer [38], Bálint and Schweizer [4]. These approaches, while model-agnostic and rigorous, do re- quire specifying a model and are not data-driven. Recently, several “model-free” ap- proaches to hedging and pricing appear in the literature. De Spiegeleer et al. [37] use Gaussian processes (GPs) to speed up valuation and interpolate implied volatil- ity surfaces. Ferguson and Green [41] use ANNs to approximate the mapping from model parameters (e.g. volatility and maturity) to prices for fast pricing. Ludkovski and Saporito [85] use GPs for estimating option Greeks. Buehler et al. [10] use a mean–variance criterion and parametrise trading strategies with an ANN. Fernan- dez Arjona and Filipovi´c [42] use a replicating martingale approach and an ANN and polynomial basis for large portfolio pricing and risk management. Gierjatowicz et al. [49] use neural SDEs to price and learn hedging strategies simultaneously. There have been some recent advances in connecting mean-ﬁeld analysis and in- teracting particle systems to learning in deep neural networks; see Mei et al. [86], Rot- skoff and Vanden-Eijnden [101], Sirignano and Spiliopoulos [107]. One of the key insights from [107] is that (stochastic) GD steps generate a ﬂow of measures (for the network parameters) and this ﬂow converges in an appropriate Skorokhod space for measure-valued processes. The choice of optimisation engine in deep learning is vitally important in obtain- ing sensible results, but a topic rarely discussed (at least within the ﬁnancial math- ematics community). An exception is Casgrain and Kratsios [30] which poses the", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 21, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p22::c0", "text": "124 S. Jaimungal meta-optimisation problem of how to construct optimisation algorithms based on the pathwise performance of the optimiser. So-called “market generators” are another area where ML models are making headway. A market generator aims to generate sample paths that reﬂect the realised dynamics of asset prices in a manner that resembles historical data. Obvious appli- cations are to risk management, but they may also be applied in e.g. portfolio opti- misation to generate supplemental simulated data for an optimisation engine. Some of the common tasks along this line are calibrating implied volatility surfaces and generating sensible sequences of surfaces. There are several approaches in the liter- ature including Bayer et al. [5], Buehler et al. [11], Cuchiero et al. [33], Xu et al. [119], Ning et al. [92]. There is also fruitful activity in modelling of high-frequency limit order book evolution; see Sirignano and Cont [105], Zhang et al. [121]. Interpretability of DL models is another area that requires much more work (see e.g. Montavon et al. [89]). What interpretability even means is a contentious topic, and there is a difference between post-hoc interpretability, where a learnt DL model is “explained” (see e.g. Ribeiro et al. [99]), and incorporating interpretability directly into the model structure (see e.g. Zhou et al. [122]). One concrete approach towards post-hoc interpretability is to project the model in various regions of the state space onto low-dimensional linear models that are more easily understood. Another ap- proach is to evaluate the impact of individual variables (the weights and biases in the afﬁne transformation) of a learnt model; see Horel and Giesecke [60]. In all, there are a multitude of directions to discover, and my hope is that we con- tinue to build tools that intimately tie ML techniques into our arsenal, not to supplant, but rather to supplement and bolster it. As a community, ﬁnancial mathematics needs a healthy ecosystem of lines of research, and I want to make it clear that I see ML as only one (albeit important) strand. Acknowledgements The author would like to thank Hariom Tatsat for producing Fig. 4 and Álvaro Cartea for helpful comments on earlier drafts of this article. References 1. Al-Aradi, A., Correia, A., Naiff, D., Jardim, G., Saporito, Y.: Solving nonlinear and high-dimensional partial differential equations via deep learning. Preprint (2018). Available online at https://arxiv.org/ abs/1811.08782 2. Al-Aradi, A., Jaimungal, S.: Outperformance and tracking: dynamic asset allocation for active and passive portfolio management. Appl. Math. Finance 25, 268–294 (2018) 3. Al-Aradi, A., Jaimungal, S.: Active and passive portfolio management with latent factors. Quant. Finance 21, 1437–1459 (2021) 4. Bálint, D.Á., Schweizer, M.: Making no-arbitrage discounting-invariant: a new FTAP beyond NFLVR and NUPBR. Swiss Finance Institute Research Paper no. 18–23 (2020). Available online at http://papers.ssrn.com/sol3/papers.cfm?abstract_id=3141770 5. Bayer, C., Horvath, B., Muguruza, A., Stemper, B., Tomas, M.: On deep calibration of (rough) stochastic volatility models. Preprint (2019). Available online at https://arxiv.org/abs/1908.08806 6. Black, F., Scholes, M.: The pricing of options and corporate liabilities. J. Polit. Econ. 81, 637–654 (1973) 7. Bo, L., Capponi, A.: Systemic risk in interbanking networks. SIAM J. Financ. Math. 6, 386–424 (2015) 8. Borovykh, A., Pascucci, A., La Rovere, S.: Systemic risk in a mean-ﬁeld model of interbank lending with self-exciting shocks. IISE Trans. 50, 806–819 (2018)", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 22, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p23::c0", "text": "Reinforcement learning and stochastic optimisation 125 9. Bouveret, G., Dumitrescu, R., Tankov, P.: Technological change in water use: a mean-ﬁeld game approach to optimal investment timing. Preprint (2020). Available online at https://papers.ssrn.com/ sol3/papers.cfm?abstract_id=3640181 10. Buehler, H., Gonon, L., Teichmann, J., Wood, B.: Deep hedging. Quant. Finance 19, 1271–1291 (2019) 11. Buehler, H., Horvath, B., Lyons, T., Perez Arribas, I., Wood, B.: A data-driven market simulator for small data environments. Preprint (2020). Available online at https://papers.ssrn.com/sol3/papers. cfm?abstract_id=3632431 12. Caines, P.E., Huang, M.: Graphon mean ﬁeld games and the GMFG equations. In: Proceedings of 2018 IEEE Conference on Decision and Control (CDC), IEEE, pp. 4129–4134. (2018) 13. Campbell, S., Chen, Y., Shrivats, A., Jaimungal, S.: Deep learning for principal-agent mean ﬁeld games. arXiv preprint 2110.01127 (2021) 14. Cardaliaguet, P., Delarue, F., Lasry, J.-M., Lions, P.-L.: The Master Equation and the Convergence Problem in Mean Field Games. Princeton University Press, Princeton (2019) 15. Cardaliaguet, P., Lehalle, C.-A.: Mean ﬁeld game of controls and an application to trade crowding. Math. Financ. Econ. 12, 335–363 (2018) 16. Carmona, R., Cooney, D., Graves, C., Laurière, M.: Stochastic graphon games: I. The static case. Preprint (2019). Available online at https://arxiv.org/abs/1911.10664 17. Carmona, R., Delarue, F.: Probabilistic Theory of Mean Field Games with Applications I. Mean Field FBSDEs, Control, and Games. Springer, Berlin (2018) 18. Carmona, R., Delarue, F.: Probabilistic Theory of Mean Field Games with Applications II. Mean Field Games with Common Noise and Master Equations. Springer, Berlin (2018) 19. Carmona, R., Fouque, J.-P., Sun, L.-H.: Mean ﬁeld games and systemic risk. Commun. Math. Sci. 13, 911–933 (2015) 20. Carmona, R., Hamidouche, K., Laurière, M., Tan, Z.: Policy optimization for linear-quadratic zero- sum mean-ﬁeld type games. In: Proceedings of 2020 59th IEEE Conference on Decision and Control (CDC), IEEE, pp. 1038–1043 (2020) 21. Carmona, R., Laurière, M.: Convergence analysis of machine learning algorithms for the numerical solution of mean ﬁeld control and games: II – The ﬁnite horizon case. Preprint (2019). Available online at https://arxiv.org/abs/1908.01613 22. Carmona, R., Laurière, M., Tan, Z.: Linear–quadratic mean-ﬁeld reinforcement learning: conver- gence of policy gradient methods. Preprint (2019). Available online at https://arxiv.org/abs/1910. 04295 23. Carmona, R., Laurière, M., Tan, Z.: Model-free mean-ﬁeld reinforcement learning: mean-ﬁeld MDP and mean-ﬁeld Q-learning. Preprint (2019). Available online at https://arxiv.org/abs/1910.12802 24. Cartea, Á., Jaimungal, S.: Incorporating order-ﬂow into optimal execution. Math. Financ. Econ. 10, 339–364 (2016) 25. Cartea, Á., Jaimungal, S., Sánchez-Betancourt, L.: Reinforcement learning for foreign exchange trading. In: Capponi, A., Lehalle, C.-A. (eds.) Machine Learning and Data Sciences for Finan- cial Markets: A Guide to Contemporary Practices. Cambridge University Press, Cambridge (2022). To appear 26. Cartea, Á., Perez Arribas, I., Sánchez-Betancourt, L.: Optimal execution of foreign securities: a double-execution problem with signatures and machine learning. Preprint (2020). Available on- line at https://ssrn.com/abstract=3562251 27. Casgrain, P., Jaimungal, S.: Mean ﬁeld games with partial information for algorithmic trading. Preprint (2018). Available online at https://arxiv.org/abs/1803.04094 28. Casgrain, P., Jaimungal, S.: Trading algorithms with learning in latent alpha models. Math. Finance 29, 735–772 (2019) 29. Casgrain, P., Jaimungal, S.: Mean-ﬁeld games with differing beliefs for algorithmic trading. Math. Finance 30, 995–1034 (2020) 30. Casgrain, P., Kratsios, A.: Optimizing optimizers: regret-optimal gradient descent algorithms. Preprint (2020). Available online at https://arxiv.org/abs/2101.00041 31. Casgrain, P., Ning, B., Jaimungal, S.: Deep Q-learning for Nash equilibria: Nash-DQN. Preprint (2019). Available online at https://arxiv.org/abs/1904.10554 32. Chauvin, Y., Rumelhart, D.E.: Backpropagation: Theory, Architectures, and Applications. Psychol- ogy Press, New York (1995) 33. Cuchiero, C., Khosrawi, W., Teichmann, J.: A generative adversarial network approach to calibration of local stochastic volatility models. Risks 8(4), 1–31 (2020)", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 23, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p24::c0", "text": "126 S. Jaimungal 34. Cuchiero, C., Larsson, M., Teichmann, J.: Deep neural networks, generic universal interpolation, and controlled ODEs. SIAM J. Math. Data Sci. 2, 901–919 (2020) 35. Cybenko, G.: Approximation by superpositions of a sigmoidal function. Math. Control Signals Syst. 2, 303–314 (1989) 36. Dabérius, K., Granat, E., Karlsson, P.: Deep execution-value and policy based reinforcement learning for trading and beating market benchmarks. Preprint (2019). Available online at https://ssrn.com/ abstract=3374766 37. De Spiegeleer, J., Madan, D.B., Reyners, S., Schoutens, W.: Machine learning for quantitative ﬁ- nance: fast derivative pricing, hedging and ﬁtting. Quant. Finance 18, 1635–1643 (2018) 38. Delbaen, F., Schachermayer, W.: A general version of the fundamental theorem of asset pricing. Math. Ann. 300, 463–520 (1994) 39. E, W., Han, J., Jentzen, A.: Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations. Commun. Math. Stat. 5, 349–380 (2017) 40. Fan, J., Wang, Z., Xie, Y., Yang, Z.: A theoretical analysis of deep Q-learning. In: Bayen, A.M., et al. (eds.) Proceedings of the 2nd Conference on Learning for Dynamics and Control, PMLR, pp. 486–489 (2020) 41. Ferguson, R., Green, A.: Deeply learning derivatives. Preprint (2018). Available online at https:// arxiv.org/abs/1809.02233 42. Fernandez Arjona, L., Filipovi´c, D.: A machine learning approach to portfolio pricing and risk man- agement for high-dimensional problems. Swiss Finance Institute Research Paper no. 20–28 (2020). Available online at https://ssrn.com/abstract=3588376 43. Féron, O., Tankov, P., Tinsi, L.: Price formation and optimal trading in intraday electricity markets with a major player. Risks 8(4), 1–21 (2020) 44. Ferreira, T.A.: Reinforced deep Markov models with applications in automatic trading. Preprint (2020). Available online at https://arxiv.org/abs/2011.04391 45. Firoozi, D., Caines, P.E.: Mean ﬁeld game ε-Nash equilibria for partially observed optimal execution problems in ﬁnance. In: Proceedings of 2016 IEEE 55th Conference on Decision and Control (CDC), IEEE, pp. 268–275 (2016) 46. Firoozi, D., Jaimungal, S.: Exploratory LQG mean ﬁeld games with entropy regularization. Auto- matica (2020, forthcoming). Available online at https://arxiv.org/abs/2011.12946 47. Gatheral, J.: No-dynamic-arbitrage and market impact. Quant. Finance 10, 749–759 (2010) 48. Germain, M., Pham, H., Warin, X.: Neural networks-based algorithms for stochastic control and PDEs in ﬁnance. Preprint (2021). Available online at https://arxiv.org/abs/2101.08068 49. Gierjatowicz, P., Sabate-Vidales, M., Šiška, D., Szpruch, L., Žuriˇc, Ž.: Robust pricing and hedging via neural SDEs. Preprint (2020). Available online at https://ssrn.com/abstract=3646241 50. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial networks. Commun. ACM 63(11), 139–144 (2020) 51. Gu, H., Guo, X., Wei, X., Xu, R.: Dynamic programming principles for mean-ﬁeld controls with learning. Preprint (2019). Available online at https://arxiv.org/abs/1911.07314v5 52. Gu, H., Guo, X., Wei, X., Xu, R.: Mean-ﬁeld controls with Q-learning for cooperative MARL: convergence and complexity analysis. Preprint (2020). Available online at https://arxiv.org/abs/2002. 04131 53. Gu, S., Lillicrap, T., Sutskever, I., Levine, S.: Continuous deep Q-learning with model-based accel- eration. In: Balcan, M.F., Weinberger, K.Q. (eds.) International Conference on Machine Learning, vol. 48, pp. 2829–2838. PMLR, New York (2016) 54. Guo, X., Hu, A., Xu, R., Zhang, J.: Learning mean-ﬁeld games. In: d’Alché-Buc, F., et al. (eds.) Advances in Neural Information Processing Systems, vol. 32, pp. 4966–4976. Curran Associates, Red Hook (2019) 55. Guo, X., Xu, R., Zariphopoulou, T.: Entropy regularization for mean ﬁeld games with learning. Preprint (2020). Available online at https://arxiv.org/abs/2010.00145 56. Han, J., Hu, R.: Deep ﬁctitious play for ﬁnding Markovian Nash equilibrium in multi-agent games. In: Lu, J., Ward, R. (eds.) Proceedings of the First Mathematical and Scientiﬁc Machine Learning Conference, PMLR, vol. 107, pp. 221–245 (2020) 57. Han, J., Long, J.: Convergence of the deep BSDE method for coupled FBSDEs. Probab. Uncertain. Quant. Risk 5, 1–33 (2020) 58. Harrison, J.M., Kreps, D.M.: Martingales and arbitrage in multiperiod securities markets. J. Econ. Theory 20, 381–408 (1979)", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 24, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p25::c0", "text": "Reinforcement learning and stochastic optimisation 127 59. Harrison, J.M., Pliska, S.R.: Martingales and stochastic integrals in the theory of continuous trading. Stoch. Process. Appl. 11, 215–260 (1981) 60. Horel, E., Giesecke, K.: Towards explainable AI: signiﬁcance tests for neural networks. Preprint (2019). Available online at https://arxiv.org/abs/1902.06021 61. Hornik, K.: Approximation capabilities of multilayer feedforward networks. Neural Netw. 4, 251–257 (1991) 62. Hornik, K., Stinchcombe, M., White, H.: Multilayer feedforward networks are universal approxima- tors. Neural Netw. 2, 359–366 (1989) 63. Hu, J., Wellman, M.P.: Nash Q-learning for general-sum stochastic games. J. Mach. Learn. Res. 4, 1039–1069 (2003) 64. Huang, M.: Large-population LQG games involving a major player: the Nash certainty equivalence principle. SIAM J. Control Optim. 48, 3318–3353 (2010) 65. Huang, M., Caines, P.E., Malhamé, R.P.: Large-population cost-coupled LQG problems with nonuni- form agents: individual-mass behavior and decentralized ε-Nash equilibria. IEEE Trans. Autom. Control 52, 1560–1571 (2007) 66. Huang, M., Malhamé, R.P., Caines, P.E.: Large population stochastic dynamic games: closed-loop McKean–Vlasov systems and the Nash certainty equivalence principle. Commun. Inf. Syst. 6, 221–252 (2006) 67. Huang, X., Jaimungal, S., Nourian, M.: Mean-ﬁeld game strategies for optimal execution. Appl. Math. Finance 26, 153–185 (2019) 68. Huré, C., Pham, H., Warin, X.: Deep backward schemes for high-dimensional nonlinear PDEs. Math. Comput. 89, 1547–1579 (2020) 69. Jaimungal, S., Pesenti, S., Wang, Y.S., Tatsat, H.: Robust risk-aware reinforcement learning. SIAM J. Financ. Math. (2021, forthcoming). https://arxiv.org/abs/2108.10403 70. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. Preprint (2014). Available online at https://arxiv.org/abs/1412.6980 71. Kreps, D.M.: Arbitrage and equilibrium in economies with inﬁnitely many commodities. J. Math. Econ. 8, 15–35 (1981) 72. Krishnan, R., Shalit, U., Sontag, D.: Structured inference networks for nonlinear state space models. In: Singh, S., Markovitch, S. (eds.) Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pp. 2101–2109. AAAI Press, Menlo Park (2017) 73. Krishnan, R.G., Shalit, U., Sontag, D.: Deep Kalman ﬁlters. Preprint (2015). Available online at https://arxiv.org/abs/1511.05121 74. Kumar, P.: Deep recurrent Q-networks for market making. Preprint (2020). Available online at http:// agi-conf.org/2020/wp-content/uploads/2020/06/AGI-20_paper_39.pdf 75. Kurutach, T., Clavera, I., Duan, Y., Tamar, A., Abbeel, P.: Model-ensemble trust-region policy opti- mization. Preprint (2018). Available online at https://arxiv.org/abs/1802.10592 76. Lasry, J.M., Lions, P.L.: Jeux à champ moyen. I – Le cas stationnaire. C. R. Acad. Sci. 343, 619–625 (2006) 77. Lasry, J.M., Lions, P.L.: Jeux à champ moyen. II – Horizon ﬁni et contrôle optimal. C. R. Acad. Sci. 343, 679–684 (2006) 78. Lasry, J.-M., Lions, P.-L.: Mean ﬁeld games. Jpn. J. Math. 2, 229–260 (2007) 79. Lehalle, C.-A., Mouzouni, C.: A mean ﬁeld game of portfolio trading and its consequences on per- ceived correlations. Preprint (2019). Available online at https://arxiv.org/abs/1902.09606 80. Lehalle, C.-A., Neuman, E.: Incorporating signals into optimal trading. Finance Stoch. 23, 275–311 (2019) 81. Li, W., Todorov, E.: Iterative linear quadratic regulator design for nonlinear biological movement systems. In: Araújo, H., et al. (eds.) Proceedings of the 1st International Conference on Informatics in Control, Automation and Robotics, pp. 222–229 (2004) 82. Li, X., Wong, T.-K.L., Chen, R.T., Duvenaud, D.: Scalable gradients for stochastic differential equa- tions. In: Chiappa, S., Calandra, R. (eds.) Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence and Statistics, PMLR, vol. 108, pp. 3870–3882 (2020) 83. Li, X., Wong, T.-K.L., Chen, R.T., Duvenaud, D.K.: Scalable gradients and variational inference for stochastic differential equations. In: Zhang, C., et al. (eds.) Proceedings of the 2nd Symposium on Advances in Approximate Bayesian Inference, PMLR, pp. 1–28 (2020) 84. Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., Wierstra, D.: Contin- uous control with deep reinforcement learning. Preprint (2015). Available online at https://arxiv.org/ abs/1509.02971", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 25, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p26::c0", "text": "128 S. Jaimungal 85. Ludkovski, M., Saporito, Y.: KrigHedge: Gaussian process surrogates for delta hedging. Preprint (2020). Available online at https://arxiv.org/abs/2010.08407v3 86. Mei, S., Montanari, A., Nguyen, P.-M.: A mean ﬁeld view of the landscape of two-layer neural networks. Proc. Natl. Acad. Sci. 115(33), E7665–E7671 (2018) 87. Merton, R.C.: Theory of rational option pricing. Bell J. Econ. Manag. Sci. 4, 141–183 (1973) 88. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Ried- miller, M., Fidjeland, A.K., Ostrovski, G.: Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015) 89. Montavon, G., Samek, W., Müller, K.-R.: Methods for interpreting and understanding deep neural networks. Digit. Signal Process. 73, 1–15 (2018) 90. Nevins, D.: Goals-based investing: integrating traditional and behavioral ﬁnance. J. Wealth Manag. 6(4), 8–23 (2004) 91. Ni, C., Li, Y., Forsyth, P., Carroll, R.: Optimal asset allocation for outperforming a stochastic bench- mark target. Preprint (2020). Available at SSRN 3619332. Available online at https://ssrn.com/ abstract=3619332 92. Ning, B., Jaimungal, S., Zhang, X., Bergeron, M.: Arbitrage-free implied volatility surface gener- ation with variational autoencoders. Preprint (2021). Available online at https://arxiv.org/abs/2108. 04941v1 93. Ning, B., Lin, F.H.T., Jaimungal, S.: Double deep Q-learning for optimal execution. Preprint (2018). Available online at https://arxiv.org/abs/1812.06600 94. Ormerod, J.T., Wand, M.P.: Explaining variational approximations. Am. Stat. 64(2), 140–153 (2010) 95. Pesenti, S.M., Jaimungal, S.: Portfolio optimisation within a Wasserstein ball. Preprint (2020). Avail- able online at https://arxiv.org/abs/2012.04500 96. Pham, H.: Continuous-Time Stochastic Control and Optimization with Financial Applications. Springer, Berlin (2009) 97. Polyak, B.T.: Some methods of speeding up the convergence of iteration methods. USSR Comput. Math. Math. Phys. 4(5), 1–17 (1964) 98. Puterman, M.L.: Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley, New York (2014) 99. Ribeiro, M.T., Singh, S., Guestrin, C.: “Why should I trust you?”: Explaining the predictions of any classiﬁer. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Dis- covery and Data Mining, pp. 1135–1144. Association for Computing Machinery, New York (2016) 100. Riedmiller, M.: Neural ﬁtted Q iteration – ﬁrst experiences with a data efﬁcient neural reinforcement learning method. In: Gama, J., et al. (eds.) Machine Learning: ECML 2005, pp. 317–328. Springer, Berlin (2005) 101. Rotskoff, G.M., Vanden-Eijnden, E.: Trainability and accuracy of neural networks: an interacting particle system approach. Preprint (2018). Available online at https://arxiv.org/abs/1805.00915 102. Saporito, Y.F., Zhang, Z.: PDGM: a neural network approach to solve path-dependent partial differ- ential equations. Preprint (2020). Available online at https://arxiv.org/abs/2003.02035 103. Shrivats, A., Firoozi, D., Jaimungal, S.: A mean-ﬁeld game approach to equilibrium pricing in solar renewable energy certiﬁcate markets. Preprint (2021). Available online at https://arxiv.org/abs/2003. 04938v5 104. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M.: Deterministic policy gradi- ent algorithms. In: Xing, E.P., Jebara, T. (eds.) Proceedings of the 31st International Conference on Machine Learning, PMLR, vol. 32, pp. 387–395 (2014) 105. Sirignano, J., Cont, R.: Universal features of price formation in ﬁnancial markets: perspectives from deep learning. Quant. Finance 19, 1449–1459 (2019) 106. Sirignano, J., Spiliopoulos, K.: DGM: a deep learning algorithm for solving partial differential equa- tions. J. Comput. Phys. 375, 1339–1364 (2018) 107. Sirignano, J., Spiliopoulos, K.: Mean ﬁeld analysis of neural networks: a law of large numbers. SIAM J. Appl. Math. 80, 725–752 (2020) 108. Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On the importance of initialization and momen- tum in deep learning. In: Dasgupta, S., McAllester, D. (eds.) International Conference on Machine Learning, PMLR, pp. 1139–1147 (2013) 109. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT Press, Cambridge (2018) 110. Sutton, R.S., McAllester, D.A., Singh, S.P., Mansour, Y.: Policy gradient methods for reinforcement learning with function approximation. In: Solla, S., et al. (eds.) Advances in Neural Information Processing Systems, vol. 12, pp. 1057–1063. MIT Press, Cambridge (2000)", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 26, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
{"chunk_id": "Reinforcement learning and stochastic optimisation::p27::c0", "text": "Reinforcement learning and stochastic optimisation 129 111. Todorov, E., Li, W.: A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems. In: Proceedings of the 2005 American Control Conference, 2005, IEEE, pp. 300–306 (2005) 112. Tsitsiklis, J.N.: Asynchronous stochastic approximation and Q-learning. Mach. Learn. 16(3), 185–202 (1994) 113. Tukey, J.W.: The future of data analysis. Ann. Math. Stat. 33, 1–67 (1962) 114. van Hasselt, H., Guez, A., Silver, D.: Deep reinforcement learning with double Q-learning. In: Pro- ceedings of Thirtieth AAAI Conference on Artiﬁcial Intelligence (AAAI-16), pp. 2094–2100 (2016) 115. Wang, H., Zariphopoulou, T., Zhou, X.Y.: Reinforcement learning in continuous time and space: a stochastic control approach. J. Mach. Learn. Res. 21, 1–34 (2020) 116. Wang, H., Zhou, X.Y.: Continuous-time mean–variance portfolio selection: a reinforcement learning framework. Math. Finance 30, 1273–1308 (2020) 117. Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., Freitas, N.: Dueling network ar- chitectures for deep reinforcement learning. In: Balcan, M.F., Weinberger, K.Q. (eds.) International Conference on Machine Learning, PMLR, pp. 1995–2003 (2016) 118. Watkins, C.J., Dayan, P.: Q-learning. Mach. Learn. 8, 279–292 (1992) 119. Xu, T., Wenliang, L.K., Munn, M., Acciaio, B.: COT-GAN: generating sequential data via causal optimal transport. Preprint (2020). Available online at https://arxiv.org/abs/2006.08571 120. Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W., Wang, J.: Mean ﬁeld multi-agent reinforcement learning. In: Dy, J., Krause, A. (eds.) Proceedings of the 35th International Conference on Machine Learning, PMLR, pp. 5571–5580 (2018) 121. Zhang, Z., Zohren, S., Roberts, S.: DeepLOB: deep convolutional neural networks for limit order books. IEEE Trans. Signal Process. 67, 3001–3012 (2019) 122. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features for discrimina- tive localization. In: Proceedings of 29th IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), pp. 2921–2929 (2016) Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations.", "meta": {"source_type": "paper", "topic": ["reinforcement_learning"], "title": "Reinforcement learning and stochastic optimisation", "page": 27, "path": "/Users/taka/research_rag/data/papers/reinforcement_learning/Reinforcement learning and stochastic optimisation.pdf"}}
